{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"attacks-vulns-list/","text":"Attack and Vulnerability Types \u00b6 The Wallarm filter node can detect many attacks and vulnerabilities. These attacks and vulnerabilities are listed below . Each entity in the list is tagged with either \u201cAttack,\u201d \u201cVulnerability,\u201d or both. The name of a particular attack can be the same as the name of the vulnerability this attack exploits. In this case, such an entity will be tagged with the combined \u201cVulnerability/Attack\u201d tag. has the Wallarm code that corresponds to this entity. Most of the vulnerabilities and attacks on this list are also accompanied by one or more codes from the list of software weakness types, also known as the Common Weakness Enumeration or CWE. Additionally, the Wallarm filter node employs several special attack and vulnerability types for the internal purpose of marking processed traffic. Such entities are not accompanied by CWE codes but are listed separately . Watch Wallarm video about how WAF protects against OWASP Top 10 The Main List of Attacks and Vulnerabilities \u00b6 Attack on XML External Entity (XXE) \u00b6 Vulnerability/Attack CWE code: CWE-611 Wallarm code: xxe Description: The XXE vulnerability allows an attacker to inject an external entity in an XML document to be evaluated by an XML parser and then executed on the target web server. As the result of a successful attack, an attacker will be able to get access to the web application's confidential data scan internal data networks read the files located on the web server perform an SSRF attack perform a Denial of Service (DoS) attack This vulnerability occurs due to a lack of restriction on the parsing of XML external entities in a web application. Remediation: You may follow these recommendations: Disable the parsing of XML external entities when working with the XML documents supplied by a user. Apply the recommendations from the OWASP XXE Prevention Cheat Sheet . Brute\u2011Force Attack \u00b6 Attack CWE codes: CWE-307 , CWE-521 , CWE-799 Wallarm code: brute Description: A brute\u2011force attack occurs when a massive number of requests with a predefined payload are sent to the server. These payloads may be generated by some means or taken from a dictionary. The server's response is then analyzed to find the right combination of the data in the payload. A successful brute\u2011force attack can potentially bypass authentication and authorization mechanisms and/or reveal a web application's hidden resources (such as directories, files, website parts, etc.), thus granting the ability to conduct other malicious actions. Remediation: You may follow these recommendations: Limit the number of requests per a certain time period for a web application. Limit the number of authentication/authorization attempts per a certain time period for a web application. Block new authentication/authorization attempts after a certain number of the failed attempts. Restrict a web application from accessing any files or directories on the server it runs on, except those within the scope of the application. How to configure Wallarm WAF to protect applications from brute force \u2192 Watch Wallarm video about brute\u2011force attacks Resource Scanning \u00b6 Attack CWE code: none Wallarm code: scanner Description: The scanner code is assigned to an HTTP request if this request is believed to be part of third\u2011party scanner software activity that is targeted to attack or scan a protected resource. The Wallarm scanner's requests are not considered to be a resource scanning attack. This information may be used later to attack these services. Remediation: You may follow these recommendations: Limit the possibility of a network perimeter scan by employing IP address whitelisting and blacklisting along with authentication/authorization mechanisms. Minimize the scan surface by placing the network perimeter behind a firewall. Define a necessary and sufficient set of ports to be opened for your services to operate. Restrict the usage of ICMP protocol on the network level. Periodically update your IT infrastructure equipment. This includes firmware of servers and other equipment operating systems other software Server\u2011Side Template Injection (SSTI) \u00b6 Vulnerability/Attack CWE codes: CWE-94 , CWE-159 Wallarm code: ssti Description: An intruder can inject an executable code into a user\u2011filled form on a web server vulnerable to SSTI attacks so that code will be parsed and executed by the web server. A successful attack may render a vulnerable web server completely compromised, potentially allowing an intruder to execute arbitrary requests, explore the server's file systems, and, under certain conditions, remotely execute arbitrary code (see \u201cRCE attack\u201d for details), as well as many other things. This vulnerability arises from the incorrect validation and parsing of user input. Remediation: You may follow the recommendation to sanitize and filter all user input to prevent an entity in the input from being executed. Logic Bomb \u00b6 Attack CWE code: CWE-511 Wallarm code: logic_bomb Description: A logic bomb is a piece of malicious code that runs under certain conditions to perform some malicious actions. A \u201ctime bomb\u201d is a variety of logic bomb that goes off at a certain time or date. An example of a logic bomb is a program that is in control of a company's salary calculation system and which attacks the company if a particular employee gets fired. Remediation: You may follow these recommendations: Use both static and dynamic code analyzers to inspect the produced code. Meticulously audit all code that is not covered by tests. Validate the integrity of any software being installed (e.g., check the software's digital signature). Cross\u2011site Scripting (XSS) \u00b6 Vulnerability/Attack CWE code: CWE-79 Wallarm code: xss Description: A cross\u2011site scripting attack allows an intruder to execute a prepared arbitrary code in a user's browser. There are a few XSS attack types: Stored XSS is when a malicious code is pre\u2011embedded in the web application's page. If the web application is vulnerable to the stored XSS attack, then it is possible for an attacker to inject a malicious code into the web application's HTML page; moreover, this code will persist and be executed by the browser of any user who requests the infected webpage. Reflected XSS is when an intruder tricks a user into opening a specially crafted link. DOM\u2011based XSS is when a JavaScript code snippet built into the web application's page parses the input and executes it as a JavaScript command due to errors in this code snippet. Exploiting any of the vulnerabilities listed above leads to the execution of an arbitrary JavaScript code. Provided that the XSS attack was successful, an intruder may steal a user's session or credentials, make requests on behalf of the user, and perform other malicious actions. This class of vulnerabilities occurs due to the incorrect validation and parsing of user input. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. While forming the web application's pages, sanitize and escape any entities that are formed dynamically. Apply the recommendations from the OWASP XSS Prevention Cheat Sheet . Insecure Direct Object References (IDOR) \u00b6 Vulnerability CWE code: CWE-639 Wallarm code: idor Description: With the IDOR vulnerability, the authentication and authorization mechanisms of a vulnerable web application do not prevent a user from accessing the data or resources of another user. This vulnerability occurs due to the web application granting the ability to access an object (e.g., a file, a directory, a database entry) by changing part of the request string and not implementing proper access control mechanisms. To exploit this vulnerability, an intruder manipulates the request string to gain unauthorized access to confidential information that belongs either to the vulnerable web application or to its users. Remediation: You may follow these recommendations: Implement proper access control mechanisms for the web application's resources. Implement role\u2011based access control mechanisms to grant access to resources based on roles that are assigned to the users. Use indirect object references. Apply the recommendations from the OWASP IDOR Prevention Cheat Sheet . Open Redirect \u00b6 Attack CWE code: CWE-601 Wallarm code: redir Description: An intruder can use an open redirect attack to redirect a user to a malicious web page via a legitimate web application. Vulnerability to this attack occurs due to incorrect filtering of URL inputs. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Notify users about all pending redirects, and ask for explicit permission. Server\u2011Side Request Forgery (SSRF) \u00b6 Vulnerability CWE code: CWE-918 Wallarm code: ssrf Description: A successful SSRF attack may allow an intruder to make requests on behalf of the attacked web server; this potentially leads to revealing the web application's network ports in use, scanning the internal networks, and bypassing authorization. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Apply the recommendations from the OWASP SSRF Prevention Cheat Sheet . Forced Browsing \u00b6 Attack CWE code: CWE-425 Wallarm code: dirbust Description: This attack belongs to the class of brute\u2011force attacks. The purpose of this attack is to detect a web application's hidden resources, namely directories and files. This is achieved by trying different file and directory names that are either generated based on some template or extracted from a prepared dictionary file. A successful forced browsing attack potentially grants access to hidden resources that are not explicitly available from the web application interface but are exposed when accessed directly. Remediation: You may follow these recommendations: Restrict or limit users' acess to those resources they are not supposed to have direct access to (e.g., by employing some authentication or authorization mechanisms). Limit the number of requests per a certain time period for the web application. Limit the number of authentication/authorization attempts per a certain time period for the web application. Block new authentication/authorization attempts after a certain number of failed attempts. Set necessary and sufficient access rights for the web application's files and directories. How to configure Wallarm WAF to protect applications from brute force \u2192 Watch Wallarm video about brute\u2011force attacks Information Exposure \u00b6 Vulnerability/Attack CWE codes: CWE-200 (see also: CWE-209 , CWE-215 , CWE-538 , CWE-541 , CWE-548 ) Wallarm code: infoleak Description: The application either intentionally or unintentionally discloses sensitive information to a subject that is not authorized to access it. The vulnerability of this type can be detected only by the method of passive detection . If the response to the request discloses sensitive information, Wallarm records an incident and an active vulnerability of the Information Exposure type. Some kinds of sensitive information that can be detected by Wallarm include: System and environment status (for example: stack trace, warnings, fatal errors) Network status and configuration The application code or internal state Metadata (for example, logging of connections or message headers) Remediation: You may follow the recommendation to prohibit a web application from having the ability to display any sensitive information. Remote Code Execution (RCE) \u00b6 Vulnerability/Attack CWE codes: CWE-78 , CWE-94 and others Wallarm code: rce Description: An intruder can inject malicious code into a request to a web application, and the application will execute this code. Also, the intruder can try to execute certain commands for the operating system that the vulnerable web application runs on. Provided that an RCE attack is successful, an intruder can perform a wide range of actions, including Compromising the confidentiality, accessibility, and integrity of the vulnerable web application's data. Taking control of the operating system and the server that the web application runs on. Other possible actions. This vulnerability occurs due to incorrect validation and parsing of user input. Remediation: You may follow the recommendation to sanitize and filter all user input to prevent an entity in the input from being executed. Authentication Bypass \u00b6 Vulnerability CWE code: CWE-288 Wallarm code: auth Description: Despite having authentication mechanisms in place, a web application can have alternative authentication methods that allow either bypassing the main authentication mechanism or exploiting its weaknesses. This combination of factors may result in an attacker gaining access with user or administrator permissions. A successful authentication bypass attack potentially leads to disclosing users' confidential data or taking control of the vulnerable application with administrator permissions. Remediation: You may follow these recommendations: Improve and strengthen existing authentication mechanisms. Eliminate any alternative authentication methods that may allow attackers to access an application while bypassing the required authentication procedure via pre\u2011defined mechanisms. Apply the recommendations from the OWASP Authentication Cheat Sheet . CRLF Injection \u00b6 Attack CWE code: CWE-93 Wallarm code: crlf Description: CRLF injections represent a class of attacks that allow an intruder to inject the Carriage Return (CR) and Line Feed (LF) characters into a request to a server (e.g., HTTP request). Combined with other factors, such CR/LF character injection can help to exploit a variety of vulnerabilities (e.g., \u201cHTTP Response Splitting\u201d CWE-113 , \u201cHTTP Response Smuggling\u201d CWE-444 ). A successful CRLF injection attack may give an intruder the ability to bypass firewalls, perform cache poisoning, replace legitimate web pages with malicious ones, perform the an \u201cOpen Redirect\u201d attack, and plenty of other actions. This vulnerability occurs due to the incorrect validation and parsing of user input. Remediation: You may follow the recommendation to sanitize and filter all user input to prevent an entity in the input from being executed. LDAP Injection \u00b6 Vulnerability/Attack CWE code: CWE-90 Wallarm code: ldapi Description: LDAP injections represent a class of attacks that allow an intruder to alter LDAP search filters by modifying requests to an LDAP server. A successful LDAP injection attack potentially grants access to the read and write operations on confidential data about LDAP users and hosts. This vulnerability occurs due to the incorrect validation and parsing of user input. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Apply the recommendations from the OWASP LDAP Injection Prevention Cheat Sheet . NoSQL Injection \u00b6 Vulnerability/Attack CWE code: CWE-943 Wallarm code: nosqli Description: Vulnerability to this attack occurs due to insufficient filtering of user input. A NoSQL injection attack is performed by injecting a specially crafted query to a NoSQL database. Remediation: You may follow the recommendation to sanitize and filter all user input to prevent an entity in the input from being executed. Path Traversal \u00b6 Vulnerability/Attack CWE code: CWE-22 Wallarm code: ptrav Description: A path traversal attack allows an intruder to access files and directories with confidential data stored in the file system where the vulnerable web application resides by altering existing paths via the web application's parameters. Vulnerability to this attack occurs due to insufficient filtering of user input when a user requests a file or directory via the web application. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Additional recommendations for mitigating such attacks are available here . SQL Injection \u00b6 Vulnerability/Attack CWE code: CWE-89 Wallarm code: sqli Description: Vulnerability to this attack occurs due to insufficient filtration of user input. An SQL injection attack is performed by injecting a specially crafted query to an SQL database. An SQL injection attack allows an intruder to inject arbitrary SQL code into an SQL query. This potentially leads to the attacker being granted access to read and modify confidential data as well as to DBMS administrator rights. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Apply the recommendations from the OWASP SQL Injection Prevention Cheat Sheet . The List of Special Attacks and Vulnerabilities \u00b6 Virtual Patch \u00b6 Attack Wallarm code: vpatch Description: A request is marked as a vpatch if it is part of an attack that was mitigated by the virtual patch mechanism . Unsafe XML Header \u00b6 Attack Wallarm code: xml_unsafe_header Description: A request is marked as an xml_unsafe_header if its body contains an XML document and the document encoding differs from the encoding stated in the XML header. Overlimiting of Computational Resources \u00b6 Attack Wallarm code: overlimit_res Description: The filter node is configured in such a way that it should spend no more than N milliseconds on incoming request processing (default value: 1000 ). If the request is not processed during the specified timeframe, then the processing of the request will be stopped and the request marked as an overlimit_res attack. You can specify the desired timeframe for the request to be processed by using the wallarm_process_time_limit Wallarm directive.","title":"Attack and vulnerability types"},{"location":"attacks-vulns-list/#attack-and-vulnerability-types","text":"The Wallarm filter node can detect many attacks and vulnerabilities. These attacks and vulnerabilities are listed below . Each entity in the list is tagged with either \u201cAttack,\u201d \u201cVulnerability,\u201d or both. The name of a particular attack can be the same as the name of the vulnerability this attack exploits. In this case, such an entity will be tagged with the combined \u201cVulnerability/Attack\u201d tag. has the Wallarm code that corresponds to this entity. Most of the vulnerabilities and attacks on this list are also accompanied by one or more codes from the list of software weakness types, also known as the Common Weakness Enumeration or CWE. Additionally, the Wallarm filter node employs several special attack and vulnerability types for the internal purpose of marking processed traffic. Such entities are not accompanied by CWE codes but are listed separately . Watch Wallarm video about how WAF protects against OWASP Top 10","title":"Attack and Vulnerability Types"},{"location":"attacks-vulns-list/#the-main-list-of-attacks-and-vulnerabilities","text":"","title":"The Main List of Attacks and Vulnerabilities"},{"location":"attacks-vulns-list/#attack-on-xml-external-entity-xxe","text":"Vulnerability/Attack CWE code: CWE-611 Wallarm code: xxe Description: The XXE vulnerability allows an attacker to inject an external entity in an XML document to be evaluated by an XML parser and then executed on the target web server. As the result of a successful attack, an attacker will be able to get access to the web application's confidential data scan internal data networks read the files located on the web server perform an SSRF attack perform a Denial of Service (DoS) attack This vulnerability occurs due to a lack of restriction on the parsing of XML external entities in a web application. Remediation: You may follow these recommendations: Disable the parsing of XML external entities when working with the XML documents supplied by a user. Apply the recommendations from the OWASP XXE Prevention Cheat Sheet .","title":"Attack on XML External Entity (XXE)"},{"location":"attacks-vulns-list/#bruteforce-attack","text":"Attack CWE codes: CWE-307 , CWE-521 , CWE-799 Wallarm code: brute Description: A brute\u2011force attack occurs when a massive number of requests with a predefined payload are sent to the server. These payloads may be generated by some means or taken from a dictionary. The server's response is then analyzed to find the right combination of the data in the payload. A successful brute\u2011force attack can potentially bypass authentication and authorization mechanisms and/or reveal a web application's hidden resources (such as directories, files, website parts, etc.), thus granting the ability to conduct other malicious actions. Remediation: You may follow these recommendations: Limit the number of requests per a certain time period for a web application. Limit the number of authentication/authorization attempts per a certain time period for a web application. Block new authentication/authorization attempts after a certain number of the failed attempts. Restrict a web application from accessing any files or directories on the server it runs on, except those within the scope of the application. How to configure Wallarm WAF to protect applications from brute force \u2192 Watch Wallarm video about brute\u2011force attacks","title":"Brute\u2011Force Attack"},{"location":"attacks-vulns-list/#resource-scanning","text":"Attack CWE code: none Wallarm code: scanner Description: The scanner code is assigned to an HTTP request if this request is believed to be part of third\u2011party scanner software activity that is targeted to attack or scan a protected resource. The Wallarm scanner's requests are not considered to be a resource scanning attack. This information may be used later to attack these services. Remediation: You may follow these recommendations: Limit the possibility of a network perimeter scan by employing IP address whitelisting and blacklisting along with authentication/authorization mechanisms. Minimize the scan surface by placing the network perimeter behind a firewall. Define a necessary and sufficient set of ports to be opened for your services to operate. Restrict the usage of ICMP protocol on the network level. Periodically update your IT infrastructure equipment. This includes firmware of servers and other equipment operating systems other software","title":"Resource Scanning"},{"location":"attacks-vulns-list/#serverside-template-injection-ssti","text":"Vulnerability/Attack CWE codes: CWE-94 , CWE-159 Wallarm code: ssti Description: An intruder can inject an executable code into a user\u2011filled form on a web server vulnerable to SSTI attacks so that code will be parsed and executed by the web server. A successful attack may render a vulnerable web server completely compromised, potentially allowing an intruder to execute arbitrary requests, explore the server's file systems, and, under certain conditions, remotely execute arbitrary code (see \u201cRCE attack\u201d for details), as well as many other things. This vulnerability arises from the incorrect validation and parsing of user input. Remediation: You may follow the recommendation to sanitize and filter all user input to prevent an entity in the input from being executed.","title":"Server\u2011Side Template Injection (SSTI)"},{"location":"attacks-vulns-list/#logic-bomb","text":"Attack CWE code: CWE-511 Wallarm code: logic_bomb Description: A logic bomb is a piece of malicious code that runs under certain conditions to perform some malicious actions. A \u201ctime bomb\u201d is a variety of logic bomb that goes off at a certain time or date. An example of a logic bomb is a program that is in control of a company's salary calculation system and which attacks the company if a particular employee gets fired. Remediation: You may follow these recommendations: Use both static and dynamic code analyzers to inspect the produced code. Meticulously audit all code that is not covered by tests. Validate the integrity of any software being installed (e.g., check the software's digital signature).","title":"Logic Bomb"},{"location":"attacks-vulns-list/#crosssite-scripting-xss","text":"Vulnerability/Attack CWE code: CWE-79 Wallarm code: xss Description: A cross\u2011site scripting attack allows an intruder to execute a prepared arbitrary code in a user's browser. There are a few XSS attack types: Stored XSS is when a malicious code is pre\u2011embedded in the web application's page. If the web application is vulnerable to the stored XSS attack, then it is possible for an attacker to inject a malicious code into the web application's HTML page; moreover, this code will persist and be executed by the browser of any user who requests the infected webpage. Reflected XSS is when an intruder tricks a user into opening a specially crafted link. DOM\u2011based XSS is when a JavaScript code snippet built into the web application's page parses the input and executes it as a JavaScript command due to errors in this code snippet. Exploiting any of the vulnerabilities listed above leads to the execution of an arbitrary JavaScript code. Provided that the XSS attack was successful, an intruder may steal a user's session or credentials, make requests on behalf of the user, and perform other malicious actions. This class of vulnerabilities occurs due to the incorrect validation and parsing of user input. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. While forming the web application's pages, sanitize and escape any entities that are formed dynamically. Apply the recommendations from the OWASP XSS Prevention Cheat Sheet .","title":"Cross\u2011site Scripting (XSS)"},{"location":"attacks-vulns-list/#insecure-direct-object-references-idor","text":"Vulnerability CWE code: CWE-639 Wallarm code: idor Description: With the IDOR vulnerability, the authentication and authorization mechanisms of a vulnerable web application do not prevent a user from accessing the data or resources of another user. This vulnerability occurs due to the web application granting the ability to access an object (e.g., a file, a directory, a database entry) by changing part of the request string and not implementing proper access control mechanisms. To exploit this vulnerability, an intruder manipulates the request string to gain unauthorized access to confidential information that belongs either to the vulnerable web application or to its users. Remediation: You may follow these recommendations: Implement proper access control mechanisms for the web application's resources. Implement role\u2011based access control mechanisms to grant access to resources based on roles that are assigned to the users. Use indirect object references. Apply the recommendations from the OWASP IDOR Prevention Cheat Sheet .","title":"Insecure Direct Object References (IDOR)"},{"location":"attacks-vulns-list/#open-redirect","text":"Attack CWE code: CWE-601 Wallarm code: redir Description: An intruder can use an open redirect attack to redirect a user to a malicious web page via a legitimate web application. Vulnerability to this attack occurs due to incorrect filtering of URL inputs. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Notify users about all pending redirects, and ask for explicit permission.","title":"Open Redirect"},{"location":"attacks-vulns-list/#serverside-request-forgery-ssrf","text":"Vulnerability CWE code: CWE-918 Wallarm code: ssrf Description: A successful SSRF attack may allow an intruder to make requests on behalf of the attacked web server; this potentially leads to revealing the web application's network ports in use, scanning the internal networks, and bypassing authorization. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Apply the recommendations from the OWASP SSRF Prevention Cheat Sheet .","title":"Server\u2011Side Request Forgery (SSRF)"},{"location":"attacks-vulns-list/#forced-browsing","text":"Attack CWE code: CWE-425 Wallarm code: dirbust Description: This attack belongs to the class of brute\u2011force attacks. The purpose of this attack is to detect a web application's hidden resources, namely directories and files. This is achieved by trying different file and directory names that are either generated based on some template or extracted from a prepared dictionary file. A successful forced browsing attack potentially grants access to hidden resources that are not explicitly available from the web application interface but are exposed when accessed directly. Remediation: You may follow these recommendations: Restrict or limit users' acess to those resources they are not supposed to have direct access to (e.g., by employing some authentication or authorization mechanisms). Limit the number of requests per a certain time period for the web application. Limit the number of authentication/authorization attempts per a certain time period for the web application. Block new authentication/authorization attempts after a certain number of failed attempts. Set necessary and sufficient access rights for the web application's files and directories. How to configure Wallarm WAF to protect applications from brute force \u2192 Watch Wallarm video about brute\u2011force attacks","title":"Forced Browsing"},{"location":"attacks-vulns-list/#information-exposure","text":"Vulnerability/Attack CWE codes: CWE-200 (see also: CWE-209 , CWE-215 , CWE-538 , CWE-541 , CWE-548 ) Wallarm code: infoleak Description: The application either intentionally or unintentionally discloses sensitive information to a subject that is not authorized to access it. The vulnerability of this type can be detected only by the method of passive detection . If the response to the request discloses sensitive information, Wallarm records an incident and an active vulnerability of the Information Exposure type. Some kinds of sensitive information that can be detected by Wallarm include: System and environment status (for example: stack trace, warnings, fatal errors) Network status and configuration The application code or internal state Metadata (for example, logging of connections or message headers) Remediation: You may follow the recommendation to prohibit a web application from having the ability to display any sensitive information.","title":"Information Exposure"},{"location":"attacks-vulns-list/#remote-code-execution-rce","text":"Vulnerability/Attack CWE codes: CWE-78 , CWE-94 and others Wallarm code: rce Description: An intruder can inject malicious code into a request to a web application, and the application will execute this code. Also, the intruder can try to execute certain commands for the operating system that the vulnerable web application runs on. Provided that an RCE attack is successful, an intruder can perform a wide range of actions, including Compromising the confidentiality, accessibility, and integrity of the vulnerable web application's data. Taking control of the operating system and the server that the web application runs on. Other possible actions. This vulnerability occurs due to incorrect validation and parsing of user input. Remediation: You may follow the recommendation to sanitize and filter all user input to prevent an entity in the input from being executed.","title":"Remote Code Execution (RCE)"},{"location":"attacks-vulns-list/#authentication-bypass","text":"Vulnerability CWE code: CWE-288 Wallarm code: auth Description: Despite having authentication mechanisms in place, a web application can have alternative authentication methods that allow either bypassing the main authentication mechanism or exploiting its weaknesses. This combination of factors may result in an attacker gaining access with user or administrator permissions. A successful authentication bypass attack potentially leads to disclosing users' confidential data or taking control of the vulnerable application with administrator permissions. Remediation: You may follow these recommendations: Improve and strengthen existing authentication mechanisms. Eliminate any alternative authentication methods that may allow attackers to access an application while bypassing the required authentication procedure via pre\u2011defined mechanisms. Apply the recommendations from the OWASP Authentication Cheat Sheet .","title":"Authentication Bypass"},{"location":"attacks-vulns-list/#crlf-injection","text":"Attack CWE code: CWE-93 Wallarm code: crlf Description: CRLF injections represent a class of attacks that allow an intruder to inject the Carriage Return (CR) and Line Feed (LF) characters into a request to a server (e.g., HTTP request). Combined with other factors, such CR/LF character injection can help to exploit a variety of vulnerabilities (e.g., \u201cHTTP Response Splitting\u201d CWE-113 , \u201cHTTP Response Smuggling\u201d CWE-444 ). A successful CRLF injection attack may give an intruder the ability to bypass firewalls, perform cache poisoning, replace legitimate web pages with malicious ones, perform the an \u201cOpen Redirect\u201d attack, and plenty of other actions. This vulnerability occurs due to the incorrect validation and parsing of user input. Remediation: You may follow the recommendation to sanitize and filter all user input to prevent an entity in the input from being executed.","title":"CRLF Injection"},{"location":"attacks-vulns-list/#ldap-injection","text":"Vulnerability/Attack CWE code: CWE-90 Wallarm code: ldapi Description: LDAP injections represent a class of attacks that allow an intruder to alter LDAP search filters by modifying requests to an LDAP server. A successful LDAP injection attack potentially grants access to the read and write operations on confidential data about LDAP users and hosts. This vulnerability occurs due to the incorrect validation and parsing of user input. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Apply the recommendations from the OWASP LDAP Injection Prevention Cheat Sheet .","title":"LDAP Injection"},{"location":"attacks-vulns-list/#nosql-injection","text":"Vulnerability/Attack CWE code: CWE-943 Wallarm code: nosqli Description: Vulnerability to this attack occurs due to insufficient filtering of user input. A NoSQL injection attack is performed by injecting a specially crafted query to a NoSQL database. Remediation: You may follow the recommendation to sanitize and filter all user input to prevent an entity in the input from being executed.","title":"NoSQL Injection"},{"location":"attacks-vulns-list/#path-traversal","text":"Vulnerability/Attack CWE code: CWE-22 Wallarm code: ptrav Description: A path traversal attack allows an intruder to access files and directories with confidential data stored in the file system where the vulnerable web application resides by altering existing paths via the web application's parameters. Vulnerability to this attack occurs due to insufficient filtering of user input when a user requests a file or directory via the web application. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Additional recommendations for mitigating such attacks are available here .","title":"Path Traversal"},{"location":"attacks-vulns-list/#sql-injection","text":"Vulnerability/Attack CWE code: CWE-89 Wallarm code: sqli Description: Vulnerability to this attack occurs due to insufficient filtration of user input. An SQL injection attack is performed by injecting a specially crafted query to an SQL database. An SQL injection attack allows an intruder to inject arbitrary SQL code into an SQL query. This potentially leads to the attacker being granted access to read and modify confidential data as well as to DBMS administrator rights. Remediation: You may follow these recommendations: Sanitize and filter all parameters that a web application receives as input to prevent an entity in the input from being executed. Apply the recommendations from the OWASP SQL Injection Prevention Cheat Sheet .","title":"SQL Injection"},{"location":"attacks-vulns-list/#the-list-of-special-attacks-and-vulnerabilities","text":"","title":"The List of Special Attacks and Vulnerabilities"},{"location":"attacks-vulns-list/#virtual-patch","text":"Attack Wallarm code: vpatch Description: A request is marked as a vpatch if it is part of an attack that was mitigated by the virtual patch mechanism .","title":"Virtual Patch"},{"location":"attacks-vulns-list/#unsafe-xml-header","text":"Attack Wallarm code: xml_unsafe_header Description: A request is marked as an xml_unsafe_header if its body contains an XML document and the document encoding differs from the encoding stated in the XML header.","title":"Unsafe XML Header"},{"location":"attacks-vulns-list/#overlimiting-of-computational-resources","text":"Attack Wallarm code: overlimit_res Description: The filter node is configured in such a way that it should spend no more than N milliseconds on incoming request processing (default value: 1000 ). If the request is not processed during the specified timeframe, then the processing of the request will be stopped and the request marked as an overlimit_res attack. You can specify the desired timeframe for the request to be processed by using the wallarm_process_time_limit Wallarm directive.","title":"Overlimiting of Computational Resources"},{"location":"glossary-en/","text":"Glossary \u00b6 Hit \u00b6 A hit is a serialized malicious request (original malicious request and metadata added by the WAF node). For example: Attack \u00b6 An attack is a single hit or multiple hits that have the same attack type, parameter with the attack vector, and the address they are sent to. Hits may come from the same or different IP addresses and have different value of the attack vector within one attack type. An example of an attack including a single hit: An example of an attack including many hits: Attack Vector \u00b6 An attack vector is a path or means by which a hacker can gain access to a network resource to deliver a payload. Vulnerability \u00b6 A vulnerability is an error made due to negligence or inadequate information when building or implementing a web application that can lead to an information security risk. The information security risks are: Unauthorized data access; for example, access to read and modify user data. Denial of service. Data corruption and other. The Internet traffic can be used to detect the vulnerabilities, which is what Wallarm does, among other functions. Security Incident \u00b6 A security incident is an occurrence of a vulnerability exploitation. An incident is an attack targeted at a confirmed vulnerability. An incident, just like an attack, is an entity external to your system and is a characteristic of the outside Internet, not the system itself. Despite the fact that the attacks targeted at existing vulnerabilities are a minority, they are of the utmost importance in terms of information security. Wallarm automatically detects the attacks targeted at existing vulnerabilities and displays them as a separate object. MITM \u00b6 A man in the middle (MITM) attack consists of an attacker secretly relaying the communication between two parties who believe they are directly communicating with each other. See OWASP . Circular Buffer \u00b6 A circular buffer is a data structure that uses a single, fixed\u2011size buffer as if it were connected end\u2011to\u2011end. See Wikipedia . LOM \u00b6 LOM stands for Local Objective Model. LOM is a set of rules for a particular web application. The set of rules is generated based on user requests to the web application and the application's responses. Invalid Request \u00b6 A request that was checked by filter node and does not match LOM rules. Reverse Proxy \u00b6 A reverse proxy is a type of proxy server that retrieves resources on behalf of a client from a server and returns the resources to the client as if they originated from the Web server itself. See Wikipedia . Certificate Authority \u00b6 A certificate authority is an entity that issues digital certificates. See Wikipedia .","title":"Glossary"},{"location":"glossary-en/#glossary","text":"","title":"Glossary"},{"location":"glossary-en/#hit","text":"A hit is a serialized malicious request (original malicious request and metadata added by the WAF node). For example:","title":"Hit"},{"location":"glossary-en/#attack","text":"An attack is a single hit or multiple hits that have the same attack type, parameter with the attack vector, and the address they are sent to. Hits may come from the same or different IP addresses and have different value of the attack vector within one attack type. An example of an attack including a single hit: An example of an attack including many hits:","title":"Attack"},{"location":"glossary-en/#attack-vector","text":"An attack vector is a path or means by which a hacker can gain access to a network resource to deliver a payload.","title":"Attack Vector"},{"location":"glossary-en/#vulnerability","text":"A vulnerability is an error made due to negligence or inadequate information when building or implementing a web application that can lead to an information security risk. The information security risks are: Unauthorized data access; for example, access to read and modify user data. Denial of service. Data corruption and other. The Internet traffic can be used to detect the vulnerabilities, which is what Wallarm does, among other functions.","title":"Vulnerability"},{"location":"glossary-en/#security-incident","text":"A security incident is an occurrence of a vulnerability exploitation. An incident is an attack targeted at a confirmed vulnerability. An incident, just like an attack, is an entity external to your system and is a characteristic of the outside Internet, not the system itself. Despite the fact that the attacks targeted at existing vulnerabilities are a minority, they are of the utmost importance in terms of information security. Wallarm automatically detects the attacks targeted at existing vulnerabilities and displays them as a separate object.","title":"Security Incident"},{"location":"glossary-en/#mitm","text":"A man in the middle (MITM) attack consists of an attacker secretly relaying the communication between two parties who believe they are directly communicating with each other. See OWASP .","title":"MITM"},{"location":"glossary-en/#circular-buffer","text":"A circular buffer is a data structure that uses a single, fixed\u2011size buffer as if it were connected end\u2011to\u2011end. See Wikipedia .","title":"Circular Buffer"},{"location":"glossary-en/#lom","text":"LOM stands for Local Objective Model. LOM is a set of rules for a particular web application. The set of rules is generated based on user requests to the web application and the application's responses.","title":"LOM"},{"location":"glossary-en/#invalid-request","text":"A request that was checked by filter node and does not match LOM rules.","title":"Invalid Request"},{"location":"glossary-en/#reverse-proxy","text":"A reverse proxy is a type of proxy server that retrieves resources on behalf of a client from a server and returns the resources to the client as if they originated from the Web server itself. See Wikipedia .","title":"Reverse Proxy"},{"location":"glossary-en/#certificate-authority","text":"A certificate authority is an entity that issues digital certificates. See Wikipedia .","title":"Certificate Authority"},{"location":"about-wallarm-waf/architecture/","text":"Wallarm WAF architecture \u00b6 Everything about components and how they connect.","title":"Wallarm architecture"},{"location":"about-wallarm-waf/architecture/#wallarm-waf-architecture","text":"Everything about components and how they connect.","title":"Wallarm WAF architecture"},{"location":"about-wallarm-waf/data-retention-policy/","text":"Data retention policy \u00b6 This policy outlines retention periods for different datasets collected by Wallarm WAF and stored in the Wallarm Cloud. Dataset Retention period Data on attacks, hits, and incidents detected by the WAF nodes 12 months Data on vulnerabilities detected by the WAF nodes or Attack rechecker 12 months Data on vulnerabilities detected by the Vulnerability Scanner 24 months Statistics on processed and blocked requests displayed on the dashboards 12 months Network perimeter elements detected by Perimeter Scanner 24 months History of whitelisted, blacklisted, and greylisted IP addresses 6 months Automatically generated or manually created rules for proccessing traffic by Wallarm WAF \u221e WAF account configuration: users , applications , integrations , triggers \u221e Activity log records 12 month","title":"Data retention policy"},{"location":"about-wallarm-waf/data-retention-policy/#data-retention-policy","text":"This policy outlines retention periods for different datasets collected by Wallarm WAF and stored in the Wallarm Cloud. Dataset Retention period Data on attacks, hits, and incidents detected by the WAF nodes 12 months Data on vulnerabilities detected by the WAF nodes or Attack rechecker 12 months Data on vulnerabilities detected by the Vulnerability Scanner 24 months Statistics on processed and blocked requests displayed on the dashboards 12 months Network perimeter elements detected by Perimeter Scanner 24 months History of whitelisted, blacklisted, and greylisted IP addresses 6 months Automatically generated or manually created rules for proccessing traffic by Wallarm WAF \u221e WAF account configuration: users , applications , integrations , triggers \u221e Activity log records 12 month","title":"Data retention policy"},{"location":"about-wallarm-waf/deployment-best-practices/","text":"Wallarm WAF deployment and maintenance best practices \u00b6 This article formulates best practices for deployment and maintenance of Wallarm WAF and API protection service. Understand the power of NGINX \u00b6 The majority of Wallarm WAF node deployment options use NGINX as the reverse proxy server (the foundation for the Wallarm WAF module), which provides a large range of functionality, modules, and performance/security guides. The following is a collection of helpful Internet articles: Awesome NGINX NGINX: Basics and Best Practices slide show How to optimize NGINX configuration 3 quick steps to optimize the performance of your NGINX server How to Build a Tough NGINX Server in 15 Steps How to Tune and Optimize Performance of NGINX Web Server Powerful ways to supercharge your NGINX server and improve its performance TLS Deployment Best Practices NGINX Web Server Security and Hardening Guide NGINX Tuning For Best Performance Top 25 NGINX Web Server Best Security Practices Follow recommended onboarding steps \u00b6 Learn about available WAF node deployment options . Learn about available options to separately manage the WAF configuration for your environments (if necessary). Deploy WAF nodes in your non-production environments with the WAF operation mode set to monitoring . Learn about how to operate, scale and monitor the WAF solution, and confirm the stability of the new network component. Deploy WAF nodes in your production environment with the WAF operation mode set to monitoring . Implement proper configuration management and monitoring processes for the new WAF component. Keep the traffic flowing via the WAF nodes in all your environments (including testing and production) for 7\u201114 days to give the WAF cloud-based backend some time to learn about your application. Enable WAF block mode in all your non-production environments and use automated or manual tests to confirm that the protected application is working as expected. Enable WAF block mode in the production environment and use available methods to confirm that the application is working as expected. Deploy the WAF nodes not just in the production environment but also in testing and staging \u00b6 The majority of Wallarm service contracts do not limit the number of WAF nodes deployed by the customer, so there is no reason to not deploy the WAF nodes across all your environments, including development, testing, staging, etc. By deploying and using the WAF nodes in all stages of your software development and/or service operation activities you have a better chance of properly testing the whole data flow and minimizing the risk of any unexpected situations in your critical production environment. Enable the libdetection WAF node library \u00b6 Analyzing requests with the libdetection library (available starting from WAF node version 2.16) significantly improves the WAF node ability to detect SQLi attacks. It is highly recommended for all Wallarm customers to upgrade to the latest version of the WAF node software and enable the libdetection library as follows: Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images, Docker node container, and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller Configure proper reporting of end-user IP addresses \u00b6 For WAF nodes located behind a load balancer or CDN please make sure to configure your WAF nodes to properly report end-user IP addresses (otherwise the IP lists functionality , Active threat verification , and some other features will not work): Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images, Docker node container, and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller Enable proper monitoring of the WAF nodes \u00b6 It is highly recommended to enable proper monitoring of Wallarm WAF nodes. The collectd service installed with every Wallarm WAF node collects the metrics listed within the link . The method for setting up the WAF node monitoring depends on its deployment option: Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller Instructions for the NGINX-based Docker image Implement proper redundancy and automatic failover functionality \u00b6 Like with every other critical component in your production environment, WAF nodes should be architected, deployed, and operated with the proper level of redundancy and automatic failover. You should have at least two active WAF nodes handling critical end-user requests. The following articles provide relevant information about the topic: Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images, Docker node container, and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller Learn how to use IP addresses whitelist, blacklist, and greylist \u00b6 In addition to blocking individual malicious requests, Wallarm WAF nodes can also block individual end-user IP addresses. Rules for IPs blocking are configured using whitelists, blacklists and greylists. More details on using IP lists \u2192 Learn how to perform gradual rollout of WAF configuration changes \u00b6 Use standard DevOps change management and gradual rollout policies for low-level configuration changes for Wallarm WAF nodes in all form-factors. For WAF rules, use a different set of application instance IDs or Host request headers. For the Define a request as an attack based on a regular expression WAF rule, in addition to the above\u2011mentioned ability to be associated with a specific application instance ID, it can be enabled in monitoring mode ( Experimental checkbox) even when the WAF is running in blocking mode. The Set traffic filtration mode WAF rule allows the control of the WAF operation mode ( monitoring or block ) from the Wallarm Console, similar to the wallarm_mode setting in the NGINX configuration (depending on the wallarm_mode_allow_override setting). Configure available integrations to receive notifications from the system \u00b6 Wallarm provides convenient native integrations with Slack, Telegram, PagerDuty, Opsgenie and other systems to quickly send you different security notifications generated by the platform, for example: Newly discovered security vulnerabilities Changes in the company network perimeter Users newly added to the company account via the Wallarm Console, etc You can also use the Triggers functionality to set up custom alerts about different events happening in the system. Learn the power of the Triggers functionality \u00b6 Depending on your specific environment we recommend you configure the following triggers : Monitoring for the increased level of malicious requests detected by the WAF. This trigger may signal one of the following potential problems: You are under attack and the WAF is successfully blocking malicious requests. You may consider reviewing the detected attacks and manually blacklist (block) reported attacker IP addresses. You have an increased level of false positive attacks detected by the WAF. You may consider escalating this to the Wallarm technical support team or manually mark the requests as false positives . If you have have the blacklisting trigger active but still receive alerts about an increased level of attacks, then the alert may signal that the trigger is not working as expected. See the configured trigger example \u2192 Notify that a new user was added to your company account in the Wallarm Console See the configured trigger example \u2192 Mark the requests as the brute-force or dirbust attack and block the IP addresses the requests were originated from Instructions on configuring brute force protection \u2192 Notify that new IP addresses were blocked See the configured trigger example \u2192 Enable SAML SSO for your account in the Wallarm Console \u00b6 You can use a SAML SSO provider like G Suite, Okta, or OneLogin to centralize the authentication of users in your Wallarm Console account. Please reach out to your Wallarm account manager or the technical support team to enable SAML SSO for your account, and after that follow these instructions to perform the SAML SSO configuration. Use the Wallarm Terraform provider for Wallarm Cloud configuration management \u00b6 Wallarm's official Terraform provider allows you to manage your Wallarm Cloud configuration (users, applications, rules, integrations, etc) using the modern Infrastructure as Code (IaC) approach. Have a plan to promptly update to newly released WAF node versions \u00b6 Wallarm is constantly working to improve the WAF node software, with new releases available about once a quarter. Please read this document for information about the recommended approach to perform the upgrades, with associated risks and relevant upgrade procedures. Learn known caveats \u00b6 All WAF nodes connected to the same Wallarm account will receive the same set of WAF rules. You still can apply different rules for different applications by using proper application instance IDs or unique HTTP request parameters like headers, query string parameters, etc. If the WAF has the trigger configured to automatically block an IP address ( trigger example ), the system will block the IP for all application instances in a Wallarm account. Similarly for other methods of changing any of the IP lists. Follow the best practices for the Active threat verification feature \u00b6 One method Wallarm uses to detect vulnerabilities is Active threat verification . Active threat verification with the main component Attack rechecker lets you turn attackers into penetration testers and discover possible security issues from their activity as they probe your apps/APIs for vulnerabilities. The module Attack rechecker finds possible vulnerabilities by probing application endpoints using real attack data from the traffic. Learn the best practices for Attack rechecker configuration \u2192","title":"Deployment best practices"},{"location":"about-wallarm-waf/deployment-best-practices/#wallarm-waf-deployment-and-maintenance-best-practices","text":"This article formulates best practices for deployment and maintenance of Wallarm WAF and API protection service.","title":"Wallarm WAF deployment and maintenance best practices"},{"location":"about-wallarm-waf/deployment-best-practices/#understand-the-power-of-nginx","text":"The majority of Wallarm WAF node deployment options use NGINX as the reverse proxy server (the foundation for the Wallarm WAF module), which provides a large range of functionality, modules, and performance/security guides. The following is a collection of helpful Internet articles: Awesome NGINX NGINX: Basics and Best Practices slide show How to optimize NGINX configuration 3 quick steps to optimize the performance of your NGINX server How to Build a Tough NGINX Server in 15 Steps How to Tune and Optimize Performance of NGINX Web Server Powerful ways to supercharge your NGINX server and improve its performance TLS Deployment Best Practices NGINX Web Server Security and Hardening Guide NGINX Tuning For Best Performance Top 25 NGINX Web Server Best Security Practices","title":"Understand the power of NGINX"},{"location":"about-wallarm-waf/deployment-best-practices/#follow-recommended-onboarding-steps","text":"Learn about available WAF node deployment options . Learn about available options to separately manage the WAF configuration for your environments (if necessary). Deploy WAF nodes in your non-production environments with the WAF operation mode set to monitoring . Learn about how to operate, scale and monitor the WAF solution, and confirm the stability of the new network component. Deploy WAF nodes in your production environment with the WAF operation mode set to monitoring . Implement proper configuration management and monitoring processes for the new WAF component. Keep the traffic flowing via the WAF nodes in all your environments (including testing and production) for 7\u201114 days to give the WAF cloud-based backend some time to learn about your application. Enable WAF block mode in all your non-production environments and use automated or manual tests to confirm that the protected application is working as expected. Enable WAF block mode in the production environment and use available methods to confirm that the application is working as expected.","title":"Follow recommended onboarding steps"},{"location":"about-wallarm-waf/deployment-best-practices/#deploy-the-waf-nodes-not-just-in-the-production-environment-but-also-in-testing-and-staging","text":"The majority of Wallarm service contracts do not limit the number of WAF nodes deployed by the customer, so there is no reason to not deploy the WAF nodes across all your environments, including development, testing, staging, etc. By deploying and using the WAF nodes in all stages of your software development and/or service operation activities you have a better chance of properly testing the whole data flow and minimizing the risk of any unexpected situations in your critical production environment.","title":"Deploy the WAF nodes not just in the production environment but also in testing and staging"},{"location":"about-wallarm-waf/deployment-best-practices/#enable-the-libdetection-waf-node-library","text":"Analyzing requests with the libdetection library (available starting from WAF node version 2.16) significantly improves the WAF node ability to detect SQLi attacks. It is highly recommended for all Wallarm customers to upgrade to the latest version of the WAF node software and enable the libdetection library as follows: Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images, Docker node container, and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller","title":"Enable the libdetection WAF node library"},{"location":"about-wallarm-waf/deployment-best-practices/#configure-proper-reporting-of-end-user-ip-addresses","text":"For WAF nodes located behind a load balancer or CDN please make sure to configure your WAF nodes to properly report end-user IP addresses (otherwise the IP lists functionality , Active threat verification , and some other features will not work): Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images, Docker node container, and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller","title":"Configure proper reporting of end-user IP addresses"},{"location":"about-wallarm-waf/deployment-best-practices/#enable-proper-monitoring-of-the-waf-nodes","text":"It is highly recommended to enable proper monitoring of Wallarm WAF nodes. The collectd service installed with every Wallarm WAF node collects the metrics listed within the link . The method for setting up the WAF node monitoring depends on its deployment option: Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller Instructions for the NGINX-based Docker image","title":"Enable proper monitoring of the WAF nodes"},{"location":"about-wallarm-waf/deployment-best-practices/#implement-proper-redundancy-and-automatic-failover-functionality","text":"Like with every other critical component in your production environment, WAF nodes should be architected, deployed, and operated with the proper level of redundancy and automatic failover. You should have at least two active WAF nodes handling critical end-user requests. The following articles provide relevant information about the topic: Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images, Docker node container, and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller","title":"Implement proper redundancy and automatic failover functionality"},{"location":"about-wallarm-waf/deployment-best-practices/#learn-how-to-use-ip-addresses-whitelist-blacklist-and-greylist","text":"In addition to blocking individual malicious requests, Wallarm WAF nodes can also block individual end-user IP addresses. Rules for IPs blocking are configured using whitelists, blacklists and greylists. More details on using IP lists \u2192","title":"Learn how to use IP addresses whitelist, blacklist, and greylist"},{"location":"about-wallarm-waf/deployment-best-practices/#learn-how-to-perform-gradual-rollout-of-waf-configuration-changes","text":"Use standard DevOps change management and gradual rollout policies for low-level configuration changes for Wallarm WAF nodes in all form-factors. For WAF rules, use a different set of application instance IDs or Host request headers. For the Define a request as an attack based on a regular expression WAF rule, in addition to the above\u2011mentioned ability to be associated with a specific application instance ID, it can be enabled in monitoring mode ( Experimental checkbox) even when the WAF is running in blocking mode. The Set traffic filtration mode WAF rule allows the control of the WAF operation mode ( monitoring or block ) from the Wallarm Console, similar to the wallarm_mode setting in the NGINX configuration (depending on the wallarm_mode_allow_override setting).","title":"Learn how to perform gradual rollout of WAF configuration changes"},{"location":"about-wallarm-waf/deployment-best-practices/#configure-available-integrations-to-receive-notifications-from-the-system","text":"Wallarm provides convenient native integrations with Slack, Telegram, PagerDuty, Opsgenie and other systems to quickly send you different security notifications generated by the platform, for example: Newly discovered security vulnerabilities Changes in the company network perimeter Users newly added to the company account via the Wallarm Console, etc You can also use the Triggers functionality to set up custom alerts about different events happening in the system.","title":"Configure available integrations to receive notifications from the system"},{"location":"about-wallarm-waf/deployment-best-practices/#learn-the-power-of-the-triggers-functionality","text":"Depending on your specific environment we recommend you configure the following triggers : Monitoring for the increased level of malicious requests detected by the WAF. This trigger may signal one of the following potential problems: You are under attack and the WAF is successfully blocking malicious requests. You may consider reviewing the detected attacks and manually blacklist (block) reported attacker IP addresses. You have an increased level of false positive attacks detected by the WAF. You may consider escalating this to the Wallarm technical support team or manually mark the requests as false positives . If you have have the blacklisting trigger active but still receive alerts about an increased level of attacks, then the alert may signal that the trigger is not working as expected. See the configured trigger example \u2192 Notify that a new user was added to your company account in the Wallarm Console See the configured trigger example \u2192 Mark the requests as the brute-force or dirbust attack and block the IP addresses the requests were originated from Instructions on configuring brute force protection \u2192 Notify that new IP addresses were blocked See the configured trigger example \u2192","title":"Learn the power of the Triggers functionality"},{"location":"about-wallarm-waf/deployment-best-practices/#enable-saml-sso-for-your-account-in-the-wallarm-console","text":"You can use a SAML SSO provider like G Suite, Okta, or OneLogin to centralize the authentication of users in your Wallarm Console account. Please reach out to your Wallarm account manager or the technical support team to enable SAML SSO for your account, and after that follow these instructions to perform the SAML SSO configuration.","title":"Enable SAML SSO for your account in the Wallarm Console"},{"location":"about-wallarm-waf/deployment-best-practices/#use-the-wallarm-terraform-provider-for-wallarm-cloud-configuration-management","text":"Wallarm's official Terraform provider allows you to manage your Wallarm Cloud configuration (users, applications, rules, integrations, etc) using the modern Infrastructure as Code (IaC) approach.","title":"Use the Wallarm Terraform provider for Wallarm Cloud configuration management"},{"location":"about-wallarm-waf/deployment-best-practices/#have-a-plan-to-promptly-update-to-newly-released-waf-node-versions","text":"Wallarm is constantly working to improve the WAF node software, with new releases available about once a quarter. Please read this document for information about the recommended approach to perform the upgrades, with associated risks and relevant upgrade procedures.","title":"Have a plan to promptly update to newly released WAF node versions"},{"location":"about-wallarm-waf/deployment-best-practices/#learn-known-caveats","text":"All WAF nodes connected to the same Wallarm account will receive the same set of WAF rules. You still can apply different rules for different applications by using proper application instance IDs or unique HTTP request parameters like headers, query string parameters, etc. If the WAF has the trigger configured to automatically block an IP address ( trigger example ), the system will block the IP for all application instances in a Wallarm account. Similarly for other methods of changing any of the IP lists.","title":"Learn known caveats"},{"location":"about-wallarm-waf/deployment-best-practices/#follow-the-best-practices-for-the-active-threat-verification-feature","text":"One method Wallarm uses to detect vulnerabilities is Active threat verification . Active threat verification with the main component Attack rechecker lets you turn attackers into penetration testers and discover possible security issues from their activity as they probe your apps/APIs for vulnerabilities. The module Attack rechecker finds possible vulnerabilities by probing application endpoints using real attack data from the traffic. Learn the best practices for Attack rechecker configuration \u2192","title":"Follow the best practices for the Active threat verification feature"},{"location":"about-wallarm-waf/detecting-vulnerabilities/","text":"Detecting vulnerabilities \u00b6 What is a vulnerability? \u00b6 A vulnerability is an error made due to negligence or inadequate information when building or implementing a web application that can lead to an information security risk. The information security risks are: Unauthorized data access: for example, access to read and modify user data Denial of service Data corruption and much more Vulnerability detection methods \u00b6 When scanning the application for open vulnerabilities, Wallarm WAF sends requests with attack signs to the protected application address and analyzes application responses. If the response matches one or more pre\u2011defined vulnerability signs, Wallarm WAF records open vulnerability. For example: if the response to the request sent to read the /etc/passwd contents returns the /etc/passwd contents, protected application is vulnerable to the Path Traversal attacks. Wallarm WAF will record the vulnerability with an appropriate type. To detect vulnerabilities in the application, Wallarm WAF sends requests with attack signs using the following methods: Passive detection : the vulnerability was found due to the security incident that occurred. Active threat verification with the main component Attack rechecker lets you turn attackers into penetration testers and discover possible security issues from their activity as they probe your apps/APIs for vulnerabilities. The module Attack rechecker finds possible vulnerabilities by probing application endpoints using real attack data from the traffic. By default this method is disabled. Vulnerability scanner : all elements of the scope are scanned for typical vulnerabilities. Passive detection \u00b6 With passive detection, Wallarm WAF detects a vulnerability due to the security incident that occurred. If an application vulnerability has been exploited during an attack, Wallarm WAF records the security incident and the exploited vulnerability. Passive vulnerability detection is enabled by default. Active threat verification \u00b6 How it works \u00b6 Based on the initial detected attacks, Attack rechecker creates a lot of new test requests with different payloads attacking the same endpoint. This mechanism allows Wallarm to detect vulnerabilities that could be potentially exploited during attacks. The Attack rechecker process will either confirm that the application is not vulnerable to the specific attack vectors or find actual application security issues. List of vulnerabilities that can be detected by the module The Attack rechecker process uses the following logic to check the protected application for possible Web and API security vulnerabilities: For every group of malicious request (every attack) detected by a Wallarm WAF node and uploaded to the connected Wallarm Cloud, the system analyzes which specific endpoint (URL, request string parameter, JSON attribute, XML field, etc) was attacked and which specific kind of vulnerability (SQLi, RCE, XSS, etc) the attacker was trying to exploit. For example, let's take a look at the following malicious GET request: https://example.com/login?token = IyEvYmluL3NoCg & user = UNION SELECT username, password From the request the system will learn the following details: The attacked URL is https://example.com/login The type of used attack is SQLi (according to the UNION SELECT username, password payload) The attacked GET request parameter is user Additional piece of information provided in the request is the request string parameter token=IyEvYmluL3NoCg (it is probably used by the application to authenticate the user) Using the collected information the Attack rechecker module will create a list of about 100-150 test requests to the originally targeted endpoint but with different types of malicious payloads for the same type of attack (like SQLi). For example: https://example.com/login?token = IyEvYmluL3NoCg & user = 1 ')+WAITFOR+DELAY+' 0 indexpt '+AND+(' wlrm '=' wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 +AND+SLEEP ( 10 ) --+wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 ) ; SELECT+PG_SLEEP ( 10 ) -- https://example.com/login?token = IyEvYmluL3NoCg & user = 1 '+OR+SLEEP(10)+AND+' wlrm '=' wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 +AND+1 =( SELECT+1+FROM+PG_SLEEP ( 10 )) https://example.com/login?token = IyEvYmluL3NoCg & user = %23 '%23\\x22%0a-sleep(10)%23 https://example.com/login?token=IyEvYmluL3NoCg&user=1' ; +WAITFOR+DELAY+ '0code:10' -- https://example.com/login?token = IyEvYmluL3NoCg & user = 1 %27%29+OR+SLEEP%280%29+AND+%28%27wlrm%27%3D%27wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = SLEEP ( 10 ) /* 'XOR(SLEEP(10))OR' | \\x 22XOR ( SLEEP ( 10 )) OR \\x 22 */ The Attack rechecker module will send generated test requests to the application bypassing the WAF protection (using the [white-listing feature][whitelist-scanner-addresses]) and verify that the application at the specific endpoint is not vulnerable to the specific attack type. If Attack rechecker suspects that the application has an actual security vulnerability, it will create an event with type incident . User-Agent HTTPS header value in Attack rechecker requests The User-Agent HTTP header in Attack rechecker requests will have the value Wallarm attack-rechecker (v1.x) . Detected security incidents are reported in the Wallarm Console and are able to be dispatched to your security team via available third-party Integrations and Triggers . Potential risks from the Attack rechecker activity \u00b6 In rare cases when a legitimate request is detected by the WAF as an attack, the request will be replayed by Attack rechecker . If the request is not idempotent (for example, an authenticated request creating a new object in the application), then the Attack rechecker requests may create many new unwanted objects in the user account or perform other unexpected operations. To minimize the risk of the described situation, Attack rechecker will automatically strip the following HTTP headers from the replayed requests: Cookie Authorization: Basic Viewstate In cases when the application uses a non-standard authentication method or does not require authenticating the requests, Attack rechecker may replay any request from the traffic and harm the system. For example: repeat 100 and more money transactions or orders. To minimize the risk of the described situation, it is recommended to use testing or staging environments for attack replaying and mask non-standard request authentication parameters . Configuration \u00b6 The module Attack rechecker is disabled by default. It should be enabled and properly configured to work correctly. Please learn the Attack rechecker configuration options and best practices for these options setup from this document . Vulnerability scanner \u00b6 How it works \u00b6 Vulnerability scanner checks all elements of the company scope for typical vulnerabilities. The scanner sends requests to application addresses from fixed IP addresses and adds the header X-Wallarm-Scanner-Info to the requests. Configuration \u00b6 The scanner can be enabled or disabled in the Wallarm Console \u2192 Scanner section. By default, the scanner is enabled. The list of vulnerabilities that can be detected by the scanner can be configured in the Wallarm Console \u2192 Scanner section. By default, vulnerability scanner detects all available vulnerabilities. The limit of requests sent from the scanner can be configured in the Wallarm Console \u2192 Scanner section. If you use additional facilities (software or hardware) to automatically filter and block traffic, it is recommended that you configure a whitelist with the IP addresses for the Wallarm Scanner. This will allow Wallarm components to seamlessly scan your resources for vulnerabilities. Scanner IP address registered in Wallarm EU Cloud Scanner IP address registered in Wallarm US Cloud If you do not use additional facilities but use Wallarm Scanner, you do not need to manually whitelist Scanner IP addresses. Starting with WAF node 3.0, Scanner IP addresses are automatically whitelisted. False positives \u00b6 False positive occurs when attack signs are detected in the legitimate request or when legitimate entity is qualified as a vulnerability. More details on false positives in attack detection \u2192 False positives in vulnerability scanning may occur due to the protected application specificities. Similar responses to similar requests may indicate an open vulnerability in one protected application and be expected behavior for another protected application. If a false positive for a vulnerability is detected, you can add an appropriate mark to the vulnerability in the Wallarm Console. A vulnerability marked as a false positive will be switched to an appropriate status and will not be rechecked. More about managing false positives via the Wallarm Console \u2192 If the detected vulnerability exists in the protected application but cannot be fixed, we recommend setting up the Create a virtual patch rule. This rule will allow blocking attacks exploiting the detected type of vulnerability and will eliminate the risk of an incident. Managing discovered vulnerabilities \u00b6 All detected vulnerabilities are displayed in the Wallarm Console \u2192 Vulnerabilities section. You can manage vulnerabilities through the interface as follows: View and analyze vulnerabilities Run vulnerability status recheck: still opened or fixed on the application side Close vulnerabilities or mark them as false positives For more information on managing vulnerabilities, see the instructions on working with vulnerabilities .","title":"Detecting vulnerabilities"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#detecting-vulnerabilities","text":"","title":"Detecting vulnerabilities"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#what-is-a-vulnerability","text":"A vulnerability is an error made due to negligence or inadequate information when building or implementing a web application that can lead to an information security risk. The information security risks are: Unauthorized data access: for example, access to read and modify user data Denial of service Data corruption and much more","title":"What is a vulnerability?"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#vulnerability-detection-methods","text":"When scanning the application for open vulnerabilities, Wallarm WAF sends requests with attack signs to the protected application address and analyzes application responses. If the response matches one or more pre\u2011defined vulnerability signs, Wallarm WAF records open vulnerability. For example: if the response to the request sent to read the /etc/passwd contents returns the /etc/passwd contents, protected application is vulnerable to the Path Traversal attacks. Wallarm WAF will record the vulnerability with an appropriate type. To detect vulnerabilities in the application, Wallarm WAF sends requests with attack signs using the following methods: Passive detection : the vulnerability was found due to the security incident that occurred. Active threat verification with the main component Attack rechecker lets you turn attackers into penetration testers and discover possible security issues from their activity as they probe your apps/APIs for vulnerabilities. The module Attack rechecker finds possible vulnerabilities by probing application endpoints using real attack data from the traffic. By default this method is disabled. Vulnerability scanner : all elements of the scope are scanned for typical vulnerabilities.","title":"Vulnerability detection methods"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#passive-detection","text":"With passive detection, Wallarm WAF detects a vulnerability due to the security incident that occurred. If an application vulnerability has been exploited during an attack, Wallarm WAF records the security incident and the exploited vulnerability. Passive vulnerability detection is enabled by default.","title":"Passive detection"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#active-threat-verification","text":"","title":"Active threat verification"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#how-it-works","text":"Based on the initial detected attacks, Attack rechecker creates a lot of new test requests with different payloads attacking the same endpoint. This mechanism allows Wallarm to detect vulnerabilities that could be potentially exploited during attacks. The Attack rechecker process will either confirm that the application is not vulnerable to the specific attack vectors or find actual application security issues. List of vulnerabilities that can be detected by the module The Attack rechecker process uses the following logic to check the protected application for possible Web and API security vulnerabilities: For every group of malicious request (every attack) detected by a Wallarm WAF node and uploaded to the connected Wallarm Cloud, the system analyzes which specific endpoint (URL, request string parameter, JSON attribute, XML field, etc) was attacked and which specific kind of vulnerability (SQLi, RCE, XSS, etc) the attacker was trying to exploit. For example, let's take a look at the following malicious GET request: https://example.com/login?token = IyEvYmluL3NoCg & user = UNION SELECT username, password From the request the system will learn the following details: The attacked URL is https://example.com/login The type of used attack is SQLi (according to the UNION SELECT username, password payload) The attacked GET request parameter is user Additional piece of information provided in the request is the request string parameter token=IyEvYmluL3NoCg (it is probably used by the application to authenticate the user) Using the collected information the Attack rechecker module will create a list of about 100-150 test requests to the originally targeted endpoint but with different types of malicious payloads for the same type of attack (like SQLi). For example: https://example.com/login?token = IyEvYmluL3NoCg & user = 1 ')+WAITFOR+DELAY+' 0 indexpt '+AND+(' wlrm '=' wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 +AND+SLEEP ( 10 ) --+wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 ) ; SELECT+PG_SLEEP ( 10 ) -- https://example.com/login?token = IyEvYmluL3NoCg & user = 1 '+OR+SLEEP(10)+AND+' wlrm '=' wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 +AND+1 =( SELECT+1+FROM+PG_SLEEP ( 10 )) https://example.com/login?token = IyEvYmluL3NoCg & user = %23 '%23\\x22%0a-sleep(10)%23 https://example.com/login?token=IyEvYmluL3NoCg&user=1' ; +WAITFOR+DELAY+ '0code:10' -- https://example.com/login?token = IyEvYmluL3NoCg & user = 1 %27%29+OR+SLEEP%280%29+AND+%28%27wlrm%27%3D%27wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = SLEEP ( 10 ) /* 'XOR(SLEEP(10))OR' | \\x 22XOR ( SLEEP ( 10 )) OR \\x 22 */ The Attack rechecker module will send generated test requests to the application bypassing the WAF protection (using the [white-listing feature][whitelist-scanner-addresses]) and verify that the application at the specific endpoint is not vulnerable to the specific attack type. If Attack rechecker suspects that the application has an actual security vulnerability, it will create an event with type incident . User-Agent HTTPS header value in Attack rechecker requests The User-Agent HTTP header in Attack rechecker requests will have the value Wallarm attack-rechecker (v1.x) . Detected security incidents are reported in the Wallarm Console and are able to be dispatched to your security team via available third-party Integrations and Triggers .","title":"How it works"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#potential-risks-from-the-attack-rechecker-activity","text":"In rare cases when a legitimate request is detected by the WAF as an attack, the request will be replayed by Attack rechecker . If the request is not idempotent (for example, an authenticated request creating a new object in the application), then the Attack rechecker requests may create many new unwanted objects in the user account or perform other unexpected operations. To minimize the risk of the described situation, Attack rechecker will automatically strip the following HTTP headers from the replayed requests: Cookie Authorization: Basic Viewstate In cases when the application uses a non-standard authentication method or does not require authenticating the requests, Attack rechecker may replay any request from the traffic and harm the system. For example: repeat 100 and more money transactions or orders. To minimize the risk of the described situation, it is recommended to use testing or staging environments for attack replaying and mask non-standard request authentication parameters .","title":"Potential risks from the Attack rechecker activity"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#configuration","text":"The module Attack rechecker is disabled by default. It should be enabled and properly configured to work correctly. Please learn the Attack rechecker configuration options and best practices for these options setup from this document .","title":"Configuration"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#vulnerability-scanner","text":"","title":"Vulnerability scanner"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#how-it-works_1","text":"Vulnerability scanner checks all elements of the company scope for typical vulnerabilities. The scanner sends requests to application addresses from fixed IP addresses and adds the header X-Wallarm-Scanner-Info to the requests.","title":"How it works"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#configuration_1","text":"The scanner can be enabled or disabled in the Wallarm Console \u2192 Scanner section. By default, the scanner is enabled. The list of vulnerabilities that can be detected by the scanner can be configured in the Wallarm Console \u2192 Scanner section. By default, vulnerability scanner detects all available vulnerabilities. The limit of requests sent from the scanner can be configured in the Wallarm Console \u2192 Scanner section. If you use additional facilities (software or hardware) to automatically filter and block traffic, it is recommended that you configure a whitelist with the IP addresses for the Wallarm Scanner. This will allow Wallarm components to seamlessly scan your resources for vulnerabilities. Scanner IP address registered in Wallarm EU Cloud Scanner IP address registered in Wallarm US Cloud If you do not use additional facilities but use Wallarm Scanner, you do not need to manually whitelist Scanner IP addresses. Starting with WAF node 3.0, Scanner IP addresses are automatically whitelisted.","title":"Configuration"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#false-positives","text":"False positive occurs when attack signs are detected in the legitimate request or when legitimate entity is qualified as a vulnerability. More details on false positives in attack detection \u2192 False positives in vulnerability scanning may occur due to the protected application specificities. Similar responses to similar requests may indicate an open vulnerability in one protected application and be expected behavior for another protected application. If a false positive for a vulnerability is detected, you can add an appropriate mark to the vulnerability in the Wallarm Console. A vulnerability marked as a false positive will be switched to an appropriate status and will not be rechecked. More about managing false positives via the Wallarm Console \u2192 If the detected vulnerability exists in the protected application but cannot be fixed, we recommend setting up the Create a virtual patch rule. This rule will allow blocking attacks exploiting the detected type of vulnerability and will eliminate the risk of an incident.","title":"False positives"},{"location":"about-wallarm-waf/detecting-vulnerabilities/#managing-discovered-vulnerabilities","text":"All detected vulnerabilities are displayed in the Wallarm Console \u2192 Vulnerabilities section. You can manage vulnerabilities through the interface as follows: View and analyze vulnerabilities Run vulnerability status recheck: still opened or fixed on the application side Close vulnerabilities or mark them as false positives For more information on managing vulnerabilities, see the instructions on working with vulnerabilities .","title":"Managing discovered vulnerabilities"},{"location":"about-wallarm-waf/overview/","text":"What is Wallarm WAF \u00b6 Wallarm is a DevOps-friendly Web Application Firewall (WAF) uniquely suited to protect your cloud applications and APIs. Wallarm's hybrid architecture safeguards your resources by offering: Protection from hacker attacks. Automatic detection of vulnerabilities . Wallarm analyzes all incoming HTTP requests and instantly blocks the malicious ones. Wallarm continuously collects metrics from the entire network traffic and processes the metrics by applying machine learning in the Wallarm cloud. Based on the processed requests, Wallarm creates an individual profile of the protected resources and applies the finely tuned security rules. The Wallarm scanner checks your company's network resources in several modes to detect vulnerabilities. Wallarm consists of the following components: The Wallarm filter node The Wallarm cloud Filter Node \u00b6 The network traffic check is done through the Wallarm filter node installed in the company's network infrastructure. The Wallarm filter node does the following: Blocks malicious requests and filters the valid ones Analyzes the company's entire network traffic Collects the network traffic metrics and uploads the metrics to the Wallarm cloud Downloads fine-tuned resource-specific rules from the Wallarm cloud Cloud \u00b6 The Wallarm cloud does the following: Processes the metrics that the filter node uploads Creates fine-tuned resource-specific rules Scans the company's protected resources to detect vulnerabilities Wallarm manages European and American cloud instances with each cloud being completely separate in terms of databases, API endpoints, client accounts, etc. A client registered in one Wallarm cloud cannot use other Wallarm cloud to manage or get access to their data stored in the first cloud. At the same time you may use both Wallarm clouds. In this case you will need to use different accounts in the Wallarm system and API endpoints to access and manage your information in individual clouds. Endpoints for the clouds are provided below. EU Cloud \u00b6 Physically located in the Netherlands. https://my.wallarm.com/ to create Wallarm account https://api.wallarm.com/ to call API methods US Cloud \u00b6 Physically located in the USA. https://us1.my.wallarm.com/ to create Wallarm account https://us1.api.wallarm.com/ to call API methods Demo videos \u00b6","title":"What is Wallarm"},{"location":"about-wallarm-waf/overview/#what-is-wallarm-waf","text":"Wallarm is a DevOps-friendly Web Application Firewall (WAF) uniquely suited to protect your cloud applications and APIs. Wallarm's hybrid architecture safeguards your resources by offering: Protection from hacker attacks. Automatic detection of vulnerabilities . Wallarm analyzes all incoming HTTP requests and instantly blocks the malicious ones. Wallarm continuously collects metrics from the entire network traffic and processes the metrics by applying machine learning in the Wallarm cloud. Based on the processed requests, Wallarm creates an individual profile of the protected resources and applies the finely tuned security rules. The Wallarm scanner checks your company's network resources in several modes to detect vulnerabilities. Wallarm consists of the following components: The Wallarm filter node The Wallarm cloud","title":"What is Wallarm WAF"},{"location":"about-wallarm-waf/overview/#filter-node","text":"The network traffic check is done through the Wallarm filter node installed in the company's network infrastructure. The Wallarm filter node does the following: Blocks malicious requests and filters the valid ones Analyzes the company's entire network traffic Collects the network traffic metrics and uploads the metrics to the Wallarm cloud Downloads fine-tuned resource-specific rules from the Wallarm cloud","title":"Filter Node"},{"location":"about-wallarm-waf/overview/#cloud","text":"The Wallarm cloud does the following: Processes the metrics that the filter node uploads Creates fine-tuned resource-specific rules Scans the company's protected resources to detect vulnerabilities Wallarm manages European and American cloud instances with each cloud being completely separate in terms of databases, API endpoints, client accounts, etc. A client registered in one Wallarm cloud cannot use other Wallarm cloud to manage or get access to their data stored in the first cloud. At the same time you may use both Wallarm clouds. In this case you will need to use different accounts in the Wallarm system and API endpoints to access and manage your information in individual clouds. Endpoints for the clouds are provided below.","title":"Cloud"},{"location":"about-wallarm-waf/overview/#eu-cloud","text":"Physically located in the Netherlands. https://my.wallarm.com/ to create Wallarm account https://api.wallarm.com/ to call API methods","title":"EU Cloud"},{"location":"about-wallarm-waf/overview/#us-cloud","text":"Physically located in the USA. https://us1.my.wallarm.com/ to create Wallarm account https://us1.api.wallarm.com/ to call API methods","title":"US Cloud"},{"location":"about-wallarm-waf/overview/#demo-videos","text":"","title":"Demo videos"},{"location":"about-wallarm-waf/protecting-against-attacks/","text":"Detecting attacks \u00b6 What is attack and what are attack components? \u00b6 Attack is a single hit or multiple hits that have the same attack type, parameter with the attack vector, and the address they are sent to. Hits may come from the same or different IP addresses and have different value of the attack vector within one attack type. Hit is a serialized malicious request (original malicious request and metadata added by the WAF node). Attack vector is a part of a malicious request containing the attack sign. Attack types \u00b6 All attacks that can be detected by Wallarm WAF are divided into groups: Input validation attacks Behavioral attacks Attack detection method depends on the attack group. To detect behavioral attacks, additional Wallarm WAF configuration is required. Input validation attacks \u00b6 Input validation attacks include SQL injection, cross\u2011site scripting, remote code execution, Path Traversal and other attack types. Each attack type are characterized by specific symbol (token) combinations sent in the requests. To detect input validation attacks, it is required to conduct syntax analysis of the requests - parse requests in order to detect specific symbol combinations. Input validation attacks are detected by the WAF node using the listed tools . Detection of input validation attacks is enabled for all clients by default. Behavioral attacks \u00b6 Behavioral attacks include classes of brute\u2011force attacks: passwords and session identifiers brute\u2011forcing, files and directories forced browsing (dirbust), credential stuffing. Behavioral attacks can be characterized by a large number of requests with different forced parameter values sent to a typical URL for a limited timeframe. For example, if an attacker forces password, many similar requests with different password values can be sent to the user authentication URL: https://example.com/login/?username = admin & password = 123456 To detect behavioral attacks, it is required to conduct syntax analysis of requests and correlation analysis of request number and time between requests. Correlation analysis is conducted when the threshold of request number sent to user authentication or resource file directory URL is exceeded. Request number threshold should be set to reduce the risk of legitimate request blocking (for example, when the user inputs incorrect password to his account several times). Correlation analysis is conducted by the Wallarm WAF postanalytics module. Comparison of the received requests number and the threshold for the requests number, and blocking of requests is conducted in the Wallarm Cloud. When behavioral attack is detected, requests sources are blocked, namely the IP addresses the requests were sent from are added to the blacklist. To protect the resource against behavioral attacks, it is required to set the threshold for correlation analysis and URLs that are vulnerable to behavioral attacks. Instructions on configuration of brute force protection \u2192 Brute force protection restrictions When searching for brute\u2011force attack signs, Wallarm WAF analyzes only HTTP requests that do not contain signs of other attack types. For example, the requests are not considered to be a part of brute-force attack in the following cases: These requests contain signs of input validation attacks . These requests match the regular expression specified in the rule Define a request as an attack based on a regular expression . Types of protected resources \u00b6 Wallarm WAF analyzes HTTP and WebSocket traffic sent to the protected resources: HTTP traffic analysis is enabled by default. Wallarm WAF analyzes HTTP traffic for input validation attacks and behavioral attacks . WebSocket traffic analysis should be enabled additionally via the directive wallarm_parse_websocket . Wallarm WAF analyzes WebSocket traffic only for input validation attacks . Protected resource API can be designed on the basis of REST, gRPC, or GraphQL technologies. Attack detection process \u00b6 To detect attacks, Wallarm WAF uses the following process: Determine the request format and parse every request part as described in the document about request parsing . Determine the endpoint the request is addressed to. Apply custom detection rules determined in the LOM file. Make a decision whether the request is malicious or not based on rules determined in proton.db and LOM . Tools for attack detection \u00b6 To detect malicious requests, Wallarm WAF analyzes all requests sent to the protected resource using the following tools: Library libproton Library libdetection Custom detection rules Library libproton \u00b6 The libproton library is a primary tool for detecting malicious requests. The library uses the component proton.db which determines different attack type signs as token sequences, for example: union select for the SQL injection attack type. If the request contains a token sequence that matches the sequence from proton.db , this request is considered to be an attack of the corresponding type. Wallarm regularly updates proton.db with token sequences for new attack types and for already described attack types. Library libdetection \u00b6 libdetection overview \u00b6 The libdetection library additionally validates attacks detected by the library libproton as follows: If libdetection confirms the attack signs detected by libproton , the attack is uploaded to the Wallarm Cloud and blocked (if the WAF node is working in the block mode). If libdetection does not confirm the attack signs detected by libproton , the request is considered legitimate, the attack is not uploaded to the Wallarm Cloud and is not blocked (if the WAF node is working in the block mode). Using libdetection ensures the double\u2011detection of attacks and reduces the number of false positives. Attack types validated by the libdetection library Currently, the library libdetection only validates SQL Injection attacks. How libdetection works \u00b6 The particular characteristic of libdetection is that it analyzes requests not only for token sequences specific for attack types, but also for context in which the token sequence was sent. The library contains the character strings of different attack type syntaxes (SQL Injection for now). The string is named as the context. Example of the context for the SQL injection attack type: SELECT example FROM table WHERE id= The library conducts the attack syntax analysis for matching the contexts. If the attack does not match the contexts, then the request will not be defined as a malicious request and will not be blocked (if the WAF node is working in the block mode). Enabling libdetection \u00b6 Analyzing requests with the libdetection library is disabled by default. To reduce the number of false positives, we recommend enabling analysis. To enable the analysis: Set the value of the directive wallarm_enable_libdetection to on . The directive can be set inside the http , server , or location block of the NGINX configuration file. Set the value of the directive proxy_request_buffering to on to allow analyzing the request body. The directive can be set inside the http , server , or location block of the NGINX configuration file. Memory consumption increase When analyzing attacks using the libdetection library, the amount of memory consumed by NGINX and Wallarm processes may increase by about 10%. Testing libdetection \u00b6 To check the operation of libdetection , you can send the following legitimate request to the protected resource: curl \"http://localhost/?id=1' UNION SELECT\" The library libproton will detect UNION SELECT as the SQL Injection attack sign. Since UNION SELECT without other commands is not a sign of the SQL Injection attack, libproton detects a false positive. If analyzing of requests with the libdetection library is enabled, the SQL Injection attack sign will not be confirmed in the request. The request will be considered legitimate, the attack will not be uploaded to the Wallarm Cloud and will not be blocked (if the WAF node is working in the block mode). Custom detection rules \u00b6 Wallarm clients can set custom detection rules based on protected application specificities. There are the following types of custom detection rules: Tag requests as a brute-force or forced browsing attack Create a virtual patch Define a request as an attack based on a regular expression Disable attack detection by the regular expressions Ignore tokens disables detection of specified attack signs ( tokens ) in certain requests. This rule is created automatically when adding a false positive marks and is not displayed in the Wallarm Console. Custom detection rules and other rules are compiled into Local Objective Model (LOM) and applied along with the standard rules from proton.db when analyzing requests. More details on LOM building \u2192 Monitoring and blocking attacks \u00b6 Wallarm WAF can process attacks in the following modes: Monitoring mode: detects attacks and displays information about attacks in the Wallarm Console. Blocking mode: detects, blocks attacks and displays information about attacks in the Wallarm Console. Wallarm WAF ensures quality request analysis and low level of false positives. However each protected application has its own specificities, so we recommend analyzing the work of the Wallarm WAF in the monitoring mode before enabling the blocking mode. To control the filtration mode, the directive wallarm_mode is used. More detailed information about filtration mode configuration is available within the link . The filtration mode for behavioral attacks is configured separately via the particular trigger . False positives \u00b6 False positive occurs when attack signs are detected in the legitimate request or when legitimate entity is qualified as a vulnerability. More details on false positives in vulnerability scanning \u2192 When analyzing requests for attacks, Wallarm WAF uses the standard rule set that provides optimal application protection with ultra\u2011low false positives. Due to protected application specificities, standard rules may mistakenly recognize attack signs in legitimate requests. For example: SQL injection attack may be detected in the request adding a post with malicious SQL query description to the Database Administrator Forum. In such cases, standard rules need to be adjusted to accommodate protected application specificities by disabling detection of certain attack signs in the requests with certain address, parameters or other elements. These rules has the action type Ignore tokens . The rule Ignore tokens is created automatically if an attack or a hit is marked as a false positive in the Wallarm Console. More details on managing false positives via the Wallarm Console \u2192 Identifying and handling false positives is a part of fine\u2011uning Wallarm WAF to protect your applications. We recommend to deploy the first WAF node in the monitoring mode and analyze detected attacks. If some attacks are mistakenly recognized as attacks, mark them as false positives and switch the WAF node to blocking mode. Managing detected attacks \u00b6 All detected attacks are displayed in the Wallarm Console \u2192 Events section by the filter attacks . You can manage attacks through the interface as follows: View and analyze attacks Increase the priority of an attack in the recheck queue Mark attacks or separate hits as false positives Create the rules for custom processing of separate hits For more information on managing attacks, see the instructions on working with attacks . Demo videos \u00b6","title":"Detecting attacks"},{"location":"about-wallarm-waf/protecting-against-attacks/#detecting-attacks","text":"","title":"Detecting attacks"},{"location":"about-wallarm-waf/protecting-against-attacks/#what-is-attack-and-what-are-attack-components","text":"Attack is a single hit or multiple hits that have the same attack type, parameter with the attack vector, and the address they are sent to. Hits may come from the same or different IP addresses and have different value of the attack vector within one attack type. Hit is a serialized malicious request (original malicious request and metadata added by the WAF node). Attack vector is a part of a malicious request containing the attack sign.","title":"What is attack and what are attack components?"},{"location":"about-wallarm-waf/protecting-against-attacks/#attack-types","text":"All attacks that can be detected by Wallarm WAF are divided into groups: Input validation attacks Behavioral attacks Attack detection method depends on the attack group. To detect behavioral attacks, additional Wallarm WAF configuration is required.","title":"Attack types"},{"location":"about-wallarm-waf/protecting-against-attacks/#input-validation-attacks","text":"Input validation attacks include SQL injection, cross\u2011site scripting, remote code execution, Path Traversal and other attack types. Each attack type are characterized by specific symbol (token) combinations sent in the requests. To detect input validation attacks, it is required to conduct syntax analysis of the requests - parse requests in order to detect specific symbol combinations. Input validation attacks are detected by the WAF node using the listed tools . Detection of input validation attacks is enabled for all clients by default.","title":"Input validation attacks"},{"location":"about-wallarm-waf/protecting-against-attacks/#behavioral-attacks","text":"Behavioral attacks include classes of brute\u2011force attacks: passwords and session identifiers brute\u2011forcing, files and directories forced browsing (dirbust), credential stuffing. Behavioral attacks can be characterized by a large number of requests with different forced parameter values sent to a typical URL for a limited timeframe. For example, if an attacker forces password, many similar requests with different password values can be sent to the user authentication URL: https://example.com/login/?username = admin & password = 123456 To detect behavioral attacks, it is required to conduct syntax analysis of requests and correlation analysis of request number and time between requests. Correlation analysis is conducted when the threshold of request number sent to user authentication or resource file directory URL is exceeded. Request number threshold should be set to reduce the risk of legitimate request blocking (for example, when the user inputs incorrect password to his account several times). Correlation analysis is conducted by the Wallarm WAF postanalytics module. Comparison of the received requests number and the threshold for the requests number, and blocking of requests is conducted in the Wallarm Cloud. When behavioral attack is detected, requests sources are blocked, namely the IP addresses the requests were sent from are added to the blacklist. To protect the resource against behavioral attacks, it is required to set the threshold for correlation analysis and URLs that are vulnerable to behavioral attacks. Instructions on configuration of brute force protection \u2192 Brute force protection restrictions When searching for brute\u2011force attack signs, Wallarm WAF analyzes only HTTP requests that do not contain signs of other attack types. For example, the requests are not considered to be a part of brute-force attack in the following cases: These requests contain signs of input validation attacks . These requests match the regular expression specified in the rule Define a request as an attack based on a regular expression .","title":"Behavioral attacks"},{"location":"about-wallarm-waf/protecting-against-attacks/#types-of-protected-resources","text":"Wallarm WAF analyzes HTTP and WebSocket traffic sent to the protected resources: HTTP traffic analysis is enabled by default. Wallarm WAF analyzes HTTP traffic for input validation attacks and behavioral attacks . WebSocket traffic analysis should be enabled additionally via the directive wallarm_parse_websocket . Wallarm WAF analyzes WebSocket traffic only for input validation attacks . Protected resource API can be designed on the basis of REST, gRPC, or GraphQL technologies.","title":"Types of protected resources"},{"location":"about-wallarm-waf/protecting-against-attacks/#attack-detection-process","text":"To detect attacks, Wallarm WAF uses the following process: Determine the request format and parse every request part as described in the document about request parsing . Determine the endpoint the request is addressed to. Apply custom detection rules determined in the LOM file. Make a decision whether the request is malicious or not based on rules determined in proton.db and LOM .","title":"Attack detection process"},{"location":"about-wallarm-waf/protecting-against-attacks/#tools-for-attack-detection","text":"To detect malicious requests, Wallarm WAF analyzes all requests sent to the protected resource using the following tools: Library libproton Library libdetection Custom detection rules","title":"Tools for attack detection"},{"location":"about-wallarm-waf/protecting-against-attacks/#library-libproton","text":"The libproton library is a primary tool for detecting malicious requests. The library uses the component proton.db which determines different attack type signs as token sequences, for example: union select for the SQL injection attack type. If the request contains a token sequence that matches the sequence from proton.db , this request is considered to be an attack of the corresponding type. Wallarm regularly updates proton.db with token sequences for new attack types and for already described attack types.","title":"Library libproton"},{"location":"about-wallarm-waf/protecting-against-attacks/#library-libdetection","text":"","title":"Library libdetection"},{"location":"about-wallarm-waf/protecting-against-attacks/#libdetection-overview","text":"The libdetection library additionally validates attacks detected by the library libproton as follows: If libdetection confirms the attack signs detected by libproton , the attack is uploaded to the Wallarm Cloud and blocked (if the WAF node is working in the block mode). If libdetection does not confirm the attack signs detected by libproton , the request is considered legitimate, the attack is not uploaded to the Wallarm Cloud and is not blocked (if the WAF node is working in the block mode). Using libdetection ensures the double\u2011detection of attacks and reduces the number of false positives. Attack types validated by the libdetection library Currently, the library libdetection only validates SQL Injection attacks.","title":"libdetection overview"},{"location":"about-wallarm-waf/protecting-against-attacks/#how-libdetection-works","text":"The particular characteristic of libdetection is that it analyzes requests not only for token sequences specific for attack types, but also for context in which the token sequence was sent. The library contains the character strings of different attack type syntaxes (SQL Injection for now). The string is named as the context. Example of the context for the SQL injection attack type: SELECT example FROM table WHERE id= The library conducts the attack syntax analysis for matching the contexts. If the attack does not match the contexts, then the request will not be defined as a malicious request and will not be blocked (if the WAF node is working in the block mode).","title":"How libdetection works"},{"location":"about-wallarm-waf/protecting-against-attacks/#enabling-libdetection","text":"Analyzing requests with the libdetection library is disabled by default. To reduce the number of false positives, we recommend enabling analysis. To enable the analysis: Set the value of the directive wallarm_enable_libdetection to on . The directive can be set inside the http , server , or location block of the NGINX configuration file. Set the value of the directive proxy_request_buffering to on to allow analyzing the request body. The directive can be set inside the http , server , or location block of the NGINX configuration file. Memory consumption increase When analyzing attacks using the libdetection library, the amount of memory consumed by NGINX and Wallarm processes may increase by about 10%.","title":"Enabling libdetection"},{"location":"about-wallarm-waf/protecting-against-attacks/#testing-libdetection","text":"To check the operation of libdetection , you can send the following legitimate request to the protected resource: curl \"http://localhost/?id=1' UNION SELECT\" The library libproton will detect UNION SELECT as the SQL Injection attack sign. Since UNION SELECT without other commands is not a sign of the SQL Injection attack, libproton detects a false positive. If analyzing of requests with the libdetection library is enabled, the SQL Injection attack sign will not be confirmed in the request. The request will be considered legitimate, the attack will not be uploaded to the Wallarm Cloud and will not be blocked (if the WAF node is working in the block mode).","title":"Testing libdetection"},{"location":"about-wallarm-waf/protecting-against-attacks/#custom-detection-rules","text":"Wallarm clients can set custom detection rules based on protected application specificities. There are the following types of custom detection rules: Tag requests as a brute-force or forced browsing attack Create a virtual patch Define a request as an attack based on a regular expression Disable attack detection by the regular expressions Ignore tokens disables detection of specified attack signs ( tokens ) in certain requests. This rule is created automatically when adding a false positive marks and is not displayed in the Wallarm Console. Custom detection rules and other rules are compiled into Local Objective Model (LOM) and applied along with the standard rules from proton.db when analyzing requests. More details on LOM building \u2192","title":"Custom detection rules"},{"location":"about-wallarm-waf/protecting-against-attacks/#monitoring-and-blocking-attacks","text":"Wallarm WAF can process attacks in the following modes: Monitoring mode: detects attacks and displays information about attacks in the Wallarm Console. Blocking mode: detects, blocks attacks and displays information about attacks in the Wallarm Console. Wallarm WAF ensures quality request analysis and low level of false positives. However each protected application has its own specificities, so we recommend analyzing the work of the Wallarm WAF in the monitoring mode before enabling the blocking mode. To control the filtration mode, the directive wallarm_mode is used. More detailed information about filtration mode configuration is available within the link . The filtration mode for behavioral attacks is configured separately via the particular trigger .","title":"Monitoring and blocking attacks"},{"location":"about-wallarm-waf/protecting-against-attacks/#false-positives","text":"False positive occurs when attack signs are detected in the legitimate request or when legitimate entity is qualified as a vulnerability. More details on false positives in vulnerability scanning \u2192 When analyzing requests for attacks, Wallarm WAF uses the standard rule set that provides optimal application protection with ultra\u2011low false positives. Due to protected application specificities, standard rules may mistakenly recognize attack signs in legitimate requests. For example: SQL injection attack may be detected in the request adding a post with malicious SQL query description to the Database Administrator Forum. In such cases, standard rules need to be adjusted to accommodate protected application specificities by disabling detection of certain attack signs in the requests with certain address, parameters or other elements. These rules has the action type Ignore tokens . The rule Ignore tokens is created automatically if an attack or a hit is marked as a false positive in the Wallarm Console. More details on managing false positives via the Wallarm Console \u2192 Identifying and handling false positives is a part of fine\u2011uning Wallarm WAF to protect your applications. We recommend to deploy the first WAF node in the monitoring mode and analyze detected attacks. If some attacks are mistakenly recognized as attacks, mark them as false positives and switch the WAF node to blocking mode.","title":"False positives"},{"location":"about-wallarm-waf/protecting-against-attacks/#managing-detected-attacks","text":"All detected attacks are displayed in the Wallarm Console \u2192 Events section by the filter attacks . You can manage attacks through the interface as follows: View and analyze attacks Increase the priority of an attack in the recheck queue Mark attacks or separate hits as false positives Create the rules for custom processing of separate hits For more information on managing attacks, see the instructions on working with attacks .","title":"Managing detected attacks"},{"location":"about-wallarm-waf/protecting-against-attacks/#demo-videos","text":"","title":"Demo videos"},{"location":"about-wallarm-waf/shared-responsibility/","text":"Security Model of Shared Responsibility for Customer Data \u00b6 Wallarm WAF relies on a shared responsibility security model. In this model, all parties (Wallarm and its clients) have different areas of responsibilities when it comes to the security of clients' data, including any Personally Identifiable Information (PII) and Cardholder Data. Wallarm WAF is a hybrid solution (part software and part SaaS) with two major components in different areas of responsibilities: Wallarm WAF filter node software, deployed in your infrastructure and managed by you. The WAF node component is responsible for filtering end user requests, sending safe requests to your application and blocking malicious requests. The WAF node passes the traffic and makes the decision locally whether a request is malicious or not. The traffic IS NOT mirrored to the Wallarm cloud for analysis. Wallarm cloud , a cloud component managed by Wallarm, is responsible for receiving meta-information about processed requests and detected attacks from the WAF nodes; as well as generating application-specific WAF rules and making them available for the nodes to download. The console UI and public API provide you with the ability to see security reports and individual events; manage WAF rules, console users, external integrations, etc. Wallarm Responsibilities \u00b6 Wallarm is responsible for the following points: The security and availability of Wallarm cloud environments, the security of Wallarm WAF node code and internal Wallarm systems. This includes, but is not limited to: server-level patching, operating the necessary services to deliver Wallarm cloud service, vulnerability testing, security event logging and monitoring, incident management, operational monitoring and 24/7 support. Wallarm is also responsible for managing server and perimeter firewall configurations (security groups) of Wallarm cloud environments. Updating the Wallarm WAF node component on a periodic basis. Please note that the application of these updates is the responsibility of the client. Providing you with a copy of the latest Wallarm SOC 2 Type II audit report if requested. Client Responsibilities \u00b6 Wallarm clients are responsible for the following points: Implementing sound and consistent internal controls regarding general IT system access and system usage appropriateness for all internal components associated with Wallarm, including Wallarm WAF node and Wallarm cloud. Practicing removal of user accounts for any users who have been terminated and were previously involved in any material functions or activities associated with Wallarm\u2019s services. Configuring proper WAF data masking rules for any sensitive data which may leave the client\u2019s security perimeter and is sent to the Wallarm cloud as a part of reporting of detected malicious requests. Ensuring that transactions for client organizations relating to Wallarm\u2019s services are appropriately authorized, and transactions are secure, timely, and complete. Notifying Wallarm in a timely manner of any changes to personnel directly involved with services performed by Wallarm. This personnel may be involved in financial, technical, or ancillary administrative functions directly associated with services provided by Wallarm. Updating WAF nodes with new software updates released by Wallarm in a timely manner. Developing, and if necessary, implementing a business continuity and disaster recovery plan (BCDRP) that will aid in the continuation of services provided by Wallarm. Masking of Sensitive Data \u00b6 As with any third-party service, it\u2019s important for a Wallarm client to understand what client data is sent to Wallarm, and be assured that sensitive data will never reach Wallarm Cloud. Wallarm clients with PCI DSS, GDPR and other requirements are recommended to mask sensitive data using special WAF rules. The only data transmitted from WAF nodes to the Wallarm Cloud that may include any sensitive details is information about detected malicious requests. It\u2019s highly unlikely that a malicious request would contain any sensitive data. However, the recommended approach is mask HTTP request fields which may contain PII or credit card details, such as token , password , api_key , email , cc_number , etc. Using this approach will guarantee that specified information fields will never leave your security perimeter. You can apply a special WAF rule called Mask sensitive data to specify what fields (in the request URI, headers or body) should be omitted when sending attack information from a WAF node to the Wallarm Cloud. For additional information about masking the data, please see the document or contact Wallarm support team .","title":"Security Model of Shared Responsibility for Customer Data"},{"location":"about-wallarm-waf/shared-responsibility/#security-model-of-shared-responsibility-for-customer-data","text":"Wallarm WAF relies on a shared responsibility security model. In this model, all parties (Wallarm and its clients) have different areas of responsibilities when it comes to the security of clients' data, including any Personally Identifiable Information (PII) and Cardholder Data. Wallarm WAF is a hybrid solution (part software and part SaaS) with two major components in different areas of responsibilities: Wallarm WAF filter node software, deployed in your infrastructure and managed by you. The WAF node component is responsible for filtering end user requests, sending safe requests to your application and blocking malicious requests. The WAF node passes the traffic and makes the decision locally whether a request is malicious or not. The traffic IS NOT mirrored to the Wallarm cloud for analysis. Wallarm cloud , a cloud component managed by Wallarm, is responsible for receiving meta-information about processed requests and detected attacks from the WAF nodes; as well as generating application-specific WAF rules and making them available for the nodes to download. The console UI and public API provide you with the ability to see security reports and individual events; manage WAF rules, console users, external integrations, etc.","title":"Security Model of Shared Responsibility for Customer Data"},{"location":"about-wallarm-waf/shared-responsibility/#wallarm-responsibilities","text":"Wallarm is responsible for the following points: The security and availability of Wallarm cloud environments, the security of Wallarm WAF node code and internal Wallarm systems. This includes, but is not limited to: server-level patching, operating the necessary services to deliver Wallarm cloud service, vulnerability testing, security event logging and monitoring, incident management, operational monitoring and 24/7 support. Wallarm is also responsible for managing server and perimeter firewall configurations (security groups) of Wallarm cloud environments. Updating the Wallarm WAF node component on a periodic basis. Please note that the application of these updates is the responsibility of the client. Providing you with a copy of the latest Wallarm SOC 2 Type II audit report if requested.","title":"Wallarm Responsibilities"},{"location":"about-wallarm-waf/shared-responsibility/#client-responsibilities","text":"Wallarm clients are responsible for the following points: Implementing sound and consistent internal controls regarding general IT system access and system usage appropriateness for all internal components associated with Wallarm, including Wallarm WAF node and Wallarm cloud. Practicing removal of user accounts for any users who have been terminated and were previously involved in any material functions or activities associated with Wallarm\u2019s services. Configuring proper WAF data masking rules for any sensitive data which may leave the client\u2019s security perimeter and is sent to the Wallarm cloud as a part of reporting of detected malicious requests. Ensuring that transactions for client organizations relating to Wallarm\u2019s services are appropriately authorized, and transactions are secure, timely, and complete. Notifying Wallarm in a timely manner of any changes to personnel directly involved with services performed by Wallarm. This personnel may be involved in financial, technical, or ancillary administrative functions directly associated with services provided by Wallarm. Updating WAF nodes with new software updates released by Wallarm in a timely manner. Developing, and if necessary, implementing a business continuity and disaster recovery plan (BCDRP) that will aid in the continuation of services provided by Wallarm.","title":"Client Responsibilities"},{"location":"about-wallarm-waf/shared-responsibility/#masking-of-sensitive-data","text":"As with any third-party service, it\u2019s important for a Wallarm client to understand what client data is sent to Wallarm, and be assured that sensitive data will never reach Wallarm Cloud. Wallarm clients with PCI DSS, GDPR and other requirements are recommended to mask sensitive data using special WAF rules. The only data transmitted from WAF nodes to the Wallarm Cloud that may include any sensitive details is information about detected malicious requests. It\u2019s highly unlikely that a malicious request would contain any sensitive data. However, the recommended approach is mask HTTP request fields which may contain PII or credit card details, such as token , password , api_key , email , cc_number , etc. Using this approach will guarantee that specified information fields will never leave your security perimeter. You can apply a special WAF rule called Mask sensitive data to specify what fields (in the request URI, headers or body) should be omitted when sending attack information from a WAF node to the Wallarm Cloud. For additional information about masking the data, please see the document or contact Wallarm support team .","title":"Masking of Sensitive Data"},{"location":"about-wallarm-waf/subscription-plans/","text":"Wallarm WAF subscription plans \u00b6 The subscription plan outlines the access conditions to Wallarm WAF: subscription period, the set of available modules, and features. This document describes the components of the Wallarm WAF subscription plan and how to configure it. Subscription plans \u00b6 The subscription plan includes a set of modules and features. You can select and include any modules listed below into your plan. The set of options in the plan is defined individually with each client. Modules \u00b6 The set of all Wallarm WAF modules is provided below. Modules can be added to any subscription plan additionally, so the set of modules is not fixed within the subscription plan. WAF continuously analyzes HTTP and HTTPS traffic and blocks malicious requests. Traffic analysis is performed with the DPI (Deep packet inspection) technology, and the decision to block a request is made in real time. Brute-force protection automatically blacklists IP addresses from which brute-force attacks are sent. Active threat verification detects open application vulnerabilities that could be exploited during an attack. For this, the module automatically replays attacks from real traffic processed by the WAF node and looks for vulnerabilities in the corresponding parts of the application. Rules configuration allows you to manually add request processing rules: block malicious requests if the WAF node is working in the monitoring mode or if any known attack vector is not detected in the malicious request / detect the attack based on the specified regular expression / cut out sensitive information such as passwords or cookies from the uploading to the Wallarm Cloud / enable and disable the blocking of requests to various parts of a web application. Scope scans the company's network perimeter: discovering new domains, IP addresses, services, and notification of new objects. Vulnerability scanner detects common types of vulnerabilities in the application in accordance with the OWASP Top 10 recommendations. The list of vulnerabilities that can be detected is available at this link . Features \u00b6 The set of features included to the subscription plan is defined individually with each client. Examples of features include: Limit for requests processed per month Multi-tenant system Logging events Integration with SIEM / SOAR / DevOps systems Authentication in Wallarm Console with SAML SSO Receiving security reports etc To define features that should be included to your subscription plan, please send the request to sales@wallarm.com . Trial period \u00b6 When a new user is registered in the Wallarm Console, a new client account with an active trial period is automatically created in the Wallarm system. The trial period is free. The trial period lasts 14 days. Wallarm WAF trial provides the maximum set of modules and features that can be included in a paid subscription to Wallarm WAF. The trial period can be extended for 14 days more only once. The trial period can be extended in the Wallarm Console \u2192 Settings \u2192 Subscriptions section and via the button from the email notifying about the end of the trial period. The email is sent only to users with the role Administrator and Global Administrator . If the trial period expired, the client account is blocked. When a paid subscription to Wallarm WAF is activated, access to the client account is restored for all users. Information about the trial period is displayed in the Wallarm Console \u2192 Settings \u2192 Subscriptions . Subscription management \u00b6 To activate, cancel, or change a subscription, please send a request to sales@wallarm.com . Information about active subscription is displayed in Wallarm Console \u2192 Settings \u2192 Subscriptions . Subscription cost is determined based on incoming traffic volume , subscription period, the set of connected modules, and features.","title":"Wallarm subscription plans"},{"location":"about-wallarm-waf/subscription-plans/#wallarm-waf-subscription-plans","text":"The subscription plan outlines the access conditions to Wallarm WAF: subscription period, the set of available modules, and features. This document describes the components of the Wallarm WAF subscription plan and how to configure it.","title":"Wallarm WAF subscription plans"},{"location":"about-wallarm-waf/subscription-plans/#subscription-plans","text":"The subscription plan includes a set of modules and features. You can select and include any modules listed below into your plan. The set of options in the plan is defined individually with each client.","title":"Subscription plans"},{"location":"about-wallarm-waf/subscription-plans/#modules","text":"The set of all Wallarm WAF modules is provided below. Modules can be added to any subscription plan additionally, so the set of modules is not fixed within the subscription plan. WAF continuously analyzes HTTP and HTTPS traffic and blocks malicious requests. Traffic analysis is performed with the DPI (Deep packet inspection) technology, and the decision to block a request is made in real time. Brute-force protection automatically blacklists IP addresses from which brute-force attacks are sent. Active threat verification detects open application vulnerabilities that could be exploited during an attack. For this, the module automatically replays attacks from real traffic processed by the WAF node and looks for vulnerabilities in the corresponding parts of the application. Rules configuration allows you to manually add request processing rules: block malicious requests if the WAF node is working in the monitoring mode or if any known attack vector is not detected in the malicious request / detect the attack based on the specified regular expression / cut out sensitive information such as passwords or cookies from the uploading to the Wallarm Cloud / enable and disable the blocking of requests to various parts of a web application. Scope scans the company's network perimeter: discovering new domains, IP addresses, services, and notification of new objects. Vulnerability scanner detects common types of vulnerabilities in the application in accordance with the OWASP Top 10 recommendations. The list of vulnerabilities that can be detected is available at this link .","title":"Modules"},{"location":"about-wallarm-waf/subscription-plans/#features","text":"The set of features included to the subscription plan is defined individually with each client. Examples of features include: Limit for requests processed per month Multi-tenant system Logging events Integration with SIEM / SOAR / DevOps systems Authentication in Wallarm Console with SAML SSO Receiving security reports etc To define features that should be included to your subscription plan, please send the request to sales@wallarm.com .","title":"Features"},{"location":"about-wallarm-waf/subscription-plans/#trial-period","text":"When a new user is registered in the Wallarm Console, a new client account with an active trial period is automatically created in the Wallarm system. The trial period is free. The trial period lasts 14 days. Wallarm WAF trial provides the maximum set of modules and features that can be included in a paid subscription to Wallarm WAF. The trial period can be extended for 14 days more only once. The trial period can be extended in the Wallarm Console \u2192 Settings \u2192 Subscriptions section and via the button from the email notifying about the end of the trial period. The email is sent only to users with the role Administrator and Global Administrator . If the trial period expired, the client account is blocked. When a paid subscription to Wallarm WAF is activated, access to the client account is restored for all users. Information about the trial period is displayed in the Wallarm Console \u2192 Settings \u2192 Subscriptions .","title":"Trial period"},{"location":"about-wallarm-waf/subscription-plans/#subscription-management","text":"To activate, cancel, or change a subscription, please send a request to sales@wallarm.com . Information about active subscription is displayed in Wallarm Console \u2192 Settings \u2192 Subscriptions . Subscription cost is determined based on incoming traffic volume , subscription period, the set of connected modules, and features.","title":"Subscription management"},{"location":"admin-en/admin-intro-en/","text":"Introduction to the Administrator Guide \u00b6 This guide provides information on the Wallarm configuration options and infrastructure integration. You must have administrator access to perform most of the operations described in this guide. You can set the administrator access on the Settings \u2192 Users tab on the Wallarm portal in the EU or US cloud.","title":"Admin intro en"},{"location":"admin-en/admin-intro-en/#introduction-to-the-administrator-guide","text":"This guide provides information on the Wallarm configuration options and infrastructure integration. You must have administrator access to perform most of the operations described in this guide. You can set the administrator access on the Settings \u2192 Users tab on the Wallarm portal in the EU or US cloud.","title":"Introduction to the Administrator Guide"},{"location":"admin-en/attack-rechecker-best-practices/","text":"Best practices for configuring the Active threat verification feature \u00b6 What is the Active threat verification feature \u00b6 One method Wallarm uses to detect vulnerabilities is Active threat verification . Active threat verification with the main component Attack rechecker lets you turn attackers into penetration testers and discover possible security issues from their activity as they probe your apps/APIs for vulnerabilities. The module Attack rechecker finds possible vulnerabilities by probing application endpoints using real attack data from the traffic. By default Active threat verification is disabled. To enable the module, know how to control the Attack rechecker . How the Active threat verification feature works \u00b6 Based on the initial detected attacks, Attack rechecker creates a lot of new test requests with different payloads attacking the same endpoint. This mechanism allows Wallarm to detect vulnerabilities that could be potentially exploited during attacks. The Attack rechecker process will either confirm that the application is not vulnerable to the specific attack vectors or find actual application security issues. List of vulnerabilities that can be detected by the module The Attack rechecker process uses the following logic to check the protected application for possible Web and API security vulnerabilities: For every group of malicious request (every attack) detected by a Wallarm WAF node and uploaded to the connected Wallarm Cloud, the system analyzes which specific endpoint (URL, request string parameter, JSON attribute, XML field, etc) was attacked and which specific kind of vulnerability (SQLi, RCE, XSS, etc) the attacker was trying to exploit. For example, let's take a look at the following malicious GET request: https://example.com/login?token = IyEvYmluL3NoCg & user = UNION SELECT username, password From the request the system will learn the following details: The attacked URL is https://example.com/login The type of used attack is SQLi (according to the UNION SELECT username, password payload) The attacked GET request parameter is user Additional piece of information provided in the request is the request string parameter token=IyEvYmluL3NoCg (it is probably used by the application to authenticate the user) Using the collected information the Attack rechecker module will create a list of about 100-150 test requests to the originally targeted endpoint but with different types of malicious payloads for the same type of attack (like SQLi). For example: https://example.com/login?token = IyEvYmluL3NoCg & user = 1 ')+WAITFOR+DELAY+' 0 indexpt '+AND+(' wlrm '=' wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 +AND+SLEEP ( 10 ) --+wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 ) ; SELECT+PG_SLEEP ( 10 ) -- https://example.com/login?token = IyEvYmluL3NoCg & user = 1 '+OR+SLEEP(10)+AND+' wlrm '=' wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 +AND+1 =( SELECT+1+FROM+PG_SLEEP ( 10 )) https://example.com/login?token = IyEvYmluL3NoCg & user = %23 '%23\\x22%0a-sleep(10)%23 https://example.com/login?token=IyEvYmluL3NoCg&user=1' ; +WAITFOR+DELAY+ '0code:10' -- https://example.com/login?token = IyEvYmluL3NoCg & user = 1 %27%29+OR+SLEEP%280%29+AND+%28%27wlrm%27%3D%27wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = SLEEP ( 10 ) /* 'XOR(SLEEP(10))OR' | \\x 22XOR ( SLEEP ( 10 )) OR \\x 22 */ The Attack rechecker module will send generated test requests to the application bypassing the WAF protection (using the [white-listing feature][whitelist-scanner-addresses]) and verify that the application at the specific endpoint is not vulnerable to the specific attack type. If Attack rechecker suspects that the application has an actual security vulnerability, it will create an event with type incident . User-Agent HTTPS header value in Attack rechecker requests The User-Agent HTTP header in Attack rechecker requests will have the value Wallarm attack-rechecker (v1.x) . Detected security incidents are reported in the Wallarm Console and are able to be dispatched to your security team via available third-party Integrations and Triggers . Potential risks from the Attack rechecker activity \u00b6 In rare cases when a legitimate request is detected by the WAF as an attack, the request will be replayed by Attack rechecker . If the request is not idempotent (for example, an authenticated request creating a new object in the application), then the Attack rechecker requests may create many new unwanted objects in the user account or perform other unexpected operations. To minimize the risk of the described situation, Attack rechecker will automatically strip the following HTTP headers from the replayed requests: Cookie Authorization: Basic Viewstate In cases when the application uses a non-standard authentication method or does not require authenticating the requests, Attack rechecker may replay any request from the traffic and harm the system. For example: repeat 100 and more money transactions or orders. To minimize the risk of the described situation, it is recommended to use testing or staging environments for attack replaying and mask non-standard request authentication parameters . Attack rechecker configuration best practices \u00b6 Configure proper data masking rules \u00b6 If your application uses non-standard types of authentication (for example, request string token or custom HTTP request header or JSON attribute in POST body), then you should configure a proper data masking rule to prevent the WAF nodes from sending the information to the Wallarm Cloud. In this case, the replayed Attack rechecker requests will be not authorized by the application and not cause any harm to the system. Know how to control the Attack rechecker \u00b6 The global on/off switch of the Attack rechecker module is located in the Wallarm Console \u2192 Scanner section . By default this module is disabled. Configure proper notification and escalation rules for detected security incidents \u00b6 Wallarm provides integrations with third-party messaging and incident management services like Slack, Telegram, PagerDuty, Opsgenie and others. It is highly recommended to configure your Wallarm Cloud instance to use the integrations to dispatch notifications about discovered security incidents to your information security team. Know how to handle potential leaks of sensitive data from WAF nodes to the Wallarm Cloud \u00b6 If you discover that your WAF nodes have dispatched some detected false positive requests with included sensitive information such as authentication tokens or username/password credentials to the Wallarm Cloud, you can ask the Wallarm technical support to delete the requests from the Wallarm Cloud storage. Also, you can configure proper data masking rules . It is not possible to modify already stored data. Optional: Enable/disable Attack rechecker tests for specific applications, domains or URLs \u00b6 If some application endpoints are not idempotent and don\u2019t use any request authentication mechanism (for example, the self-registration of a new customer account) it is recommended to disable the Attack rechecker feature for the specific endpoints. Wallarm also provides the customers with the ability to control which specific customer applications, domains or URLs should have the Attack rechecker scanner enabled or disabled. For now, the configuration can be managed only via Wallarm technical support team - please feel free to reach out using email support@wallarm.com . Optional: Configure Attack rechecker request rewriting rules (run tests against a copy of the application) \u00b6 If you want to run checks against the copy of the application and completely avoid scanning the production application, then it is possible to create a rule which instructs the Attack rechecker to modify certain elements in replayed attack requests.","title":"Attack rechecker best practices"},{"location":"admin-en/attack-rechecker-best-practices/#best-practices-for-configuring-the-active-threat-verification-feature","text":"","title":"Best practices for configuring the Active threat verification feature"},{"location":"admin-en/attack-rechecker-best-practices/#what-is-the-active-threat-verification-feature","text":"One method Wallarm uses to detect vulnerabilities is Active threat verification . Active threat verification with the main component Attack rechecker lets you turn attackers into penetration testers and discover possible security issues from their activity as they probe your apps/APIs for vulnerabilities. The module Attack rechecker finds possible vulnerabilities by probing application endpoints using real attack data from the traffic. By default Active threat verification is disabled. To enable the module, know how to control the Attack rechecker .","title":"What is the Active threat verification feature"},{"location":"admin-en/attack-rechecker-best-practices/#how-the-active-threat-verification-feature-works","text":"Based on the initial detected attacks, Attack rechecker creates a lot of new test requests with different payloads attacking the same endpoint. This mechanism allows Wallarm to detect vulnerabilities that could be potentially exploited during attacks. The Attack rechecker process will either confirm that the application is not vulnerable to the specific attack vectors or find actual application security issues. List of vulnerabilities that can be detected by the module The Attack rechecker process uses the following logic to check the protected application for possible Web and API security vulnerabilities: For every group of malicious request (every attack) detected by a Wallarm WAF node and uploaded to the connected Wallarm Cloud, the system analyzes which specific endpoint (URL, request string parameter, JSON attribute, XML field, etc) was attacked and which specific kind of vulnerability (SQLi, RCE, XSS, etc) the attacker was trying to exploit. For example, let's take a look at the following malicious GET request: https://example.com/login?token = IyEvYmluL3NoCg & user = UNION SELECT username, password From the request the system will learn the following details: The attacked URL is https://example.com/login The type of used attack is SQLi (according to the UNION SELECT username, password payload) The attacked GET request parameter is user Additional piece of information provided in the request is the request string parameter token=IyEvYmluL3NoCg (it is probably used by the application to authenticate the user) Using the collected information the Attack rechecker module will create a list of about 100-150 test requests to the originally targeted endpoint but with different types of malicious payloads for the same type of attack (like SQLi). For example: https://example.com/login?token = IyEvYmluL3NoCg & user = 1 ')+WAITFOR+DELAY+' 0 indexpt '+AND+(' wlrm '=' wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 +AND+SLEEP ( 10 ) --+wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 ) ; SELECT+PG_SLEEP ( 10 ) -- https://example.com/login?token = IyEvYmluL3NoCg & user = 1 '+OR+SLEEP(10)+AND+' wlrm '=' wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = 1 +AND+1 =( SELECT+1+FROM+PG_SLEEP ( 10 )) https://example.com/login?token = IyEvYmluL3NoCg & user = %23 '%23\\x22%0a-sleep(10)%23 https://example.com/login?token=IyEvYmluL3NoCg&user=1' ; +WAITFOR+DELAY+ '0code:10' -- https://example.com/login?token = IyEvYmluL3NoCg & user = 1 %27%29+OR+SLEEP%280%29+AND+%28%27wlrm%27%3D%27wlrm https://example.com/login?token = IyEvYmluL3NoCg & user = SLEEP ( 10 ) /* 'XOR(SLEEP(10))OR' | \\x 22XOR ( SLEEP ( 10 )) OR \\x 22 */ The Attack rechecker module will send generated test requests to the application bypassing the WAF protection (using the [white-listing feature][whitelist-scanner-addresses]) and verify that the application at the specific endpoint is not vulnerable to the specific attack type. If Attack rechecker suspects that the application has an actual security vulnerability, it will create an event with type incident . User-Agent HTTPS header value in Attack rechecker requests The User-Agent HTTP header in Attack rechecker requests will have the value Wallarm attack-rechecker (v1.x) . Detected security incidents are reported in the Wallarm Console and are able to be dispatched to your security team via available third-party Integrations and Triggers .","title":"How the Active threat verification feature works"},{"location":"admin-en/attack-rechecker-best-practices/#potential-risks-from-the-attack-rechecker-activity","text":"In rare cases when a legitimate request is detected by the WAF as an attack, the request will be replayed by Attack rechecker . If the request is not idempotent (for example, an authenticated request creating a new object in the application), then the Attack rechecker requests may create many new unwanted objects in the user account or perform other unexpected operations. To minimize the risk of the described situation, Attack rechecker will automatically strip the following HTTP headers from the replayed requests: Cookie Authorization: Basic Viewstate In cases when the application uses a non-standard authentication method or does not require authenticating the requests, Attack rechecker may replay any request from the traffic and harm the system. For example: repeat 100 and more money transactions or orders. To minimize the risk of the described situation, it is recommended to use testing or staging environments for attack replaying and mask non-standard request authentication parameters .","title":"Potential risks from the Attack rechecker activity"},{"location":"admin-en/attack-rechecker-best-practices/#attack-rechecker-configuration-best-practices","text":"","title":"Attack rechecker configuration best practices"},{"location":"admin-en/attack-rechecker-best-practices/#configure-proper-data-masking-rules","text":"If your application uses non-standard types of authentication (for example, request string token or custom HTTP request header or JSON attribute in POST body), then you should configure a proper data masking rule to prevent the WAF nodes from sending the information to the Wallarm Cloud. In this case, the replayed Attack rechecker requests will be not authorized by the application and not cause any harm to the system.","title":"Configure proper data masking rules"},{"location":"admin-en/attack-rechecker-best-practices/#know-how-to-control-the-attack-rechecker","text":"The global on/off switch of the Attack rechecker module is located in the Wallarm Console \u2192 Scanner section . By default this module is disabled.","title":"Know how to control the Attack rechecker"},{"location":"admin-en/attack-rechecker-best-practices/#configure-proper-notification-and-escalation-rules-for-detected-security-incidents","text":"Wallarm provides integrations with third-party messaging and incident management services like Slack, Telegram, PagerDuty, Opsgenie and others. It is highly recommended to configure your Wallarm Cloud instance to use the integrations to dispatch notifications about discovered security incidents to your information security team.","title":"Configure proper notification and escalation rules for detected security incidents"},{"location":"admin-en/attack-rechecker-best-practices/#know-how-to-handle-potential-leaks-of-sensitive-data-from-waf-nodes-to-the-wallarm-cloud","text":"If you discover that your WAF nodes have dispatched some detected false positive requests with included sensitive information such as authentication tokens or username/password credentials to the Wallarm Cloud, you can ask the Wallarm technical support to delete the requests from the Wallarm Cloud storage. Also, you can configure proper data masking rules . It is not possible to modify already stored data.","title":"Know how to handle potential leaks of sensitive data from WAF nodes to the Wallarm Cloud"},{"location":"admin-en/attack-rechecker-best-practices/#optional-enabledisable-attack-rechecker-tests-for-specific-applications-domains-or-urls","text":"If some application endpoints are not idempotent and don\u2019t use any request authentication mechanism (for example, the self-registration of a new customer account) it is recommended to disable the Attack rechecker feature for the specific endpoints. Wallarm also provides the customers with the ability to control which specific customer applications, domains or URLs should have the Attack rechecker scanner enabled or disabled. For now, the configuration can be managed only via Wallarm technical support team - please feel free to reach out using email support@wallarm.com .","title":"Optional: Enable/disable Attack rechecker tests for specific applications, domains or URLs"},{"location":"admin-en/attack-rechecker-best-practices/#optional-configure-attack-rechecker-request-rewriting-rules-run-tests-against-a-copy-of-the-application","text":"If you want to run checks against the copy of the application and completely avoid scanning the production application, then it is possible to create a rule which instructs the Attack rechecker to modify certain elements in replayed attack requests.","title":"Optional: Configure Attack rechecker request rewriting rules (run tests against a copy of the application)"},{"location":"admin-en/config-files/","text":"WAF node configuration files \u00b6 DEB and RPM packages: /etc/nginx/conf.d/default.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings ... Docker images: /etc/nginx/sites-enabled/default","title":"Configuration files"},{"location":"admin-en/config-files/#waf-node-configuration-files","text":"DEB and RPM packages: /etc/nginx/conf.d/default.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings ... Docker images: /etc/nginx/sites-enabled/default","title":"WAF node configuration files"},{"location":"admin-en/configure-backup-en/","text":"Configuring a Failover Method \u00b6 Deploying a filter node as a reverse proxy requires that the filter node is highly available. The filter node failure, for example due to power outage, limits the web application's operation. To ensure the high availability of Wallarm, you are recommended to use one of the failover methods described in this section. A failover method introduces additional nodes to which the traffic is automatically forwarded if the main filter node fails. Data Center Failover \u00b6 If the web application and filter nodes are in a data center, use the data center's \"Failover IP\" service VRRP or CARP \u00b6 On each filter node, start a keepalived or ucarp daemon that monitors the availability of the nodes and starts forwarding traffic if the nodes go down. This is a standard high availability method that can also be used for traffic load balancing by starting a failover\u2011IP on each node and distributing the traffic with DNS balancing. Working with NGINX Plus Wallarm can be set up to work on NGINX Plus with a custom VRRP wrapper. Most of the Linux distributions, including CentOS and Debian, have custom packages that can install this build. To find out about installation of Wallarm with NGINX Plus, see detailed instructions on the \u00abInstalling with NGINX Plus\u00bb page. Hardware L3 or L4 Load Balancer \u00b6 A layer 3 or layer 4 load balancer is a good high availability solution. DNS Load Balancing \u00b6 Specify several IP addresses in the DNS settings. While this method targets load balancing, you may also find it useful as high availability method.","title":"Configuring a Failover Method"},{"location":"admin-en/configure-backup-en/#configuring-a-failover-method","text":"Deploying a filter node as a reverse proxy requires that the filter node is highly available. The filter node failure, for example due to power outage, limits the web application's operation. To ensure the high availability of Wallarm, you are recommended to use one of the failover methods described in this section. A failover method introduces additional nodes to which the traffic is automatically forwarded if the main filter node fails.","title":"Configuring a Failover Method"},{"location":"admin-en/configure-backup-en/#data-center-failover","text":"If the web application and filter nodes are in a data center, use the data center's \"Failover IP\" service","title":"Data Center Failover"},{"location":"admin-en/configure-backup-en/#vrrp-or-carp","text":"On each filter node, start a keepalived or ucarp daemon that monitors the availability of the nodes and starts forwarding traffic if the nodes go down. This is a standard high availability method that can also be used for traffic load balancing by starting a failover\u2011IP on each node and distributing the traffic with DNS balancing. Working with NGINX Plus Wallarm can be set up to work on NGINX Plus with a custom VRRP wrapper. Most of the Linux distributions, including CentOS and Debian, have custom packages that can install this build. To find out about installation of Wallarm with NGINX Plus, see detailed instructions on the \u00abInstalling with NGINX Plus\u00bb page.","title":"VRRP or CARP"},{"location":"admin-en/configure-backup-en/#hardware-l3-or-l4-load-balancer","text":"A layer 3 or layer 4 load balancer is a good high availability solution.","title":"Hardware L3 or L4 Load Balancer"},{"location":"admin-en/configure-backup-en/#dns-load-balancing","text":"Specify several IP addresses in the DNS settings. While this method targets load balancing, you may also find it useful as high availability method.","title":"DNS Load Balancing"},{"location":"admin-en/configure-cloud-node-synchronization-en/","text":"WAF node and Wallarm Cloud synchronization configuration \u00b6 The WAF node regularly synchronizes with the Wallarm Cloud to: Get updates for traffic processing rules (LOM) Get updates of proton.db Send data on detected attacks and vulnerabilities Send metrics for processed traffic These instructions describe parameters and methods used to configure WAF node and Wallarm Cloud synchronization. The set of parameters and the method of its configuration depend on the deployed WAF node type: Cloud WAF node created by the addcloudnode script Regular WAF node created by the addnode script Cloud WAF node and Wallarm Cloud synchronization \u00b6 The /etc/wallarm/syncnode file contains environment variables that define the way the cloud WAF node will synchronize with the Wallarm Cloud. The /etc/wallarm/syncnode file containing the variable WALLARM_API_TOKEN with the cloud WAF node token is created after running the addcloudnode script. The wallarm-synccloud service applies the changes made to the /etc/wallarm/syncnode file to the synchronization process and runs the synchronization with the new configuration. Available environment variables \u00b6 The list of environment variables available for the cloud WAF node and Wallarm Cloud synchronization configuration is provided below. To get the list of available environment variables, you can also run the following command: /usr/share/wallarm-common/synccloud --help Variable Description WALLARM_API_HOST Wallarm API endpoint. Can be: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud Default value is api.wallarm.com . This variable is required to be set in the file /etc/wallarm/syncnode . WALLARM_API_PORT Wallarm API port. Default value is 444 . WALLARM_API_TOKEN Cloud WAF node token to access the Wallarm API. WALLARM_API_CA_VERIFY Whether to enable/disable Wallarm API server certificate verification. Can be: true , yes , and 1 to enable verification Any other value to disable verification Default value is yes . WALLARM_API_CA_PATH Path to the Wallarm API certificate authority file. WALLARM_SYNCNODE Whether to enable/disable WAF node\u2011specific data synchronization. If the synchronization is enabled, the files for the cloud WAF node operation (such as LOM file ) will be periodically downloaded from the Cloud. If the synchronization is disabled, the files for the cloud WAF node operation will not be downloaded. Can be: true , yes , and 1 to enable synchronization Any other value to disable synchronization Default value is yes . WALLARM_SYNCNODE_INTERVAL Interval between WAF node and Wallarm Cloud synchronizations in seconds. The value cannot be less than the default value. Default value is 120 . WALLARM_SYNCNODE_RAND_DELAY Synchronization delay jitter in seconds. Default value is 120 . WALLARM_SYNCNODE_TIMEOUT Synchronization duration limit. This limit allows interrupting the synchronization if any issues occur during the process of downloading the files for the cloud WAF node operation. For example, such issues can be caused by network outages. Default value is 900 . WALLARM_SYNCNODE_OWNER Owner for the files needed for the cloud WAF node operation. Default value is root . WALLARM_SYNCNODE_GROUP Group for the files needed for the cloud WAF node operation. Default value is wallarm . WALLARM_SYNCNODE_MODE Access rights to the files needed for the cloud WAF node operation. Default value is 0640 . Configuration of the access rights to the files needed for the cloud WAF node operation Make sure that after configuring access rights using the WALLARM_SYNCNODE_OWNER , WALLARM_SYNCNODE_GROUP , and WALLARM_SYNCNODE_MODE variables, the wallarm-worker and nginx services can read content of the files needed for the cloud WAF node operation. Configuring synchronization parameters \u00b6 To change synchronization parameters, proceed with the following steps: Make changes to the /etc/wallarm/syncnode file by adding the required environment variables and assigning the desired values to them. The valid /etc/wallarm/syncnode contents: WALLARM_API_TOKEN = K85iHWi0SXRxJTb+xxxxxxxxxxxxxxxxxxxxfiwo9twr9I5/+sjZ9v2UlRRgwwMD WALLARM_SYNCNODE_INTERVAL = 800 WALLARM_SYNCNODE_TIMEOUT = 600 Restart the wallarm-synccloud service to apply updated settings to the synchronization process: sudo /bin/systemctl restart wallarm-synccloud The service will apply the values assigned to the environment variables in the /etc/wallarm/syncnode file as new parameters for the cloud WAF node and Wallarm Cloud synchronization. After the command execution, the WAF node will be performing the synchronization procedure according to the new parameters. Regular WAF node and Wallarm Cloud synchronization \u00b6 Configuration of the regular WAF node and Wallarm Cloud synchronization is set in the following way: Credentials to access the Wallarm Cloud are set in the node.yaml file. The node.yaml file containing the regular WAF node name and UUID, and secret key to access Wallarm API is created after running the addnode script. Default path to the file is /etc/wallarm/node.yaml . This path can be changed via the wallarm_api_conf directive. Interval between WAF node and Wallarm Cloud synchronizations is set via the system environment variable WALLARM_SYNCNODE_INTERVAL . Variable value should be set in the /etc/environment file. Default variable value is 120 seconds. Credentials to access the Wallarm Cloud \u00b6 The node.yaml file may contain the following parameters for accessing the regular WAF node to the Wallarm Cloud: Parameter Description hostname Regular WAF node name. This variable is required to be set in the file node.yaml . uuid Regular WAF node UUID. This variable is required to be set in the file node.yaml . secret Secret key to access the Wallarm API. This variable is required to be set in the file node.yaml . api.host Wallarm API endpoint. Can be: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud Default value is api.wallarm.com . api.port Wallarm API port. Default value is 444 . api.ca_verify Whether to enable/disable Wallarm API server certificate verification. Can be: true to enable verification false to disable verification Default value is true . api.local_host Local IP address of the network interface through which requests to Wallarm API are sent. This parameter is required if the network interface used by default restricts access to Wallarm API (for example, access to the Internet may be closed). api.local_port Port of the network interface through which requests to Wallarm API are sent. This parameter is required if the network interface used by default restricts access to Wallarm API (for example, access to the Internet may be closed). syncnode.owner Owner for the files needed for the regular WAF node operation. Default value is root . syncnode.group Group for the files needed for the regular WAF node operation. Default value is wallarm . syncnode.mode Access rights to the files needed for the regular WAF node operation. Default value is 0640 . To change synchronization parameters, proceed with the following steps: Make changes to the node.yaml file by adding the required parameters and assigning the desired values to them. The valid node.yaml contents: hostname: example-node-name uuid: ea1xa0xe-xxxx-42a0-xxxx-b1b446xxxxxx secret: b827axxxxxxxxxxxcbe45c855c71389a2a5564920xxxxxxxxxxxxxxxxxxc4613260 api: host: api.wallarm.com port: 444 ca_verify: true syncnode: owner: root group: wallarm mode: 0640 Restart NGINX to apply updated settings to the synchronization process: Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx Interval between WAF node and Wallarm Cloud synchronizations \u00b6 By default, the WAF node synchronizes with the Wallarm Cloud every 120\u2011240 seconds (2\u20114 minutes). You can change the synchronization interval via the system environment variable WALLARM_SYNCNODE_INTERVAL . To change the interval between regular WAF node and Wallarm Cloud synchronizations: Open the file /etc/environment . Add the WALLARM_SYNCNODE_INTERVAL variable to the file and set a desired value to the variable in seconds. The value cannot be less than the default value ( 120 seconds). For example: WALLARM_SYNCNODE_INTERVAL = 800 Save the changed file /etc/environment . New interval value will be applied to the synchronization process automatically.","title":"Node and Wallarm Cloud synchronization configuration"},{"location":"admin-en/configure-cloud-node-synchronization-en/#waf-node-and-wallarm-cloud-synchronization-configuration","text":"The WAF node regularly synchronizes with the Wallarm Cloud to: Get updates for traffic processing rules (LOM) Get updates of proton.db Send data on detected attacks and vulnerabilities Send metrics for processed traffic These instructions describe parameters and methods used to configure WAF node and Wallarm Cloud synchronization. The set of parameters and the method of its configuration depend on the deployed WAF node type: Cloud WAF node created by the addcloudnode script Regular WAF node created by the addnode script","title":"WAF node and Wallarm Cloud synchronization configuration"},{"location":"admin-en/configure-cloud-node-synchronization-en/#cloud-waf-node-and-wallarm-cloud-synchronization","text":"The /etc/wallarm/syncnode file contains environment variables that define the way the cloud WAF node will synchronize with the Wallarm Cloud. The /etc/wallarm/syncnode file containing the variable WALLARM_API_TOKEN with the cloud WAF node token is created after running the addcloudnode script. The wallarm-synccloud service applies the changes made to the /etc/wallarm/syncnode file to the synchronization process and runs the synchronization with the new configuration.","title":"Cloud WAF node and Wallarm Cloud synchronization"},{"location":"admin-en/configure-cloud-node-synchronization-en/#available-environment-variables","text":"The list of environment variables available for the cloud WAF node and Wallarm Cloud synchronization configuration is provided below. To get the list of available environment variables, you can also run the following command: /usr/share/wallarm-common/synccloud --help Variable Description WALLARM_API_HOST Wallarm API endpoint. Can be: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud Default value is api.wallarm.com . This variable is required to be set in the file /etc/wallarm/syncnode . WALLARM_API_PORT Wallarm API port. Default value is 444 . WALLARM_API_TOKEN Cloud WAF node token to access the Wallarm API. WALLARM_API_CA_VERIFY Whether to enable/disable Wallarm API server certificate verification. Can be: true , yes , and 1 to enable verification Any other value to disable verification Default value is yes . WALLARM_API_CA_PATH Path to the Wallarm API certificate authority file. WALLARM_SYNCNODE Whether to enable/disable WAF node\u2011specific data synchronization. If the synchronization is enabled, the files for the cloud WAF node operation (such as LOM file ) will be periodically downloaded from the Cloud. If the synchronization is disabled, the files for the cloud WAF node operation will not be downloaded. Can be: true , yes , and 1 to enable synchronization Any other value to disable synchronization Default value is yes . WALLARM_SYNCNODE_INTERVAL Interval between WAF node and Wallarm Cloud synchronizations in seconds. The value cannot be less than the default value. Default value is 120 . WALLARM_SYNCNODE_RAND_DELAY Synchronization delay jitter in seconds. Default value is 120 . WALLARM_SYNCNODE_TIMEOUT Synchronization duration limit. This limit allows interrupting the synchronization if any issues occur during the process of downloading the files for the cloud WAF node operation. For example, such issues can be caused by network outages. Default value is 900 . WALLARM_SYNCNODE_OWNER Owner for the files needed for the cloud WAF node operation. Default value is root . WALLARM_SYNCNODE_GROUP Group for the files needed for the cloud WAF node operation. Default value is wallarm . WALLARM_SYNCNODE_MODE Access rights to the files needed for the cloud WAF node operation. Default value is 0640 . Configuration of the access rights to the files needed for the cloud WAF node operation Make sure that after configuring access rights using the WALLARM_SYNCNODE_OWNER , WALLARM_SYNCNODE_GROUP , and WALLARM_SYNCNODE_MODE variables, the wallarm-worker and nginx services can read content of the files needed for the cloud WAF node operation.","title":"Available environment variables"},{"location":"admin-en/configure-cloud-node-synchronization-en/#configuring-synchronization-parameters","text":"To change synchronization parameters, proceed with the following steps: Make changes to the /etc/wallarm/syncnode file by adding the required environment variables and assigning the desired values to them. The valid /etc/wallarm/syncnode contents: WALLARM_API_TOKEN = K85iHWi0SXRxJTb+xxxxxxxxxxxxxxxxxxxxfiwo9twr9I5/+sjZ9v2UlRRgwwMD WALLARM_SYNCNODE_INTERVAL = 800 WALLARM_SYNCNODE_TIMEOUT = 600 Restart the wallarm-synccloud service to apply updated settings to the synchronization process: sudo /bin/systemctl restart wallarm-synccloud The service will apply the values assigned to the environment variables in the /etc/wallarm/syncnode file as new parameters for the cloud WAF node and Wallarm Cloud synchronization. After the command execution, the WAF node will be performing the synchronization procedure according to the new parameters.","title":"Configuring synchronization parameters"},{"location":"admin-en/configure-cloud-node-synchronization-en/#regular-waf-node-and-wallarm-cloud-synchronization","text":"Configuration of the regular WAF node and Wallarm Cloud synchronization is set in the following way: Credentials to access the Wallarm Cloud are set in the node.yaml file. The node.yaml file containing the regular WAF node name and UUID, and secret key to access Wallarm API is created after running the addnode script. Default path to the file is /etc/wallarm/node.yaml . This path can be changed via the wallarm_api_conf directive. Interval between WAF node and Wallarm Cloud synchronizations is set via the system environment variable WALLARM_SYNCNODE_INTERVAL . Variable value should be set in the /etc/environment file. Default variable value is 120 seconds.","title":"Regular WAF node and Wallarm Cloud synchronization"},{"location":"admin-en/configure-cloud-node-synchronization-en/#credentials-to-access-the-wallarm-cloud","text":"The node.yaml file may contain the following parameters for accessing the regular WAF node to the Wallarm Cloud: Parameter Description hostname Regular WAF node name. This variable is required to be set in the file node.yaml . uuid Regular WAF node UUID. This variable is required to be set in the file node.yaml . secret Secret key to access the Wallarm API. This variable is required to be set in the file node.yaml . api.host Wallarm API endpoint. Can be: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud Default value is api.wallarm.com . api.port Wallarm API port. Default value is 444 . api.ca_verify Whether to enable/disable Wallarm API server certificate verification. Can be: true to enable verification false to disable verification Default value is true . api.local_host Local IP address of the network interface through which requests to Wallarm API are sent. This parameter is required if the network interface used by default restricts access to Wallarm API (for example, access to the Internet may be closed). api.local_port Port of the network interface through which requests to Wallarm API are sent. This parameter is required if the network interface used by default restricts access to Wallarm API (for example, access to the Internet may be closed). syncnode.owner Owner for the files needed for the regular WAF node operation. Default value is root . syncnode.group Group for the files needed for the regular WAF node operation. Default value is wallarm . syncnode.mode Access rights to the files needed for the regular WAF node operation. Default value is 0640 . To change synchronization parameters, proceed with the following steps: Make changes to the node.yaml file by adding the required parameters and assigning the desired values to them. The valid node.yaml contents: hostname: example-node-name uuid: ea1xa0xe-xxxx-42a0-xxxx-b1b446xxxxxx secret: b827axxxxxxxxxxxcbe45c855c71389a2a5564920xxxxxxxxxxxxxxxxxxc4613260 api: host: api.wallarm.com port: 444 ca_verify: true syncnode: owner: root group: wallarm mode: 0640 Restart NGINX to apply updated settings to the synchronization process: Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx","title":"Credentials to access the Wallarm Cloud"},{"location":"admin-en/configure-cloud-node-synchronization-en/#interval-between-waf-node-and-wallarm-cloud-synchronizations","text":"By default, the WAF node synchronizes with the Wallarm Cloud every 120\u2011240 seconds (2\u20114 minutes). You can change the synchronization interval via the system environment variable WALLARM_SYNCNODE_INTERVAL . To change the interval between regular WAF node and Wallarm Cloud synchronizations: Open the file /etc/environment . Add the WALLARM_SYNCNODE_INTERVAL variable to the file and set a desired value to the variable in seconds. The value cannot be less than the default value ( 120 seconds). For example: WALLARM_SYNCNODE_INTERVAL = 800 Save the changed file /etc/environment . New interval value will be applied to the synchronization process automatically.","title":"Interval between WAF node and Wallarm Cloud synchronizations"},{"location":"admin-en/configure-dynamic-dns-resolution-nginx/","text":"Configuring dynamic DNS resolution in NGINX \u00b6 If the domain name is passed in the proxy_pass directive of the NGINX configuration file, then NGINX resolves the IP address of the host only once after the start. If the DNS server changes the IP address of the host, then NGINX will be still using the old IP address until NGINX will be reloaded or restarted. Before that, NGINX will send requests to the wrong IP address. For example: location / { proxy_pass https://demo-app.com ; include proxy_params ; } For dynamic DNS resolution, you can set a proxy_pass directive as the variable. In this case, NGINX will use the DNS address that is set in the resolver directive when calculating the variable. Impact of dynamic DNS resolution on traffic processing NGINX configuration with the resolver directive and variable in the proxy_pass directive slows down request processing since it will be the additional step of dynamic DNS resolution in the request processing. If the DNS server is down, NGINX will not process the traffic. For example: location / { resolver 172 .43.1.2 ; set $backend https://demo-app.com $uri$is_args$args ; proxy_pass $backend ; include proxy_params ; } Dynamic DNS resolution in NGINX Plus NGINX Plus supports a dynamic resolution of domain names by default.","title":"Configuring dynamic DNS resolution in NGINX"},{"location":"admin-en/configure-dynamic-dns-resolution-nginx/#configuring-dynamic-dns-resolution-in-nginx","text":"If the domain name is passed in the proxy_pass directive of the NGINX configuration file, then NGINX resolves the IP address of the host only once after the start. If the DNS server changes the IP address of the host, then NGINX will be still using the old IP address until NGINX will be reloaded or restarted. Before that, NGINX will send requests to the wrong IP address. For example: location / { proxy_pass https://demo-app.com ; include proxy_params ; } For dynamic DNS resolution, you can set a proxy_pass directive as the variable. In this case, NGINX will use the DNS address that is set in the resolver directive when calculating the variable. Impact of dynamic DNS resolution on traffic processing NGINX configuration with the resolver directive and variable in the proxy_pass directive slows down request processing since it will be the additional step of dynamic DNS resolution in the request processing. If the DNS server is down, NGINX will not process the traffic. For example: location / { resolver 172 .43.1.2 ; set $backend https://demo-app.com $uri$is_args$args ; proxy_pass $backend ; include proxy_params ; } Dynamic DNS resolution in NGINX Plus NGINX Plus supports a dynamic resolution of domain names by default.","title":"Configuring dynamic DNS resolution in NGINX"},{"location":"admin-en/configure-kubernetes-en/","text":"Fine\u2011tuning of Wallarm Ingress Controller \u00b6 Official documentation for NGINX Ingress Controller The fine\u2011tuning of Wallarm Ingress Controller is quite similar to that of NGINX Ingress Controller described in the official documentation . When working with Wallarm, all options for setting up the original NGINX Ingress Controller are available. Additional Settings for Helm Chart \u00b6 The settings are performed via the values.yaml file. By default, the file looks as follows: controller: wallarm: enabled: false apiHost: api.wallarm.com apiPort: 444 apiSSL: true token: \"\" tarantool: kind: Deployment service: annotations: {} replicaCount: 1 arena: \"0.2\" livenessProbe: failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: {} metrics: enabled: false service: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: /wallarm-metrics prometheus.io/port: \"18080\" ## List of IP addresses at which the stats-exporter service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \"\" loadBalancerSourceRanges: [] servicePort: 9913 type: ClusterIP synccloud: resources: {} collectd: resources: {} A description of the main parameters you can set up is provided below. Other parameters come with default value and rarely need to be changed; their descriptions are provided at this link . wallarm.enabled \u00b6 Allows you to enable or disable Wallarm functions. Default value : false wallarm.apiHost \u00b6 Wallarm API endpoint. Can be: api.wallarm.com for the EU cloud , us1.api.wallarm.com for the US cloud . Default value : api.wallarm.com wallarm.token \u00b6 The Cloud Node token is created on the Wallarm portal in the EU or US cloud. It is required to access to Wallarm API. Default value : not specified wallarm.tarantool.replicaCount \u00b6 The number of running pods for postanalytics. Postanalytics is used for the behavior\u2011based attack detection. Default value : 1 wallarm.tarantool.arena \u00b6 Specifies the amount of memory allocated for postanalytics service. It is recommended to set up a value sufficient to store requests data for the last 5-15 minutes. Default value : 0.2 wallarm.metrics.enabled \u00b6 This switch toggles information and metrics collection. If Prometheus is installed in the Kubernetes cluster, no additional configuration is required. Default value : false Global Controller Settings \u00b6 Implemented via ConfigMap . Besides the standard ones, the following additional parameters are supported: enable-wallarm - enables the Wallarm module in NGINX wallarm-upstream-connect-attempts wallarm-upstream-reconnect-interval wallarm-process-time-limit wallarm-process-time-limit-block wallarm-request-memory-limit Ingress Annotations \u00b6 These annotations are used for setting up parameters for processing individual instances of Ingress. Besides the standard ones , the following additional annotations are supported: nginx.ingress.kubernetes.io/wallarm-mode , default: off nginx.ingress.kubernetes.io/wallarm-mode-allow-override nginx.ingress.kubernetes.io/wallarm-fallback nginx.ingress.kubernetes.io/wallarm-instance nginx.ingress.kubernetes.io/wallarm-block-page nginx.ingress.kubernetes.io/wallarm-parse-response nginx.ingress.kubernetes.io/wallarm-parse-websocket nginx.ingress.kubernetes.io/wallarm-unpack-response nginx.ingress.kubernetes.io/wallarm-parser-disable Applying annotation to the Ingress resource \u00b6 To apply the settings to your Ingress, please use the following command: kubectl annotate --overwrite ingress YOUR_INGRESS_NAME ANNOTATION_NAME=VALUE YOUR_INGRESS_NAME is the name of your Ingress, ANNOTATION_NAME is the name of the annotation from the list above, VALUE is the value of the annotation from the list above. Annotation examples \u00b6 Configuring the blocking page and error code \u00b6 The annotation nginx.ingress.kubernetes.io/wallarm-block-page is used to configure the blocking page and error code returned in the response to the request blocked for the following reasons: Request contains malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . Request containing malicious payloads from the list above is originated from greylisted IP address and the WAF node filters requests in the safe blocking mode . Request is originated from the blacklisted IP address . For example, to return the default Wallarm blocking page and the error code 445 in the response to any blocked request: kubectl annotate ingress <YOUR_INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = \"&/usr/share/nginx/html/wallarm_blocked.html response_code=445 type=attack,acl_ip,acl_source\" More details on the blocking page and error code configuration methods \u2192 Enabling attack analysis with libdetection \u00b6 The libdetection library additionally validates attacks detected by the library libproton . Using libdetection ensures the double\u2011detection of attacks and reduces the number of false positives. To allow libdetection to parse and check the request body, buffering of a client request body must be enabled ( proxy_request_buffering on ). There are two options to enable attack analysis with libdetection : Applying the following nginx.ingress.kubernetes.io/server-snippet annotation to the Ingress resource: kubectl annotate --overwrite ingress <YOUR_INGRESS_NAME> nginx.ingress.kubernetes.io/server-snippet = \"wallarm_enable_libdetection on; proxy_request_buffering on;\" Adding the following snippet to the config object in values.yaml of the cloned Wallarm Helm chart repository : config: { server-snippet: 'wallarm_enable_libdetection on; proxy_request_buffering on;' }","title":"Inress controller configuration parameters"},{"location":"admin-en/configure-kubernetes-en/#finetuning-of-wallarm-ingress-controller","text":"Official documentation for NGINX Ingress Controller The fine\u2011tuning of Wallarm Ingress Controller is quite similar to that of NGINX Ingress Controller described in the official documentation . When working with Wallarm, all options for setting up the original NGINX Ingress Controller are available.","title":"Fine\u2011tuning of Wallarm Ingress Controller"},{"location":"admin-en/configure-kubernetes-en/#additional-settings-for-helm-chart","text":"The settings are performed via the values.yaml file. By default, the file looks as follows: controller: wallarm: enabled: false apiHost: api.wallarm.com apiPort: 444 apiSSL: true token: \"\" tarantool: kind: Deployment service: annotations: {} replicaCount: 1 arena: \"0.2\" livenessProbe: failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: {} metrics: enabled: false service: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: /wallarm-metrics prometheus.io/port: \"18080\" ## List of IP addresses at which the stats-exporter service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \"\" loadBalancerSourceRanges: [] servicePort: 9913 type: ClusterIP synccloud: resources: {} collectd: resources: {} A description of the main parameters you can set up is provided below. Other parameters come with default value and rarely need to be changed; their descriptions are provided at this link .","title":"Additional Settings for Helm Chart"},{"location":"admin-en/configure-kubernetes-en/#wallarmenabled","text":"Allows you to enable or disable Wallarm functions. Default value : false","title":"wallarm.enabled"},{"location":"admin-en/configure-kubernetes-en/#wallarmapihost","text":"Wallarm API endpoint. Can be: api.wallarm.com for the EU cloud , us1.api.wallarm.com for the US cloud . Default value : api.wallarm.com","title":"wallarm.apiHost"},{"location":"admin-en/configure-kubernetes-en/#wallarmtoken","text":"The Cloud Node token is created on the Wallarm portal in the EU or US cloud. It is required to access to Wallarm API. Default value : not specified","title":"wallarm.token"},{"location":"admin-en/configure-kubernetes-en/#wallarmtarantoolreplicacount","text":"The number of running pods for postanalytics. Postanalytics is used for the behavior\u2011based attack detection. Default value : 1","title":"wallarm.tarantool.replicaCount"},{"location":"admin-en/configure-kubernetes-en/#wallarmtarantoolarena","text":"Specifies the amount of memory allocated for postanalytics service. It is recommended to set up a value sufficient to store requests data for the last 5-15 minutes. Default value : 0.2","title":"wallarm.tarantool.arena"},{"location":"admin-en/configure-kubernetes-en/#wallarmmetricsenabled","text":"This switch toggles information and metrics collection. If Prometheus is installed in the Kubernetes cluster, no additional configuration is required. Default value : false","title":"wallarm.metrics.enabled"},{"location":"admin-en/configure-kubernetes-en/#global-controller-settings","text":"Implemented via ConfigMap . Besides the standard ones, the following additional parameters are supported: enable-wallarm - enables the Wallarm module in NGINX wallarm-upstream-connect-attempts wallarm-upstream-reconnect-interval wallarm-process-time-limit wallarm-process-time-limit-block wallarm-request-memory-limit","title":"Global Controller Settings"},{"location":"admin-en/configure-kubernetes-en/#ingress-annotations","text":"These annotations are used for setting up parameters for processing individual instances of Ingress. Besides the standard ones , the following additional annotations are supported: nginx.ingress.kubernetes.io/wallarm-mode , default: off nginx.ingress.kubernetes.io/wallarm-mode-allow-override nginx.ingress.kubernetes.io/wallarm-fallback nginx.ingress.kubernetes.io/wallarm-instance nginx.ingress.kubernetes.io/wallarm-block-page nginx.ingress.kubernetes.io/wallarm-parse-response nginx.ingress.kubernetes.io/wallarm-parse-websocket nginx.ingress.kubernetes.io/wallarm-unpack-response nginx.ingress.kubernetes.io/wallarm-parser-disable","title":"Ingress Annotations"},{"location":"admin-en/configure-kubernetes-en/#applying-annotation-to-the-ingress-resource","text":"To apply the settings to your Ingress, please use the following command: kubectl annotate --overwrite ingress YOUR_INGRESS_NAME ANNOTATION_NAME=VALUE YOUR_INGRESS_NAME is the name of your Ingress, ANNOTATION_NAME is the name of the annotation from the list above, VALUE is the value of the annotation from the list above.","title":"Applying annotation to the Ingress resource"},{"location":"admin-en/configure-kubernetes-en/#annotation-examples","text":"","title":"Annotation examples"},{"location":"admin-en/configure-kubernetes-en/#configuring-the-blocking-page-and-error-code","text":"The annotation nginx.ingress.kubernetes.io/wallarm-block-page is used to configure the blocking page and error code returned in the response to the request blocked for the following reasons: Request contains malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . Request containing malicious payloads from the list above is originated from greylisted IP address and the WAF node filters requests in the safe blocking mode . Request is originated from the blacklisted IP address . For example, to return the default Wallarm blocking page and the error code 445 in the response to any blocked request: kubectl annotate ingress <YOUR_INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = \"&/usr/share/nginx/html/wallarm_blocked.html response_code=445 type=attack,acl_ip,acl_source\" More details on the blocking page and error code configuration methods \u2192","title":"Configuring the blocking page and error code"},{"location":"admin-en/configure-kubernetes-en/#enabling-attack-analysis-with-libdetection","text":"The libdetection library additionally validates attacks detected by the library libproton . Using libdetection ensures the double\u2011detection of attacks and reduces the number of false positives. To allow libdetection to parse and check the request body, buffering of a client request body must be enabled ( proxy_request_buffering on ). There are two options to enable attack analysis with libdetection : Applying the following nginx.ingress.kubernetes.io/server-snippet annotation to the Ingress resource: kubectl annotate --overwrite ingress <YOUR_INGRESS_NAME> nginx.ingress.kubernetes.io/server-snippet = \"wallarm_enable_libdetection on; proxy_request_buffering on;\" Adding the following snippet to the config object in values.yaml of the cloned Wallarm Helm chart repository : config: { server-snippet: 'wallarm_enable_libdetection on; proxy_request_buffering on;' }","title":"Enabling attack analysis with libdetection"},{"location":"admin-en/configure-logging/","text":"Working with Filter Node Logs \u00b6 Where Filter Node Logs are Stored \u00b6 A filter node stores the following log files in the /var/log/wallarm directory: brute-detect.log and sync-brute-clusters.log : the log of fetching the brute force attack-related counters in the filter node cluster. export-attacks.log : the log of exporting the attacks' data from the postanalytics module to the Wallarm cloud. export-clusterization-data.log : the log of exporting the filter node cluster's data. export-counters.log : the log of exporting the counters' data (see \u201cMonitoring the Filter Node\u201d ). syncnode.log : the log of syncing the filter node with the Wallarm cloud (this includes fetching the LOM and proton.db files from the cloud). tarantool.log : the log of the postanalytics module operations. sync-blacklist.log : the log of syncing the WAF node with IP addresses added to IP lists as single objects or subnets. sync-mmdb.log : the log of syncing the WAF node with IP addresses registered in countries and data centers from IP lists . Configuring Extended Logging for the NGINX\u2011Based Filter Node \u00b6 NGINX writes logs of the processed requests (access logs) into a separate log file, using the predefined combined logging format by default. log_format combined '$remote_addr - $remote_user [$time_local] ' '\"$request\" $request_id $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\"'; You can define and use a custom logging format by including one or several filter node variables (as well as other NGINX variables if needed). The NGINX log file will allow for much faster filter node diagnostics. Filter Node Variables \u00b6 You may use the following filter node variables when defining the NGINX logging format: Name Type Value request_id String Request identifier Has the following value form: a79199bcea606040cc79f913325401fb wallarm_request_time Float Request execution time in seconds wallarm_serialized_size Integer Size of the serialized request in bytes wallarm_is_input_valid Integer Request validity 0 : request is valid. The request has been checked by filter node and matches LOM rules. 1 : request is invalid. The request has been checked by filter node and does not match LOM rules. wallarm_attack_type_list String Attack types detected in the request with the library libproton . Types are presented in text format: xss sqli rce xxe ptrav crlf redir nosqli infoleak overlimit_res logic_bomb vpatch ldapi scanner If several attack types are detected in a request, they are listed with the symbol | . For example: if XSS and SQLi attacks are detected, the variable value is xss|sqli . wallarm_attack_type Integer Attack types detected in the request with the library libproton . Types are presented in bit string format: 0x00000000 : no attack: \"0\"``\"0\" 0x00000002 : xss: \"2\" 0x00000004 : sqli: \"4\" 0x00000008 : rce: \"8\" 0x00000010 : xxe: \"16\" 0x00000020 : ptrav: \"32\" 0x00000040 : crlf: \"64\" 0x00000080 : redir: \"128\" 0x00000100 : nosqli: \"256\" 0x00000200 : infoleak: \"512\" 0x20000000 : overlimit_res: \"536870912\" 0x40000000 : logic_bomb: \"1073741824\" 0x80000000 : vpatch: \"2147483648\" 0x00002000 : ldapi: \"8192\" 0x4000 : scanner: \"16384\" If several attack types are detected in a request, the values are summarized. For example: if XSS and SQLi attacks are detected, the variable value is 6 . Configuration Example \u00b6 Let us assume that you need to specify the extended logging format named wallarm_combined that includes the following variables: all variables used in the combined format all filter node variables To do this, perform the following actions: The lines below describe the desired logging format. Add them into the http block of the NGINX configuration file. log_format wallarm_combined '$remote_addr - $remote_user [$time_local] ' '\"$request\" $request_id $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\"' '$wallarm_request_time $wallarm_serialized_size $wallarm_is_input_valid $wallarm_attack_type $wallarm_attack_type_list'; Enable the extended logging format by adding the following directive into the same block as in the first step: access_log /var/log/nginx/access.log wallarm_combined; Processed request logs will be written in the wallarm_combined format into the /var/log/nginx/access.log file. Conditional Logging With the directive listed above, all processed requests will be logged to a log file, including these that are not related to an attack. You can configure conditional logging to write logs only for the requests that are part of an attack (the wallarm_attack_type variable value is not zero for these requests). To do so, add a condition to the aforementioned directive: access_log /var/log/nginx/access.log wallarm_combined if=$wallarm_attack_type; This may be useful if you want to reduce a log file size, or if you integrate a filter node with one of SIEM solutions. Restart NGINX by running one of the following commands depending on the OS you are using: Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx Detailed information To see detailed information about configuring logging in NGINX, proceed to this link .","title":"Configuring logging"},{"location":"admin-en/configure-logging/#working-with-filter-node-logs","text":"","title":"Working with Filter Node Logs"},{"location":"admin-en/configure-logging/#where-filter-node-logs-are-stored","text":"A filter node stores the following log files in the /var/log/wallarm directory: brute-detect.log and sync-brute-clusters.log : the log of fetching the brute force attack-related counters in the filter node cluster. export-attacks.log : the log of exporting the attacks' data from the postanalytics module to the Wallarm cloud. export-clusterization-data.log : the log of exporting the filter node cluster's data. export-counters.log : the log of exporting the counters' data (see \u201cMonitoring the Filter Node\u201d ). syncnode.log : the log of syncing the filter node with the Wallarm cloud (this includes fetching the LOM and proton.db files from the cloud). tarantool.log : the log of the postanalytics module operations. sync-blacklist.log : the log of syncing the WAF node with IP addresses added to IP lists as single objects or subnets. sync-mmdb.log : the log of syncing the WAF node with IP addresses registered in countries and data centers from IP lists .","title":"Where Filter Node Logs are Stored"},{"location":"admin-en/configure-logging/#configuring-extended-logging-for-the-nginxbased-filter-node","text":"NGINX writes logs of the processed requests (access logs) into a separate log file, using the predefined combined logging format by default. log_format combined '$remote_addr - $remote_user [$time_local] ' '\"$request\" $request_id $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\"'; You can define and use a custom logging format by including one or several filter node variables (as well as other NGINX variables if needed). The NGINX log file will allow for much faster filter node diagnostics.","title":"Configuring Extended Logging for the NGINX\u2011Based Filter Node"},{"location":"admin-en/configure-logging/#filter-node-variables","text":"You may use the following filter node variables when defining the NGINX logging format: Name Type Value request_id String Request identifier Has the following value form: a79199bcea606040cc79f913325401fb wallarm_request_time Float Request execution time in seconds wallarm_serialized_size Integer Size of the serialized request in bytes wallarm_is_input_valid Integer Request validity 0 : request is valid. The request has been checked by filter node and matches LOM rules. 1 : request is invalid. The request has been checked by filter node and does not match LOM rules. wallarm_attack_type_list String Attack types detected in the request with the library libproton . Types are presented in text format: xss sqli rce xxe ptrav crlf redir nosqli infoleak overlimit_res logic_bomb vpatch ldapi scanner If several attack types are detected in a request, they are listed with the symbol | . For example: if XSS and SQLi attacks are detected, the variable value is xss|sqli . wallarm_attack_type Integer Attack types detected in the request with the library libproton . Types are presented in bit string format: 0x00000000 : no attack: \"0\"``\"0\" 0x00000002 : xss: \"2\" 0x00000004 : sqli: \"4\" 0x00000008 : rce: \"8\" 0x00000010 : xxe: \"16\" 0x00000020 : ptrav: \"32\" 0x00000040 : crlf: \"64\" 0x00000080 : redir: \"128\" 0x00000100 : nosqli: \"256\" 0x00000200 : infoleak: \"512\" 0x20000000 : overlimit_res: \"536870912\" 0x40000000 : logic_bomb: \"1073741824\" 0x80000000 : vpatch: \"2147483648\" 0x00002000 : ldapi: \"8192\" 0x4000 : scanner: \"16384\" If several attack types are detected in a request, the values are summarized. For example: if XSS and SQLi attacks are detected, the variable value is 6 .","title":"Filter Node Variables"},{"location":"admin-en/configure-logging/#configuration-example","text":"Let us assume that you need to specify the extended logging format named wallarm_combined that includes the following variables: all variables used in the combined format all filter node variables To do this, perform the following actions: The lines below describe the desired logging format. Add them into the http block of the NGINX configuration file. log_format wallarm_combined '$remote_addr - $remote_user [$time_local] ' '\"$request\" $request_id $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\"' '$wallarm_request_time $wallarm_serialized_size $wallarm_is_input_valid $wallarm_attack_type $wallarm_attack_type_list'; Enable the extended logging format by adding the following directive into the same block as in the first step: access_log /var/log/nginx/access.log wallarm_combined; Processed request logs will be written in the wallarm_combined format into the /var/log/nginx/access.log file. Conditional Logging With the directive listed above, all processed requests will be logged to a log file, including these that are not related to an attack. You can configure conditional logging to write logs only for the requests that are part of an attack (the wallarm_attack_type variable value is not zero for these requests). To do so, add a condition to the aforementioned directive: access_log /var/log/nginx/access.log wallarm_combined if=$wallarm_attack_type; This may be useful if you want to reduce a log file size, or if you integrate a filter node with one of SIEM solutions. Restart NGINX by running one of the following commands depending on the OS you are using: Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx Detailed information To see detailed information about configuring logging in NGINX, proceed to this link .","title":"Configuration Example"},{"location":"admin-en/configure-parameters-en/","text":"Configuration Options for the NGINX\u2011Based Filter Node \u00b6 NGINX official documentation The Wallarm configuration is very similar to the NGINX configuration. See the official NGINX documentation . Along with the Wallarm specific configuration options, you have the full capabilities of the NGINX configuration. Wallarm Directives \u00b6 disable_acl \u00b6 Description NGINX blocks Default value Detailed instructions Allows disabling analysis of requests origins. If disabled ( on ), the WAF node does not download IP lists from the Wallarm Cloud and skips request source IPs analysis. http, server, location off - wallarm_block_page \u00b6 Description NGINX blocks Default value Detailed instructions Lets you set up the response to the blocked request. http, server, location - Blocking page andd error code configuration wallarm_api_conf \u00b6 A path to the node.yaml file, which contains access requirements for the Wallarm API. Example : wallarm_api_conf /etc/wallarm/node.yaml Used to upload serialized requests from the filtering node directly to the Wallarm API (cloud) instead of uploading into the postanalytics module (Tarantool). Only requests with attacks are sent to the API. Requests without attacks are not saved. Example of the node.yaml file content: # API connection credentials hostname: <some name> uuid: <some uuid> secret: <some secret> # API connection parameters (the parameters below are used by default) api: host: api.wallarm.com port: 444 ca_verify: true wallarm_block_page_add_dynamic_path \u00b6 This directive is used to initialize the blocking page that has NGINX variables in its code and the path to this blocking page is also set using a variable. Otherwise, the directive is not used. More details on the blocking page and error code configuration \u2192 Info The directive can be set inside the http block of the NGINX configuration file. wallarm_cache_path \u00b6 A directory in which the backup catalog for the proton.db and LOM copy storage is created when the server starts. This directory must be writable for the client that runs NGINX. Info This parameter is configured inside the http block only. wallarm_enable_libdetection \u00b6 Enables additional validation of the SQL Injection attacks via the libdetection library. Using libdetection ensures the double\u2011detection of attacks and reduces the number of false positives. Analyzing of requests with the libdetection library is disabled by default. To reduce the number of false positives, we recommend to enable analysis ( wallarm_enable_libdetection on ). More details on libdetection \u2192 To allow libdetection to analyze the request body, please ensure that buffering of a client request body is enabled ( proxy_request_buffering on ). Example: wallarm_enable_libdetection on; proxy_request_buffering on; Memory consumption increase When analyzing attacks using the libdetection library, the amount of memory consumed by NGINX and Wallarm processes may increase by about 10%. Info This parameter can be set inside the http, server, and location blocks. To enable libdetection in the Wallarm Ingresss controller, it is required to apply the nginx.ingress.kubernetes.io/server-snippet annotation with this parameter to the Ingress resource. Default value is off . wallarm_fallback \u00b6 With the value set to on , NGINX has the ability to enter an emergency mode; if proton.db or LOM cannot be downloaded, this setting disables the Wallarm module for the http, server, and location blocks, for which the data fails to download. NGINX keeps functioning. Info This parameter can be set inside the http, server, and location blocks. wallarm_force \u00b6 Sets the requests' analysis and LOM rules generation based on the NGINX mirrored traffic. See Analyzing mirrored traffic with NGINX . wallarm_global_trainingset_path \u00b6 A path to the proton.db file that has the global settings for request filtering, which do not depend on the application structure. Info This parameter can be set inside the http, server, and location blocks. Default value : /etc/wallarm/proton.db wallarm_file_check_interval \u00b6 Defines an interval between checking new data in proton.db and LOM . The unit of measure is specified in the suffix as follows: no suffix for minutes, s for seconds, ms for milliseconds. Info This parameter is configured only inside the http block. Default value : 1 (one minute) wallarm_instance \u00b6 An application identifier. The directive is used to visually separate the data of different applications on the Dashboard . Numeric values only. The application identifiers are used solely for the convenience of data presentation. To correctly separate the data of different applications, the same application identifiers must be set in the Wallarm interface. Any filter node will filter traffic for any number of applications without additional configuration. Info This parameter can be set inside the http, server, and location blocks. wallarm_key_path \u00b6 A path to the Wallarm license key. Info Default value : /etc/wallarm/license.key wallarm_local_trainingset_path \u00b6 A path to the LOM file that contains information on the protected application and the filter node settings. Info This parameter can be set inside the http, server, and location blocks. Default value : /etc/wallarm/lom wallarm_mode \u00b6 Traffic processing mode: off \u2192 the WAF node: Does not analyze whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . Blocks all requests originated from blacklisted IP addresses . monitoring \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks all requests originated from blacklisted IP addresses . safe_blocking \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks all requests originated from blacklisted IP addresses . Blocks requests containing malicious payloads if they are originated from greylisted IP addresses . block \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks requests containing malicious payloads. Blocks all requests originated from blacklisted IP addresses . Usage of wallarm_mode can be restricted by the wallarm_mode_allow_override directive. Detailed instructions on filtration mode configuration \u2192 Info This parameter can be set inside the http, server, and location blocks. Default value depends on the WAF node deployment method (can be off or monitoring ) wallarm_mode_allow_override \u00b6 Manages the ability to override the wallarm_mode values via filtering rules downloaded from the Wallarm cloud (LOM): off : the rules set in LOM are ignored. strict : LOM can only strengthen the operation mode. on : it is possible to both strengthen and soften the operation mode. For example, with wallarm_mode monitoring and wallarm_mode_allow_override strict set, the Wallarm Console can be used to enable blocking of some requests, but the attack analysis cannot be fully disabled. Detailed instructions on filtration mode configuration \u2192 Info This parameter can be set inside the http, server, and location blocks. Default value : on wallarm_parse_response \u00b6 The mode of processing web server responses; by default, only the client's request to the web server can be processed. Possible values: on : analysis of web server responses by a passive vulnerability scanner without sending requests from the Wallarm cloud). off : responses are not analyzed. Info This parameter can be set inside http, server, and location blocks. Default value : on Improve performance You are recommended to disable processing of static files through location to improve performance. wallarm_parse_websocket \u00b6 Wallarm has full WebSockets support. By default, the WebSockets' messages are not analyzed for attacks. To force the feature, use the wallarm_parse_websocket directive. Possible values: - on : message analyses is enabled - off : message analyses is disabled. Info This parameter can be set inside the http, server, and location blocks. Default value : off wallarm_parser_disable \u00b6 Allows to disable parsers. The following parsers are currently supported: cookie zlib htmljs json multipart base64 percent urlenc xml Example wallarm_parser_disable base64; wallarm_parser_disable xml; location /ab { wallarm_parser_disable json; wallarm_parser_disable base64; proxy_pass http://example.com; } location /zy { wallarm_parser_disable json; proxy_pass http://example.com; } Info This parameter can be set inside the http, server, and location blocks. wallarm_parse_html_response \u00b6 Lets you enable and disable an HTML parser for responses to requests. Can be: on off Info This parameter can be set inside the http, server, and location blocks. Default value : on wallarm_stalled_worker_timeout \u00b6 Sets the time limit for processing a single request for an NGINX worker in seconds. If the time exceeds the limit, data about NGINX workers is written to the stalled_workers_count and stalled_workers statistic parameters. Info This parameter can be set inside the http, server, and location blocks. Default value : 5 (five seconds) wallarm_process_time_limit \u00b6 Sets the time limit of a single request processing in milliseconds. If the time exceeds the limit, an error is recorded into the log and the request is marked as an overlimit_res attack. The requests are blocked in the blocking mode ( wallarm_mode block; ) and ignored in the monitoring mode ( wallarm_mode monitoring; ). Info This parameter can be set inside the http, server, and location blocks. Default value : 1000ms (one second). wallarm_process_time_limit_block \u00b6 The ability to manage the blocking of requests, which exceed the time limit set in the wallarm_process_time_limit directive: on : the requests are always blocked off : the requests are always ignored attack : depends on the attack blocking mode set in the wallarm-mode directive: off : the requests are not processed monitoring : the requests are ignored block : the requests are blocked Info This parameter can be set inside the http, server, and location blocks. Default value : wallarm_process_time_limit_block attack wallarm_request_memory_limit \u00b6 Set a limit for the maximum amount of memory that can be used for processing of a single request. If the limit is exceeded, the request processing will be interrupted and a user will get a 500 error. The following suffixes can be used in this parameter: k or K for kilobytes m or M for megabytes g or G for gigabytes Value of 0 turns the limit off. By default, limits are off. Info This parameter can be set inside the http, server, and/or location blocks. wallarm_proton_log_mask_master \u00b6 Settings for the debug logging of the NGINX master process. Using the directive You need to configure the directive only if you are told to do so by the Wallarm support team member. They will provide you with the value to use with the directive. Info The parameter can only be configured at the main level. wallarm_proton_log_mask_worker \u00b6 Settings of the debug logging for a NGINX worker process. Using the directive You need to configure the directive only if you are told to do so by the Wallarm support team member. They will provide you with the value to use with the directive. Info The parameter can only be configured at the main level. wallarm_request_chunk_size \u00b6 Limits the size of the part of the request that is processed during one iteration. You can set up a custom value of the wallarm_request_chunk_size directive in bytes by assigning an integer to it. The directive also supports the following postfixes: k or K for kilobytes m or M for megabytes g or G for gigabytes Info This parameter can be set inside the http, server, and location blocks. Default value : 8k (8 kilobytes). wallarm_tarantool_connect_attempts \u00b6 Deprecated Use the wallarm_upstream_connect_attempts directive instead. wallarm_tarantool_connect_interval \u00b6 Deprecated Use the wallarm_upstream_reconnect_interval directive instead. wallarm_tarantool_upstream \u00b6 With the wallarm_tarantool_upstream , you can balance the requests between several postanalytics servers. Example: upstream wallarm_tarantool { server 127 .0.0.1:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 1 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; See also Module ngx_http_upstream_module . Required conditions It is required that the following conditions are satisfied for the max_conns and the keepalive parameters: The value of the keepalive parameter must not be lower than the number of the tarantool servers. The value of the max_conns parameter must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. Info The parameter is configured inside the http block only. wallarm_timeslice \u00b6 A limit on the time that a filter node spends on one iteration of processing a request before it switches to the next request. Upon reaching the time limit, the filter node proceeds to process the next request in the queue. After performing one iteration on each of the requests in the queue, the node performs the second iteration of processing on the first request in the queue. You can use time intervals suffixes that are described in the nginx documentation to assign different time unit values to the directive. Info This parameter can be set inside the http, server, and location blocks. Default value : 0 (time limit for single iteration is disabled). Warning Due to NGINX server limitations, it is necessary to disable the buffering request by assigning the off value to the proxy_request_buffering NGINX directive for the wallarm_timeslice directive to work. wallarm_ts_request_memory_limit \u00b6 Set a limit for the maximum amount of memory that can be used by one instance of proton.db and LOM. If the memory limit is exceeded while processing some request, the user will get a 500 error. The following suffixes can be used in this parameter: k or K for kilobytes m or M for megabytes g or G for gigabyte Value of 0 turns the limit off. Info This parameter can be set inside the http, server, and/or location blocks. Default value : 1 GB wallarm_unpack_response \u00b6 If the backend sends compressed data, the value on decompresses the data before processing. The value off turns off the decompressing. Info Default value : on . wallarm_upstream_backend \u00b6 A method for sending serialized requests. Requests can be sent either to the Tarantool or to the API. Possible values of the directive: tarantool api Depending on the other directives, the default value will be assigned as follows: tarantool \u2014if there is no wallarm_api_conf directive in the configuration. api \u2014if there is a wallarm_api_conf directive, but there is no wallarm_tarantool_upstream directive in the configuration. Note If the wallarm_api_conf and wallarm_tarantool_upstream directives are present simultaneously in the configuration, a configuration error of the directive ambiguous wallarm upstream backend form will occur. Info This parameter can be set inside the http block only. wallarm_upstream_connect_attempts \u00b6 Defines the number of immediate reconnects to the Tarantool or Wallarm API. If a connection to the Tarantool or API is terminated, then the attempt to reconnect will not occur. However, this is not the case when there aren't anymore connections and the serialized request queue is not empty. Note Reconnection may occur through another server, because the \u201cupstream\u201d subsystem is responsible for choosing the server. This parameter can be set inside the http block only. wallarm_upstream_reconnect_interval \u00b6 Defines the interval between attempts to reconnect to the Tarantool or Wallarm API after the number of unsuccessful attempts has exceeded the wallarm_upstream_connect_attempts threshold. Info This parameter can be set inside the http block only. wallarm_upstream_connect_timeout \u00b6 Defines a timeout for connecting to the Tarantool or Wallarm API. Info This parameter can be set inside the http block only. wallarm_upstream_queue_limit \u00b6 Defines a limit to the number of serialized requests. Simultaneously setting the wallarm_upstream_queue_limit parameter and not setting the wallarm_upstream_queue_memory_limit parameter means that there will be no limit on the latter. Info This parameter can be set inside the http block only. wallarm_upstream_queue_memory_limit \u00b6 Defines a limit to the total volume of serialized requests. Simultaneously setting the wallarm_upstream_queue_memory_limit parameter and not setting the wallarm_upstream_queue_limit parameter means that there will be no limit on the latter. Info Default value: 100m . This parameter can be set inside the http block only. wallarm_worker_rlimit_vmem \u00b6 Deprecated It is now an alias for the wallarm_ts_request_memory_limit directive.","title":"NGINX directives"},{"location":"admin-en/configure-parameters-en/#configuration-options-for-the-nginxbased-filter-node","text":"NGINX official documentation The Wallarm configuration is very similar to the NGINX configuration. See the official NGINX documentation . Along with the Wallarm specific configuration options, you have the full capabilities of the NGINX configuration.","title":"Configuration Options for the NGINX\u2011Based Filter Node"},{"location":"admin-en/configure-parameters-en/#wallarm-directives","text":"","title":"Wallarm Directives"},{"location":"admin-en/configure-parameters-en/#disable_acl","text":"Description NGINX blocks Default value Detailed instructions Allows disabling analysis of requests origins. If disabled ( on ), the WAF node does not download IP lists from the Wallarm Cloud and skips request source IPs analysis. http, server, location off -","title":"disable_acl"},{"location":"admin-en/configure-parameters-en/#wallarm_block_page","text":"Description NGINX blocks Default value Detailed instructions Lets you set up the response to the blocked request. http, server, location - Blocking page andd error code configuration","title":"wallarm_block_page"},{"location":"admin-en/configure-parameters-en/#wallarm_api_conf","text":"A path to the node.yaml file, which contains access requirements for the Wallarm API. Example : wallarm_api_conf /etc/wallarm/node.yaml Used to upload serialized requests from the filtering node directly to the Wallarm API (cloud) instead of uploading into the postanalytics module (Tarantool). Only requests with attacks are sent to the API. Requests without attacks are not saved. Example of the node.yaml file content: # API connection credentials hostname: <some name> uuid: <some uuid> secret: <some secret> # API connection parameters (the parameters below are used by default) api: host: api.wallarm.com port: 444 ca_verify: true","title":"wallarm_api_conf"},{"location":"admin-en/configure-parameters-en/#wallarm_block_page_add_dynamic_path","text":"This directive is used to initialize the blocking page that has NGINX variables in its code and the path to this blocking page is also set using a variable. Otherwise, the directive is not used. More details on the blocking page and error code configuration \u2192 Info The directive can be set inside the http block of the NGINX configuration file.","title":"wallarm_block_page_add_dynamic_path"},{"location":"admin-en/configure-parameters-en/#wallarm_cache_path","text":"A directory in which the backup catalog for the proton.db and LOM copy storage is created when the server starts. This directory must be writable for the client that runs NGINX. Info This parameter is configured inside the http block only.","title":"wallarm_cache_path"},{"location":"admin-en/configure-parameters-en/#wallarm_enable_libdetection","text":"Enables additional validation of the SQL Injection attacks via the libdetection library. Using libdetection ensures the double\u2011detection of attacks and reduces the number of false positives. Analyzing of requests with the libdetection library is disabled by default. To reduce the number of false positives, we recommend to enable analysis ( wallarm_enable_libdetection on ). More details on libdetection \u2192 To allow libdetection to analyze the request body, please ensure that buffering of a client request body is enabled ( proxy_request_buffering on ). Example: wallarm_enable_libdetection on; proxy_request_buffering on; Memory consumption increase When analyzing attacks using the libdetection library, the amount of memory consumed by NGINX and Wallarm processes may increase by about 10%. Info This parameter can be set inside the http, server, and location blocks. To enable libdetection in the Wallarm Ingresss controller, it is required to apply the nginx.ingress.kubernetes.io/server-snippet annotation with this parameter to the Ingress resource. Default value is off .","title":"wallarm_enable_libdetection"},{"location":"admin-en/configure-parameters-en/#wallarm_fallback","text":"With the value set to on , NGINX has the ability to enter an emergency mode; if proton.db or LOM cannot be downloaded, this setting disables the Wallarm module for the http, server, and location blocks, for which the data fails to download. NGINX keeps functioning. Info This parameter can be set inside the http, server, and location blocks.","title":"wallarm_fallback"},{"location":"admin-en/configure-parameters-en/#wallarm_force","text":"Sets the requests' analysis and LOM rules generation based on the NGINX mirrored traffic. See Analyzing mirrored traffic with NGINX .","title":"wallarm_force"},{"location":"admin-en/configure-parameters-en/#wallarm_global_trainingset_path","text":"A path to the proton.db file that has the global settings for request filtering, which do not depend on the application structure. Info This parameter can be set inside the http, server, and location blocks. Default value : /etc/wallarm/proton.db","title":"wallarm_global_trainingset_path"},{"location":"admin-en/configure-parameters-en/#wallarm_file_check_interval","text":"Defines an interval between checking new data in proton.db and LOM . The unit of measure is specified in the suffix as follows: no suffix for minutes, s for seconds, ms for milliseconds. Info This parameter is configured only inside the http block. Default value : 1 (one minute)","title":"wallarm_file_check_interval"},{"location":"admin-en/configure-parameters-en/#wallarm_instance","text":"An application identifier. The directive is used to visually separate the data of different applications on the Dashboard . Numeric values only. The application identifiers are used solely for the convenience of data presentation. To correctly separate the data of different applications, the same application identifiers must be set in the Wallarm interface. Any filter node will filter traffic for any number of applications without additional configuration. Info This parameter can be set inside the http, server, and location blocks.","title":"wallarm_instance"},{"location":"admin-en/configure-parameters-en/#wallarm_key_path","text":"A path to the Wallarm license key. Info Default value : /etc/wallarm/license.key","title":"wallarm_key_path"},{"location":"admin-en/configure-parameters-en/#wallarm_local_trainingset_path","text":"A path to the LOM file that contains information on the protected application and the filter node settings. Info This parameter can be set inside the http, server, and location blocks. Default value : /etc/wallarm/lom","title":"wallarm_local_trainingset_path"},{"location":"admin-en/configure-parameters-en/#wallarm_mode","text":"Traffic processing mode: off \u2192 the WAF node: Does not analyze whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . Blocks all requests originated from blacklisted IP addresses . monitoring \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks all requests originated from blacklisted IP addresses . safe_blocking \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks all requests originated from blacklisted IP addresses . Blocks requests containing malicious payloads if they are originated from greylisted IP addresses . block \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks requests containing malicious payloads. Blocks all requests originated from blacklisted IP addresses . Usage of wallarm_mode can be restricted by the wallarm_mode_allow_override directive. Detailed instructions on filtration mode configuration \u2192 Info This parameter can be set inside the http, server, and location blocks. Default value depends on the WAF node deployment method (can be off or monitoring )","title":"wallarm_mode"},{"location":"admin-en/configure-parameters-en/#wallarm_mode_allow_override","text":"Manages the ability to override the wallarm_mode values via filtering rules downloaded from the Wallarm cloud (LOM): off : the rules set in LOM are ignored. strict : LOM can only strengthen the operation mode. on : it is possible to both strengthen and soften the operation mode. For example, with wallarm_mode monitoring and wallarm_mode_allow_override strict set, the Wallarm Console can be used to enable blocking of some requests, but the attack analysis cannot be fully disabled. Detailed instructions on filtration mode configuration \u2192 Info This parameter can be set inside the http, server, and location blocks. Default value : on","title":"wallarm_mode_allow_override"},{"location":"admin-en/configure-parameters-en/#wallarm_parse_response","text":"The mode of processing web server responses; by default, only the client's request to the web server can be processed. Possible values: on : analysis of web server responses by a passive vulnerability scanner without sending requests from the Wallarm cloud). off : responses are not analyzed. Info This parameter can be set inside http, server, and location blocks. Default value : on Improve performance You are recommended to disable processing of static files through location to improve performance.","title":"wallarm_parse_response"},{"location":"admin-en/configure-parameters-en/#wallarm_parse_websocket","text":"Wallarm has full WebSockets support. By default, the WebSockets' messages are not analyzed for attacks. To force the feature, use the wallarm_parse_websocket directive. Possible values: - on : message analyses is enabled - off : message analyses is disabled. Info This parameter can be set inside the http, server, and location blocks. Default value : off","title":"wallarm_parse_websocket"},{"location":"admin-en/configure-parameters-en/#wallarm_parser_disable","text":"Allows to disable parsers. The following parsers are currently supported: cookie zlib htmljs json multipart base64 percent urlenc xml Example wallarm_parser_disable base64; wallarm_parser_disable xml; location /ab { wallarm_parser_disable json; wallarm_parser_disable base64; proxy_pass http://example.com; } location /zy { wallarm_parser_disable json; proxy_pass http://example.com; } Info This parameter can be set inside the http, server, and location blocks.","title":"wallarm_parser_disable"},{"location":"admin-en/configure-parameters-en/#wallarm_parse_html_response","text":"Lets you enable and disable an HTML parser for responses to requests. Can be: on off Info This parameter can be set inside the http, server, and location blocks. Default value : on","title":"wallarm_parse_html_response"},{"location":"admin-en/configure-parameters-en/#wallarm_stalled_worker_timeout","text":"Sets the time limit for processing a single request for an NGINX worker in seconds. If the time exceeds the limit, data about NGINX workers is written to the stalled_workers_count and stalled_workers statistic parameters. Info This parameter can be set inside the http, server, and location blocks. Default value : 5 (five seconds)","title":"wallarm_stalled_worker_timeout"},{"location":"admin-en/configure-parameters-en/#wallarm_process_time_limit","text":"Sets the time limit of a single request processing in milliseconds. If the time exceeds the limit, an error is recorded into the log and the request is marked as an overlimit_res attack. The requests are blocked in the blocking mode ( wallarm_mode block; ) and ignored in the monitoring mode ( wallarm_mode monitoring; ). Info This parameter can be set inside the http, server, and location blocks. Default value : 1000ms (one second).","title":"wallarm_process_time_limit"},{"location":"admin-en/configure-parameters-en/#wallarm_process_time_limit_block","text":"The ability to manage the blocking of requests, which exceed the time limit set in the wallarm_process_time_limit directive: on : the requests are always blocked off : the requests are always ignored attack : depends on the attack blocking mode set in the wallarm-mode directive: off : the requests are not processed monitoring : the requests are ignored block : the requests are blocked Info This parameter can be set inside the http, server, and location blocks. Default value : wallarm_process_time_limit_block attack","title":"wallarm_process_time_limit_block"},{"location":"admin-en/configure-parameters-en/#wallarm_request_memory_limit","text":"Set a limit for the maximum amount of memory that can be used for processing of a single request. If the limit is exceeded, the request processing will be interrupted and a user will get a 500 error. The following suffixes can be used in this parameter: k or K for kilobytes m or M for megabytes g or G for gigabytes Value of 0 turns the limit off. By default, limits are off. Info This parameter can be set inside the http, server, and/or location blocks.","title":"wallarm_request_memory_limit"},{"location":"admin-en/configure-parameters-en/#wallarm_proton_log_mask_master","text":"Settings for the debug logging of the NGINX master process. Using the directive You need to configure the directive only if you are told to do so by the Wallarm support team member. They will provide you with the value to use with the directive. Info The parameter can only be configured at the main level.","title":"wallarm_proton_log_mask_master"},{"location":"admin-en/configure-parameters-en/#wallarm_proton_log_mask_worker","text":"Settings of the debug logging for a NGINX worker process. Using the directive You need to configure the directive only if you are told to do so by the Wallarm support team member. They will provide you with the value to use with the directive. Info The parameter can only be configured at the main level.","title":"wallarm_proton_log_mask_worker"},{"location":"admin-en/configure-parameters-en/#wallarm_request_chunk_size","text":"Limits the size of the part of the request that is processed during one iteration. You can set up a custom value of the wallarm_request_chunk_size directive in bytes by assigning an integer to it. The directive also supports the following postfixes: k or K for kilobytes m or M for megabytes g or G for gigabytes Info This parameter can be set inside the http, server, and location blocks. Default value : 8k (8 kilobytes).","title":"wallarm_request_chunk_size"},{"location":"admin-en/configure-parameters-en/#wallarm_tarantool_connect_attempts","text":"Deprecated Use the wallarm_upstream_connect_attempts directive instead.","title":"wallarm_tarantool_connect_attempts"},{"location":"admin-en/configure-parameters-en/#wallarm_tarantool_connect_interval","text":"Deprecated Use the wallarm_upstream_reconnect_interval directive instead.","title":"wallarm_tarantool_connect_interval"},{"location":"admin-en/configure-parameters-en/#wallarm_tarantool_upstream","text":"With the wallarm_tarantool_upstream , you can balance the requests between several postanalytics servers. Example: upstream wallarm_tarantool { server 127 .0.0.1:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 1 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; See also Module ngx_http_upstream_module . Required conditions It is required that the following conditions are satisfied for the max_conns and the keepalive parameters: The value of the keepalive parameter must not be lower than the number of the tarantool servers. The value of the max_conns parameter must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. Info The parameter is configured inside the http block only.","title":"wallarm_tarantool_upstream"},{"location":"admin-en/configure-parameters-en/#wallarm_timeslice","text":"A limit on the time that a filter node spends on one iteration of processing a request before it switches to the next request. Upon reaching the time limit, the filter node proceeds to process the next request in the queue. After performing one iteration on each of the requests in the queue, the node performs the second iteration of processing on the first request in the queue. You can use time intervals suffixes that are described in the nginx documentation to assign different time unit values to the directive. Info This parameter can be set inside the http, server, and location blocks. Default value : 0 (time limit for single iteration is disabled). Warning Due to NGINX server limitations, it is necessary to disable the buffering request by assigning the off value to the proxy_request_buffering NGINX directive for the wallarm_timeslice directive to work.","title":"wallarm_timeslice"},{"location":"admin-en/configure-parameters-en/#wallarm_ts_request_memory_limit","text":"Set a limit for the maximum amount of memory that can be used by one instance of proton.db and LOM. If the memory limit is exceeded while processing some request, the user will get a 500 error. The following suffixes can be used in this parameter: k or K for kilobytes m or M for megabytes g or G for gigabyte Value of 0 turns the limit off. Info This parameter can be set inside the http, server, and/or location blocks. Default value : 1 GB","title":"wallarm_ts_request_memory_limit"},{"location":"admin-en/configure-parameters-en/#wallarm_unpack_response","text":"If the backend sends compressed data, the value on decompresses the data before processing. The value off turns off the decompressing. Info Default value : on .","title":"wallarm_unpack_response"},{"location":"admin-en/configure-parameters-en/#wallarm_upstream_backend","text":"A method for sending serialized requests. Requests can be sent either to the Tarantool or to the API. Possible values of the directive: tarantool api Depending on the other directives, the default value will be assigned as follows: tarantool \u2014if there is no wallarm_api_conf directive in the configuration. api \u2014if there is a wallarm_api_conf directive, but there is no wallarm_tarantool_upstream directive in the configuration. Note If the wallarm_api_conf and wallarm_tarantool_upstream directives are present simultaneously in the configuration, a configuration error of the directive ambiguous wallarm upstream backend form will occur. Info This parameter can be set inside the http block only.","title":"wallarm_upstream_backend"},{"location":"admin-en/configure-parameters-en/#wallarm_upstream_connect_attempts","text":"Defines the number of immediate reconnects to the Tarantool or Wallarm API. If a connection to the Tarantool or API is terminated, then the attempt to reconnect will not occur. However, this is not the case when there aren't anymore connections and the serialized request queue is not empty. Note Reconnection may occur through another server, because the \u201cupstream\u201d subsystem is responsible for choosing the server. This parameter can be set inside the http block only.","title":"wallarm_upstream_connect_attempts"},{"location":"admin-en/configure-parameters-en/#wallarm_upstream_reconnect_interval","text":"Defines the interval between attempts to reconnect to the Tarantool or Wallarm API after the number of unsuccessful attempts has exceeded the wallarm_upstream_connect_attempts threshold. Info This parameter can be set inside the http block only.","title":"wallarm_upstream_reconnect_interval"},{"location":"admin-en/configure-parameters-en/#wallarm_upstream_connect_timeout","text":"Defines a timeout for connecting to the Tarantool or Wallarm API. Info This parameter can be set inside the http block only.","title":"wallarm_upstream_connect_timeout"},{"location":"admin-en/configure-parameters-en/#wallarm_upstream_queue_limit","text":"Defines a limit to the number of serialized requests. Simultaneously setting the wallarm_upstream_queue_limit parameter and not setting the wallarm_upstream_queue_memory_limit parameter means that there will be no limit on the latter. Info This parameter can be set inside the http block only.","title":"wallarm_upstream_queue_limit"},{"location":"admin-en/configure-parameters-en/#wallarm_upstream_queue_memory_limit","text":"Defines a limit to the total volume of serialized requests. Simultaneously setting the wallarm_upstream_queue_memory_limit parameter and not setting the wallarm_upstream_queue_limit parameter means that there will be no limit on the latter. Info Default value: 100m . This parameter can be set inside the http block only.","title":"wallarm_upstream_queue_memory_limit"},{"location":"admin-en/configure-parameters-en/#wallarm_worker_rlimit_vmem","text":"Deprecated It is now an alias for the wallarm_ts_request_memory_limit directive.","title":"wallarm_worker_rlimit_vmem"},{"location":"admin-en/configure-selinux/","text":"Configuring SELinux \u00b6 If the SELinux mechanism is enabled on a host with a filter node, it may interfere with the filter node, rendering it inoperable: The filter node's RPS (requests per second) and APS (attacks per second) values will not be exported to the Wallarm cloud. It will not be possible to export filter node metrics to monitoring systems via the TCP protocol (see \u201cMonitoring the Filter Node\u201d ). SELinux is installed and enabled by default on RedHat\u2011based Linux distributions (e.g., CentOS or Amazon Linux 2). SELinux can also be installed on other Linux distributions, such as Debian or Ubuntu. It is mandatory to either disable SELinux or configure SELinux so it does not disrupt the filter node operation. Check SELinux Status \u00b6 Execute the following command: sestatus Examine the output: SELinux status: enabled SELinux status: disabled Configure SELinux \u00b6 Allow the collectd utility to use a TCP socket to make the filter node operable with SELinux enabled. To do so, execute the following command: setsebool -P collectd_tcp_network_connect 1 Check if the aforementioned command executed successfully by running the following command: semanage export | grep collectd_tcp_network_connect The output should contain this string: boolean -m -1 collectd_tcp_network_connect Disable SELinux \u00b6 To set SELinux to a disabled state either execute the setenforce 0 command (SELinux will be disabled until the next reboot) or set the value of the SELINUX variable to disabled in the /etc/selinux/config file, then reboot (SELinux will be disabled permanently).","title":"Configuring SELinux"},{"location":"admin-en/configure-selinux/#configuring-selinux","text":"If the SELinux mechanism is enabled on a host with a filter node, it may interfere with the filter node, rendering it inoperable: The filter node's RPS (requests per second) and APS (attacks per second) values will not be exported to the Wallarm cloud. It will not be possible to export filter node metrics to monitoring systems via the TCP protocol (see \u201cMonitoring the Filter Node\u201d ). SELinux is installed and enabled by default on RedHat\u2011based Linux distributions (e.g., CentOS or Amazon Linux 2). SELinux can also be installed on other Linux distributions, such as Debian or Ubuntu. It is mandatory to either disable SELinux or configure SELinux so it does not disrupt the filter node operation.","title":"Configuring SELinux"},{"location":"admin-en/configure-selinux/#check-selinux-status","text":"Execute the following command: sestatus Examine the output: SELinux status: enabled SELinux status: disabled","title":"Check SELinux Status"},{"location":"admin-en/configure-selinux/#configure-selinux","text":"Allow the collectd utility to use a TCP socket to make the filter node operable with SELinux enabled. To do so, execute the following command: setsebool -P collectd_tcp_network_connect 1 Check if the aforementioned command executed successfully by running the following command: semanage export | grep collectd_tcp_network_connect The output should contain this string: boolean -m -1 collectd_tcp_network_connect","title":"Configure SELinux"},{"location":"admin-en/configure-selinux/#disable-selinux","text":"To set SELinux to a disabled state either execute the setenforce 0 command (SELinux will be disabled until the next reboot) or set the value of the SELINUX variable to disabled in the /etc/selinux/config file, then reboot (SELinux will be disabled permanently).","title":"Disable SELinux"},{"location":"admin-en/configure-statistics-service/","text":"Configuration of the Statistics Service \u00b6 To obtain statistics about the filter node, use the wallarm_status directive, which is written in the NGINX configuration file. Configuring the Statistics Service \u00b6 Important It is highly recommended to configure the statistics service in the separate configuration file wallarm-status.conf and not to use the wallarm_status directive in other files that you use when setting up NGINX, because the latter may be insecure. Also, it is strongly advised not to alter any of the existing lines of the default wallarm-status configuration as it may corrupt the process of metric data upload to the Wallarm cloud. When using the directive, statistics can be given in JSON format or in a format compatible with Prometheus . Usage: wallarm_status [on|off] [format=json|prometheus]; Info The directive can be configured in the context of server and/or location . When configuring the wallarm_status directive, you can specify the IP addresses from which you can request statistics. By default, access is denied from anywhere except for the IP addresses 127.0.0.1 and ::1 , which allow executing the request only from the server where Wallarm is installed. An example of a secure configuration of the filter node statistics service ( wallarm-status.conf ) is shown below: server { listen 127.0.0.8:80; server_name localhost; allow 127.0.0.0/8; # Access is only available for loopback addresses of the filter node server deny all; wallarm_mode off; access_log off; location /wallarm-status { wallarm_status on; } } Changing the listen directive Note that if you change the IP address of the listen directive (in the example above, 127.0.0.8 ), you will also need to change the following settings: Adjust the monitoring settings of the WAF node to the new IP address in the file /etc/collectd/collectd.conf.d/nginx-wallarm.conf Add or change the allow directive to allow access from addresses other than loopback addresses (the above configuration file allows access only to loopback addresses) To allow requests from another server, add the allow instruction with the IP address of the desired server in the configuration. For example: allow 10.41.29.0; Working with the Statistics Service \u00b6 To obtain the filter node statistics, make a request from one of the allowed IP addresses (see above): curl http://127.0.0.8/wallarm-status As a result, you will get a response of the type: { \"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0, \"requests_lost\":0,\"overlimits_time\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0, \"proton_errors\":0,\"time_detect\":0,\"db_id\":73,\"lom_id\":102,\"db_apply_time\":1598525865, \"lom_apply_time\":1598525870,\"proton_instances\": { \"total\":3,\"success\":3,\"fallback\":0, \"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[],\"ts_files\":[{\"id\":102, \"size\":12624136,\"mod_time\":1598525870,\"fname\":\"\\/etc\\/wallarm\\/lom\"}],\"db_files\": [{\"id\":73,\"size\":139094,\"mod_time\":1598525865,\"fname\":\"\\/etc\\/wallarm\\/proton.db\"}] } The following response parameters are available: requests : the number of requests that have been processed by the filter node. attacks : the number of recorded attacks. blocked : the number of blocked requests. abnormal : the number of requests the application deems abnormal. requests_lost : the number of requests that were not analyzed in a post-analytics module and transferred to API. For these requests, blocking parameters were applied (i.e., malicious requests were blocked if the system was operating in blocking mode); however, data on these events is not visible in the UI. This parameter is only used when the Wallarm Node works with a local post-analytics module. overlimits_time : the number of attacks with the type Overlimiting of computational resources detected by the WAF node. tnt_errors : the number of requests not analyzed by a post-analytics module. For these requests, the reasons for blocking are recorded, but the requests themselves are not counted in statistics and behavior checks. api_errors : the number of requests that were not submitted to the API for further analysis. For these requests, blocking parameters were applied (i.e., malicious requests were blocked if the system was operating in blocking mode); however, data on these events is not visible in the UI. This parameter is only used when the Wallarm Node works with a local post-analytics module. segfaults : the number of issues that led to the emergency termination of the worker process. memfaults : the number of issues where the virtual memory limits were reached. time_detect : the total time of requests analysis. db_id : proton.db version. lom_id : LOM version. db_apply_time : Unix time of the last update of the proton.db file. lom_apply_time : Unix time of the last update of the LOM file. proton_instances : information about proton.db + LOM pairs: total : the number of proton.db + LOM pairs. success : the number of the successfully uploaded proton.db + LOM pairs. fallback : the number of proton.db + LOM pairs loaded from the last saved files. failed : the number of proton.db + LOM pairs that were not initialized and run in the \u201cdo not analyze\u201d mode. stalled_workers_count : the quantity of workers that exceeded the time limit for request processing (the limit is set in the wallarm_stalled_worker_timeout directive). stalled_workers : the list of the workers that exceeded the time limit for request processing (the limit is set in the wallarm_stalled_worker_timeout directive) and the amount of time spent on request processing. ts_files : information about the LOM file: id : used LOM version. size : LOM file size in bytes. mod_time : Unix time of the last update of the LOM file. fname : path to the LOM file. db_files : information about the proton.db file: id : used proton.db version. size : proton.db file size in bytes. mod_time : Unix time of the last update of the proton.db file. fname : path to the proton.db file. startid : randomly-generated unique ID of the WAF node. The data of all counters is accumulated from the moment NGINX is started. If Wallarm has been installed in a ready-made infrastructure with NGINX, the NGINX server must be restarted to start Wallarm.","title":"Configure statistics service"},{"location":"admin-en/configure-statistics-service/#configuration-of-the-statistics-service","text":"To obtain statistics about the filter node, use the wallarm_status directive, which is written in the NGINX configuration file.","title":"Configuration of the Statistics Service"},{"location":"admin-en/configure-statistics-service/#configuring-the-statistics-service","text":"Important It is highly recommended to configure the statistics service in the separate configuration file wallarm-status.conf and not to use the wallarm_status directive in other files that you use when setting up NGINX, because the latter may be insecure. Also, it is strongly advised not to alter any of the existing lines of the default wallarm-status configuration as it may corrupt the process of metric data upload to the Wallarm cloud. When using the directive, statistics can be given in JSON format or in a format compatible with Prometheus . Usage: wallarm_status [on|off] [format=json|prometheus]; Info The directive can be configured in the context of server and/or location . When configuring the wallarm_status directive, you can specify the IP addresses from which you can request statistics. By default, access is denied from anywhere except for the IP addresses 127.0.0.1 and ::1 , which allow executing the request only from the server where Wallarm is installed. An example of a secure configuration of the filter node statistics service ( wallarm-status.conf ) is shown below: server { listen 127.0.0.8:80; server_name localhost; allow 127.0.0.0/8; # Access is only available for loopback addresses of the filter node server deny all; wallarm_mode off; access_log off; location /wallarm-status { wallarm_status on; } } Changing the listen directive Note that if you change the IP address of the listen directive (in the example above, 127.0.0.8 ), you will also need to change the following settings: Adjust the monitoring settings of the WAF node to the new IP address in the file /etc/collectd/collectd.conf.d/nginx-wallarm.conf Add or change the allow directive to allow access from addresses other than loopback addresses (the above configuration file allows access only to loopback addresses) To allow requests from another server, add the allow instruction with the IP address of the desired server in the configuration. For example: allow 10.41.29.0;","title":"Configuring the Statistics Service"},{"location":"admin-en/configure-statistics-service/#working-with-the-statistics-service","text":"To obtain the filter node statistics, make a request from one of the allowed IP addresses (see above): curl http://127.0.0.8/wallarm-status As a result, you will get a response of the type: { \"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0, \"requests_lost\":0,\"overlimits_time\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0, \"proton_errors\":0,\"time_detect\":0,\"db_id\":73,\"lom_id\":102,\"db_apply_time\":1598525865, \"lom_apply_time\":1598525870,\"proton_instances\": { \"total\":3,\"success\":3,\"fallback\":0, \"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[],\"ts_files\":[{\"id\":102, \"size\":12624136,\"mod_time\":1598525870,\"fname\":\"\\/etc\\/wallarm\\/lom\"}],\"db_files\": [{\"id\":73,\"size\":139094,\"mod_time\":1598525865,\"fname\":\"\\/etc\\/wallarm\\/proton.db\"}] } The following response parameters are available: requests : the number of requests that have been processed by the filter node. attacks : the number of recorded attacks. blocked : the number of blocked requests. abnormal : the number of requests the application deems abnormal. requests_lost : the number of requests that were not analyzed in a post-analytics module and transferred to API. For these requests, blocking parameters were applied (i.e., malicious requests were blocked if the system was operating in blocking mode); however, data on these events is not visible in the UI. This parameter is only used when the Wallarm Node works with a local post-analytics module. overlimits_time : the number of attacks with the type Overlimiting of computational resources detected by the WAF node. tnt_errors : the number of requests not analyzed by a post-analytics module. For these requests, the reasons for blocking are recorded, but the requests themselves are not counted in statistics and behavior checks. api_errors : the number of requests that were not submitted to the API for further analysis. For these requests, blocking parameters were applied (i.e., malicious requests were blocked if the system was operating in blocking mode); however, data on these events is not visible in the UI. This parameter is only used when the Wallarm Node works with a local post-analytics module. segfaults : the number of issues that led to the emergency termination of the worker process. memfaults : the number of issues where the virtual memory limits were reached. time_detect : the total time of requests analysis. db_id : proton.db version. lom_id : LOM version. db_apply_time : Unix time of the last update of the proton.db file. lom_apply_time : Unix time of the last update of the LOM file. proton_instances : information about proton.db + LOM pairs: total : the number of proton.db + LOM pairs. success : the number of the successfully uploaded proton.db + LOM pairs. fallback : the number of proton.db + LOM pairs loaded from the last saved files. failed : the number of proton.db + LOM pairs that were not initialized and run in the \u201cdo not analyze\u201d mode. stalled_workers_count : the quantity of workers that exceeded the time limit for request processing (the limit is set in the wallarm_stalled_worker_timeout directive). stalled_workers : the list of the workers that exceeded the time limit for request processing (the limit is set in the wallarm_stalled_worker_timeout directive) and the amount of time spent on request processing. ts_files : information about the LOM file: id : used LOM version. size : LOM file size in bytes. mod_time : Unix time of the last update of the LOM file. fname : path to the LOM file. db_files : information about the proton.db file: id : used proton.db version. size : proton.db file size in bytes. mod_time : Unix time of the last update of the proton.db file. fname : path to the proton.db file. startid : randomly-generated unique ID of the WAF node. The data of all counters is accumulated from the moment NGINX is started. If Wallarm has been installed in a ready-made infrastructure with NGINX, the NGINX server must be restarted to start Wallarm.","title":"Working with the Statistics Service"},{"location":"admin-en/configure-wallarm-mode/","text":"Filtration mode configuration \u00b6 Filtration mode defines the WAF node behavior when processing incoming requests. These instructions describe available filtration modes and their configuration methods. Available filtration modes \u00b6 The WAF node can process incoming requests in the following modes (from the mildest to the strictest): Disabled ( off ) \u2192 the WAF node: Does not analyze whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . Blocks all requests originated from blacklisted IP addresses . Monitoring ( monitoring ) \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks all requests originated from blacklisted IP addresses . Safe blocking ( safe_blocking ) \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks all requests originated from blacklisted IP addresses . Blocks requests containing malicious payloads if they are originated from greylisted IP addresses . Blocking ( block ) \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks requests containing malicious payloads. Blocks all requests originated from blacklisted IP addresses . Methods of the filtration mode configuration \u00b6 The filtration mode can be configured in the following ways: Assign a value to the wallarm_mode directive in the WAF node configuration file Define the general filtration rule in the Wallarm Console Create a filtration mode rule in the Rules section of the Wallarm Console Priorities of the filtration mode configuration methods are determined in the wallarm_mode_allow_override directive . By default, the settings specified in the Wallarm Console have a higher priority than the walalrm_mode directive regardless of its value severity. Specifying the filtration mode in the wallarm_mode directive \u00b6 Using the wallarm_mode directive in the WAF node configuration file, you can define filtration modes for different contexts. These contexts are ordered from the most global to the most local in the following list: http : the directives inside the http block are applied to the requests sent to the HTTP server. server : the directives inside the server block are applied to the requests sent to the virtual server. location : the directives inside the location block are only applied to the requests containing that particular path. If different wallarm_mode directive values are defined for the http , server , and location blocks, the most local configuration has the highest priority. The wallarm_mode directive usage example: http { wallarm_mode monitoring ; server { server_name SERVER_A ; } server { server_name SERVER_B ; wallarm_mode off ; } server { server_name SERVER_C ; wallarm_mode off ; location /main/content { wallarm_mode monitoring ; } location /main/login { wallarm_mode block ; } location /main/reset-password { wallarm_mode safe_blocking ; } } } In this example, the filtration modes are defined for the resources as follows: The monitoring mode is applied to the requests sent to the HTTP server. The monitoring mode is applied to the requests sent to the virtual server SERVER_A . The off mode is applied to the requests sent to the virtual server SERVER_B . The off mode is applied to the requests sent to the virtual server SERVER_C , except for the requests that contain the /main/content , /main/login , or the /main/reset-password path. The monitoring mode is applied to the requests sent to the virtual server SERVER_C that contain the /main/content path. The block mode is applied to the requests sent to the virtual server SERVER_C that contain the /main/login path. The safe_blocking mode is applied to the requests sent to the virtual server SERVER_C that contain the /main/reset-password path. Setting up the general filtration rule in the Wallarm Console \u00b6 The radio buttons on the General tab of the Wallarm Console settings in the EU Wallarm Cloud or US Wallarm Cloud define the general filtration mode for all incoming requests. The wallarm_mode directive value defined in the http block in the configuration file has the same action scope as these buttons. The local filtration mode settings on the Rules tab of the Wallarm Console have higher priority than the global settings on the Global tab. On the General tab, you can specify one of the following filtration modes: Local settings (default) : filtration mode defined using the wallarm_mode directive is applied Monitoring Safe blocking Blocking The Wallarm Cloud and WAF node synchronization The rules defined in the Wallarm Console are applied during the Wallarm Cloud and WAF node synchronization process, which is conducted once every 2\u20114 minutes. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 Setting up the filtration rules on the \"Rules\" tab \u00b6 You can fine-tune the filtration mode for processing requests that meet your custom conditions on the Rules tab of the Wallarm Console. These rules have higher priority than the general filtration rule set in the Wallarm Console . Details on working with rules on the Rules tab \u2192 Step-by-step guide for creating a rule that manages the filtration mode \u2192 The Wallarm Cloud and WAF node synchronization The rules defined in the Wallarm Console are applied during the Wallarm Cloud and WAF node synchronization process, which is conducted once every 2\u20114 minutes. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 Setting up priorities of the filtration mode configuration methods using wallarm_mode_allow_override \u00b6 The wallarm_mode_allow_override directive manages the ability to apply rules that are defined on the Wallarm Console instead of using the wallarm_mode directive values from the WAF node configuration file. The following values are valid for the wallarm_mode_allow_override directive: off : rules specified in the Wallarm Console are ignored. Rules specified by the wallarm_mode directive in the configuration file are applied. strict : only the rules specified in the Wallarm Cloud that define stricter filtration modes than those defined by the wallarm_mode directive in the configuration file are applied. The available filtration modes ordered from the mildest to the strictest are listed above . on (by default): rules specified in the Wallarm Console are applied. Rules specified by the wallarm_mode directive in the configuration file are ignored. The contexts in which the wallarm_mode_allow_override directive value can be defined, in order from the most global to the most local, are presented in the following list: http : the directives inside the http block are applied to the requests sent to the HTTP server. server : the directives inside the server block are applied to the requests sent to the virtual server. location : the directives inside the location block are only applied to the requests containing that particular path. If different wallarm_mode_allow_override directive values are defined in the http , server , and location blocks, the most local configuration has the highest priority. The wallarm_mode_allow_override directive usage example: http { wallarm_mode monitoring ; server { server_name SERVER_A ; wallarm_mode_allow_override off ; } server { server_name SERVER_B ; wallarm_mode_allow_override on ; location /main/login { wallarm_mode_allow_override strict ; } } } This configuration example results in the following applications of the filtration mode rules from the Wallarm Console: The filtration mode rules defined in the Wallarm Console are ignored for requests sent to the virtual server SERVER_A . There is no wallarm_mode directive specified in the server block that corresponds to the SERVER_A server, which is why the monitoring filtration mode specified in the http block is applied for such requests. The filtration mode rules defined in the Wallarm Console are applied to the requests sent to the virtual server SERVER_B except for the requests that contain the /main/login path. For those requests that are sent to the virtual server SERVER_B and contain the /main/login path, the filtration mode rules defined in the Wallarm Console are only applied if they define a filtration mode that is stricter than the monitoring mode. Filtration mode configuration example \u00b6 Let us consider the example of a filtration mode configuration that uses all of the methods mentioned above. Setting up filtration mode in the WAF node configuration file \u00b6 http { wallarm_mode block ; server { server_name SERVER_A ; wallarm_mode monitoring ; wallarm_mode_allow_override off ; location /main/login { wallarm_mode block ; wallarm_mode_allow_override strict ; } location /main/signup { wallarm_mode_allow_override strict ; } location /main/apply { wallarm_mode block ; wallarm_mode_allow_override on ; } } } Setting up filtration mode in the Wallarm Console \u00b6 General filtration rule : Monitoring . Filtration rules : If the request meets the following conditions: Method: POST First part of the path: main Second part of the path: apply , then apply the Default filtration mode. If the request meets the following condition: First part of the path: main , then apply the Blocking filtration mode. If the request meets the following conditions: First part of the path: main Second part of the path: login , then apply the Monitoring filtration mode. Examples of requests sent to the server SERVER_A \u00b6 Examples of the requests sent to the configured server SERVER_A and the actions that the Wallarm WAF node applies to them are the following: The malicious request with the /news path is processed but not blocked due to the wallarm_mode monitoring; setting for the server SERVER_A . The malicious request with the /main path is processed but not blocked due to the wallarm_mode monitoring; setting for the server SERVER_A . The Blocking rule defined in the Wallarm Console is not applied to it due to the wallarm_mode_allow_override off; setting for the server SERVER_A . The malicious request with the /main/login path is blocked due to the wallarm_mode block; setting for the requests with the /main/login path. The Monitoring rule defined in the Wallarm Console is not applied to it due to the wallarm_mode_allow_override strict; setting in the WAF node configuration file. The malicious request with the /main/signup path is blocked due to the wallarm_mode_allow_override strict; setting for the requests with the /main/signup path and the Blocking rule defined in the Wallarm Console for the requests with the /main path. The malicious request with the /main/apply path and the GET method is blocked due to the wallarm_mode_allow_override on; setting for the requests with the /main/apply path and the Blocking rule defined in the Wallarm Console for the requests with the /main path. The malicious request with the /main/apply path and the POST method is blocked due to the wallarm_mode_allow_override on; setting for those requests with the /main/apply path, the Default rule defined in the Wallarm Console, and the wallarm_mode block; setting for the requests with the /main/apply path in the WAF node configuration file.","title":"Filtration mode configuration"},{"location":"admin-en/configure-wallarm-mode/#filtration-mode-configuration","text":"Filtration mode defines the WAF node behavior when processing incoming requests. These instructions describe available filtration modes and their configuration methods.","title":"Filtration mode configuration"},{"location":"admin-en/configure-wallarm-mode/#available-filtration-modes","text":"The WAF node can process incoming requests in the following modes (from the mildest to the strictest): Disabled ( off ) \u2192 the WAF node: Does not analyze whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . Blocks all requests originated from blacklisted IP addresses . Monitoring ( monitoring ) \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks all requests originated from blacklisted IP addresses . Safe blocking ( safe_blocking ) \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks all requests originated from blacklisted IP addresses . Blocks requests containing malicious payloads if they are originated from greylisted IP addresses . Blocking ( block ) \u2192 the WAF node: Analyzes whether incoming requests contain malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . If malicious requests are detected, the WAF node uploads them to the Wallarm Cloud. Blocks requests containing malicious payloads. Blocks all requests originated from blacklisted IP addresses .","title":"Available filtration modes"},{"location":"admin-en/configure-wallarm-mode/#methods-of-the-filtration-mode-configuration","text":"The filtration mode can be configured in the following ways: Assign a value to the wallarm_mode directive in the WAF node configuration file Define the general filtration rule in the Wallarm Console Create a filtration mode rule in the Rules section of the Wallarm Console Priorities of the filtration mode configuration methods are determined in the wallarm_mode_allow_override directive . By default, the settings specified in the Wallarm Console have a higher priority than the walalrm_mode directive regardless of its value severity.","title":"Methods of the filtration mode configuration"},{"location":"admin-en/configure-wallarm-mode/#specifying-the-filtration-mode-in-the-wallarm_mode-directive","text":"Using the wallarm_mode directive in the WAF node configuration file, you can define filtration modes for different contexts. These contexts are ordered from the most global to the most local in the following list: http : the directives inside the http block are applied to the requests sent to the HTTP server. server : the directives inside the server block are applied to the requests sent to the virtual server. location : the directives inside the location block are only applied to the requests containing that particular path. If different wallarm_mode directive values are defined for the http , server , and location blocks, the most local configuration has the highest priority. The wallarm_mode directive usage example: http { wallarm_mode monitoring ; server { server_name SERVER_A ; } server { server_name SERVER_B ; wallarm_mode off ; } server { server_name SERVER_C ; wallarm_mode off ; location /main/content { wallarm_mode monitoring ; } location /main/login { wallarm_mode block ; } location /main/reset-password { wallarm_mode safe_blocking ; } } } In this example, the filtration modes are defined for the resources as follows: The monitoring mode is applied to the requests sent to the HTTP server. The monitoring mode is applied to the requests sent to the virtual server SERVER_A . The off mode is applied to the requests sent to the virtual server SERVER_B . The off mode is applied to the requests sent to the virtual server SERVER_C , except for the requests that contain the /main/content , /main/login , or the /main/reset-password path. The monitoring mode is applied to the requests sent to the virtual server SERVER_C that contain the /main/content path. The block mode is applied to the requests sent to the virtual server SERVER_C that contain the /main/login path. The safe_blocking mode is applied to the requests sent to the virtual server SERVER_C that contain the /main/reset-password path.","title":"Specifying the filtration mode in the wallarm_mode directive"},{"location":"admin-en/configure-wallarm-mode/#setting-up-the-general-filtration-rule-in-the-wallarm-console","text":"The radio buttons on the General tab of the Wallarm Console settings in the EU Wallarm Cloud or US Wallarm Cloud define the general filtration mode for all incoming requests. The wallarm_mode directive value defined in the http block in the configuration file has the same action scope as these buttons. The local filtration mode settings on the Rules tab of the Wallarm Console have higher priority than the global settings on the Global tab. On the General tab, you can specify one of the following filtration modes: Local settings (default) : filtration mode defined using the wallarm_mode directive is applied Monitoring Safe blocking Blocking The Wallarm Cloud and WAF node synchronization The rules defined in the Wallarm Console are applied during the Wallarm Cloud and WAF node synchronization process, which is conducted once every 2\u20114 minutes. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192","title":"Setting up the general filtration rule in the Wallarm Console"},{"location":"admin-en/configure-wallarm-mode/#setting-up-the-filtration-rules-on-the-rules-tab","text":"You can fine-tune the filtration mode for processing requests that meet your custom conditions on the Rules tab of the Wallarm Console. These rules have higher priority than the general filtration rule set in the Wallarm Console . Details on working with rules on the Rules tab \u2192 Step-by-step guide for creating a rule that manages the filtration mode \u2192 The Wallarm Cloud and WAF node synchronization The rules defined in the Wallarm Console are applied during the Wallarm Cloud and WAF node synchronization process, which is conducted once every 2\u20114 minutes. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192","title":"Setting up the filtration rules on the \"Rules\" tab"},{"location":"admin-en/configure-wallarm-mode/#setting-up-priorities-of-the-filtration-mode-configuration-methods-using-wallarm_mode_allow_override","text":"The wallarm_mode_allow_override directive manages the ability to apply rules that are defined on the Wallarm Console instead of using the wallarm_mode directive values from the WAF node configuration file. The following values are valid for the wallarm_mode_allow_override directive: off : rules specified in the Wallarm Console are ignored. Rules specified by the wallarm_mode directive in the configuration file are applied. strict : only the rules specified in the Wallarm Cloud that define stricter filtration modes than those defined by the wallarm_mode directive in the configuration file are applied. The available filtration modes ordered from the mildest to the strictest are listed above . on (by default): rules specified in the Wallarm Console are applied. Rules specified by the wallarm_mode directive in the configuration file are ignored. The contexts in which the wallarm_mode_allow_override directive value can be defined, in order from the most global to the most local, are presented in the following list: http : the directives inside the http block are applied to the requests sent to the HTTP server. server : the directives inside the server block are applied to the requests sent to the virtual server. location : the directives inside the location block are only applied to the requests containing that particular path. If different wallarm_mode_allow_override directive values are defined in the http , server , and location blocks, the most local configuration has the highest priority. The wallarm_mode_allow_override directive usage example: http { wallarm_mode monitoring ; server { server_name SERVER_A ; wallarm_mode_allow_override off ; } server { server_name SERVER_B ; wallarm_mode_allow_override on ; location /main/login { wallarm_mode_allow_override strict ; } } } This configuration example results in the following applications of the filtration mode rules from the Wallarm Console: The filtration mode rules defined in the Wallarm Console are ignored for requests sent to the virtual server SERVER_A . There is no wallarm_mode directive specified in the server block that corresponds to the SERVER_A server, which is why the monitoring filtration mode specified in the http block is applied for such requests. The filtration mode rules defined in the Wallarm Console are applied to the requests sent to the virtual server SERVER_B except for the requests that contain the /main/login path. For those requests that are sent to the virtual server SERVER_B and contain the /main/login path, the filtration mode rules defined in the Wallarm Console are only applied if they define a filtration mode that is stricter than the monitoring mode.","title":"Setting up priorities of the filtration mode configuration methods using wallarm_mode_allow_override"},{"location":"admin-en/configure-wallarm-mode/#filtration-mode-configuration-example","text":"Let us consider the example of a filtration mode configuration that uses all of the methods mentioned above.","title":"Filtration mode configuration example"},{"location":"admin-en/configure-wallarm-mode/#setting-up-filtration-mode-in-the-waf-node-configuration-file","text":"http { wallarm_mode block ; server { server_name SERVER_A ; wallarm_mode monitoring ; wallarm_mode_allow_override off ; location /main/login { wallarm_mode block ; wallarm_mode_allow_override strict ; } location /main/signup { wallarm_mode_allow_override strict ; } location /main/apply { wallarm_mode block ; wallarm_mode_allow_override on ; } } }","title":"Setting up filtration mode in the WAF node configuration file"},{"location":"admin-en/configure-wallarm-mode/#setting-up-filtration-mode-in-the-wallarm-console","text":"General filtration rule : Monitoring . Filtration rules : If the request meets the following conditions: Method: POST First part of the path: main Second part of the path: apply , then apply the Default filtration mode. If the request meets the following condition: First part of the path: main , then apply the Blocking filtration mode. If the request meets the following conditions: First part of the path: main Second part of the path: login , then apply the Monitoring filtration mode.","title":"Setting up filtration mode in the Wallarm Console"},{"location":"admin-en/configure-wallarm-mode/#examples-of-requests-sent-to-the-server-server_a","text":"Examples of the requests sent to the configured server SERVER_A and the actions that the Wallarm WAF node applies to them are the following: The malicious request with the /news path is processed but not blocked due to the wallarm_mode monitoring; setting for the server SERVER_A . The malicious request with the /main path is processed but not blocked due to the wallarm_mode monitoring; setting for the server SERVER_A . The Blocking rule defined in the Wallarm Console is not applied to it due to the wallarm_mode_allow_override off; setting for the server SERVER_A . The malicious request with the /main/login path is blocked due to the wallarm_mode block; setting for the requests with the /main/login path. The Monitoring rule defined in the Wallarm Console is not applied to it due to the wallarm_mode_allow_override strict; setting in the WAF node configuration file. The malicious request with the /main/signup path is blocked due to the wallarm_mode_allow_override strict; setting for the requests with the /main/signup path and the Blocking rule defined in the Wallarm Console for the requests with the /main path. The malicious request with the /main/apply path and the GET method is blocked due to the wallarm_mode_allow_override on; setting for the requests with the /main/apply path and the Blocking rule defined in the Wallarm Console for the requests with the /main path. The malicious request with the /main/apply path and the POST method is blocked due to the wallarm_mode_allow_override on; setting for those requests with the /main/apply path, the Default rule defined in the Wallarm Console, and the wallarm_mode block; setting for the requests with the /main/apply path in the WAF node configuration file.","title":"Examples of requests sent to the server SERVER_A"},{"location":"admin-en/installation-ami-en/","text":"Deploying as an Amazon Machine Image (AMI) \u00b6 To deploy an Amazon Machine Image with a filter node, perform the following steps: Log in to your Amazon Web Services account. Create a pair of SSH keys. Create a security group. Launch a filter node instance. Connect to the filter node instance via SSH. Connect the filter node to the Wallarm Cloud. Set up the filter node for using a proxy server. Set up filtering and proxying rules. Allocate instance memory for the Wallarm Node. Configure logging. Restart NGINX. 1. Log In to Your Amazon Web Services Account \u00b6 Log in to aws.amazon.com . 2. Create a Pair of SSH Keys \u00b6 During the deploying process, you will need to connect to the virtual machine via SSH. Amazon EC2 allows creating a named pair of public and private SSH keys that can be used to connect to the instance. To create a key pair, do the following: Navigate to the Key pairs tab on the Amazon EC2 dashboard. Click the Create Key Pair button. Enter a key pair name and click the Create button. A private SSH key in PEM format will automatically start to download. Save the key to connect to the created instance in the future. Creating SSH keys To see detailed information about creating SSH keys, proceed to this link . 3. Create a Security Group \u00b6 A Security Group defines allowed and forbidden incoming and outgoing connections for virtual machines. The final list of connections depends on the protected application (e.g., allowing all of the incoming connections to the TCP/80 and TCP/443 ports). Rules for outgoing connections from the security group When creating a security group, all of the outgoing connections are allowed by default. If you restrict outgoing connections from the filter node, make sure that it is granted access to a Wallarm API server. The choice of a Wallarm API server depends on the Wallarm Cloud you are using: If you are using the EU cloud, your node needs to be granted access to api.wallarm.com:444 . If you are using the US cloud, your node needs to be granted access to us1.api.wallarm.com:444 . The filter node requires access to a Wallarm API server for proper operation. Create a security group for the filter node. To do this, proceed with the following steps: Navigate to the Security Groups tab on the Amazon EC2 dashboard and click the Create Security Group button. Enter a security group name and an optional description into the dialog window that appears. Select the required VPC. Configure incoming and outgoing connections rules on the Inbound and Outbound tabs. Click the Create button to create the security group. To see detailed information about creating a security group, proceed to this link . 4. Launch a Filter Node Instance \u00b6 If Wallarm WAF is already deployed If you launch Wallarm WAF instead of the already existing Wallarm WAF or need to duplicate the deployment in the same environment, please keep the same WAF version as currently used or update the version of all deployments to the latest. To check the launched version, connect to the running instance and execute the following command: apt list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update all Wallarm WAF instances to the latest version. If the version 2.16.x or lower is installed, then please update all Wallarm WAF instances to the latest version. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . To launch an instance with the WAF node, proceed to this link and subscribe to the WAF node 3.0.0. When creating an instance, you need to specify the previously created security group. To do this, perform the following actions: While working with the Launch Instance Wizard, proceed to the 6. Configure Security Group instance launch step by clicking the corresponding tab. Choose the \u201cSelect an existing security group\u201d option in the Assign a security group setting. Select the security group from the list that appears. After specifying all of the required instance settings, click the \u201cReview and Launch\u201d button, make sure that instance is configured correctly, and click the \u201cLaunch\u201d button. In the window that appears, specify the previously created key pair by performing the following actions: In the first drop-down list, select the \u201cChoose an existing key pair\u201d option. In the second drop-down list, select the name of the key pair. Make sure you have access to the private key in PEM format from the key pair you specified in the second drop-down list and tick the checkbox to confirm this. Click the Launch Instances button. The instance will launch with the preinstalled filter node. To see detailed information about launching instances in AWS, proceed to this link . 5. Connect to the Filter Node Instance via SSH \u00b6 You need to use the \u201cadmin\u201d username to connect to the instance. Using the key to connect via SSH Use the private key in PEM format that you created earlier to connect to the instance via SSH. This must be the private key from the SSH key pair that you specified when creating an instance. To see detailed information about ways to connect to an instance, proceed to this link . 6. Connect the Filter Node to the Wallarm Cloud \u00b6 The filter node interacts with the Wallarm cloud. There are two ways of connecting the node to the cloud: Using the filter node token Using your cloud account login and password Required access rights Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled, therefore allowing you to connect a filter node to the cloud. You can check the aforementioned parameters by navigating to the user account list in the Wallarm console. If you are using https://my.wallarm.com/ , proceed to the following link to check your user settings. If you are using https://us1.my.wallarm.com/ , proceed to the following link to check your user settings. Connecting Using the Filter Node Token \u00b6 To connect the node to the cloud using the token, proceed with the following steps: Create a new node on the Nodes tab of Wallarm web interface. Click the Create new node button. In the form that appears, enter the node name into the corresponding field and select the \u201cCloud\u201d type of installation from the drop-down list. Click the Create button. In the window that appears, click the Copy button next to the field with the token to add the token of the newly created filter node to your clipboard. On the virtual machine run the addcloudnode script: Info You have to pick which script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the EU Cloud tab below. If you are using https://us1.my.wallarm.com/ , run the script from the US Cloud tab below. EU Cloud sudo /usr/share/wallarm-common/addcloudnode US Cloud sudo /usr/share/wallarm-common/addcloudnode -H us1.api.wallarm.com Paste the filter node token from your clipboard. Your WAF node will now synchronize with the Cloud every 2\u20114 minutes according to the default synchronization configuration. WAF node and Cloud synchronization configuration After running the addcloudnode script, the /etc/wallarm/syncnode file containing the WAF node and Cloud synchronization settings will be created. The settings of the WAF node and Cloud synchronization can be changed via the /etc/wallarm/syncnode file. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 Connecting Using Your Cloud Account Login and Password \u00b6 To connect the node to the cloud using your cloud account requisites, proceed with the following steps: On the virtual machine run the addnode script: Info You have to pick which script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the \u00ab EU Cloud tab below. If you are using https://us1.my.wallarm.com/ , run the script from the US Cloud tab below. EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Provide your Wallarm account\u2019s login and password when prompted. API Access The API choice for your filter node depends on the Cloud you are using. Please, select the API accordingly: If you are using https://my.wallarm.com/ , your node requires access to https://api.wallarm.com:444 . If you are using https://us1.my.wallarm.com/ , your node requires access to https://us1.api.wallarm.com:444 . Ensure the access is not blocked by a firewall. Your WAF node will now synchronize with the Cloud every 2\u20114 minutes according to the default synchronization configuration. WAF node and Cloud synchronization configuration After running the addnode script, the /etc/wallarm/node.yaml file containing the WAF node and Cloud synchronization settings and other settings required for a correct WAF node operation will be created. The settings of the WAF node and Cloud synchronization can be changed via the /etc/wallarm/node.yaml file and system environment variables. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 7. Set Up the Filter Node for Using a Proxy Server \u00b6 Info This setup step is intended for users who use their own proxy server for the operation of the protected web applications. If you do not use a proxy server, skip this step of the setup. You need to assign new values to the environment variables, which define the proxy server used, to configure Wallarm node for using your proxy server. Add new values of the environment variables to the /etc/environment file: Add https_proxy to define a proxy for the https protocol. Add http_proxy to define a proxy for the http protocol. Add no_proxy to define the list of the resources proxy should not be used for. Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables. <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for. <proxy_user> defines the username for proxy authorization. <proxy_pass> defines the password for proxy authorization. <host> defines a host of the proxy server. <port> defines a port of the proxy server. Assign a \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" array value, where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains, to the no_proxy variable to define a list of the resources which proxy should not be used for. This array should consist of IP addresses and/or domains. Resources that need to be addressed without a proxy Add the following IP addresses and domain to the list of the resources that have to be addressed without a proxy for the system to operate correctly: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . The 127.0.0.8 and 127.0.0.9 IP addresses are used for the operation of the Wallarm filter node. The example of the correct /etc/environment file contents below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy=http://admin:01234@1.2.3.4:1234 http_proxy=http://admin:01234@1.2.3.4:1234 no_proxy=\"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\" 8. Set Up Filtering and Proxying Rules \u00b6 The following files contain NGINX and WAF node settings: /etc/nginx/nginx.conf defines the configuration of NGINX /etc/nginx/conf.d/wallarm.conf defines the global configuration of Wallarm WAF node /etc/nginx/conf.d/wallarm-status.conf defines the WAF node monitoring service configuration You can create your own configuration files to define the operation of NGINX and Wallarm. It is recommended to create a separate configuration file with the server block for each group of the domains that should be processed in the same way. To see detailed information about working with NGINX configuration files, proceed to the official NGINX documentation . Wallarm directives define the operation logic of the Wallarm filter node. To see the list of Wallarm directives available, proceed to the Wallarm configuration options page. A Configuration File Example \u00b6 Let us suppose that you need to configure the server to work in the following conditions: Only HTTP traffic is processed. There are no HTTPS requests processed. The following domains receive the requests: example.com and www.example.com . All requests must be passed to the server 10.80.0.5 . All incoming requests are considered less than 1MB in size (default setting). The processing of a request takes no more than 60 seconds (default setting). Wallarm must operate in the monitor mode. Clients access the filter node directly, without an intermediate HTTP load balancer. Creating a configuration file You can create a custom NGINX configuration file (e.g. example.com.conf ) or modify the default NGINX configuration file ( default.conf ). When creating a custom configuration file, make sure that NGINX listens to the incoming connections on the free port. To meet the listed conditions, the contents of the configuration file must be the following: server { listen 80; listen [::]:80 ipv6only=on; # the domains for which traffic is processed server_name example.com; server_name www.example.com; # turn on the monitoring mode of traffic processing wallarm_mode monitoring; # wallarm_instance 1; location / { # setting the address for request forwarding proxy_pass http://10.80.0.5; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 9. Instance Memory Allocation for the Wallarm Node \u00b6 Filter node uses the in-memory storage Tarantool. By default, the amount of RAM allocated to Tarantool is 40% of the total instance memory. You can change the amount of RAM allocated for Tarantool. To allocate the instance RAM to Tarantool: Open the Tarantool configuration file: sudo vim /etc/default/wallarm-tarantool Set the amount of allocated RAM in the SLAB_ALLOC_ARENA in GB. The value can be an integer or a float (a dot . is a decimal separator). For example, to set 24 GB: SLAB_ALLOC_ARENA=24 To apply changes, restart the Tarantool daemon: sudo systemctl restart wallarm-tarantool 10. Configure Logging \u00b6 Configure the filter node variables logging using NGINX. This will allow to perform a quick filter node diagnostics with the help of the NGINX log file. 11. Restart NGINX \u00b6 Restart NGINX by running the following command: sudo systemctl restart nginx The Installation Is Complete \u00b6 The installation is now complete. Check that the filter node runs and filters the traffic. See Check the filter node operation . Default Settings A freshly installed filter node operates in blocking mode (see the wallarm_mode directive description) by default. This may result in the inoperable Wallarm scanner . If you plan to use the scanner, then you need to perform additional actions to render scanner operational. Additional Settings \u00b6 The filter node may require some additional configuration after installation. The document below lists a few of the typical setups that you can apply if needed. To get more information about other available settings, proceed to the \u201cConfiguration\u201d section of the Administrator\u2019s Guide . Configuring the Display of the Client's Real IP \u00b6 If the filter node is deployed behind a proxy server or load balancer without any additional configuration, the request source address may not be equal to the actual IP address of the client. Instead, it may be equal to one of the IP addresses of the proxy server or the load balancer. In this case, if you want the filter node to receive the client's IP address as a request source address, you need to perform an additional configuration of the proxy server or the load balancer. Limiting the Single Request Processing Time \u00b6 Use the wallarm_process_time_limit Wallarm directive to specify the limit of the duration for processing a single request by the filter node. If processing the request consumes more time than specified in the directive, then the information on the error is entered into the log file and the request is marked as an overlimit_res attack. Limiting the Server Reply Waiting Time \u00b6 Use the proxy_read_timeout NGINX directive to specify the timeout for reading the proxy server reply. If the server sends nothing during this time, the connection is closed. Limiting the Maximum Request Size \u00b6 Use the client_max_body_size NGINX directive to specify the limit for the maximum size of the body of the client's request. If this limit is exceeded, NGINX replies to the client with the 413 ( Payload Too Large ) code, also known as the Request Entity Too Large message.","title":"AWS Marketplace image deployment"},{"location":"admin-en/installation-ami-en/#deploying-as-an-amazon-machine-image-ami","text":"To deploy an Amazon Machine Image with a filter node, perform the following steps: Log in to your Amazon Web Services account. Create a pair of SSH keys. Create a security group. Launch a filter node instance. Connect to the filter node instance via SSH. Connect the filter node to the Wallarm Cloud. Set up the filter node for using a proxy server. Set up filtering and proxying rules. Allocate instance memory for the Wallarm Node. Configure logging. Restart NGINX.","title":"Deploying as an Amazon Machine Image (AMI)"},{"location":"admin-en/installation-ami-en/#1-log-in-to-your-amazon-web-services-account","text":"Log in to aws.amazon.com .","title":"1. Log In to Your Amazon Web Services Account"},{"location":"admin-en/installation-ami-en/#2-create-a-pair-of-ssh-keys","text":"During the deploying process, you will need to connect to the virtual machine via SSH. Amazon EC2 allows creating a named pair of public and private SSH keys that can be used to connect to the instance. To create a key pair, do the following: Navigate to the Key pairs tab on the Amazon EC2 dashboard. Click the Create Key Pair button. Enter a key pair name and click the Create button. A private SSH key in PEM format will automatically start to download. Save the key to connect to the created instance in the future. Creating SSH keys To see detailed information about creating SSH keys, proceed to this link .","title":"2. Create a Pair of SSH Keys"},{"location":"admin-en/installation-ami-en/#3-create-a-security-group","text":"A Security Group defines allowed and forbidden incoming and outgoing connections for virtual machines. The final list of connections depends on the protected application (e.g., allowing all of the incoming connections to the TCP/80 and TCP/443 ports). Rules for outgoing connections from the security group When creating a security group, all of the outgoing connections are allowed by default. If you restrict outgoing connections from the filter node, make sure that it is granted access to a Wallarm API server. The choice of a Wallarm API server depends on the Wallarm Cloud you are using: If you are using the EU cloud, your node needs to be granted access to api.wallarm.com:444 . If you are using the US cloud, your node needs to be granted access to us1.api.wallarm.com:444 . The filter node requires access to a Wallarm API server for proper operation. Create a security group for the filter node. To do this, proceed with the following steps: Navigate to the Security Groups tab on the Amazon EC2 dashboard and click the Create Security Group button. Enter a security group name and an optional description into the dialog window that appears. Select the required VPC. Configure incoming and outgoing connections rules on the Inbound and Outbound tabs. Click the Create button to create the security group. To see detailed information about creating a security group, proceed to this link .","title":"3. Create a Security Group"},{"location":"admin-en/installation-ami-en/#4-launch-a-filter-node-instance","text":"If Wallarm WAF is already deployed If you launch Wallarm WAF instead of the already existing Wallarm WAF or need to duplicate the deployment in the same environment, please keep the same WAF version as currently used or update the version of all deployments to the latest. To check the launched version, connect to the running instance and execute the following command: apt list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update all Wallarm WAF instances to the latest version. If the version 2.16.x or lower is installed, then please update all Wallarm WAF instances to the latest version. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . To launch an instance with the WAF node, proceed to this link and subscribe to the WAF node 3.0.0. When creating an instance, you need to specify the previously created security group. To do this, perform the following actions: While working with the Launch Instance Wizard, proceed to the 6. Configure Security Group instance launch step by clicking the corresponding tab. Choose the \u201cSelect an existing security group\u201d option in the Assign a security group setting. Select the security group from the list that appears. After specifying all of the required instance settings, click the \u201cReview and Launch\u201d button, make sure that instance is configured correctly, and click the \u201cLaunch\u201d button. In the window that appears, specify the previously created key pair by performing the following actions: In the first drop-down list, select the \u201cChoose an existing key pair\u201d option. In the second drop-down list, select the name of the key pair. Make sure you have access to the private key in PEM format from the key pair you specified in the second drop-down list and tick the checkbox to confirm this. Click the Launch Instances button. The instance will launch with the preinstalled filter node. To see detailed information about launching instances in AWS, proceed to this link .","title":"4. Launch a Filter Node Instance"},{"location":"admin-en/installation-ami-en/#5-connect-to-the-filter-node-instance-via-ssh","text":"You need to use the \u201cadmin\u201d username to connect to the instance. Using the key to connect via SSH Use the private key in PEM format that you created earlier to connect to the instance via SSH. This must be the private key from the SSH key pair that you specified when creating an instance. To see detailed information about ways to connect to an instance, proceed to this link .","title":"5. Connect to the Filter Node Instance via SSH"},{"location":"admin-en/installation-ami-en/#6-connect-the-filter-node-to-the-wallarm-cloud","text":"The filter node interacts with the Wallarm cloud. There are two ways of connecting the node to the cloud: Using the filter node token Using your cloud account login and password Required access rights Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled, therefore allowing you to connect a filter node to the cloud. You can check the aforementioned parameters by navigating to the user account list in the Wallarm console. If you are using https://my.wallarm.com/ , proceed to the following link to check your user settings. If you are using https://us1.my.wallarm.com/ , proceed to the following link to check your user settings.","title":"6. Connect the Filter Node to the Wallarm Cloud"},{"location":"admin-en/installation-ami-en/#connecting-using-the-filter-node-token","text":"To connect the node to the cloud using the token, proceed with the following steps: Create a new node on the Nodes tab of Wallarm web interface. Click the Create new node button. In the form that appears, enter the node name into the corresponding field and select the \u201cCloud\u201d type of installation from the drop-down list. Click the Create button. In the window that appears, click the Copy button next to the field with the token to add the token of the newly created filter node to your clipboard. On the virtual machine run the addcloudnode script: Info You have to pick which script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the EU Cloud tab below. If you are using https://us1.my.wallarm.com/ , run the script from the US Cloud tab below. EU Cloud sudo /usr/share/wallarm-common/addcloudnode US Cloud sudo /usr/share/wallarm-common/addcloudnode -H us1.api.wallarm.com Paste the filter node token from your clipboard. Your WAF node will now synchronize with the Cloud every 2\u20114 minutes according to the default synchronization configuration. WAF node and Cloud synchronization configuration After running the addcloudnode script, the /etc/wallarm/syncnode file containing the WAF node and Cloud synchronization settings will be created. The settings of the WAF node and Cloud synchronization can be changed via the /etc/wallarm/syncnode file. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192","title":"Connecting Using the Filter Node Token"},{"location":"admin-en/installation-ami-en/#connecting-using-your-cloud-account-login-and-password","text":"To connect the node to the cloud using your cloud account requisites, proceed with the following steps: On the virtual machine run the addnode script: Info You have to pick which script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the \u00ab EU Cloud tab below. If you are using https://us1.my.wallarm.com/ , run the script from the US Cloud tab below. EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Provide your Wallarm account\u2019s login and password when prompted. API Access The API choice for your filter node depends on the Cloud you are using. Please, select the API accordingly: If you are using https://my.wallarm.com/ , your node requires access to https://api.wallarm.com:444 . If you are using https://us1.my.wallarm.com/ , your node requires access to https://us1.api.wallarm.com:444 . Ensure the access is not blocked by a firewall. Your WAF node will now synchronize with the Cloud every 2\u20114 minutes according to the default synchronization configuration. WAF node and Cloud synchronization configuration After running the addnode script, the /etc/wallarm/node.yaml file containing the WAF node and Cloud synchronization settings and other settings required for a correct WAF node operation will be created. The settings of the WAF node and Cloud synchronization can be changed via the /etc/wallarm/node.yaml file and system environment variables. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192","title":"Connecting Using Your Cloud Account Login and Password"},{"location":"admin-en/installation-ami-en/#7-set-up-the-filter-node-for-using-a-proxy-server","text":"Info This setup step is intended for users who use their own proxy server for the operation of the protected web applications. If you do not use a proxy server, skip this step of the setup. You need to assign new values to the environment variables, which define the proxy server used, to configure Wallarm node for using your proxy server. Add new values of the environment variables to the /etc/environment file: Add https_proxy to define a proxy for the https protocol. Add http_proxy to define a proxy for the http protocol. Add no_proxy to define the list of the resources proxy should not be used for. Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables. <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for. <proxy_user> defines the username for proxy authorization. <proxy_pass> defines the password for proxy authorization. <host> defines a host of the proxy server. <port> defines a port of the proxy server. Assign a \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" array value, where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains, to the no_proxy variable to define a list of the resources which proxy should not be used for. This array should consist of IP addresses and/or domains. Resources that need to be addressed without a proxy Add the following IP addresses and domain to the list of the resources that have to be addressed without a proxy for the system to operate correctly: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . The 127.0.0.8 and 127.0.0.9 IP addresses are used for the operation of the Wallarm filter node. The example of the correct /etc/environment file contents below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy=http://admin:01234@1.2.3.4:1234 http_proxy=http://admin:01234@1.2.3.4:1234 no_proxy=\"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\"","title":"7. Set Up the Filter Node for Using a Proxy Server"},{"location":"admin-en/installation-ami-en/#8-set-up-filtering-and-proxying-rules","text":"The following files contain NGINX and WAF node settings: /etc/nginx/nginx.conf defines the configuration of NGINX /etc/nginx/conf.d/wallarm.conf defines the global configuration of Wallarm WAF node /etc/nginx/conf.d/wallarm-status.conf defines the WAF node monitoring service configuration You can create your own configuration files to define the operation of NGINX and Wallarm. It is recommended to create a separate configuration file with the server block for each group of the domains that should be processed in the same way. To see detailed information about working with NGINX configuration files, proceed to the official NGINX documentation . Wallarm directives define the operation logic of the Wallarm filter node. To see the list of Wallarm directives available, proceed to the Wallarm configuration options page.","title":"8. Set Up Filtering and Proxying Rules"},{"location":"admin-en/installation-ami-en/#a-configuration-file-example","text":"Let us suppose that you need to configure the server to work in the following conditions: Only HTTP traffic is processed. There are no HTTPS requests processed. The following domains receive the requests: example.com and www.example.com . All requests must be passed to the server 10.80.0.5 . All incoming requests are considered less than 1MB in size (default setting). The processing of a request takes no more than 60 seconds (default setting). Wallarm must operate in the monitor mode. Clients access the filter node directly, without an intermediate HTTP load balancer. Creating a configuration file You can create a custom NGINX configuration file (e.g. example.com.conf ) or modify the default NGINX configuration file ( default.conf ). When creating a custom configuration file, make sure that NGINX listens to the incoming connections on the free port. To meet the listed conditions, the contents of the configuration file must be the following: server { listen 80; listen [::]:80 ipv6only=on; # the domains for which traffic is processed server_name example.com; server_name www.example.com; # turn on the monitoring mode of traffic processing wallarm_mode monitoring; # wallarm_instance 1; location / { # setting the address for request forwarding proxy_pass http://10.80.0.5; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } }","title":"A Configuration File Example"},{"location":"admin-en/installation-ami-en/#9-instance-memory-allocation-for-the-wallarm-node","text":"Filter node uses the in-memory storage Tarantool. By default, the amount of RAM allocated to Tarantool is 40% of the total instance memory. You can change the amount of RAM allocated for Tarantool. To allocate the instance RAM to Tarantool: Open the Tarantool configuration file: sudo vim /etc/default/wallarm-tarantool Set the amount of allocated RAM in the SLAB_ALLOC_ARENA in GB. The value can be an integer or a float (a dot . is a decimal separator). For example, to set 24 GB: SLAB_ALLOC_ARENA=24 To apply changes, restart the Tarantool daemon: sudo systemctl restart wallarm-tarantool","title":"9. Instance Memory Allocation for the Wallarm Node"},{"location":"admin-en/installation-ami-en/#10-configure-logging","text":"Configure the filter node variables logging using NGINX. This will allow to perform a quick filter node diagnostics with the help of the NGINX log file.","title":"10. Configure Logging"},{"location":"admin-en/installation-ami-en/#11-restart-nginx","text":"Restart NGINX by running the following command: sudo systemctl restart nginx","title":"11. Restart NGINX"},{"location":"admin-en/installation-ami-en/#the-installation-is-complete","text":"The installation is now complete. Check that the filter node runs and filters the traffic. See Check the filter node operation . Default Settings A freshly installed filter node operates in blocking mode (see the wallarm_mode directive description) by default. This may result in the inoperable Wallarm scanner . If you plan to use the scanner, then you need to perform additional actions to render scanner operational.","title":"The Installation Is Complete"},{"location":"admin-en/installation-ami-en/#additional-settings","text":"The filter node may require some additional configuration after installation. The document below lists a few of the typical setups that you can apply if needed. To get more information about other available settings, proceed to the \u201cConfiguration\u201d section of the Administrator\u2019s Guide .","title":"Additional Settings"},{"location":"admin-en/installation-ami-en/#configuring-the-display-of-the-clients-real-ip","text":"If the filter node is deployed behind a proxy server or load balancer without any additional configuration, the request source address may not be equal to the actual IP address of the client. Instead, it may be equal to one of the IP addresses of the proxy server or the load balancer. In this case, if you want the filter node to receive the client's IP address as a request source address, you need to perform an additional configuration of the proxy server or the load balancer.","title":"Configuring the Display of the Client's Real IP"},{"location":"admin-en/installation-ami-en/#limiting-the-single-request-processing-time","text":"Use the wallarm_process_time_limit Wallarm directive to specify the limit of the duration for processing a single request by the filter node. If processing the request consumes more time than specified in the directive, then the information on the error is entered into the log file and the request is marked as an overlimit_res attack.","title":"Limiting the Single Request Processing Time"},{"location":"admin-en/installation-ami-en/#limiting-the-server-reply-waiting-time","text":"Use the proxy_read_timeout NGINX directive to specify the timeout for reading the proxy server reply. If the server sends nothing during this time, the connection is closed.","title":"Limiting the Server Reply Waiting Time"},{"location":"admin-en/installation-ami-en/#limiting-the-maximum-request-size","text":"Use the client_max_body_size NGINX directive to specify the limit for the maximum size of the body of the client's request. If this limit is exceeded, NGINX replies to the client with the 413 ( Payload Too Large ) code, also known as the Request Entity Too Large message.","title":"Limiting the Maximum Request Size"},{"location":"admin-en/installation-check-operation-en/","text":"Checking the Filter Node Operation \u00b6 If everything is configured correctly, Wallarm filters the requests and proxies the filtered requests in accordance with the configuration file settings. To check the correct operation, you must: Execute the wallarm-status request. Run a test attack. 1. Execute the wallarm-status Request \u00b6 You can get filter node operation statistics by requesting the /wallarm-status URL. Run the command: curl http://127.0.0.8/wallarm-status The output will be like: { \"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0, \"requests_lost\":0,\"segfaults\":0,\"memfaults\":0, \"softmemfaults\":0,\"time_detect\":0,\"db_id\":46, \"lom_id\":16767,\"proton_instances\": { \"total\":1,\"success\":1,\"fallback\":0,\"failed\":0 }, \"stalled_workers_count\":0,\"stalled_workers\":[] } This means that the filter node statistics service is running and working properly. The Statistics Service You can read more about the statistics service and how to configure it here . 2. Run a Test Attack \u00b6 To check if Wallarm correctly detects attacks, send an invalid request to the protected resource. For example: http://<resource_URL>/?id='or+1=1--a-<script>prompt(1)</script>' Wallarm must detect in the request the following: SQLI XSS Now the counter of the number of attacks will increase when a request for wallarm-status is executed, which means that the filter node is operating normally. To learn more about the Wallarm filter node settings, see the Configuration Options chapter.","title":"Installation check operation en"},{"location":"admin-en/installation-check-operation-en/#checking-the-filter-node-operation","text":"If everything is configured correctly, Wallarm filters the requests and proxies the filtered requests in accordance with the configuration file settings. To check the correct operation, you must: Execute the wallarm-status request. Run a test attack.","title":"Checking the Filter Node Operation"},{"location":"admin-en/installation-check-operation-en/#1-execute-the-wallarm-status-request","text":"You can get filter node operation statistics by requesting the /wallarm-status URL. Run the command: curl http://127.0.0.8/wallarm-status The output will be like: { \"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0, \"requests_lost\":0,\"segfaults\":0,\"memfaults\":0, \"softmemfaults\":0,\"time_detect\":0,\"db_id\":46, \"lom_id\":16767,\"proton_instances\": { \"total\":1,\"success\":1,\"fallback\":0,\"failed\":0 }, \"stalled_workers_count\":0,\"stalled_workers\":[] } This means that the filter node statistics service is running and working properly. The Statistics Service You can read more about the statistics service and how to configure it here .","title":"1. Execute the wallarm-status Request"},{"location":"admin-en/installation-check-operation-en/#2-run-a-test-attack","text":"To check if Wallarm correctly detects attacks, send an invalid request to the protected resource. For example: http://<resource_URL>/?id='or+1=1--a-<script>prompt(1)</script>' Wallarm must detect in the request the following: SQLI XSS Now the counter of the number of attacks will increase when a request for wallarm-status is executed, which means that the filter node is operating normally. To learn more about the Wallarm filter node settings, see the Configuration Options chapter.","title":"2. Run a Test Attack"},{"location":"admin-en/installation-docker-en/","text":"Running Docker NGINX\u2011based image \u00b6 Image overview \u00b6 The WAF node can be deployed as a Docker container. The Docker container is fat and contains all subsystems of the WAF node. The functionality of the WAF node installed inside the Docker container is completely identical to the functionality of the other deployment options. If the Wallarm WAF image is already deployed in your environment If you deploy the Wallarm WAF image instead of the already deployed image or need to duplicate the deployment, please keep the same WAF version as currently used or update the version of all images to the latest. To check the installed version, run the following command in the container: apt list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update the packages to the latest version in all deployments. If the version 2.16.x or lower is installed, then please update the packages to the latest version in all deployments. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Requirements \u00b6 Access to the account with the Deploy or Administrator role and two\u2011factor authentication disabled in Wallarm Console in the EU Cloud or US Cloud Access to https://api.wallarm.com:444 if working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 if working with US Wallarm Cloud. Please ensure the access is not blocked by a firewall Options for running the container \u00b6 The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container. Run the container passing the environment variables \u00b6 You can pass the following basic WAF node settings to the container via the option -e : Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No To run the image, use the command: EU Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e NGINX_BACKEND = 'example.com' -e TARANTOOL_MEMORY_GB = 16 -p 80 :80 wallarm/node:3.0.0-2 US Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e NGINX_BACKEND = 'example.com' -e WALLARM_API_HOST = 'us1.api.wallarm.com' -e TARANTOOL_MEMORY_GB = 16 -p 80 :80 wallarm/node:3.0.0-2 The command does the following: Automatically creates new WAF node in the Wallarm Cloud. Created WAF node will be displayed in the Wallarm Console \u2192 Nodes . Creates the file default with minimal NGINX configuration and passes WAF node configuration in the /etc/nginx/sites-enabled container directory. Creates files with WAF node credentials to access the Wallarm Cloud in the /etc/wallarm container directory: node.yaml with WAF node UUID and secret key license.key with Wallarm license key Protects the resource http://NGINX_BACKEND:80 . Run the container mounting the configuration file \u00b6 You can mount the prepared configuration file to the Docker container via the -v option. The file must contain the following settings: WAF node directives NGINX settings See an example of the mounted file with minimal settings server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } To run the image: Pass required environment variables to the container via the -e option: Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Mount the directory with the configuration file default to the /etc/nginx/sites-enabled container directory via the -v option. EU Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -v /configs/default:/etc/nginx/sites-enabled/default -p 80 :80 wallarm/node:3.0.0-2 US Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v /configs/default:/etc/nginx/sites-enabled/default -p 80 :80 wallarm/node:3.0.0-2 The command does the following: Automatically creates new WAF node in Wallarm Cloud. Created WAF node will be displayed in the Wallarm Console \u2192 Nodes . Mounts the file default into the /etc/nginx/sites-enabled container directory. Creates files with WAF node credentials to access Wallarm Cloud in the /etc/wallarm container directory: node.yaml with WAF node UUID and secret key license.key with Wallarm license key Protects the resource http://example.com . Mounting other configuration files The container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files If required, you can mount any files to the listed container directories. The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. Logging configuration \u00b6 The logging is enabled by default. The log directories are: /var/log/nginx \u2014 NGINX logs /var/log/wallarm \u2014 Wallarm WAF logs To configure extended logging of the WAF node variables, please use these instructions . By default, the logs rotate once every 24 hours. To set up the log rotation, change the configuration files in /etc/logrotate.d/ . Changing the rotation parameters through environment variables is not possible. Monitoring configuration \u00b6 To monitor the WAF node, there are Nagios\u2011compatible scripts inside the container. See details in Monitoring the WAF node . Example of running the scripts: docker exec -it wallarm-node /usr/lib/nagios-plugins/check_wallarm_tarantool_timeframe -w 1800 -c 900 docker exec -it wallarm-node /usr/lib/nagios-plugins/check_wallarm_export_delay -w 120 -c 300 Testing WAF node operation \u00b6 Send the request with test SQLI and XSS attacks to the protected resource address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Configuring the use cases \u00b6 The configuration file mounted to the Docker container should describe the WAF node configuration in the available directive . Below are some commonly used WAF node configuration options: Configuration of the filtration mode Logging WAF node variables Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Double\u2011detection of attacks with libdetection","title":"Running Docker NGINX\u2011based image"},{"location":"admin-en/installation-docker-en/#running-docker-nginxbased-image","text":"","title":"Running Docker NGINX\u2011based image"},{"location":"admin-en/installation-docker-en/#image-overview","text":"The WAF node can be deployed as a Docker container. The Docker container is fat and contains all subsystems of the WAF node. The functionality of the WAF node installed inside the Docker container is completely identical to the functionality of the other deployment options. If the Wallarm WAF image is already deployed in your environment If you deploy the Wallarm WAF image instead of the already deployed image or need to duplicate the deployment, please keep the same WAF version as currently used or update the version of all images to the latest. To check the installed version, run the following command in the container: apt list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update the packages to the latest version in all deployments. If the version 2.16.x or lower is installed, then please update the packages to the latest version in all deployments. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"Image overview"},{"location":"admin-en/installation-docker-en/#requirements","text":"Access to the account with the Deploy or Administrator role and two\u2011factor authentication disabled in Wallarm Console in the EU Cloud or US Cloud Access to https://api.wallarm.com:444 if working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 if working with US Wallarm Cloud. Please ensure the access is not blocked by a firewall","title":"Requirements"},{"location":"admin-en/installation-docker-en/#options-for-running-the-container","text":"The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container.","title":"Options for running the container"},{"location":"admin-en/installation-docker-en/#run-the-container-passing-the-environment-variables","text":"You can pass the following basic WAF node settings to the container via the option -e : Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No To run the image, use the command: EU Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e NGINX_BACKEND = 'example.com' -e TARANTOOL_MEMORY_GB = 16 -p 80 :80 wallarm/node:3.0.0-2 US Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e NGINX_BACKEND = 'example.com' -e WALLARM_API_HOST = 'us1.api.wallarm.com' -e TARANTOOL_MEMORY_GB = 16 -p 80 :80 wallarm/node:3.0.0-2 The command does the following: Automatically creates new WAF node in the Wallarm Cloud. Created WAF node will be displayed in the Wallarm Console \u2192 Nodes . Creates the file default with minimal NGINX configuration and passes WAF node configuration in the /etc/nginx/sites-enabled container directory. Creates files with WAF node credentials to access the Wallarm Cloud in the /etc/wallarm container directory: node.yaml with WAF node UUID and secret key license.key with Wallarm license key Protects the resource http://NGINX_BACKEND:80 .","title":"Run the container passing the environment variables"},{"location":"admin-en/installation-docker-en/#run-the-container-mounting-the-configuration-file","text":"You can mount the prepared configuration file to the Docker container via the -v option. The file must contain the following settings: WAF node directives NGINX settings See an example of the mounted file with minimal settings server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } To run the image: Pass required environment variables to the container via the -e option: Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Mount the directory with the configuration file default to the /etc/nginx/sites-enabled container directory via the -v option. EU Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -v /configs/default:/etc/nginx/sites-enabled/default -p 80 :80 wallarm/node:3.0.0-2 US Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v /configs/default:/etc/nginx/sites-enabled/default -p 80 :80 wallarm/node:3.0.0-2 The command does the following: Automatically creates new WAF node in Wallarm Cloud. Created WAF node will be displayed in the Wallarm Console \u2192 Nodes . Mounts the file default into the /etc/nginx/sites-enabled container directory. Creates files with WAF node credentials to access Wallarm Cloud in the /etc/wallarm container directory: node.yaml with WAF node UUID and secret key license.key with Wallarm license key Protects the resource http://example.com . Mounting other configuration files The container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files If required, you can mount any files to the listed container directories. The WAF node directives should be described in the /etc/nginx/sites-enabled/default file.","title":"Run the container mounting the configuration file"},{"location":"admin-en/installation-docker-en/#logging-configuration","text":"The logging is enabled by default. The log directories are: /var/log/nginx \u2014 NGINX logs /var/log/wallarm \u2014 Wallarm WAF logs To configure extended logging of the WAF node variables, please use these instructions . By default, the logs rotate once every 24 hours. To set up the log rotation, change the configuration files in /etc/logrotate.d/ . Changing the rotation parameters through environment variables is not possible.","title":"Logging configuration"},{"location":"admin-en/installation-docker-en/#monitoring-configuration","text":"To monitor the WAF node, there are Nagios\u2011compatible scripts inside the container. See details in Monitoring the WAF node . Example of running the scripts: docker exec -it wallarm-node /usr/lib/nagios-plugins/check_wallarm_tarantool_timeframe -w 1800 -c 900 docker exec -it wallarm-node /usr/lib/nagios-plugins/check_wallarm_export_delay -w 120 -c 300","title":"Monitoring configuration"},{"location":"admin-en/installation-docker-en/#testing-waf-node-operation","text":"Send the request with test SQLI and XSS attacks to the protected resource address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"Testing WAF node operation"},{"location":"admin-en/installation-docker-en/#configuring-the-use-cases","text":"The configuration file mounted to the Docker container should describe the WAF node configuration in the available directive . Below are some commonly used WAF node configuration options: Configuration of the filtration mode Logging WAF node variables Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Double\u2011detection of attacks with libdetection","title":"Configuring the use cases"},{"location":"admin-en/installation-gcp-en/","text":"Deploying on Google Cloud Platform (GCP) \u00b6 To deploy a filter node on the Google Cloud Platform, perform the following steps: Log in to your Google Cloud Platform account. Launch a filter node instance. Configure the filter node instance. Connect to the filter node instance via SSH. Connect the filter node to the Wallarm Cloud. Set up the filter node for using a proxy server. Set up filtering and proxying rules Allocate more memory for the Wallarm Node. Configure logging. Restart NGINX. 1. Log In to Your Google Cloud Platform Account \u00b6 Log in to console.cloud.google.com . 2. Launch a Filter Node Instance \u00b6 If Wallarm WAF is already deployed If you launch Wallarm WAF instead of the already existing Wallarm WAF or need to duplicate the deployment in the same environment, please keep the same WAF version as currently used or update the version of all deployments to the latest. To check the launched version, connect to the running instance and execute the following command: apt list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update all Wallarm WAF instances to the latest version. If the version 2.16.x or lower is installed, then please update all Wallarm WAF instances to the latest version. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Launch the instance via the Google Cloud UI \u00b6 To launch the WAF node instance via the Google Cloud UI, please open the WAF node image on the Google Cloud Marketplace and click LAUNCH . The instance will launch with a preinstalled WAF node. To see detailed information on launching instances in the Google Cloud, please proceed to the official Google Cloud Platform documentation . Launch the instance via Terraform or other tools \u00b6 When using a tool like Terraform to launch the WAF node instance using Wallarm GCP image, you may need to provide the name of this image in the Terraform configuration. Image name has the following format: wallarm-node-195710/wallarm-node-<IMAGE_VERSION>-build To launch the instance with the WAF node version 3.0, please use the following image name: wallarm-node-195710/wallarm-node-3-0-20210628-090819 To get the image name, you can also follow these steps: Install Google Cloud SDK . Execute the command gcloud compute images list with the following parameters: gcloud compute images list --project wallarm-node-195710 --filter = \"name~'wallarm-node-3-0-*'\" --no-standard-images Copy the version value from the name of the latest available image and paste the copied value into the provided image name format. For example, the WAF node version 3.0 image will have the following name: wallarm-node-195710/wallarm-node-3-0-20210628-090819 3. Configure the Filter Node Instance \u00b6 Perform the following actions to configure the launched filter node instance: Navigate to the VM instances page in the Compute Engine section of the menu. Select the launched filter node instance and click the Edit button. Allow the required types of incoming traffic by ticking the corresponding checkboxes in the Firewalls setting. If necessary, you can restrict connecting to the instance with the project SSH keys and use a custom SSH key pair for connecting to this instance. To do this, perform the following actions: Tick the \u201cBlock project-wide\u201d checkbox in the SSH Keys setting. Click the Show and edit button in the SSH Keys setting to expand the field for entering an SSH key. Generate a pair of public and private SSH keys. For example, you can use the ssh-keygen and PuTTYgen utilities. Copy an open key in OpenSSH format from the interface of the used key generator (in the current example, the generated public key should be copied from the Public key for pasting into OpenSSH authorized_keys file area of the PuTTYgen interface) and paste it into the field containing the \u201cEnter entire key data\u201d hint. Save the private key. It will be required for connecting to the configured instance in the future. Click the Save button at the bottom of the page to apply the changes. 4. Connect to the Filter Node Instance via SSH \u00b6 To see detailed information about ways of connecting to instances, proceed to this link . Connecting to the instance via a custom private key If during base instance creation process you have enabled connection to the instance via a custom SSH key pair, make sure you have access to the private key from this key pair. 5. Connect the Filter Node to the Wallarm Cloud \u00b6 The filter node interacts with the Wallarm cloud. There are two ways of connecting the node to the cloud: Using the filter node token Using your cloud account login and password Required access rights Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled, therefore allowing you to connect a filter node to the cloud. You can check the aforementioned parameters by navigating to the user account list in the Wallarm console. If you are using https://my.wallarm.com/ , proceed to the following link to check your user settings. If you are using https://us1.my.wallarm.com/ , proceed to the following link to check your user settings. Connecting Using the Filter Node Token \u00b6 To connect the node to the cloud using the token, proceed with the following steps: Create a new node on the Nodes tab of Wallarm web interface. Click the Create new node button. In the form that appears, enter the node name into the corresponding field and select the \u201cCloud\u201d type of installation from the drop-down list. Click the Create button. In the window that appears, click the Copy button next to the field with the token to add the token of the newly created filter node to your clipboard. On the virtual machine run the addcloudnode script: Info You have to pick which script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the EU Cloud tab below. If you are using https://us1.my.wallarm.com/ , run the script from the US Cloud tab below. EU Cloud sudo /usr/share/wallarm-common/addcloudnode US Cloud sudo /usr/share/wallarm-common/addcloudnode -H us1.api.wallarm.com Paste the filter node token from your clipboard. Your WAF node will now synchronize with the Cloud every 2\u20114 minutes according to the default synchronization configuration. WAF node and Cloud synchronization configuration After running the addcloudnode script, the /etc/wallarm/syncnode file containing the WAF node and Cloud synchronization settings will be created. The settings of the WAF node and Cloud synchronization can be changed via the /etc/wallarm/syncnode file. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 Connecting Using Your Cloud Account Login and Password \u00b6 To connect the node to the cloud using your cloud account requisites, proceed with the following steps: On the virtual machine run the addnode script: Info You have to pick which script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the \u00ab EU Cloud tab below. If you are using https://us1.my.wallarm.com/ , run the script from the US Cloud tab below. EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Provide your Wallarm account\u2019s login and password when prompted. API Access The API choice for your filter node depends on the Cloud you are using. Please, select the API accordingly: If you are using https://my.wallarm.com/ , your node requires access to https://api.wallarm.com:444 . If you are using https://us1.my.wallarm.com/ , your node requires access to https://us1.api.wallarm.com:444 . Ensure the access is not blocked by a firewall. Your WAF node will now synchronize with the Cloud every 2\u20114 minutes according to the default synchronization configuration. WAF node and Cloud synchronization configuration After running the addnode script, the /etc/wallarm/node.yaml file containing the WAF node and Cloud synchronization settings and other settings required for a correct WAF node operation will be created. The settings of the WAF node and Cloud synchronization can be changed via the /etc/wallarm/node.yaml file and system environment variables. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 6. Set up the Filter Node for Using a Proxy Server \u00b6 Info This setup step is intended for users who use their own proxy server for the operation of the protected web applications. If you do not use a proxy server, skip this step of the setup. You need to assign new values to the environment variables, which define the proxy server used, to configure Wallarm node for using your proxy server. Add new values of the environment variables to the /etc/environment file: Add https_proxy to define a proxy for the https protocol. Add http_proxy to define a proxy for the http protocol. Add no_proxy to define the list of the resources proxy should not be used for. Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables. <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for. <proxy_user> defines the username for proxy authorization. <proxy_pass> defines the password for proxy authorization. <host> defines a host of the proxy server. <port> defines a port of the proxy server. Assign a \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" array value, where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains, to the no_proxy variable to define a list of the resources which proxy should not be used for. This array should consist of IP addresses and/or domains. Resources that need to be addressed without a proxy Add the following IP addresses and domain to the list of the resources that have to be addressed without a proxy for the system to operate correctly: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . The 127.0.0.8 and 127.0.0.9 IP addresses are used for the operation of the Wallarm filter node. The example of the correct /etc/environment file contents below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy=http://admin:01234@1.2.3.4:1234 http_proxy=http://admin:01234@1.2.3.4:1234 no_proxy=\"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\" 7. Set Up Filtering and Proxying Rules \u00b6 The following files contain NGINX and WAF node settings: /etc/nginx/nginx.conf defines the configuration of NGINX /etc/nginx/conf.d/wallarm.conf defines the global configuration of Wallarm WAF node /etc/nginx/conf.d/wallarm-status.conf defines the WAF node monitoring service configuration You can create your own configuration files to define the operation of NGINX and Wallarm. It is recommended to create a separate configuration file with the server block for each group of the domains that should be processed in the same way. To see detailed information about working with NGINX configuration files, proceed to the official NGINX documentation . Wallarm directives define the operation logic of the Wallarm filter node. To see the list of Wallarm directives available, proceed to the Wallarm configuration options page. A Configuration File Example \u00b6 Let us suppose that you need to configure the server to work in the following conditions: Only HTTP traffic is processed. There are no HTTPS requests processed. The following domains receive the requests: example.com and www.example.com . All requests must be passed to the server 10.80.0.5 . All incoming requests are considered less than 1MB in size (default setting). The processing of a request takes no more than 60 seconds (default setting). Wallarm must operate in the monitor mode. Clients access the filter node directly, without an intermediate HTTP load balancer. Creating a configuration file You can create a custom NGINX configuration file (e.g. example.com.conf ) or modify the default NGINX configuration file ( default.conf ). When creating a custom configuration file, make sure that NGINX listens to the incoming connections on the free port. To meet the listed conditions, the contents of the configuration file must be the following: server { listen 80; listen [::]:80 ipv6only=on; # the domains for which traffic is processed server_name example.com; server_name www.example.com; # turn on the monitoring mode of traffic processing wallarm_mode monitoring; # wallarm_instance 1; location / { # setting the address for request forwarding proxy_pass http://10.80.0.5; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 8. Allocate More Memory for the Wallarm Node \u00b6 The Wallarm Node uses Tarantool, an open\u2011source in-memory database, to calculate traffic metrics required for automated adjusting of security rules. By default, the amount of RAM allocated to Tarantool is 40% of the total instance memory. You can change the amount of RAM allocated for Tarantool. To allocate the instance RAM to Tarantool: Open the Tarantool configuration file: sudo vim /etc/default/wallarm-tarantool Set the amount of allocated RAM in the SLAB_ALLOC_ARENA in GB. The value can be an integer or a float (a dot . is a decimal separator). For example, to set 24 GB: SLAB_ALLOC_ARENA=24 To apply changes, restart the Tarantool daemon: sudo systemctl restart wallarm-tarantool 9. Configure Logging \u00b6 Configure the filter node variables logging using NGINX. This will allow to perform a quick filter node diagnostics with the help of the NGINX log file. 10. Restart NGINX \u00b6 Restart NGINX by running the following command: sudo systemctl restart nginx The Installation Is Complete \u00b6 The installation is now complete. Check that the filter node runs and filters the traffic. See Check the filter node operation . Default Settings A freshly installed filter node operates in blocking mode (see the wallarm_mode directive description) by default. This may result in the inoperable Wallarm scanner . If you plan to use the scanner, then you need to perform additional actions to render scanner operational. Additional Settings \u00b6 The filter node may require some additional configuration after installation. The document below lists a few of the typical setups that you can apply if needed. To get more information about other available settings, proceed to the \u201cConfiguration\u201d section of the Administrator\u2019s Guide . Configuring the Display of the Client's Real IP \u00b6 If the filter node is deployed behind a proxy server or load balancer without any additional configuration, the request source address may not be equal to the actual IP address of the client. Instead, it may be equal to one of the IP addresses of the proxy server or the load balancer. In this case, if you want the filter node to receive the client's IP address as a request source address, you need to perform an additional configuration of the proxy server or the load balancer. Limiting the Single Request Processing Time \u00b6 Use the wallarm_process_time_limit Wallarm directive to specify the limit of the duration for processing a single request by the filter node. If processing the request consumes more time than specified in the directive, then the information on the error is entered into the log file and the request is marked as an overlimit_res attack. Limiting the Server Reply Waiting Time \u00b6 Use the proxy_read_timeout NGINX directive to specify the timeout for reading the proxy server reply. If the server sends nothing during this time, the connection is closed. Limiting the Maximum Request Size \u00b6 Use the client_max_body_size NGINX directive to specify the limit for the maximum size of the body of the client's request. If this limit is exceeded, NGINX replies to the client with the 413 ( Payload Too Large ) code, also known as the Request Entity Too Large message.","title":"GCP Marketplace image deployment"},{"location":"admin-en/installation-gcp-en/#deploying-on-google-cloud-platform-gcp","text":"To deploy a filter node on the Google Cloud Platform, perform the following steps: Log in to your Google Cloud Platform account. Launch a filter node instance. Configure the filter node instance. Connect to the filter node instance via SSH. Connect the filter node to the Wallarm Cloud. Set up the filter node for using a proxy server. Set up filtering and proxying rules Allocate more memory for the Wallarm Node. Configure logging. Restart NGINX.","title":"Deploying on Google Cloud Platform (GCP)"},{"location":"admin-en/installation-gcp-en/#1-log-in-to-your-google-cloud-platform-account","text":"Log in to console.cloud.google.com .","title":"1. Log In to Your Google Cloud Platform Account"},{"location":"admin-en/installation-gcp-en/#2-launch-a-filter-node-instance","text":"If Wallarm WAF is already deployed If you launch Wallarm WAF instead of the already existing Wallarm WAF or need to duplicate the deployment in the same environment, please keep the same WAF version as currently used or update the version of all deployments to the latest. To check the launched version, connect to the running instance and execute the following command: apt list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update all Wallarm WAF instances to the latest version. If the version 2.16.x or lower is installed, then please update all Wallarm WAF instances to the latest version. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"2. Launch a Filter Node Instance"},{"location":"admin-en/installation-gcp-en/#launch-the-instance-via-the-google-cloud-ui","text":"To launch the WAF node instance via the Google Cloud UI, please open the WAF node image on the Google Cloud Marketplace and click LAUNCH . The instance will launch with a preinstalled WAF node. To see detailed information on launching instances in the Google Cloud, please proceed to the official Google Cloud Platform documentation .","title":"Launch the instance via the Google Cloud UI"},{"location":"admin-en/installation-gcp-en/#launch-the-instance-via-terraform-or-other-tools","text":"When using a tool like Terraform to launch the WAF node instance using Wallarm GCP image, you may need to provide the name of this image in the Terraform configuration. Image name has the following format: wallarm-node-195710/wallarm-node-<IMAGE_VERSION>-build To launch the instance with the WAF node version 3.0, please use the following image name: wallarm-node-195710/wallarm-node-3-0-20210628-090819 To get the image name, you can also follow these steps: Install Google Cloud SDK . Execute the command gcloud compute images list with the following parameters: gcloud compute images list --project wallarm-node-195710 --filter = \"name~'wallarm-node-3-0-*'\" --no-standard-images Copy the version value from the name of the latest available image and paste the copied value into the provided image name format. For example, the WAF node version 3.0 image will have the following name: wallarm-node-195710/wallarm-node-3-0-20210628-090819","title":"Launch the instance via Terraform or other tools"},{"location":"admin-en/installation-gcp-en/#3-configure-the-filter-node-instance","text":"Perform the following actions to configure the launched filter node instance: Navigate to the VM instances page in the Compute Engine section of the menu. Select the launched filter node instance and click the Edit button. Allow the required types of incoming traffic by ticking the corresponding checkboxes in the Firewalls setting. If necessary, you can restrict connecting to the instance with the project SSH keys and use a custom SSH key pair for connecting to this instance. To do this, perform the following actions: Tick the \u201cBlock project-wide\u201d checkbox in the SSH Keys setting. Click the Show and edit button in the SSH Keys setting to expand the field for entering an SSH key. Generate a pair of public and private SSH keys. For example, you can use the ssh-keygen and PuTTYgen utilities. Copy an open key in OpenSSH format from the interface of the used key generator (in the current example, the generated public key should be copied from the Public key for pasting into OpenSSH authorized_keys file area of the PuTTYgen interface) and paste it into the field containing the \u201cEnter entire key data\u201d hint. Save the private key. It will be required for connecting to the configured instance in the future. Click the Save button at the bottom of the page to apply the changes.","title":"3. Configure the Filter Node Instance"},{"location":"admin-en/installation-gcp-en/#4-connect-to-the-filter-node-instance-via-ssh","text":"To see detailed information about ways of connecting to instances, proceed to this link . Connecting to the instance via a custom private key If during base instance creation process you have enabled connection to the instance via a custom SSH key pair, make sure you have access to the private key from this key pair.","title":"4. Connect to the Filter Node Instance via SSH"},{"location":"admin-en/installation-gcp-en/#5-connect-the-filter-node-to-the-wallarm-cloud","text":"The filter node interacts with the Wallarm cloud. There are two ways of connecting the node to the cloud: Using the filter node token Using your cloud account login and password Required access rights Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled, therefore allowing you to connect a filter node to the cloud. You can check the aforementioned parameters by navigating to the user account list in the Wallarm console. If you are using https://my.wallarm.com/ , proceed to the following link to check your user settings. If you are using https://us1.my.wallarm.com/ , proceed to the following link to check your user settings.","title":"5. Connect the Filter Node to the Wallarm Cloud"},{"location":"admin-en/installation-gcp-en/#connecting-using-the-filter-node-token","text":"To connect the node to the cloud using the token, proceed with the following steps: Create a new node on the Nodes tab of Wallarm web interface. Click the Create new node button. In the form that appears, enter the node name into the corresponding field and select the \u201cCloud\u201d type of installation from the drop-down list. Click the Create button. In the window that appears, click the Copy button next to the field with the token to add the token of the newly created filter node to your clipboard. On the virtual machine run the addcloudnode script: Info You have to pick which script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the EU Cloud tab below. If you are using https://us1.my.wallarm.com/ , run the script from the US Cloud tab below. EU Cloud sudo /usr/share/wallarm-common/addcloudnode US Cloud sudo /usr/share/wallarm-common/addcloudnode -H us1.api.wallarm.com Paste the filter node token from your clipboard. Your WAF node will now synchronize with the Cloud every 2\u20114 minutes according to the default synchronization configuration. WAF node and Cloud synchronization configuration After running the addcloudnode script, the /etc/wallarm/syncnode file containing the WAF node and Cloud synchronization settings will be created. The settings of the WAF node and Cloud synchronization can be changed via the /etc/wallarm/syncnode file. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192","title":"Connecting Using the Filter Node Token"},{"location":"admin-en/installation-gcp-en/#connecting-using-your-cloud-account-login-and-password","text":"To connect the node to the cloud using your cloud account requisites, proceed with the following steps: On the virtual machine run the addnode script: Info You have to pick which script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the \u00ab EU Cloud tab below. If you are using https://us1.my.wallarm.com/ , run the script from the US Cloud tab below. EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Provide your Wallarm account\u2019s login and password when prompted. API Access The API choice for your filter node depends on the Cloud you are using. Please, select the API accordingly: If you are using https://my.wallarm.com/ , your node requires access to https://api.wallarm.com:444 . If you are using https://us1.my.wallarm.com/ , your node requires access to https://us1.api.wallarm.com:444 . Ensure the access is not blocked by a firewall. Your WAF node will now synchronize with the Cloud every 2\u20114 minutes according to the default synchronization configuration. WAF node and Cloud synchronization configuration After running the addnode script, the /etc/wallarm/node.yaml file containing the WAF node and Cloud synchronization settings and other settings required for a correct WAF node operation will be created. The settings of the WAF node and Cloud synchronization can be changed via the /etc/wallarm/node.yaml file and system environment variables. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192","title":"Connecting Using Your Cloud Account Login and Password"},{"location":"admin-en/installation-gcp-en/#6-set-up-the-filter-node-for-using-a-proxy-server","text":"Info This setup step is intended for users who use their own proxy server for the operation of the protected web applications. If you do not use a proxy server, skip this step of the setup. You need to assign new values to the environment variables, which define the proxy server used, to configure Wallarm node for using your proxy server. Add new values of the environment variables to the /etc/environment file: Add https_proxy to define a proxy for the https protocol. Add http_proxy to define a proxy for the http protocol. Add no_proxy to define the list of the resources proxy should not be used for. Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables. <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for. <proxy_user> defines the username for proxy authorization. <proxy_pass> defines the password for proxy authorization. <host> defines a host of the proxy server. <port> defines a port of the proxy server. Assign a \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" array value, where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains, to the no_proxy variable to define a list of the resources which proxy should not be used for. This array should consist of IP addresses and/or domains. Resources that need to be addressed without a proxy Add the following IP addresses and domain to the list of the resources that have to be addressed without a proxy for the system to operate correctly: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . The 127.0.0.8 and 127.0.0.9 IP addresses are used for the operation of the Wallarm filter node. The example of the correct /etc/environment file contents below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy=http://admin:01234@1.2.3.4:1234 http_proxy=http://admin:01234@1.2.3.4:1234 no_proxy=\"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\"","title":"6. Set up the Filter Node for Using a Proxy Server"},{"location":"admin-en/installation-gcp-en/#7-set-up-filtering-and-proxying-rules","text":"The following files contain NGINX and WAF node settings: /etc/nginx/nginx.conf defines the configuration of NGINX /etc/nginx/conf.d/wallarm.conf defines the global configuration of Wallarm WAF node /etc/nginx/conf.d/wallarm-status.conf defines the WAF node monitoring service configuration You can create your own configuration files to define the operation of NGINX and Wallarm. It is recommended to create a separate configuration file with the server block for each group of the domains that should be processed in the same way. To see detailed information about working with NGINX configuration files, proceed to the official NGINX documentation . Wallarm directives define the operation logic of the Wallarm filter node. To see the list of Wallarm directives available, proceed to the Wallarm configuration options page.","title":"7. Set Up Filtering and Proxying Rules"},{"location":"admin-en/installation-gcp-en/#a-configuration-file-example","text":"Let us suppose that you need to configure the server to work in the following conditions: Only HTTP traffic is processed. There are no HTTPS requests processed. The following domains receive the requests: example.com and www.example.com . All requests must be passed to the server 10.80.0.5 . All incoming requests are considered less than 1MB in size (default setting). The processing of a request takes no more than 60 seconds (default setting). Wallarm must operate in the monitor mode. Clients access the filter node directly, without an intermediate HTTP load balancer. Creating a configuration file You can create a custom NGINX configuration file (e.g. example.com.conf ) or modify the default NGINX configuration file ( default.conf ). When creating a custom configuration file, make sure that NGINX listens to the incoming connections on the free port. To meet the listed conditions, the contents of the configuration file must be the following: server { listen 80; listen [::]:80 ipv6only=on; # the domains for which traffic is processed server_name example.com; server_name www.example.com; # turn on the monitoring mode of traffic processing wallarm_mode monitoring; # wallarm_instance 1; location / { # setting the address for request forwarding proxy_pass http://10.80.0.5; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } }","title":"A Configuration File Example"},{"location":"admin-en/installation-gcp-en/#8-allocate-more-memory-for-the-wallarm-node","text":"The Wallarm Node uses Tarantool, an open\u2011source in-memory database, to calculate traffic metrics required for automated adjusting of security rules. By default, the amount of RAM allocated to Tarantool is 40% of the total instance memory. You can change the amount of RAM allocated for Tarantool. To allocate the instance RAM to Tarantool: Open the Tarantool configuration file: sudo vim /etc/default/wallarm-tarantool Set the amount of allocated RAM in the SLAB_ALLOC_ARENA in GB. The value can be an integer or a float (a dot . is a decimal separator). For example, to set 24 GB: SLAB_ALLOC_ARENA=24 To apply changes, restart the Tarantool daemon: sudo systemctl restart wallarm-tarantool","title":"8. Allocate More Memory for the Wallarm Node"},{"location":"admin-en/installation-gcp-en/#9-configure-logging","text":"Configure the filter node variables logging using NGINX. This will allow to perform a quick filter node diagnostics with the help of the NGINX log file.","title":"9. Configure Logging"},{"location":"admin-en/installation-gcp-en/#10-restart-nginx","text":"Restart NGINX by running the following command: sudo systemctl restart nginx","title":"10. Restart NGINX"},{"location":"admin-en/installation-gcp-en/#the-installation-is-complete","text":"The installation is now complete. Check that the filter node runs and filters the traffic. See Check the filter node operation . Default Settings A freshly installed filter node operates in blocking mode (see the wallarm_mode directive description) by default. This may result in the inoperable Wallarm scanner . If you plan to use the scanner, then you need to perform additional actions to render scanner operational.","title":"The Installation Is Complete"},{"location":"admin-en/installation-gcp-en/#additional-settings","text":"The filter node may require some additional configuration after installation. The document below lists a few of the typical setups that you can apply if needed. To get more information about other available settings, proceed to the \u201cConfiguration\u201d section of the Administrator\u2019s Guide .","title":"Additional Settings"},{"location":"admin-en/installation-gcp-en/#configuring-the-display-of-the-clients-real-ip","text":"If the filter node is deployed behind a proxy server or load balancer without any additional configuration, the request source address may not be equal to the actual IP address of the client. Instead, it may be equal to one of the IP addresses of the proxy server or the load balancer. In this case, if you want the filter node to receive the client's IP address as a request source address, you need to perform an additional configuration of the proxy server or the load balancer.","title":"Configuring the Display of the Client's Real IP"},{"location":"admin-en/installation-gcp-en/#limiting-the-single-request-processing-time","text":"Use the wallarm_process_time_limit Wallarm directive to specify the limit of the duration for processing a single request by the filter node. If processing the request consumes more time than specified in the directive, then the information on the error is entered into the log file and the request is marked as an overlimit_res attack.","title":"Limiting the Single Request Processing Time"},{"location":"admin-en/installation-gcp-en/#limiting-the-server-reply-waiting-time","text":"Use the proxy_read_timeout NGINX directive to specify the timeout for reading the proxy server reply. If the server sends nothing during this time, the connection is closed.","title":"Limiting the Server Reply Waiting Time"},{"location":"admin-en/installation-gcp-en/#limiting-the-maximum-request-size","text":"Use the client_max_body_size NGINX directive to specify the limit for the maximum size of the body of the client's request. If this limit is exceeded, NGINX replies to the client with the 413 ( Payload Too Large ) code, also known as the Request Entity Too Large message.","title":"Limiting the Maximum Request Size"},{"location":"admin-en/installation-kong-en/","text":"Installing with Kong \u00b6 Prerequisites Requirements for the Kong platform: Kong version 1.4.3 or lower Kong installed on a platform supported by Wallarm according to Kong's official instructions One of the following points is required for proper Kong operation: prepared configuration files, configured database. Please make sure that the installed Kong meets the prerequisites before proceeding with Wallarm installation. The official Kong documentation is available at this link . Known Limitations The wallarm_block_page directive is not supported. Wallarm configuration via Kong Admin API is not supported. Installation \u00b6 Installation of postanalytics on a separate server If you are planning to install postanalytics on a separate server, you must install postanalytics first. See details in Separate postanalytics module installation . To install the Wallarm module with Kong, you need to: Add Wallarm repositories. Install Wallarm packages. Configure postanalytics. Set up the filter node for using a proxy server. Connect the filter node to the Wallarm cloud. Configure the postanalytics server addresses. Configure the filtration mode. Configure logging. Prerequisites Prior to taking any steps listed below, either disable or configure SELinux if it is installed on the operating system. Make sure that you execute all commands below as superuser (e.g. root ). If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of an already existing Wallarm WAF or need to duplicate the installation in the same environment, then please keep the same WAF version as currently used or update all installations to the latest version. For the postanalytics installed separately, versions of substite or duplicate installations must be the same as already installed postanalytics too. To check the installed version of WAF node and postanalytics installed on the same server: Debian apt list wallarm-node Ubuntu apt list wallarm-node CentOS yum list wallarm-node To check the versions of WAF node and postanalytics installed on different servers: Debian # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool Ubuntu # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool CentOS # run from the server with installed WAF node yum list wallarm-node-nginx # run from the server with installed postanalytics yum list wallarm-node-tarantool If the version 3.0.x is installed, then follow the current instructions for the WAF node and for separate postanalytics . If the version 2.18.x is installed, then follow the instructions for WAF node 2.18 and for separate postanalytics 2.18 or update WAF node packages and separate postanalytics packages to the latest version in all installations. If the version 2.16.x or lower is installed, then please update the WAF node packages and separate postanalytics packages to the latest version in all installations. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . 1. Add Wallarm Repositories \u00b6 The filter node installs and updates from the Wallarm repositories. Depending on your operating system, run one of the following commands: Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Repository access Your system must have access to https://repo.wallarm.com to download the packages. Ensure the access is not blocked by a firewall. 2. Install Wallarm Packages \u00b6 To install the filter node and postanalytics on the same server, run the command: Debian 9.x (stretch) sudo apt install --no-install-recommends wallarm-node kong-module-wallarm Ubuntu 18.04 LTS (bionic) sudo apt install --no-install-recommends wallarm-node kong-module-wallarm CentOS 7.x sudo yum install wallarm-node kong-module-wallarm To install the filter node alone, run the command: Debian 9.x (stretch) sudo apt install --no-install-recommends wallarm-node-nginx kong-module-wallarm Ubuntu 18.04 LTS (bionic) sudo apt install --no-install-recommends wallarm-node-nginx kong-module-wallarm CentOS 7.x sudo yum install wallarm-node-nginx kong-module-wallarm 3. Configure Postanalytics \u00b6 Info Skip this step if you installed postanalytics on a separate server as you already have your postanalytics configured. The amount of memory determines the quality of work of the statistical algorithms. The recommended value is 75% of the total server memory. For example, if the server has 32 GB of memory, the recommended allocation size is 24 GB. Allocate the operating memory size for Tarantool: Open for editing the configuration file of Tarantool: Debian 9.x (stretch) sudo vim /etc/default/wallarm-tarantool Ubuntu 18.04 LTS (bionic) sudo vim /etc/default/wallarm-tarantool CentOS 7.x sudo vim /etc/sysconfig/wallarm-tarantool Set the allocated memory size in the configuration file of Tarantool via the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example: SLAB_ALLOC_ARENA=24 Restart Tarantool: Debian 9.x (stretch) sudo systemctl restart wallarm-tarantool Ubuntu 18.04 LTS (bionic) sudo service wallarm-tarantool restart CentOS 7.x sudo systemctl restart wallarm-tarantool 4. Set up the Filter Node for Using a Proxy Server \u00b6 Info This setup step is intended for users who use their own proxy server for the operation of the protected web applications. If you do not use a proxy server, skip this step of the setup. You need to assign new values to the environment variables, which define the proxy server used, to configure Wallarm node for using your proxy server. Add new values of the environment variables to the /etc/environment file: Add https_proxy to define a proxy for the https protocol. Add http_proxy to define a proxy for the http protocol. Add no_proxy to define the list of the resources proxy should not be used for. Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables. <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for. <proxy_user> defines the username for proxy authorization. <proxy_pass> defines the password for proxy authorization. <host> defines a host of the proxy server. <port> defines a port of the proxy server. Assign a \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" array value, where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains, to the no_proxy variable to define a list of the resources which proxy should not be used for. This array should consist of IP addresses and/or domains. Resources that need to be addressed without a proxy Add the following IP addresses and domain to the list of the resources that have to be addressed without a proxy for the system to operate correctly: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . The 127.0.0.8 and 127.0.0.9 IP addresses are used for the operation of the Wallarm filter node. The example of the correct /etc/environment file contents below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy=http://admin:01234@1.2.3.4:1234 http_proxy=http://admin:01234@1.2.3.4:1234 no_proxy=\"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\" 5. Connect the Filter Node to the Wallarm Cloud \u00b6 API Access The API choice for your filter node depends on the Cloud you are using. Please, select the API accordingly: If you are using https://my.wallarm.com/ , your node requires access to https://api.wallarm.com:444 . If you are using https://us1.my.wallarm.com/ , your node requires access to https://us1.api.wallarm.com:444 . Ensure the access is not blocked by a firewall. The filter node interacts with the Wallarm cloud. To connect the node to the cloud using your cloud account requisites, proceed with the following steps: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled, therefore allowing you to connect a filter node to the cloud. You can check the above mentioned parameters by navigating to the user account list in the Wallarm console. If you are using https://my.wallarm.com/ , proceed to the following link to check your user settings. If you are using https://us1.my.wallarm.com/ , proceed to the following link to check your user settings. Run the addnode script in a system with the filter node: Info You have to pick the script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the \u00abEU Cloud\u00bb tab below. If you are using https://us1.my.wallarm.com/ , run the script from the \u00abUS Cloud\u00bb tab below. EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com To specify the name of the created node, use the -n <node name> option. Provide your Wallarm account\u2019s login and password when prompted. 6. Configure the Postanalytics Server Addresses \u00b6 Info Skip this step if you installed postanalytics and the filter node on the same server. Do this step if you installed postanalytics and the filter node on separate servers. Add the server address of postanalytics to /etc/kong/nginx-wallarm.template : upstream wallarm_tarantool { server <ip1>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; server <ip2>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 2 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; Required conditions It is required that the following conditions are satisfied for the max_conns and the keepalive parameters: The value of the keepalive parameter must not be lower than the number of the tarantool servers. The value of the max_conns parameter must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. 7. Set up the Filtration Mode \u00b6 The filtering and proxying rules are configured in the /etc/kong/nginx-wallarm.template file. To see detailed information about working with NGINX configuration files, proceed to the official NGINX documentation . Wallarm directives define the operation logic of the Wallarm filter node. To see the list of Wallarm directives available, proceed to the Wallarm configuration options page. A Configuration File Example \u00b6 Let us suppose that you need to configure the server to work in the following conditions: Only HTTP traffic is processed. There are no HTTPS requests processed. The following domains receive the requests: example.com and www.example.com . All requests must be passed to the server 10.80.0.5 . All incoming requests are considered less than 1MB in size (default setting). The processing of a request takes no more than 60 seconds (default setting). Wallarm must operate in the monitor mode. Clients access the filter node directly, without an intermediate HTTP load balancer. To meet the listed conditions, the contents of the configuration file must be the following: server { listen 80; listen [::]:80 ipv6only=on; # the domains for which traffic is processed server_name example.com; server_name www.example.com; # turn on the monitoring mode of traffic processing wallarm_mode monitoring; # wallarm_instance 1; location / { # setting the address for request forwarding proxy_pass http://10.80.0.5; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 8. Configure Logging \u00b6 Configure the filter node variables logging using NGINX. This will allow to perform a quick filter node diagnostics with the help of the NGINX log file. Start Kong \u00b6 To start Kong with the installed Wallarm module, run the command: kong start --nginx-conf /etc/kong/nginx-wallarm.template The Installation Is Complete \u00b6 Check that the filter node runs and filters the traffic. See Check the filter node operation . Default Settings A freshly installed filter node operates in blocking mode (see the wallarm_mode directive description) by default. This may result in the inoperable Wallarm scanner . If you plan to use the scanner, then you need to perform additional actions to render scanner operational. Additional Settings \u00b6 The filter node may require some additional configuration after installation. The document below lists a few of the typical setups that you can apply if needed. To get more information about other available settings, proceed to the \u201cConfiguration\u201d section of the Administrator\u2019s Guide . Configuring the Display of the Client's Real IP \u00b6 If the filter node is deployed behind a proxy server or load balancer without any additional configuration, the request source address may not be equal to the actual IP address of the client. Instead, it may be equal to one of the IP addresses of the proxy server or the load balancer. In this case, if you want the filter node to receive the client's IP address as a request source address, you need to perform an additional configuration of the proxy server or the load balancer. Limiting the Single Request Processing Time \u00b6 Use the wallarm_process_time_limit Wallarm directive to specify the limit of the duration for processing a single request by the filter node. If processing the request consumes more time than specified in the directive, then the information on the error is entered into the log file and the request is marked as an overlimit_res attack. Limiting the Server Reply Waiting Time \u00b6 Use the proxy_read_timeout NGINX directive to specify the timeout for reading the proxy server reply. If the server sends nothing during this time, the connection is closed. Limiting the Maximum Request Size \u00b6 Use the client_max_body_size NGINX directive to specify the limit for the maximum size of the body of the client's request. If this limit is exceeded, NGINX replies to the client with the 413 ( Payload Too Large ) code, also known as the Request Entity Too Large message.","title":"Installing on the Kong platform"},{"location":"admin-en/installation-kong-en/#installing-with-kong","text":"Prerequisites Requirements for the Kong platform: Kong version 1.4.3 or lower Kong installed on a platform supported by Wallarm according to Kong's official instructions One of the following points is required for proper Kong operation: prepared configuration files, configured database. Please make sure that the installed Kong meets the prerequisites before proceeding with Wallarm installation. The official Kong documentation is available at this link . Known Limitations The wallarm_block_page directive is not supported. Wallarm configuration via Kong Admin API is not supported.","title":"Installing with Kong"},{"location":"admin-en/installation-kong-en/#installation","text":"Installation of postanalytics on a separate server If you are planning to install postanalytics on a separate server, you must install postanalytics first. See details in Separate postanalytics module installation . To install the Wallarm module with Kong, you need to: Add Wallarm repositories. Install Wallarm packages. Configure postanalytics. Set up the filter node for using a proxy server. Connect the filter node to the Wallarm cloud. Configure the postanalytics server addresses. Configure the filtration mode. Configure logging. Prerequisites Prior to taking any steps listed below, either disable or configure SELinux if it is installed on the operating system. Make sure that you execute all commands below as superuser (e.g. root ). If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of an already existing Wallarm WAF or need to duplicate the installation in the same environment, then please keep the same WAF version as currently used or update all installations to the latest version. For the postanalytics installed separately, versions of substite or duplicate installations must be the same as already installed postanalytics too. To check the installed version of WAF node and postanalytics installed on the same server: Debian apt list wallarm-node Ubuntu apt list wallarm-node CentOS yum list wallarm-node To check the versions of WAF node and postanalytics installed on different servers: Debian # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool Ubuntu # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool CentOS # run from the server with installed WAF node yum list wallarm-node-nginx # run from the server with installed postanalytics yum list wallarm-node-tarantool If the version 3.0.x is installed, then follow the current instructions for the WAF node and for separate postanalytics . If the version 2.18.x is installed, then follow the instructions for WAF node 2.18 and for separate postanalytics 2.18 or update WAF node packages and separate postanalytics packages to the latest version in all installations. If the version 2.16.x or lower is installed, then please update the WAF node packages and separate postanalytics packages to the latest version in all installations. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"Installation"},{"location":"admin-en/installation-kong-en/#1-add-wallarm-repositories","text":"The filter node installs and updates from the Wallarm repositories. Depending on your operating system, run one of the following commands: Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Repository access Your system must have access to https://repo.wallarm.com to download the packages. Ensure the access is not blocked by a firewall.","title":"1. Add Wallarm Repositories"},{"location":"admin-en/installation-kong-en/#2-install-wallarm-packages","text":"To install the filter node and postanalytics on the same server, run the command: Debian 9.x (stretch) sudo apt install --no-install-recommends wallarm-node kong-module-wallarm Ubuntu 18.04 LTS (bionic) sudo apt install --no-install-recommends wallarm-node kong-module-wallarm CentOS 7.x sudo yum install wallarm-node kong-module-wallarm To install the filter node alone, run the command: Debian 9.x (stretch) sudo apt install --no-install-recommends wallarm-node-nginx kong-module-wallarm Ubuntu 18.04 LTS (bionic) sudo apt install --no-install-recommends wallarm-node-nginx kong-module-wallarm CentOS 7.x sudo yum install wallarm-node-nginx kong-module-wallarm","title":"2. Install Wallarm Packages"},{"location":"admin-en/installation-kong-en/#3-configure-postanalytics","text":"Info Skip this step if you installed postanalytics on a separate server as you already have your postanalytics configured. The amount of memory determines the quality of work of the statistical algorithms. The recommended value is 75% of the total server memory. For example, if the server has 32 GB of memory, the recommended allocation size is 24 GB. Allocate the operating memory size for Tarantool: Open for editing the configuration file of Tarantool: Debian 9.x (stretch) sudo vim /etc/default/wallarm-tarantool Ubuntu 18.04 LTS (bionic) sudo vim /etc/default/wallarm-tarantool CentOS 7.x sudo vim /etc/sysconfig/wallarm-tarantool Set the allocated memory size in the configuration file of Tarantool via the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example: SLAB_ALLOC_ARENA=24 Restart Tarantool: Debian 9.x (stretch) sudo systemctl restart wallarm-tarantool Ubuntu 18.04 LTS (bionic) sudo service wallarm-tarantool restart CentOS 7.x sudo systemctl restart wallarm-tarantool","title":"3. Configure Postanalytics"},{"location":"admin-en/installation-kong-en/#4-set-up-the-filter-node-for-using-a-proxy-server","text":"Info This setup step is intended for users who use their own proxy server for the operation of the protected web applications. If you do not use a proxy server, skip this step of the setup. You need to assign new values to the environment variables, which define the proxy server used, to configure Wallarm node for using your proxy server. Add new values of the environment variables to the /etc/environment file: Add https_proxy to define a proxy for the https protocol. Add http_proxy to define a proxy for the http protocol. Add no_proxy to define the list of the resources proxy should not be used for. Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables. <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for. <proxy_user> defines the username for proxy authorization. <proxy_pass> defines the password for proxy authorization. <host> defines a host of the proxy server. <port> defines a port of the proxy server. Assign a \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" array value, where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains, to the no_proxy variable to define a list of the resources which proxy should not be used for. This array should consist of IP addresses and/or domains. Resources that need to be addressed without a proxy Add the following IP addresses and domain to the list of the resources that have to be addressed without a proxy for the system to operate correctly: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . The 127.0.0.8 and 127.0.0.9 IP addresses are used for the operation of the Wallarm filter node. The example of the correct /etc/environment file contents below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy=http://admin:01234@1.2.3.4:1234 http_proxy=http://admin:01234@1.2.3.4:1234 no_proxy=\"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\"","title":"4. Set up the Filter Node for Using a Proxy Server"},{"location":"admin-en/installation-kong-en/#5-connect-the-filter-node-to-the-wallarm-cloud","text":"API Access The API choice for your filter node depends on the Cloud you are using. Please, select the API accordingly: If you are using https://my.wallarm.com/ , your node requires access to https://api.wallarm.com:444 . If you are using https://us1.my.wallarm.com/ , your node requires access to https://us1.api.wallarm.com:444 . Ensure the access is not blocked by a firewall. The filter node interacts with the Wallarm cloud. To connect the node to the cloud using your cloud account requisites, proceed with the following steps: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled, therefore allowing you to connect a filter node to the cloud. You can check the above mentioned parameters by navigating to the user account list in the Wallarm console. If you are using https://my.wallarm.com/ , proceed to the following link to check your user settings. If you are using https://us1.my.wallarm.com/ , proceed to the following link to check your user settings. Run the addnode script in a system with the filter node: Info You have to pick the script to run depending on the Cloud you are using. If you are using https://my.wallarm.com/ , run the script from the \u00abEU Cloud\u00bb tab below. If you are using https://us1.my.wallarm.com/ , run the script from the \u00abUS Cloud\u00bb tab below. EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com To specify the name of the created node, use the -n <node name> option. Provide your Wallarm account\u2019s login and password when prompted.","title":"5. Connect the Filter Node to the Wallarm Cloud"},{"location":"admin-en/installation-kong-en/#6-configure-the-postanalytics-server-addresses","text":"Info Skip this step if you installed postanalytics and the filter node on the same server. Do this step if you installed postanalytics and the filter node on separate servers. Add the server address of postanalytics to /etc/kong/nginx-wallarm.template : upstream wallarm_tarantool { server <ip1>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; server <ip2>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 2 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; Required conditions It is required that the following conditions are satisfied for the max_conns and the keepalive parameters: The value of the keepalive parameter must not be lower than the number of the tarantool servers. The value of the max_conns parameter must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections.","title":"6. Configure the Postanalytics Server Addresses"},{"location":"admin-en/installation-kong-en/#7-set-up-the-filtration-mode","text":"The filtering and proxying rules are configured in the /etc/kong/nginx-wallarm.template file. To see detailed information about working with NGINX configuration files, proceed to the official NGINX documentation . Wallarm directives define the operation logic of the Wallarm filter node. To see the list of Wallarm directives available, proceed to the Wallarm configuration options page.","title":"7. Set up the Filtration Mode"},{"location":"admin-en/installation-kong-en/#a-configuration-file-example","text":"Let us suppose that you need to configure the server to work in the following conditions: Only HTTP traffic is processed. There are no HTTPS requests processed. The following domains receive the requests: example.com and www.example.com . All requests must be passed to the server 10.80.0.5 . All incoming requests are considered less than 1MB in size (default setting). The processing of a request takes no more than 60 seconds (default setting). Wallarm must operate in the monitor mode. Clients access the filter node directly, without an intermediate HTTP load balancer. To meet the listed conditions, the contents of the configuration file must be the following: server { listen 80; listen [::]:80 ipv6only=on; # the domains for which traffic is processed server_name example.com; server_name www.example.com; # turn on the monitoring mode of traffic processing wallarm_mode monitoring; # wallarm_instance 1; location / { # setting the address for request forwarding proxy_pass http://10.80.0.5; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } }","title":"A Configuration File Example"},{"location":"admin-en/installation-kong-en/#8-configure-logging","text":"Configure the filter node variables logging using NGINX. This will allow to perform a quick filter node diagnostics with the help of the NGINX log file.","title":"8. Configure Logging"},{"location":"admin-en/installation-kong-en/#start-kong","text":"To start Kong with the installed Wallarm module, run the command: kong start --nginx-conf /etc/kong/nginx-wallarm.template","title":"Start Kong"},{"location":"admin-en/installation-kong-en/#the-installation-is-complete","text":"Check that the filter node runs and filters the traffic. See Check the filter node operation . Default Settings A freshly installed filter node operates in blocking mode (see the wallarm_mode directive description) by default. This may result in the inoperable Wallarm scanner . If you plan to use the scanner, then you need to perform additional actions to render scanner operational.","title":"The Installation Is Complete"},{"location":"admin-en/installation-kong-en/#additional-settings","text":"The filter node may require some additional configuration after installation. The document below lists a few of the typical setups that you can apply if needed. To get more information about other available settings, proceed to the \u201cConfiguration\u201d section of the Administrator\u2019s Guide .","title":"Additional Settings"},{"location":"admin-en/installation-kong-en/#configuring-the-display-of-the-clients-real-ip","text":"If the filter node is deployed behind a proxy server or load balancer without any additional configuration, the request source address may not be equal to the actual IP address of the client. Instead, it may be equal to one of the IP addresses of the proxy server or the load balancer. In this case, if you want the filter node to receive the client's IP address as a request source address, you need to perform an additional configuration of the proxy server or the load balancer.","title":"Configuring the Display of the Client's Real IP"},{"location":"admin-en/installation-kong-en/#limiting-the-single-request-processing-time","text":"Use the wallarm_process_time_limit Wallarm directive to specify the limit of the duration for processing a single request by the filter node. If processing the request consumes more time than specified in the directive, then the information on the error is entered into the log file and the request is marked as an overlimit_res attack.","title":"Limiting the Single Request Processing Time"},{"location":"admin-en/installation-kong-en/#limiting-the-server-reply-waiting-time","text":"Use the proxy_read_timeout NGINX directive to specify the timeout for reading the proxy server reply. If the server sends nothing during this time, the connection is closed.","title":"Limiting the Server Reply Waiting Time"},{"location":"admin-en/installation-kong-en/#limiting-the-maximum-request-size","text":"Use the client_max_body_size NGINX directive to specify the limit for the maximum size of the body of the client's request. If this limit is exceeded, NGINX replies to the client with the 413 ( Payload Too Large ) code, also known as the Request Entity Too Large message.","title":"Limiting the Maximum Request Size"},{"location":"admin-en/installation-kubernetes-en/","text":"Installation in the Kubernetes Cluster \u00b6 System Requirements \u00b6 Kubernetes platform version 1.20 and lower Helm package manager Compatibility of your services with the official NGINX Ingress Controller version 0.26.2 See also What is Ingress? Installation of Helm Known Restrictions \u00b6 Operation without the postanalytics service is not supported. Scaling down postanalytics service may result in a partial loss of attack data. Installation \u00b6 Install the Wallarm Ingress controller. Enable traffic analysis for your Ingress. Check the Wallarm Ingress controller operation. Step 1: Installing the Wallarm Ingress Controller \u00b6 Go to the Wallarm Console \u2192 the Nodes tab via the link below: https://my.wallarm.com/nodes for the EU cloud https://us1.my.wallarm.com/nodes for the US cloud Create a WAF node with the Cloud type and copy the token. Clone the repository of Wallarm Helm chart: git clone https://github.com/wallarm/ingress-chart --branch 3.0.0-3 --single-branch Install the Wallarm Ingress controller: EU Cloud helm install --set controller.wallarm.enabled = true,controller.wallarm.token = <YOUR_CLOUD_NODE_TOKEN> <INGRESS_CONTROLLER_NAME> ingress-chart/wallarm-ingress -n <KUBERNETES_NAMESPACE> US Cloud helm install --set controller.wallarm.enabled = true,controller.wallarm.token = <YOUR_CLOUD_NODE_TOKEN>,controller.wallarm.apiHost = us1.api.wallarm.com <INGRESS_CONTROLLER_NAME> ingress-chart/wallarm-ingress -n <KUBERNETES_NAMESPACE> <YOUR_CLOUD_NODE_TOKEN> is the cloud WAF node token <INGRESS_CONTROLLER_NAME> is the name of the Wallarm Ingress controller <KUBERNETES_NAMESPACE> is the namespace of your Ingress Step 2: Enabling Traffic Analysis for Your Ingress \u00b6 kubectl annotate ingress <YOUR_INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-mode = monitoring kubectl annotate ingress <YOUR_INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-instance = <INSTANCE> <YOUR_INGRESS_NAME> is the name of your Ingress <INSTANCE> is a positive number that is unique to each of your applications or application groups. This will allow you to obtain separate statistics and to distinguish between attacks aimed at the corresponding applications Step 3: Checking the Wallarm Ingress Controller Operation \u00b6 Get the list of pods specifying the name of the Wallarm Ingress controller in <INGRESS_CONTROLLER_NAME> : kubectl get pods -l release=<INGRESS_CONTROLLER_NAME> Each pod should display the following: \"STATUS: Running\" and \"READY: N/N\". For example: NAME READY STATUS RESTARTS AGE ingress-controller-nginx-ingress-controller-675c68d46d-cfck8 3/3 Running 0 5m ingress-controller-nginx-ingress-controller-wallarm-tarantljj8g 8/8 Running 0 5m ingress-controller-nginx-ingress-default-backend-584ffc6c7xj5xx 1/1 Running 0 5m Send the request with test SQLI and XSS attacks to the Wallarm Ingress controller address: curl http://<INGRESS_CONTROLLER_IP>/?id = 'or+1=1--a-<script>prompt(1)</script>' If the WAF node is working in the block mode, the code 403 Forbidden will be returned in the response to the request and attacks will be displayed in Wallarm Console \u2192 Nodes . Configuration \u00b6 After the Wallarm Ingress controller is successfully installed and checked, you can make advanced configurations to the solution such as: Proper Reporting of End User Public IP Address Management of IP Addresses Blocking High Availability Considerations Ingress Controller Monitoring To find parameters used for advanced configuration and appropriate instructions, please follow the link .","title":"Deploying the NGINX-based node as the K8s Ingress Controller"},{"location":"admin-en/installation-kubernetes-en/#installation-in-the-kubernetes-cluster","text":"","title":"Installation in the Kubernetes Cluster"},{"location":"admin-en/installation-kubernetes-en/#system-requirements","text":"Kubernetes platform version 1.20 and lower Helm package manager Compatibility of your services with the official NGINX Ingress Controller version 0.26.2 See also What is Ingress? Installation of Helm","title":"System Requirements"},{"location":"admin-en/installation-kubernetes-en/#known-restrictions","text":"Operation without the postanalytics service is not supported. Scaling down postanalytics service may result in a partial loss of attack data.","title":"Known Restrictions"},{"location":"admin-en/installation-kubernetes-en/#installation","text":"Install the Wallarm Ingress controller. Enable traffic analysis for your Ingress. Check the Wallarm Ingress controller operation.","title":"Installation"},{"location":"admin-en/installation-kubernetes-en/#step-1-installing-the-wallarm-ingress-controller","text":"Go to the Wallarm Console \u2192 the Nodes tab via the link below: https://my.wallarm.com/nodes for the EU cloud https://us1.my.wallarm.com/nodes for the US cloud Create a WAF node with the Cloud type and copy the token. Clone the repository of Wallarm Helm chart: git clone https://github.com/wallarm/ingress-chart --branch 3.0.0-3 --single-branch Install the Wallarm Ingress controller: EU Cloud helm install --set controller.wallarm.enabled = true,controller.wallarm.token = <YOUR_CLOUD_NODE_TOKEN> <INGRESS_CONTROLLER_NAME> ingress-chart/wallarm-ingress -n <KUBERNETES_NAMESPACE> US Cloud helm install --set controller.wallarm.enabled = true,controller.wallarm.token = <YOUR_CLOUD_NODE_TOKEN>,controller.wallarm.apiHost = us1.api.wallarm.com <INGRESS_CONTROLLER_NAME> ingress-chart/wallarm-ingress -n <KUBERNETES_NAMESPACE> <YOUR_CLOUD_NODE_TOKEN> is the cloud WAF node token <INGRESS_CONTROLLER_NAME> is the name of the Wallarm Ingress controller <KUBERNETES_NAMESPACE> is the namespace of your Ingress","title":"Step 1: Installing the Wallarm Ingress Controller"},{"location":"admin-en/installation-kubernetes-en/#step-2-enabling-traffic-analysis-for-your-ingress","text":"kubectl annotate ingress <YOUR_INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-mode = monitoring kubectl annotate ingress <YOUR_INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-instance = <INSTANCE> <YOUR_INGRESS_NAME> is the name of your Ingress <INSTANCE> is a positive number that is unique to each of your applications or application groups. This will allow you to obtain separate statistics and to distinguish between attacks aimed at the corresponding applications","title":"Step 2: Enabling Traffic Analysis for Your Ingress"},{"location":"admin-en/installation-kubernetes-en/#step-3-checking-the-wallarm-ingress-controller-operation","text":"Get the list of pods specifying the name of the Wallarm Ingress controller in <INGRESS_CONTROLLER_NAME> : kubectl get pods -l release=<INGRESS_CONTROLLER_NAME> Each pod should display the following: \"STATUS: Running\" and \"READY: N/N\". For example: NAME READY STATUS RESTARTS AGE ingress-controller-nginx-ingress-controller-675c68d46d-cfck8 3/3 Running 0 5m ingress-controller-nginx-ingress-controller-wallarm-tarantljj8g 8/8 Running 0 5m ingress-controller-nginx-ingress-default-backend-584ffc6c7xj5xx 1/1 Running 0 5m Send the request with test SQLI and XSS attacks to the Wallarm Ingress controller address: curl http://<INGRESS_CONTROLLER_IP>/?id = 'or+1=1--a-<script>prompt(1)</script>' If the WAF node is working in the block mode, the code 403 Forbidden will be returned in the response to the request and attacks will be displayed in Wallarm Console \u2192 Nodes .","title":"Step 3: Checking the Wallarm Ingress Controller Operation"},{"location":"admin-en/installation-kubernetes-en/#configuration","text":"After the Wallarm Ingress controller is successfully installed and checked, you can make advanced configurations to the solution such as: Proper Reporting of End User Public IP Address Management of IP Addresses Blocking High Availability Considerations Ingress Controller Monitoring To find parameters used for advanced configuration and appropriate instructions, please follow the link .","title":"Configuration"},{"location":"admin-en/installation-nginx-overview/","text":"Installation Options Overview \u00b6 The filter node that is used with NGINX or NGINX Plus consists of the following modules: The module that connects to NGINX (NGINX Plus) The postanalytics module The modules installation and configuration order depends on the way you install NGINX or NGINX Plus. This document contains the following sections: Modules Overview Links to particular module installation and configuration documents Modules Overview \u00b6 When the filter node is used to process requests, incoming traffic sequentially proceeds through initial processing and then processing by Wallarm modules. The initial traffic processing is performed by the module that connects to NGINX or NGINX Plus that is already installed in the system. Further traffic processing is conducted by the postanalytics module , which requires a significant amount of memory to work properly. Therefore, you can pick one of the following installation options: Installed on the same servers as NGINX/NGINX Plus (if server configurations allow this) Installed on a group of servers separate from NGINX/NGINX Plus Installing and Configuring the Modules \u00b6 Module for NGINX \u00b6 Selecting the Module to Install The Wallarm module installation and connection procedures depend on the NGINX installation method you are using. The Wallarm module for NGINX can be connected by one of the following installation methods (links to instructions for each of the installation options are listed in the parenthesis): Building NGINX from the source files ( instruction ) Installing NGINX packages from the NGINX repository ( instruction ) Installing NGINX packages from the Debian repository ( instruction ) Installing NGINX packages from the CentOS repository ( instruction ) Module for NGINX Plus \u00b6 These instructions describe how to connect Wallarm to an NGINX Plus module. Postanalytics Module \u00b6 Instructions on the postanalytics module installation and configuration (either on the same server with NGINX/NGINX Plus or on a separate server) are located in the NGINX module installation and the NGINX Plus module installation sections.","title":"Installation nginx overview"},{"location":"admin-en/installation-nginx-overview/#installation-options-overview","text":"The filter node that is used with NGINX or NGINX Plus consists of the following modules: The module that connects to NGINX (NGINX Plus) The postanalytics module The modules installation and configuration order depends on the way you install NGINX or NGINX Plus. This document contains the following sections: Modules Overview Links to particular module installation and configuration documents","title":"Installation Options Overview"},{"location":"admin-en/installation-nginx-overview/#modules-overview","text":"When the filter node is used to process requests, incoming traffic sequentially proceeds through initial processing and then processing by Wallarm modules. The initial traffic processing is performed by the module that connects to NGINX or NGINX Plus that is already installed in the system. Further traffic processing is conducted by the postanalytics module , which requires a significant amount of memory to work properly. Therefore, you can pick one of the following installation options: Installed on the same servers as NGINX/NGINX Plus (if server configurations allow this) Installed on a group of servers separate from NGINX/NGINX Plus","title":"Modules Overview"},{"location":"admin-en/installation-nginx-overview/#installing-and-configuring-the-modules","text":"","title":"Installing and Configuring the Modules"},{"location":"admin-en/installation-nginx-overview/#module-for-nginx","text":"Selecting the Module to Install The Wallarm module installation and connection procedures depend on the NGINX installation method you are using. The Wallarm module for NGINX can be connected by one of the following installation methods (links to instructions for each of the installation options are listed in the parenthesis): Building NGINX from the source files ( instruction ) Installing NGINX packages from the NGINX repository ( instruction ) Installing NGINX packages from the Debian repository ( instruction ) Installing NGINX packages from the CentOS repository ( instruction )","title":"Module for NGINX"},{"location":"admin-en/installation-nginx-overview/#module-for-nginx-plus","text":"These instructions describe how to connect Wallarm to an NGINX Plus module.","title":"Module for NGINX Plus"},{"location":"admin-en/installation-nginx-overview/#postanalytics-module","text":"Instructions on the postanalytics module installation and configuration (either on the same server with NGINX/NGINX Plus or on a separate server) are located in the NGINX module installation and the NGINX Plus module installation sections.","title":"Postanalytics Module"},{"location":"admin-en/installation-postanalytics-en/","text":"Separate postanalytics module installation \u00b6 The processing of requests in the WAF is divided into two stages: Primary processing in the NGINX-Wallarm module. The processing is not memory demanding and can be put on frontend servers without changing the server requirements. Statistical analysis of the processed requests in the postanalytics module. Postanalytics is memory demanding, which may require changes in the server configuration or installation of postanalytics on a separate server. Depending on the system architecture, the NGINX-Wallarm and postanalytics modules can be installed on the same server or on different servers . These instructions provide the steps to install the postanalytics module on a separate server. Requirements \u00b6 NGINX-Wallarm module installed with NGINX stable from NGINX repository , NGINX from Debian/CentOS repositories or NGINX Plus Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud SELinux disabled or configured upon the instructions Executing all commands as a superuser (e.g. root ) Access to https://repo.wallarm.com to download packages. Ensure the access is not blocked by a firewall Access to https://api.wallarm.com:444 if working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 if working with US Wallarm Cloud. If access can be configured only via the proxy server, then use the instructions Access to GCP storage addresses to download an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers Installed text editor vim , nano , or any other. In the instruction, vim is used Installation \u00b6 1. Add Wallarm WAF repositories \u00b6 The postanalytics module, like other Wallarm WAF modules, is installed and updated from the Wallarm repositories. To add repositories, use the commands for your platform: Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 10.x (buster) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 20.04 LTS (focal) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node focal/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Amazon Linux 2 sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm 2. Install packages for the postanalytics module \u00b6 Install the wallarm-node-tarantool package from the Wallarm repository for the postanalytics module and Tarantool database: Debian sudo apt install --no-install-recommends wallarm-node-tarantool Ubuntu sudo apt install --no-install-recommends wallarm-node-tarantool CentOS or Amazon Linux 2 sudo yum install wallarm-node-tarantool 3. Connect the postanalytics module to Wallarm Cloud \u00b6 The postanalytics module interacts with the Wallarm Cloud. To connect the postanalytics module to the Cloud, it is required to create a separate WAF node for the postanalytics module. Created WAF node will get the security rules from the Cloud and upload attacks data to the Cloud. To create the WAF node and connect the postanalytics module to the Cloud: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled in the Wallarm Console. You can check mentioned settings by navigating to the users list in the EU Cloud or US Cloud . Run the addnode script in a system with the installed postanalytics module packages: EU Cloud sudo /usr/share/wallarm-common/addnode --no-sync US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com --no-sync Input the email and password for your account in the Wallarm Console. Input the postanalytics WAF node name or click Enter to use automatically generated name. Open the Wallarm Console \u2192 Nodes section in the EU Cloud or US Cloud and ensure a new WAF node is added to the list. 4. Update postanalytics module configuration \u00b6 The configuration files of the postanalytics module are located in the paths: /etc/default/wallarm-tarantool for Debian and Ubuntu operating systems /etc/sysconfig/wallarm-tarantool for CentOS and Amazon Linux 2 operating systems To open the file in the editing mode, please use the command: Debian sudo vim /etc/default/wallarm-tarantool Ubuntu sudo vim /etc/default/wallarm-tarantool CentOS or Amazon Linux 2 sudo vim /etc/sysconfig/wallarm-tarantool Memory \u00b6 The postanalytics module uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 75% of the total server memory. The allocated memory size is set in GB via the SLAB_ALLOC_ARENA directive in the wallarm-tarantool configuration file. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 Detailed recommendations about allocating memory for Tarantool are described in these instructions . Address of the separate postanalytics server \u00b6 To set the address of the separate postanalytics server: Uncomment the HOST and PORT variables in the wallarm-tarantool configuration file and set them the following values: # address and port for bind HOST = '0.0.0.0' PORT = 3313 If the configuration file of Tarantool is set up to accept connections on the IP addresses different from 0.0.0.0 or 127.0.0.1 , then please provide the addresses in /etc/wallarm/node.yaml : hostname: <name of postanalytics WAF node> uuid: <UUID of postanalytics WAF node> secret: <secret key of postanalytics WAF node> tarantool: host: '<IP address of Tarantool>' port: 3313 Add the address of the postanalytics module server to the configuration files on the server with the NGINX\u2011Wallarm package as described in the instructions for proper installation forms: NGINX stable from NGINX repository NGINX from Debian/CentOS repositories NGINX Plus 5. Restart Wallarm WAF services \u00b6 To apply the settings to the postanalytics and NGINX\u2011Wallarm modules: Restart the wallarm-tarantool service on the server with separate postanalytics module: Debian sudo systemctl restart wallarm-tarantool Ubuntu sudo systemctl restart wallarm-tarantool CentOS or Amazon Linux 2 sudo systemctl restart wallarm-tarantool Restart the NGINX service on the server with the NGINX\u2011Wallarm module: Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx 6. Check the NGINX\u2011Wallarm and separate postanalytics modules interaction \u00b6 To check the NGINX\u2011Wallarm and separate postanalytics modules interaction, you can send the request with test attack to the address of the protected application: curl http://localhost/?id = 'or+1=1--a-<script>prompt(1)</script>' If the NGINX\u2011Wallarm and separate postanalytics modules are configured properly, the attack will be uploaded to the Wallarm Cloud and displayed in the Events section of the Wallarm Console: If the attack was not uploaded to the Cloud, please check that there are no errors in the services operation: Make sure that the postanalytics service wallarm-tarantool is in the status active sudo systemctl status wallarm-tarantool Analyze the postanalytics module logs sudo cat /var/log/wallarm/tarantool.log If there is the record like SystemError binary: failed to bind: Cannot assign requested address , make sure that the server accepts connection on specified address and port. On the server with the NGINX\u2011Wallarm module, analyze the NGINX logs: sudo cat /var/log/nginx/error.log If there is the record like [error] wallarm: <address> connect() failed , make sure that the address of separate postanalytics module is specified correctly in the NGINX\u2011Wallarm module configuration files and separate postanalytics server accepts connection on specified address and port. On the server with the NGINX\u2011Wallarm module, get the statistics on processed requests using the command below and make sure that the value of tnt_errors is 0 curl http://127.0.0.8/wallarm-status Description of all parameters returned by the statistics service \u2192 Postanalytics module protection \u00b6 Protect installed postanalytics module We highly recommend to protect a newly installed Wallarm postanalytics module with a firewall. Otherwise, there is a risk of getting unauthorized access to the service that may result in: disclosure of information about processed requests possibility of executing arbitrary Lua code and operating system commands Please note that no such risk exists if you are deploying the postanalytics module alongside with the NGINX-Wallarm module on the same server. This holds true because the postanalytics module will listen to the port 3313 . Here are the firewall settings that should be applied to the separately installed postanalytics module: Allow the HTTPS traffic to and from the Wallarm API servers, so the postanalytics module can interact with these servers: api.wallarm.com:444 is the API server in the EU Wallarm Cloud us1.api.wallarm.com:444 is the API server in the US Wallarm Cloud Restrict the access to the 3313 Tarantool port via TCP and UDP protocols by allowing connections only from the IP addresses of the Wallarm WAF nodes.","title":"Separate postanalytics module installation"},{"location":"admin-en/installation-postanalytics-en/#separate-postanalytics-module-installation","text":"The processing of requests in the WAF is divided into two stages: Primary processing in the NGINX-Wallarm module. The processing is not memory demanding and can be put on frontend servers without changing the server requirements. Statistical analysis of the processed requests in the postanalytics module. Postanalytics is memory demanding, which may require changes in the server configuration or installation of postanalytics on a separate server. Depending on the system architecture, the NGINX-Wallarm and postanalytics modules can be installed on the same server or on different servers . These instructions provide the steps to install the postanalytics module on a separate server.","title":"Separate postanalytics module installation"},{"location":"admin-en/installation-postanalytics-en/#requirements","text":"NGINX-Wallarm module installed with NGINX stable from NGINX repository , NGINX from Debian/CentOS repositories or NGINX Plus Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud SELinux disabled or configured upon the instructions Executing all commands as a superuser (e.g. root ) Access to https://repo.wallarm.com to download packages. Ensure the access is not blocked by a firewall Access to https://api.wallarm.com:444 if working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 if working with US Wallarm Cloud. If access can be configured only via the proxy server, then use the instructions Access to GCP storage addresses to download an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers Installed text editor vim , nano , or any other. In the instruction, vim is used","title":"Requirements"},{"location":"admin-en/installation-postanalytics-en/#installation","text":"","title":"Installation"},{"location":"admin-en/installation-postanalytics-en/#1-add-wallarm-waf-repositories","text":"The postanalytics module, like other Wallarm WAF modules, is installed and updated from the Wallarm repositories. To add repositories, use the commands for your platform: Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 10.x (buster) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 20.04 LTS (focal) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node focal/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Amazon Linux 2 sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm","title":"1. Add Wallarm WAF repositories"},{"location":"admin-en/installation-postanalytics-en/#2-install-packages-for-the-postanalytics-module","text":"Install the wallarm-node-tarantool package from the Wallarm repository for the postanalytics module and Tarantool database: Debian sudo apt install --no-install-recommends wallarm-node-tarantool Ubuntu sudo apt install --no-install-recommends wallarm-node-tarantool CentOS or Amazon Linux 2 sudo yum install wallarm-node-tarantool","title":"2. Install packages for the postanalytics module"},{"location":"admin-en/installation-postanalytics-en/#3-connect-the-postanalytics-module-to-wallarm-cloud","text":"The postanalytics module interacts with the Wallarm Cloud. To connect the postanalytics module to the Cloud, it is required to create a separate WAF node for the postanalytics module. Created WAF node will get the security rules from the Cloud and upload attacks data to the Cloud. To create the WAF node and connect the postanalytics module to the Cloud: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled in the Wallarm Console. You can check mentioned settings by navigating to the users list in the EU Cloud or US Cloud . Run the addnode script in a system with the installed postanalytics module packages: EU Cloud sudo /usr/share/wallarm-common/addnode --no-sync US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com --no-sync Input the email and password for your account in the Wallarm Console. Input the postanalytics WAF node name or click Enter to use automatically generated name. Open the Wallarm Console \u2192 Nodes section in the EU Cloud or US Cloud and ensure a new WAF node is added to the list.","title":"3. Connect the postanalytics module to Wallarm Cloud"},{"location":"admin-en/installation-postanalytics-en/#4-update-postanalytics-module-configuration","text":"The configuration files of the postanalytics module are located in the paths: /etc/default/wallarm-tarantool for Debian and Ubuntu operating systems /etc/sysconfig/wallarm-tarantool for CentOS and Amazon Linux 2 operating systems To open the file in the editing mode, please use the command: Debian sudo vim /etc/default/wallarm-tarantool Ubuntu sudo vim /etc/default/wallarm-tarantool CentOS or Amazon Linux 2 sudo vim /etc/sysconfig/wallarm-tarantool","title":"4. Update postanalytics module configuration"},{"location":"admin-en/installation-postanalytics-en/#memory","text":"The postanalytics module uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 75% of the total server memory. The allocated memory size is set in GB via the SLAB_ALLOC_ARENA directive in the wallarm-tarantool configuration file. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 Detailed recommendations about allocating memory for Tarantool are described in these instructions .","title":"Memory"},{"location":"admin-en/installation-postanalytics-en/#address-of-the-separate-postanalytics-server","text":"To set the address of the separate postanalytics server: Uncomment the HOST and PORT variables in the wallarm-tarantool configuration file and set them the following values: # address and port for bind HOST = '0.0.0.0' PORT = 3313 If the configuration file of Tarantool is set up to accept connections on the IP addresses different from 0.0.0.0 or 127.0.0.1 , then please provide the addresses in /etc/wallarm/node.yaml : hostname: <name of postanalytics WAF node> uuid: <UUID of postanalytics WAF node> secret: <secret key of postanalytics WAF node> tarantool: host: '<IP address of Tarantool>' port: 3313 Add the address of the postanalytics module server to the configuration files on the server with the NGINX\u2011Wallarm package as described in the instructions for proper installation forms: NGINX stable from NGINX repository NGINX from Debian/CentOS repositories NGINX Plus","title":"Address of the separate postanalytics server"},{"location":"admin-en/installation-postanalytics-en/#5-restart-wallarm-waf-services","text":"To apply the settings to the postanalytics and NGINX\u2011Wallarm modules: Restart the wallarm-tarantool service on the server with separate postanalytics module: Debian sudo systemctl restart wallarm-tarantool Ubuntu sudo systemctl restart wallarm-tarantool CentOS or Amazon Linux 2 sudo systemctl restart wallarm-tarantool Restart the NGINX service on the server with the NGINX\u2011Wallarm module: Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx","title":"5. Restart Wallarm WAF services"},{"location":"admin-en/installation-postanalytics-en/#6-check-the-nginxwallarm-and-separate-postanalytics-modules-interaction","text":"To check the NGINX\u2011Wallarm and separate postanalytics modules interaction, you can send the request with test attack to the address of the protected application: curl http://localhost/?id = 'or+1=1--a-<script>prompt(1)</script>' If the NGINX\u2011Wallarm and separate postanalytics modules are configured properly, the attack will be uploaded to the Wallarm Cloud and displayed in the Events section of the Wallarm Console: If the attack was not uploaded to the Cloud, please check that there are no errors in the services operation: Make sure that the postanalytics service wallarm-tarantool is in the status active sudo systemctl status wallarm-tarantool Analyze the postanalytics module logs sudo cat /var/log/wallarm/tarantool.log If there is the record like SystemError binary: failed to bind: Cannot assign requested address , make sure that the server accepts connection on specified address and port. On the server with the NGINX\u2011Wallarm module, analyze the NGINX logs: sudo cat /var/log/nginx/error.log If there is the record like [error] wallarm: <address> connect() failed , make sure that the address of separate postanalytics module is specified correctly in the NGINX\u2011Wallarm module configuration files and separate postanalytics server accepts connection on specified address and port. On the server with the NGINX\u2011Wallarm module, get the statistics on processed requests using the command below and make sure that the value of tnt_errors is 0 curl http://127.0.0.8/wallarm-status Description of all parameters returned by the statistics service \u2192","title":"6. Check the NGINX\u2011Wallarm and separate postanalytics modules interaction"},{"location":"admin-en/installation-postanalytics-en/#postanalytics-module-protection","text":"Protect installed postanalytics module We highly recommend to protect a newly installed Wallarm postanalytics module with a firewall. Otherwise, there is a risk of getting unauthorized access to the service that may result in: disclosure of information about processed requests possibility of executing arbitrary Lua code and operating system commands Please note that no such risk exists if you are deploying the postanalytics module alongside with the NGINX-Wallarm module on the same server. This holds true because the postanalytics module will listen to the port 3313 . Here are the firewall settings that should be applied to the separately installed postanalytics module: Allow the HTTPS traffic to and from the Wallarm API servers, so the postanalytics module can interact with these servers: api.wallarm.com:444 is the API server in the EU Wallarm Cloud us1.api.wallarm.com:444 is the API server in the US Wallarm Cloud Restrict the access to the 3313 Tarantool port via TCP and UDP protocols by allowing connections only from the IP addresses of the Wallarm WAF nodes.","title":"Postanalytics module protection"},{"location":"admin-en/mirror-traffic-en/","text":"Analyzing Mirrored Traffic with NGINX \u00b6 Starting with NGINX 1.13 you can mirror the traffic to an additional backend. Installing a Wallarm node as the addtional backend lets you run an analysis of the traffic copy without considerable changes to the production system. This setup is useful to: Quickly pilot a project. Be sure that the protection service will not affect application's performance. Train the system on the traffic copy before running the module on the production system. Limitations: Only requests are analyzed; responses are not analyzed. The attacks are not blocked in real time on the request level. Configuration \u00b6 General Diagram \u00b6 Configure NGINX: install the module and configure the request mirroring. Install and configure the Wallarm node. See Installing as a dynamic module for NGINX . On step 1, install the mirroring module ngx_http_mirror_module and configure the request mirroring to an additional backend. On step 2, as an additional backend, install the Wallarm node. Configure NGINX \u00b6 Configure mirroring with the mirror directive that you can set inside the location or server block. Example of mirrored requests to location / on location /mirror-test : location / { mirror /mirror-test; mirror_request_body on; root /usr/share/nginx/html; index index.html index.htm; } location /mirror-test { internal; #proxy_pass http://111.11.111.1$request_uri; proxy_pass http://222.222.222.222$request_uri; proxy_set_header X-SERVER-PORT $server_port; proxy_set_header X-SERVER-ADDR $server_addr; proxy_set_header HOST $http_host; proxy_set_header X-REAL-IP $remote_addr; proxy_set_header X-Request-ID $request_id; } In location , to send the mirrored traffic, you must list the headers that will be sent. As an IP address, set the machine with the Wallarm node that receives the traffic copy. Configure the Wallarm Node to Receive the Mirrored Traffic \u00b6 To have the Wallarm interface display the IP addresses of the attackers, configure real_ip_header and disable the request processing \u2013 the requests will not be analyzed with the wallarm_force_response_* directives as only copies of the requests are sent. Example of the configured real_ip_header with the disabled requests handling: wallarm_force server_addr $http_x_server_addr; wallarm_force server_port $http_x_server_port; #Change 222.222.222.22 to the mirror server address set_real_ip_from 222.222.222.22; real_ip_header X-REAL-IP; #real_ip_recursive on; wallarm_force response_status 0; wallarm_force response_time 0; wallarm_force response_size 0;","title":"Mirror traffic en"},{"location":"admin-en/mirror-traffic-en/#analyzing-mirrored-traffic-with-nginx","text":"Starting with NGINX 1.13 you can mirror the traffic to an additional backend. Installing a Wallarm node as the addtional backend lets you run an analysis of the traffic copy without considerable changes to the production system. This setup is useful to: Quickly pilot a project. Be sure that the protection service will not affect application's performance. Train the system on the traffic copy before running the module on the production system. Limitations: Only requests are analyzed; responses are not analyzed. The attacks are not blocked in real time on the request level.","title":"Analyzing Mirrored Traffic with NGINX"},{"location":"admin-en/mirror-traffic-en/#configuration","text":"","title":"Configuration"},{"location":"admin-en/mirror-traffic-en/#general-diagram","text":"Configure NGINX: install the module and configure the request mirroring. Install and configure the Wallarm node. See Installing as a dynamic module for NGINX . On step 1, install the mirroring module ngx_http_mirror_module and configure the request mirroring to an additional backend. On step 2, as an additional backend, install the Wallarm node.","title":"General Diagram"},{"location":"admin-en/mirror-traffic-en/#configure-nginx","text":"Configure mirroring with the mirror directive that you can set inside the location or server block. Example of mirrored requests to location / on location /mirror-test : location / { mirror /mirror-test; mirror_request_body on; root /usr/share/nginx/html; index index.html index.htm; } location /mirror-test { internal; #proxy_pass http://111.11.111.1$request_uri; proxy_pass http://222.222.222.222$request_uri; proxy_set_header X-SERVER-PORT $server_port; proxy_set_header X-SERVER-ADDR $server_addr; proxy_set_header HOST $http_host; proxy_set_header X-REAL-IP $remote_addr; proxy_set_header X-Request-ID $request_id; } In location , to send the mirrored traffic, you must list the headers that will be sent. As an IP address, set the machine with the Wallarm node that receives the traffic copy.","title":"Configure NGINX"},{"location":"admin-en/mirror-traffic-en/#configure-the-wallarm-node-to-receive-the-mirrored-traffic","text":"To have the Wallarm interface display the IP addresses of the attackers, configure real_ip_header and disable the request processing \u2013 the requests will not be analyzed with the wallarm_force_response_* directives as only copies of the requests are sent. Example of the configured real_ip_header with the disabled requests handling: wallarm_force server_addr $http_x_server_addr; wallarm_force server_port $http_x_server_port; #Change 222.222.222.22 to the mirror server address set_real_ip_from 222.222.222.22; real_ip_header X-REAL-IP; #real_ip_recursive on; wallarm_force response_status 0; wallarm_force response_time 0; wallarm_force response_size 0;","title":"Configure the Wallarm Node to Receive the Mirrored Traffic"},{"location":"admin-en/scanner-address-en/","text":"Scanner Addresses for EU Cloud \u00b6 Info This list only works for users who use the EU Cloud https://my.wallarm.com . If you are using the US Cloud https://us1.my.wallarm.com , proceed to the list of scanner addresses for US Cloud . The list of IP addresses for EU Cloud from which the Wallarm Scanner scans company resources for vulnerabilities . 139.162.148.184 139.162.151.155 139.162.159.244 139.162.151.10 139.162.159.137 139.162.157.131 139.162.144.202 139.162.130.66 139.162.158.79 139.162.156.102 85.90.246.120 172.104.128.215 139.162.174.26 139.162.182.20 139.162.190.22 139.162.168.17 139.162.167.19 139.162.170.84 139.162.191.89 139.162.176.169 139.162.184.225 139.162.185.243 172.104.143.34 139.162.178.148 139.162.186.136 139.162.163.61 139.162.171.141 139.162.179.214 139.162.187.138 139.162.164.41 139.162.172.35 139.162.180.37 139.162.188.246 172.104.139.18 172.104.152.28 139.162.177.83 172.104.240.115 172.105.64.135 139.162.153.16 172.104.241.162 139.162.167.48 172.104.233.100 172.104.157.26 172.105.65.182 172.104.138.5 172.104.150.243 139.162.190.165 139.162.166.202 139.162.174.220 139.162.182.156 139.162.190.86 139.162.167.51 139.162.175.71 172.104.152.54 172.104.151.59 172.104.128.67 172.104.152.96 139.162.145.238 172.104.128.103 172.104.139.37 139.162.184.33 139.162.186.129 172.104.154.128 172.104.146.90 172.104.128.44 85.90.246.49 139.162.146.245 139.162.171.208 172.104.252.112 139.162.132.87 139.162.162.71 172.104.229.59 172.104.152.244 172.104.250.27 139.162.130.123 IP addresses used by Wallarm to recheck attacks and vulnerabilities: 34.90.246.212 34.91.42.9 34.91.17.225 35.204.191.122 34.91.120.174 34.90.36.233 34.91.38.9 35.204.162.101 34.90.29.52 34.91.79.224 34.91.110.164 34.90.123.93 34.91.54.247 34.91.133.93 34.90.114.30 35.204.60.30 Get the list of IP addresses Download the plain text file containing list of scanners' IP addresses.","title":"Scanner address en"},{"location":"admin-en/scanner-address-en/#scanner-addresses-for-eu-cloud","text":"Info This list only works for users who use the EU Cloud https://my.wallarm.com . If you are using the US Cloud https://us1.my.wallarm.com , proceed to the list of scanner addresses for US Cloud . The list of IP addresses for EU Cloud from which the Wallarm Scanner scans company resources for vulnerabilities . 139.162.148.184 139.162.151.155 139.162.159.244 139.162.151.10 139.162.159.137 139.162.157.131 139.162.144.202 139.162.130.66 139.162.158.79 139.162.156.102 85.90.246.120 172.104.128.215 139.162.174.26 139.162.182.20 139.162.190.22 139.162.168.17 139.162.167.19 139.162.170.84 139.162.191.89 139.162.176.169 139.162.184.225 139.162.185.243 172.104.143.34 139.162.178.148 139.162.186.136 139.162.163.61 139.162.171.141 139.162.179.214 139.162.187.138 139.162.164.41 139.162.172.35 139.162.180.37 139.162.188.246 172.104.139.18 172.104.152.28 139.162.177.83 172.104.240.115 172.105.64.135 139.162.153.16 172.104.241.162 139.162.167.48 172.104.233.100 172.104.157.26 172.105.65.182 172.104.138.5 172.104.150.243 139.162.190.165 139.162.166.202 139.162.174.220 139.162.182.156 139.162.190.86 139.162.167.51 139.162.175.71 172.104.152.54 172.104.151.59 172.104.128.67 172.104.152.96 139.162.145.238 172.104.128.103 172.104.139.37 139.162.184.33 139.162.186.129 172.104.154.128 172.104.146.90 172.104.128.44 85.90.246.49 139.162.146.245 139.162.171.208 172.104.252.112 139.162.132.87 139.162.162.71 172.104.229.59 172.104.152.244 172.104.250.27 139.162.130.123 IP addresses used by Wallarm to recheck attacks and vulnerabilities: 34.90.246.212 34.91.42.9 34.91.17.225 35.204.191.122 34.91.120.174 34.90.36.233 34.91.38.9 35.204.162.101 34.90.29.52 34.91.79.224 34.91.110.164 34.90.123.93 34.91.54.247 34.91.133.93 34.90.114.30 35.204.60.30 Get the list of IP addresses Download the plain text file containing list of scanners' IP addresses.","title":"Scanner Addresses for EU Cloud"},{"location":"admin-en/scanner-address-us-en/","text":"Scanner Addresses for US Cloud \u00b6 Info This list only works for users who use the US Cloud https://us1.my.wallarm.com . If you are using the EU Cloud https://my.wallarm.com , proceed to the list of scanner addresses for EU Cloud . The list of IP addresses for US Cloud from which the Wallarm Scanner scans for vulnerabilities . 50.116.11.251 45.79.143.18 172.104.21.210 74.207.237.202 45.79.186.159 45.79.216.187 45.33.16.32 96.126.127.23 172.104.208.113 192.81.135.28 104.237.155.105 45.56.71.221 45.79.194.128 104.237.151.202 45.33.15.249 45.33.43.225 45.79.10.15 45.33.79.18 45.79.75.59 23.239.30.236 172.104.22.150 45.33.86.254 45.56.72.191 45.79.75.91 192.155.92.134 23.239.4.41 45.79.93.164 45.56.122.184 96.126.124.141 45.79.115.178 66.228.36.28 192.81.134.116 45.79.138.122 45.56.68.241 69.164.216.244 104.237.139.12 23.239.18.250 45.56.123.144 50.116.35.43 50.116.43.110 72.14.181.105 45.56.102.9 173.255.192.83 173.255.200.80 173.230.156.200 173.255.214.180 45.33.81.109 173.230.158.207 50.116.42.181 72.14.184.100 66.175.222.237 69.164.202.55 45.56.113.41 192.155.82.205 23.92.30.204 45.33.65.37 45.56.114.24 173.255.193.92 45.33.64.71 104.200.29.36 45.56.119.39 45.33.105.35 45.33.73.43 45.33.72.81 45.33.80.65 104.237.151.23 45.33.88.42 72.14.191.76 45.33.98.89 173.230.130.253 45.33.97.86 23.92.18.13 45.33.33.19 23.239.11.21 45.33.41.31 173.230.138.206 66.228.58.101 45.56.104.7 50.116.23.110 45.33.115.7 45.79.16.240 45.56.69.211 IP addresses used by Wallarm to recheck attacks and vulnerabilities: 34.94.100.64 35.235.101.133 34.94.16.235 35.236.51.79 35.236.55.214 35.236.127.211 35.236.126.84 35.236.3.158 34.94.218.5 35.236.118.146 35.236.1.4 35.236.20.89 Get the list of IP addresses Download the plain text file containing list of scanners' IP addresses.","title":"Scanner address us en"},{"location":"admin-en/scanner-address-us-en/#scanner-addresses-for-us-cloud","text":"Info This list only works for users who use the US Cloud https://us1.my.wallarm.com . If you are using the EU Cloud https://my.wallarm.com , proceed to the list of scanner addresses for EU Cloud . The list of IP addresses for US Cloud from which the Wallarm Scanner scans for vulnerabilities . 50.116.11.251 45.79.143.18 172.104.21.210 74.207.237.202 45.79.186.159 45.79.216.187 45.33.16.32 96.126.127.23 172.104.208.113 192.81.135.28 104.237.155.105 45.56.71.221 45.79.194.128 104.237.151.202 45.33.15.249 45.33.43.225 45.79.10.15 45.33.79.18 45.79.75.59 23.239.30.236 172.104.22.150 45.33.86.254 45.56.72.191 45.79.75.91 192.155.92.134 23.239.4.41 45.79.93.164 45.56.122.184 96.126.124.141 45.79.115.178 66.228.36.28 192.81.134.116 45.79.138.122 45.56.68.241 69.164.216.244 104.237.139.12 23.239.18.250 45.56.123.144 50.116.35.43 50.116.43.110 72.14.181.105 45.56.102.9 173.255.192.83 173.255.200.80 173.230.156.200 173.255.214.180 45.33.81.109 173.230.158.207 50.116.42.181 72.14.184.100 66.175.222.237 69.164.202.55 45.56.113.41 192.155.82.205 23.92.30.204 45.33.65.37 45.56.114.24 173.255.193.92 45.33.64.71 104.200.29.36 45.56.119.39 45.33.105.35 45.33.73.43 45.33.72.81 45.33.80.65 104.237.151.23 45.33.88.42 72.14.191.76 45.33.98.89 173.230.130.253 45.33.97.86 23.92.18.13 45.33.33.19 23.239.11.21 45.33.41.31 173.230.138.206 66.228.58.101 45.56.104.7 50.116.23.110 45.33.115.7 45.79.16.240 45.56.69.211 IP addresses used by Wallarm to recheck attacks and vulnerabilities: 34.94.100.64 35.235.101.133 34.94.16.235 35.236.51.79 35.236.55.214 35.236.127.211 35.236.126.84 35.236.3.158 34.94.218.5 35.236.118.146 35.236.1.4 35.236.20.89 Get the list of IP addresses Download the plain text file containing list of scanners' IP addresses.","title":"Scanner Addresses for US Cloud"},{"location":"admin-en/scanner-complaint-en/","text":"Contacting Wallarm Support to Stop the Resource Scanner \u00b6 If the Wallarm scanner scans your company's resources that you never set for discovery, contact Wallarm Support to exclude the resource from scanning.","title":"Scanner complaint en"},{"location":"admin-en/scanner-complaint-en/#contacting-wallarm-support-to-stop-the-resource-scanner","text":"If the Wallarm scanner scans your company's resources that you never set for discovery, contact Wallarm Support to exclude the resource from scanning.","title":"Contacting Wallarm Support to Stop the Resource Scanner"},{"location":"admin-en/split-by-apps/","text":"Splitting traffic processing by applications \u00b6 Splitting is for ... To split traffic processing, add wallarm_instances ...","title":"Splitting traffic processing by applications or environment"},{"location":"admin-en/split-by-apps/#splitting-traffic-processing-by-applications","text":"Splitting is for ... To split traffic processing, add wallarm_instances ...","title":"Splitting traffic processing by applications"},{"location":"admin-en/supported-platforms/","text":"Wallarm WAF deployment options \u00b6 Web servers and API gateways \u00b6 NGINX Stable Deploy WAF node compatible to NGINX Stable with... AWS, GCP, Azure, etc Docker container Kubernetes DEB or RPM packages NGINX Plus Deploy WAF node compatible to NGINX Plus with... AWS, GCP, Azure, etc DEB or RPM packages Kong Deploy WAF node compatible to Kong with... AWS, GCP, Azure, etc DEB or RPM packages Envoy Deploy WAF node compatible to Envoy with... Docker container Cloud platforms \u00b6 AWS Deploy WAF node distributed as... AWS Marketplace image Docker container DEB or RPM packages Google Cloud Platform Deploy WAF node distributed as... GCP Marketplace image Docker container DEB or RPM packages Yandex.Cloud Deploy WAF node distributed as... Yandex.Cloud Marketplace image Docker container DEB or RPM packages Microsoft Azure Deploy WAF node distributed as... Docker container DEB or RPM packages Alibaba Cloud Deploy WAF node distributed as... Docker container DEB or RPM packages Private clouds Deploy WAF node distributed as... Docker container DEB or RPM packages Docker images \u00b6 Docker image (NGINX) Deploy the NGINX-based WAF node as a Docker container Docker image (Envoy) Deploy the Envoy-based WAF node as a Docker container Kubernetes \u00b6 K8s Ingress controller Deploy the Wallarm Ingress controller based on the official NGINX Ingress controller K8s Sidecar container Run the NGINX-based WAF node as the K8s sidecar container DEB and RPM packages \u00b6 Debian 9.x Stretch Install the WAF module for... NGINX Stable NGINX Plus NGINX from Debian repo Kong Debian 9.x Stretch (backports) Install the WAF module for... NGINX from Debian repo Debian 10.x Buster Install the WAF module for... NGINX Stable NGINX Plus NGINX from Debian repo Ubuntu 18.04 Bionic Install the WAF module for... NGINX Stable NGINX Plus Kong Ubuntu 20.04 Focal Install the WAF module for... NGINX Stable NGINX Plus CentOS 7.x Install the WAF module for... NGINX Stable NGINX Plus NGINX from CentOS repo Kong CentOS 8.x Install the WAF module for... NGINX Stable NGINX Plus NGINX from CentOS repo Amazon Linux 2 Install the WAF module for... NGINX Stable NGINX Plus","title":"Deployment options"},{"location":"admin-en/supported-platforms/#wallarm-waf-deployment-options","text":"","title":"Wallarm WAF deployment options"},{"location":"admin-en/supported-platforms/#web-servers-and-api-gateways","text":"","title":"Web servers and API gateways"},{"location":"admin-en/supported-platforms/#cloud-platforms","text":"","title":"Cloud platforms"},{"location":"admin-en/supported-platforms/#docker-images","text":"","title":"Docker images"},{"location":"admin-en/supported-platforms/#kubernetes","text":"","title":"Kubernetes"},{"location":"admin-en/supported-platforms/#deb-and-rpm-packages","text":"","title":"DEB and RPM packages"},{"location":"admin-en/uat-checklist-en/","text":"Wallarm User Acceptance Testing Checklist \u00b6 This section provides you with a checklist to ensure your Wallarm instance operates correctly. Operation Expected behavior Check Wallarm node detects attacks Attacks are detected You can log into the Wallarm interface You can log in Wallarm interface shows requests per second You see the requests stats Wallarm marks requests as false and stops blocking them Wallarm does not block the requests Wallarm detects vulnerabilities and creates security incidents Security incidents are created Wallarm detects perimeter Scope is discovered IP whitelisting, blacklisting, and greylisting work IP addresses are blocked Users can be configured and have proper access rights Users can be created and updated User activity log has records The log has records Reporting works You receive reports Wallarm Node Detects Attacks \u00b6 Send a malicious request to your resource: http://<resource_URL>/?id='or+1=1--a-<script>prompt(1)</script>' Run the following command to check if the attack count increased: curl http://127.0.0.8/wallarm-status See also Checking the filter node operation You Can Log into the Wallarm Interface \u00b6 Proceed to the link that corresponds to the cloud you are using: If you are using the EU cloud, proceed to the https://my.wallarm.com/ link. If you are using the US cloud, proceed to the https://us1.my.wallarm.com link. See if you can log in successfully. See also Dashboard overview . Wallarm Interface Shows Requests per Second \u00b6 Send a request to your resource: curl http://<resource_URL> Or send several requests with a bash script: for (( i=0 ; $i<10 ; i++ )) ; do curl http://<resource_URL> ; done This example is for 10 requests. Check if the Wallarm interface shows detected requests per second. See also The \"WAF\" Dashboard . Wallarm Marks Requests as False and Stops Blocking them \u00b6 Expand an attack on the Attacks tab. Select a hit and click False . Wait for around 3 minutes. Resend the request and check if Wallarm detects it as an attack and blocks it. See also Working with false attacks . Wallarm Detects Vulnerabilities and Creates Security Incidents \u00b6 Ensure you have an open vulnerability on your resource. Send a malicious request to exploit the vulnerability. Check if there is an incident detected in the Wallarm interface. See also Checking attacks and incidents . Wallarm Detects Perimeter \u00b6 On the Scanner tab, add your resource's domain. Check if Wallarm discovers all resources associated with the added domain. See also Working with the scanner . IP whitelisting, blacklisting, and greylisting work \u00b6 Learn core logic of IP lists . Add IP addresses to the whitelist , blacklist , and greylist . Check that the WAF node correctly processes requests originated from IPs added to the lists. Users Can Be Configured and Have Proper Access Rights \u00b6 Ensure you have the Administrator role in the Wallarm system. Create, change role, disable, and delete a user as described in Configuring users . See also Configuring users . User Activity Log Has Records \u00b6 Go to Settings \u2013> Users . Check that User Activity Log has records. See also User activity log . Reporting Works \u00b6 On the Attacks tab, put in a search query. Click the report button on the right. Put in your email and click the report button again. Check if you receive the report. See also Creating a custom report .","title":"Uat checklist en"},{"location":"admin-en/uat-checklist-en/#wallarm-user-acceptance-testing-checklist","text":"This section provides you with a checklist to ensure your Wallarm instance operates correctly. Operation Expected behavior Check Wallarm node detects attacks Attacks are detected You can log into the Wallarm interface You can log in Wallarm interface shows requests per second You see the requests stats Wallarm marks requests as false and stops blocking them Wallarm does not block the requests Wallarm detects vulnerabilities and creates security incidents Security incidents are created Wallarm detects perimeter Scope is discovered IP whitelisting, blacklisting, and greylisting work IP addresses are blocked Users can be configured and have proper access rights Users can be created and updated User activity log has records The log has records Reporting works You receive reports","title":"Wallarm User Acceptance Testing Checklist"},{"location":"admin-en/uat-checklist-en/#wallarm-node-detects-attacks","text":"Send a malicious request to your resource: http://<resource_URL>/?id='or+1=1--a-<script>prompt(1)</script>' Run the following command to check if the attack count increased: curl http://127.0.0.8/wallarm-status See also Checking the filter node operation","title":"Wallarm Node Detects Attacks"},{"location":"admin-en/uat-checklist-en/#you-can-log-into-the-wallarm-interface","text":"Proceed to the link that corresponds to the cloud you are using: If you are using the EU cloud, proceed to the https://my.wallarm.com/ link. If you are using the US cloud, proceed to the https://us1.my.wallarm.com link. See if you can log in successfully. See also Dashboard overview .","title":"You Can Log into the Wallarm Interface"},{"location":"admin-en/uat-checklist-en/#wallarm-interface-shows-requests-per-second","text":"Send a request to your resource: curl http://<resource_URL> Or send several requests with a bash script: for (( i=0 ; $i<10 ; i++ )) ; do curl http://<resource_URL> ; done This example is for 10 requests. Check if the Wallarm interface shows detected requests per second. See also The \"WAF\" Dashboard .","title":"Wallarm Interface Shows Requests per Second"},{"location":"admin-en/uat-checklist-en/#wallarm-marks-requests-as-false-and-stops-blocking-them","text":"Expand an attack on the Attacks tab. Select a hit and click False . Wait for around 3 minutes. Resend the request and check if Wallarm detects it as an attack and blocks it. See also Working with false attacks .","title":"Wallarm Marks Requests as False and Stops Blocking them"},{"location":"admin-en/uat-checklist-en/#wallarm-detects-vulnerabilities-and-creates-security-incidents","text":"Ensure you have an open vulnerability on your resource. Send a malicious request to exploit the vulnerability. Check if there is an incident detected in the Wallarm interface. See also Checking attacks and incidents .","title":"Wallarm Detects Vulnerabilities and Creates Security Incidents"},{"location":"admin-en/uat-checklist-en/#wallarm-detects-perimeter","text":"On the Scanner tab, add your resource's domain. Check if Wallarm discovers all resources associated with the added domain. See also Working with the scanner .","title":"Wallarm Detects Perimeter"},{"location":"admin-en/uat-checklist-en/#ip-whitelisting-blacklisting-and-greylisting-work","text":"Learn core logic of IP lists . Add IP addresses to the whitelist , blacklist , and greylist . Check that the WAF node correctly processes requests originated from IPs added to the lists.","title":"IP whitelisting, blacklisting, and greylisting work"},{"location":"admin-en/uat-checklist-en/#users-can-be-configured-and-have-proper-access-rights","text":"Ensure you have the Administrator role in the Wallarm system. Create, change role, disable, and delete a user as described in Configuring users . See also Configuring users .","title":"Users Can Be Configured and Have Proper Access Rights"},{"location":"admin-en/uat-checklist-en/#user-activity-log-has-records","text":"Go to Settings \u2013> Users . Check that User Activity Log has records. See also User activity log .","title":"User Activity Log Has Records"},{"location":"admin-en/uat-checklist-en/#reporting-works","text":"On the Attacks tab, put in a search query. Click the report button on the right. Put in your email and click the report button again. Check if you receive the report. See also Creating a custom report .","title":"Reporting Works"},{"location":"admin-en/using-proxy-or-balancer-en/","text":"Settings for Using a Balancer or Proxy in Front of the Filter Node \u00b6 Who's this document for? This document contains information for users who have a proxy server or balancer installed that receives requests and proxies them to the Wallarm filter nodes. If your system does not have such a balancer, you can skip this configuration step. By default, the Wallarm filter node considers the IP address from which the request originated to be the IP address of the request source. If the request passed through a proxy server or load balancer before being sent to the node, the IP address of the balancer will be displayed in the web interface as the IP address of the request source. To correctly display the IP address of the request source in the Wallarm web interface, configure the balancer and the filter node to transmit the IP address of the source in the request header. The figure below shows an example using the X-Client-IP header by the HAProxy server to send the client IP address. To configure sending a client IP address in the request header by a proxy server or a balancer, follow the steps described in the following sections: Configuring a proxy server or load balancer Configuring the filter node Configuring a Proxy Server or Load Balancer \u00b6 Configure a proxy server or load balancer to write the IP address from which the request was received to the header of this request and send the request with the header to the filter node. To learn how to configure your proxy server or balancer, refer to its official documentation. The example below demonstrates how to configure the X-Client-IP header for the HAProxy balancer. HAProxy Balancer Setup Example \u00b6 The option forwardfor directive tells the HAProxy balancer that a header must be added to the request with the IP address of the client. You can use the X-Client-IP header for this purpose. In the /etc/haproxy/haproxy.cfg configuration file, insert the option forwardfor header X-Client-IP line into the backend directive block, which is responsible for connecting HAProxy to the Wallarm filter node. Details of the directive You can find detailed information about the option forwardfor directive in the official HAProxy documentation . An example fragment of the /etc/haproxy/haproxy.cfg configuration file is given below: # Public IP address for receiving requests frontend my_frontend bind <haproxy-ip> mode http default_backend my_backend # Backend with the Wallarm filter node backend my_backend mode http option forwardfor header X-Client-IP server wallarm-node <node-ip> In the example above <haproxy-ip> is the IP address of the HAProxy server to receive client requests; <node-ip> is the IP address of the Wallarm filter node to receive requests from the HAProxy server. Configuring the Filter Node \u00b6 For the Wallarm filter node to recognize the value of the X-Client-IP header as the request source address, add the set_real_ip_from and real_ip_header directives to the NGINX configuration file. The real_ip_header directive reports that the real IP address of the client that sent the request is transmitted in the X-Client-IP header. The set_real_ip_from directive specifies the IP address of your proxy server or a balancer from which requests with the X-Client-IP header are sent. If your system has several proxies or balancers, specify several set_real_ip_from directives with their IP addresses. You can also specify IP address ranges (for example, 1.2.3.0/24 ). Details of the directives You can find detailed information about the set_real_ip_from and real_ip_header directives in the NGINX official documentation . An example fragment of the /etc/nginx/conf.d/default.conf configuration file is given below: location / { # Setting of proxy and filtration mode of the node wallarm_mode block; # Settings of proxying requests to the protected application proxy_pass http://<app-ip>; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # Settings of determining the true source IP address of requests set_real_ip_from <proxy-ip1>; set_real_ip_from <proxy-ip2>; real_ip_header X-Client-IP; } In the example above <app-ip> is the IP address of the protected application for requests from the filter node; <proxy-ip1> , <proxy-ip2> is the IP addresses of proxies that pass requests to the Wallarm filter node. After you save the modified NGINX configuration file, restart NGINX: service nginx restart Checking results \u00b6 Perform a test attack and verify that the IP address of the request source is correctly displayed in the Wallarm web interface:","title":"Cofiguring real IP reporting"},{"location":"admin-en/using-proxy-or-balancer-en/#settings-for-using-a-balancer-or-proxy-in-front-of-the-filter-node","text":"Who's this document for? This document contains information for users who have a proxy server or balancer installed that receives requests and proxies them to the Wallarm filter nodes. If your system does not have such a balancer, you can skip this configuration step. By default, the Wallarm filter node considers the IP address from which the request originated to be the IP address of the request source. If the request passed through a proxy server or load balancer before being sent to the node, the IP address of the balancer will be displayed in the web interface as the IP address of the request source. To correctly display the IP address of the request source in the Wallarm web interface, configure the balancer and the filter node to transmit the IP address of the source in the request header. The figure below shows an example using the X-Client-IP header by the HAProxy server to send the client IP address. To configure sending a client IP address in the request header by a proxy server or a balancer, follow the steps described in the following sections: Configuring a proxy server or load balancer Configuring the filter node","title":"Settings for Using a Balancer or Proxy in Front of the Filter Node"},{"location":"admin-en/using-proxy-or-balancer-en/#configuring-a-proxy-server-or-load-balancer","text":"Configure a proxy server or load balancer to write the IP address from which the request was received to the header of this request and send the request with the header to the filter node. To learn how to configure your proxy server or balancer, refer to its official documentation. The example below demonstrates how to configure the X-Client-IP header for the HAProxy balancer.","title":"Configuring a Proxy Server or Load Balancer"},{"location":"admin-en/using-proxy-or-balancer-en/#haproxy-balancer-setup-example","text":"The option forwardfor directive tells the HAProxy balancer that a header must be added to the request with the IP address of the client. You can use the X-Client-IP header for this purpose. In the /etc/haproxy/haproxy.cfg configuration file, insert the option forwardfor header X-Client-IP line into the backend directive block, which is responsible for connecting HAProxy to the Wallarm filter node. Details of the directive You can find detailed information about the option forwardfor directive in the official HAProxy documentation . An example fragment of the /etc/haproxy/haproxy.cfg configuration file is given below: # Public IP address for receiving requests frontend my_frontend bind <haproxy-ip> mode http default_backend my_backend # Backend with the Wallarm filter node backend my_backend mode http option forwardfor header X-Client-IP server wallarm-node <node-ip> In the example above <haproxy-ip> is the IP address of the HAProxy server to receive client requests; <node-ip> is the IP address of the Wallarm filter node to receive requests from the HAProxy server.","title":"HAProxy Balancer Setup Example"},{"location":"admin-en/using-proxy-or-balancer-en/#configuring-the-filter-node","text":"For the Wallarm filter node to recognize the value of the X-Client-IP header as the request source address, add the set_real_ip_from and real_ip_header directives to the NGINX configuration file. The real_ip_header directive reports that the real IP address of the client that sent the request is transmitted in the X-Client-IP header. The set_real_ip_from directive specifies the IP address of your proxy server or a balancer from which requests with the X-Client-IP header are sent. If your system has several proxies or balancers, specify several set_real_ip_from directives with their IP addresses. You can also specify IP address ranges (for example, 1.2.3.0/24 ). Details of the directives You can find detailed information about the set_real_ip_from and real_ip_header directives in the NGINX official documentation . An example fragment of the /etc/nginx/conf.d/default.conf configuration file is given below: location / { # Setting of proxy and filtration mode of the node wallarm_mode block; # Settings of proxying requests to the protected application proxy_pass http://<app-ip>; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # Settings of determining the true source IP address of requests set_real_ip_from <proxy-ip1>; set_real_ip_from <proxy-ip2>; real_ip_header X-Client-IP; } In the example above <app-ip> is the IP address of the protected application for requests from the filter node; <proxy-ip1> , <proxy-ip2> is the IP addresses of proxies that pass requests to the Wallarm filter node. After you save the modified NGINX configuration file, restart NGINX: service nginx restart","title":"Configuring the Filter Node"},{"location":"admin-en/using-proxy-or-balancer-en/#checking-results","text":"Perform a test attack and verify that the IP address of the request source is correctly displayed in the Wallarm web interface:","title":"Checking results"},{"location":"admin-en/configuration-guides/access-to-wallarm-api-via-proxy/","text":"Access to Wallarm API via Proxy \u00b6 These instructions describe the steps to configure access to Wallarm API via the proxy server. https://api.wallarm.com/ for the EU cloud https://us1.api.wallarm.com/ for the US cloud To configure access, please assign new values to the environment variables defining the proxy server used in the /etc/environment file: https_proxy to define a proxy for the HTTPS protocol http_proxy to define a proxy for the HTTP protocol no_proxy to define the list of the resources proxy should not be used for https_proxy and http_proxy values \u00b6 Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables: <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for <proxy_user> defines the username for proxy authorization <proxy_pass> defines the password for proxy authorization <host> defines a host of the proxy server <port> defines a port of the proxy server no_proxy value \u00b6 To the no_proxy variable, assign the array of IP addresses and/or domains of the resources which proxy should not be used for: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 and localhost for correct WAF node operation additional addresses in the format: \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains Example of the file /etc/environment \u00b6 An example of the file /etc/environment below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy = http://admin:01234@1.2.3.4:1234 http_proxy = http://admin:01234@1.2.3.4:1234 no_proxy = \"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\"","title":"Access to wallarm api via proxy"},{"location":"admin-en/configuration-guides/access-to-wallarm-api-via-proxy/#access-to-wallarm-api-via-proxy","text":"These instructions describe the steps to configure access to Wallarm API via the proxy server. https://api.wallarm.com/ for the EU cloud https://us1.api.wallarm.com/ for the US cloud To configure access, please assign new values to the environment variables defining the proxy server used in the /etc/environment file: https_proxy to define a proxy for the HTTPS protocol http_proxy to define a proxy for the HTTP protocol no_proxy to define the list of the resources proxy should not be used for","title":"Access to Wallarm API via Proxy"},{"location":"admin-en/configuration-guides/access-to-wallarm-api-via-proxy/#https_proxy-and-http_proxy-values","text":"Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables: <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for <proxy_user> defines the username for proxy authorization <proxy_pass> defines the password for proxy authorization <host> defines a host of the proxy server <port> defines a port of the proxy server","title":"https_proxy and http_proxy values"},{"location":"admin-en/configuration-guides/access-to-wallarm-api-via-proxy/#no_proxy-value","text":"To the no_proxy variable, assign the array of IP addresses and/or domains of the resources which proxy should not be used for: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 and localhost for correct WAF node operation additional addresses in the format: \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains","title":"no_proxy value"},{"location":"admin-en/configuration-guides/access-to-wallarm-api-via-proxy/#example-of-the-file-etcenvironment","text":"An example of the file /etc/environment below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy = http://admin:01234@1.2.3.4:1234 http_proxy = http://admin:01234@1.2.3.4:1234 no_proxy = \"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\"","title":"Example of the file /etc/environment"},{"location":"admin-en/configuration-guides/allocate-resources-for-waf-node/","text":"Allocating Resources for WAF Node \u00b6 The amount of memory allocated for the WAF node determines the quality and speed of request processing. These instructions describe the recommendations for WAF node memory allocation. In a WAF node there are two main memory consumers: Tarantool , also called postanalytics module . This is the local data analytics backend and the primary memory consumer in a WAF node. NGINX is the main WAF node and reverse proxy component. Tarantool \u00b6 Postanalytics uses the in-memory storage Tarantool. The Tarantool database is used to keep in a circular buffer a local copy of the data stream processed by a WAF node, including request/response headers and request bodies (but not response bodies). The recommended value is 75% of the total server memory. However, there is a more accurate way to define the required value. To make a WAF node efficient, the database should keep at least 15 minutes of transmitted data with about 2x overhead for data serialization. Following these points, the amount of memory can be estimated by the formula: Speed of request processing per minute in bytes * 15 * 2 For example, if a WAF node is handling at peak 50 MBps of end user requests, the required Tarantool database memory consumption can be estimated as the following: 50 MBps / 8 (bits in a byte) * 60 (seconds in a minute) * 15 * 2 = 11,250,000,000 bytes (or ~ 10.4 GB) Allocating Resources in Kubernetes Ingress Controller \u00b6 Tarantool memory is configured for the ingress-controller-wallarm-tarantool pod using the following sections in the values.yaml file: To set up memory in GB: controller: wallarm: tarantool: arena: \"0.2\" To set up memory in CPU: controller: wallarm: tarantool: resources: limits: cpu: 1000m memory: 1640Mi requests: cpu: 1000m memory: 1640Mi Allocating Resources in Other Deployment Options \u00b6 The sizing of Tarantool memory is controlled using the SLAB_ALLOC_ARENA attribute in the /etc/default/wallarm-tarantool configuration file. To allocate memory: Open for editing the configuration file of Tarantool: Debian 9.x (stretch) sudo vim /etc/default/wallarm-tarantool Debian 10.x (buster) sudo vim /etc/default/wallarm-tarantool Ubuntu 18.04 LTS (bionic) sudo vim /etc/default/wallarm-tarantool Ubuntu 20.04 LTS (focal) sudo vim /etc/default/wallarm-tarantool CentOS 7.x sudo vim /etc/sysconfig/wallarm-tarantool Amazon Linux 2 sudo vim /etc/sysconfig/wallarm-tarantool CentOS 8.x sudo vim /etc/sysconfig/wallarm-tarantool Set the SLAB_ALLOC_ARENA attribute to memory size. The value can be an integer or a float (a dot . is a decimal separator). For example: SLAB_ALLOC_ARENA=10.4 Restart Tarantool: Debian 9.x (stretch) sudo systemctl restart wallarm-tarantool Debian 10.x (buster) sudo systemctl restart wallarm-tarantool Ubuntu 18.04 LTS (bionic) sudo service wallarm-tarantool restart Ubuntu 20.04 LTS (focal) sudo service wallarm-tarantool restart CentOS 7.x sudo systemctl restart wallarm-tarantool Amazon Linux 2 sudo systemctl restart wallarm-tarantool CentOS 8.x sudo systemctl restart wallarm-tarantool To learn how long a Tarantool instance is capable of keeping traffic details with the current level of WAF node load, you can use the wallarm-tarantool/gauge-timeframe_size monitoring metric. NGINX \u00b6 NGINX memory consumption depends on many factors. On average it can be estimated as the following: Number of concurrent request * Average request size * 3 For example: WAF node is processing at peak 10000 concurrent requests, average request size is 5 kB. The NGINX memory consumption can be estimated as follows: 10000 * 5 kB * 3 = 150000 kB (or ~150 MB) To allocate the amount of memory: for the NGINX Ingress controller pod ( ingress-controller ), use the following sections in the values.yaml file: controller: resources: limits: cpu: 1000m memory: 1640Mi requests: cpu: 1000m memory: 1640Mi for other deployment options, use the NGINX configuration files. Recommendations from the CPU utilization perspective When running in production mode, it is recommended to allocate at least one CPU core for the NGINX process and one core for the Tarantool process. Actual NGINX CPU utilization depends on many factors like RPS level, average size of request and response, number of LOM rules handled by the node, types and layers of employed data encodings like Base64 or data compression, etc. On average, one CPU core can handle about 500 RPS. In the majority of cases it is recommended to initially over-provision a WAF node, see the actual CPU and memory usage for real production traffic levels, and gradually reduce allocated resources to a reasonable level (with at least 2x headroom for traffic spikes and node redundancy).","title":"Allocating resources"},{"location":"admin-en/configuration-guides/allocate-resources-for-waf-node/#allocating-resources-for-waf-node","text":"The amount of memory allocated for the WAF node determines the quality and speed of request processing. These instructions describe the recommendations for WAF node memory allocation. In a WAF node there are two main memory consumers: Tarantool , also called postanalytics module . This is the local data analytics backend and the primary memory consumer in a WAF node. NGINX is the main WAF node and reverse proxy component.","title":"Allocating Resources for WAF Node"},{"location":"admin-en/configuration-guides/allocate-resources-for-waf-node/#tarantool","text":"Postanalytics uses the in-memory storage Tarantool. The Tarantool database is used to keep in a circular buffer a local copy of the data stream processed by a WAF node, including request/response headers and request bodies (but not response bodies). The recommended value is 75% of the total server memory. However, there is a more accurate way to define the required value. To make a WAF node efficient, the database should keep at least 15 minutes of transmitted data with about 2x overhead for data serialization. Following these points, the amount of memory can be estimated by the formula: Speed of request processing per minute in bytes * 15 * 2 For example, if a WAF node is handling at peak 50 MBps of end user requests, the required Tarantool database memory consumption can be estimated as the following: 50 MBps / 8 (bits in a byte) * 60 (seconds in a minute) * 15 * 2 = 11,250,000,000 bytes (or ~ 10.4 GB)","title":"Tarantool"},{"location":"admin-en/configuration-guides/allocate-resources-for-waf-node/#allocating-resources-in-kubernetes-ingress-controller","text":"Tarantool memory is configured for the ingress-controller-wallarm-tarantool pod using the following sections in the values.yaml file: To set up memory in GB: controller: wallarm: tarantool: arena: \"0.2\" To set up memory in CPU: controller: wallarm: tarantool: resources: limits: cpu: 1000m memory: 1640Mi requests: cpu: 1000m memory: 1640Mi","title":"Allocating Resources in Kubernetes Ingress Controller"},{"location":"admin-en/configuration-guides/allocate-resources-for-waf-node/#allocating-resources-in-other-deployment-options","text":"The sizing of Tarantool memory is controlled using the SLAB_ALLOC_ARENA attribute in the /etc/default/wallarm-tarantool configuration file. To allocate memory: Open for editing the configuration file of Tarantool: Debian 9.x (stretch) sudo vim /etc/default/wallarm-tarantool Debian 10.x (buster) sudo vim /etc/default/wallarm-tarantool Ubuntu 18.04 LTS (bionic) sudo vim /etc/default/wallarm-tarantool Ubuntu 20.04 LTS (focal) sudo vim /etc/default/wallarm-tarantool CentOS 7.x sudo vim /etc/sysconfig/wallarm-tarantool Amazon Linux 2 sudo vim /etc/sysconfig/wallarm-tarantool CentOS 8.x sudo vim /etc/sysconfig/wallarm-tarantool Set the SLAB_ALLOC_ARENA attribute to memory size. The value can be an integer or a float (a dot . is a decimal separator). For example: SLAB_ALLOC_ARENA=10.4 Restart Tarantool: Debian 9.x (stretch) sudo systemctl restart wallarm-tarantool Debian 10.x (buster) sudo systemctl restart wallarm-tarantool Ubuntu 18.04 LTS (bionic) sudo service wallarm-tarantool restart Ubuntu 20.04 LTS (focal) sudo service wallarm-tarantool restart CentOS 7.x sudo systemctl restart wallarm-tarantool Amazon Linux 2 sudo systemctl restart wallarm-tarantool CentOS 8.x sudo systemctl restart wallarm-tarantool To learn how long a Tarantool instance is capable of keeping traffic details with the current level of WAF node load, you can use the wallarm-tarantool/gauge-timeframe_size monitoring metric.","title":"Allocating Resources in Other Deployment Options"},{"location":"admin-en/configuration-guides/allocate-resources-for-waf-node/#nginx","text":"NGINX memory consumption depends on many factors. On average it can be estimated as the following: Number of concurrent request * Average request size * 3 For example: WAF node is processing at peak 10000 concurrent requests, average request size is 5 kB. The NGINX memory consumption can be estimated as follows: 10000 * 5 kB * 3 = 150000 kB (or ~150 MB) To allocate the amount of memory: for the NGINX Ingress controller pod ( ingress-controller ), use the following sections in the values.yaml file: controller: resources: limits: cpu: 1000m memory: 1640Mi requests: cpu: 1000m memory: 1640Mi for other deployment options, use the NGINX configuration files. Recommendations from the CPU utilization perspective When running in production mode, it is recommended to allocate at least one CPU core for the NGINX process and one core for the Tarantool process. Actual NGINX CPU utilization depends on many factors like RPS level, average size of request and response, number of LOM rules handled by the node, types and layers of employed data encodings like Base64 or data compression, etc. On average, one CPU core can handle about 500 RPS. In the majority of cases it is recommended to initially over-provision a WAF node, see the actual CPU and memory usage for real production traffic levels, and gradually reduce allocated resources to a reasonable level (with at least 2x headroom for traffic spikes and node redundancy).","title":"NGINX"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/","text":"Configuration of the blocking page and error code (NGINX) \u00b6 These instructions describe the method to customize the blocking page and error code returned in the response to the request blocked for the following reasons: Request contains malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . Request containing malicious payloads from the list above is originated from greylisted IP address and the WAF node filters requests in the safe blocking mode . Request is originated from the blacklisted IP address . Configuration limitations \u00b6 Configuration of the blocking page and error code is supported in NGINX-based WAF node deployments but is not supported in Envoy-based WAF node deployments. Envoy-based WAF node always returns code 403 in the response to the blocked request. Configuration methods \u00b6 By default, the response code 403 and default NGINX blocking page are returned to the client. You can change default settings by using the following NGINX directives: wallarm_block_page wallarm_block_page_add_dynamic_path NGINX directive wallarm_block_page \u00b6 You can configure the blocking page and error code passing the following parameters in the wallarm_block_page NGINX directive: Path to the HTM or HTML file of the blocking page. You can specify the path either to a custom blocking page or the default blocking page provided by Wallarm. The Wallarm blocking page located in the file /usr/share/nginx/html/wallarm_blocked.html looks as follows: The text of the message to be returned in response to a blocked request. URL for the client redirection. response_code : response code. type : the type of the blocked request in response to which the specified configuration must be returned. The parameter accepts one or several values (separated by commas) from the list: attack (by default): for requests blocked by the WAF node when filtering requests in the blocking or safe blocking mode . acl_ip : for requests originated from IP addresses that are added to the blacklist as a single object or a subnet. acl_source : for requests originated from IP addresses that are registered in blacklisted countries or data centers. The wallarm_block_page directive accepts the listed parameters in the following formats: Path to the HTM or HTML file, error code (optional), and blocked request type (optional) wallarm_block_page & /<PATH_TO_FILE/HTML_HTM_FILE_NAME> response_code = <CUSTOM_CODE> type = <BLOCKED_REQUEST_TYPE> ; You can use NGINX variables on the blocking page. For this, add the variable name in the format ${variable_name} to the blocking page code. For example, ${remote_addr} displays the IP address from which the blocked request was originated. Wallarm provides the default blocking page. To use this page, please specify the path &/usr/share/nginx/html/wallarm_blocked.html in the directive value. Important information for Debian and CentOS users If you use an NGINX version lower than 1.11 installed from CentOS/Debian repositories, you should remove the request_id variable from the page code to display the dynamic blocking page correctly: UUID ${request_id} This applies to both wallarm_blocked.html and to the custom block page. Example of configuration \u2192 URL for the client redirection and blocked request type (optional) wallarm_block_page /<REDIRECT_URL> type = <BLOCKED_REQUEST_TYPE> ; Example of configuration \u2192 Named NGINX location and blocked request type (optional) wallarm_block_page @<NAMED_LOCATION> type = <BLOCKED_REQUEST_TYPE> ; Example of configuration \u2192 Name of the variable setting the path to the HTM or HTML file, error code (optional), and blocked request type (optional) wallarm_block_page & <VARIABLE_NAME> response_code = <CUSTOM_CODE> type = <BLOCKED_REQUEST_TYPE> ; Initializing the blocking page with NGINX variables in the code If using this method to set the blocking page with NGINX variables in its code, please initialize this page via the directive wallarm_block_page_add_dynamic_path . Example of configuration \u2192 The directive wallarm_block_page can be set inside the http , server , location blocks of the NGINX configuration file. NGINX directive wallarm_block_page_add_dynamic_path \u00b6 The directive wallarm_block_page_add_dynamic_path is used to initialize the blocking page that has NGINX variables in its code and the path to this blocking page is also set using a variable. Otherwise, the directive is not used. The directive can be set inside the http block of the NGINX configuration file. Configuration examples \u00b6 Below are examples of configuring the blocking page and error code via the directives wallarm_block_page and wallarm_block_page_add_dynamic_path . The type parameter of the wallarm_block_page directive is explicitly specified in each example. If you remove the type parameter, then configured block page, message, etc will be returned only in the response to the request blocked by the WAF node in the blocking or safe blocking mode . Path to the HTM or HTML file with the blocking page and error code \u00b6 This example shows the following response settings: Default Wallarm blocking page and the error code 445 returned if the request is blocked by the WAF node in the blocking or safe blocking mode. Custom blocking page /usr/share/nginx/html/block.html and the error code 445 returned if the request is originated from any blacklisted IP address. NGINX configuration file \u00b6 wallarm_block_page & /usr/share/nginx/html/wallarm_blocked.html response_code = 445 type = attack ; wallarm_block_page & /usr/share/nginx/html/block.html response_code = 445 type = acl_ip,acl_source ; To apply the settings to the Docker container, the NGINX configuration file with appropriate settings should be mounted to the container. If configuring the custom blocking page, this page should also be mounted to the container. Running the container mounting the configuration file \u2192 To apply the settings to Wallarm sidecar container, the directive should be passed in Wallarm ConfigMap (see the instructions for Kubernetes deployment based on Helm charts or Manifests ). Ingress annotations \u00b6 Before adding the Ingress annotation: Create ConfigMap from the file block.html . Mount created ConfigMap to the pod with Wallarm Ingress controller. For this, please update the Deployment object relevant for Wallarm Ingress controller following the instructions . Directory for mounted ConfigMap Since existing files in the directory used to mount ConfigMap can be deleted, it is recommended to create a new directory for the files mounted via ConfigMap. Ingress annotations: kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = \"&/usr/share/nginx/html/wallarm_blocked.html response_code=445 type=attack;&/usr/share/nginx/html/block.html response_code=445 type=acl_ip,acl_source\" URL for the client redirection \u00b6 This example shows settings to redirect the client to the page host/err445 if the WAF node blocks the request originated from blacklisted countries or data centers. NGINX configuration file \u00b6 wallarm_block_page /err445 type = acl_source ; To apply the settings to the Docker container, the NGINX configuration file with appropriate settings should be mounted to the container. Running the container mounting the configuration file \u2192 To apply the settings to Wallarm sidecar container, the directive should be passed in Wallarm ConfigMap (see the instructions for Kubernetes deployment based on Helm charts or Manifests ). Ingress annotations \u00b6 kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = \"/err445 type=acl_source\" Named NGINX location \u00b6 This example shows settings to return to the client the message The page is blocked and the error code 445 regardless of the reason for request blocking (blocking or safe blocking mode, origin blacklisted as a single IP / subnet / country / data center). NGINX configuration file \u00b6 wallarm_block_page @block type = attack,acl_ip,acl_source ; location @block { return 445 'The page is blocked' ; } To apply the settings to the Docker container, the NGINX configuration file with appropriate settings should be mounted to the container. Running the container mounting the configuration file \u2192 To apply the settings to Wallarm sidecar container, the directive should be passed in Wallarm ConfigMap (see the instructions for Kubernetes deployment based on Helm charts or Manifests ). Ingress annotations \u00b6 kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/server-snippet = \"location @block {return 445 'The page is blocked';}\" kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = \"@block type=attack,acl_ip,acl_source\" Variable and error code \u00b6 This configuration is returned to the client if the request is originated from the source blacklisted as a single IP or subnet. The WAF node returns the code 445 and the blocking page with the content that depends on the User-Agent header value: By default, the default Wallarm blocking page /usr/share/nginx/html/wallarm_blocked.html is returned. Since NGINX variables are used in the blocking page code, this page should be initialized via the directive wallarm_block_page_add_dynamic_path . For users of Firefox \u2014 /usr/share/nginx/html/block_page_firefox.html : You are blocked! IP ${ remote_addr } Blocked on ${ time_iso8601 } UUID ${ request_id } Since NGINX variables are used in the blocking page code, this page should be initialized via the directive wallarm_block_page_add_dynamic_path . For users of Chrome \u2014 /usr/share/nginx/html/block_page_chrome.html : You are blocked! Since NGINX variables are NOT used in the blocking page code, this page should NOT be initialized. NGINX configuration file \u00b6 wallarm_block_page_add_dynamic_path /usr/share/nginx/html/block_page_firefox.html /usr/share/nginx/html/wallarm_blocked.html ; map $http_user_agent $block_page { \"~Firefox\" & /usr/share/nginx/html/block_page_firefox.html ; \"~Chrome\" & /usr/share/nginx/html/block_page_chrome.html ; default & /usr/share/nginx/html/wallarm_blocked.html ; } wallarm_block_page $block_page response_code = 445 type = acl_ip ; To apply the settings to the Docker container, the NGINX configuration file with appropriate settings should be mounted to the container. If configuring the custom blocking page, this page should also be mounted to the container. Running the container mounting the configuration file \u2192 To apply the settings to Wallarm sidecar container, the directives should be passed in Wallarm ConfigMap (see the instructions for Kubernetes deployment based on Helm charts or Manifests ). Ingress controller \u00b6 Add the following parameter to the config object in values.yaml of the cloned Wallarm Helm chart repository : config: { http-snippet: 'wallarm_block_page_add_dynamic_path /usr/test-block-page/blocked.html /usr/share/nginx/html/wallarm_blocked.html; map $http_user_agent $block_page { \"~Firefox\" &/usr/share/nginx/html/block_page_firefox.html; \"~Chrome\" &/usr/share/nginx/html/block_page_chrome.html; default &/usr/share/nginx/html/wallarm_blocked.html;}' } Execute the command helm install as described in step 4 of the installation instructions . Create ConfigMap from the files block_page_firefox.html and block_page_chrome.html . Mount created ConfigMap to the pod with Wallarm Ingress controller. For this, please update the Deployment object relevant for Wallarm Ingress controller following the instructions . Directory for mounted ConfigMap Since existing files in the directory used to mount ConfigMap can be deleted, it is recommended to create a new directory for the files mounted via ConfigMap. Add the following annotation to the Ingress: kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = '$block_page response_code=445 type=acl_ip'","title":"Configuration of the blocking page and error code"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#configuration-of-the-blocking-page-and-error-code-nginx","text":"These instructions describe the method to customize the blocking page and error code returned in the response to the request blocked for the following reasons: Request contains malicious payloads of the following types: input validation attacks , vpatch attacks , or attacks detected based on regular expressions . Request containing malicious payloads from the list above is originated from greylisted IP address and the WAF node filters requests in the safe blocking mode . Request is originated from the blacklisted IP address .","title":"Configuration of the blocking page and error code (NGINX)"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#configuration-limitations","text":"Configuration of the blocking page and error code is supported in NGINX-based WAF node deployments but is not supported in Envoy-based WAF node deployments. Envoy-based WAF node always returns code 403 in the response to the blocked request.","title":"Configuration limitations"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#configuration-methods","text":"By default, the response code 403 and default NGINX blocking page are returned to the client. You can change default settings by using the following NGINX directives: wallarm_block_page wallarm_block_page_add_dynamic_path","title":"Configuration methods"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#nginx-directive-wallarm_block_page","text":"You can configure the blocking page and error code passing the following parameters in the wallarm_block_page NGINX directive: Path to the HTM or HTML file of the blocking page. You can specify the path either to a custom blocking page or the default blocking page provided by Wallarm. The Wallarm blocking page located in the file /usr/share/nginx/html/wallarm_blocked.html looks as follows: The text of the message to be returned in response to a blocked request. URL for the client redirection. response_code : response code. type : the type of the blocked request in response to which the specified configuration must be returned. The parameter accepts one or several values (separated by commas) from the list: attack (by default): for requests blocked by the WAF node when filtering requests in the blocking or safe blocking mode . acl_ip : for requests originated from IP addresses that are added to the blacklist as a single object or a subnet. acl_source : for requests originated from IP addresses that are registered in blacklisted countries or data centers. The wallarm_block_page directive accepts the listed parameters in the following formats: Path to the HTM or HTML file, error code (optional), and blocked request type (optional) wallarm_block_page & /<PATH_TO_FILE/HTML_HTM_FILE_NAME> response_code = <CUSTOM_CODE> type = <BLOCKED_REQUEST_TYPE> ; You can use NGINX variables on the blocking page. For this, add the variable name in the format ${variable_name} to the blocking page code. For example, ${remote_addr} displays the IP address from which the blocked request was originated. Wallarm provides the default blocking page. To use this page, please specify the path &/usr/share/nginx/html/wallarm_blocked.html in the directive value. Important information for Debian and CentOS users If you use an NGINX version lower than 1.11 installed from CentOS/Debian repositories, you should remove the request_id variable from the page code to display the dynamic blocking page correctly: UUID ${request_id} This applies to both wallarm_blocked.html and to the custom block page. Example of configuration \u2192 URL for the client redirection and blocked request type (optional) wallarm_block_page /<REDIRECT_URL> type = <BLOCKED_REQUEST_TYPE> ; Example of configuration \u2192 Named NGINX location and blocked request type (optional) wallarm_block_page @<NAMED_LOCATION> type = <BLOCKED_REQUEST_TYPE> ; Example of configuration \u2192 Name of the variable setting the path to the HTM or HTML file, error code (optional), and blocked request type (optional) wallarm_block_page & <VARIABLE_NAME> response_code = <CUSTOM_CODE> type = <BLOCKED_REQUEST_TYPE> ; Initializing the blocking page with NGINX variables in the code If using this method to set the blocking page with NGINX variables in its code, please initialize this page via the directive wallarm_block_page_add_dynamic_path . Example of configuration \u2192 The directive wallarm_block_page can be set inside the http , server , location blocks of the NGINX configuration file.","title":"NGINX directive wallarm_block_page"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#nginx-directive-wallarm_block_page_add_dynamic_path","text":"The directive wallarm_block_page_add_dynamic_path is used to initialize the blocking page that has NGINX variables in its code and the path to this blocking page is also set using a variable. Otherwise, the directive is not used. The directive can be set inside the http block of the NGINX configuration file.","title":"NGINX directive wallarm_block_page_add_dynamic_path"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#configuration-examples","text":"Below are examples of configuring the blocking page and error code via the directives wallarm_block_page and wallarm_block_page_add_dynamic_path . The type parameter of the wallarm_block_page directive is explicitly specified in each example. If you remove the type parameter, then configured block page, message, etc will be returned only in the response to the request blocked by the WAF node in the blocking or safe blocking mode .","title":"Configuration examples"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#path-to-the-htm-or-html-file-with-the-blocking-page-and-error-code","text":"This example shows the following response settings: Default Wallarm blocking page and the error code 445 returned if the request is blocked by the WAF node in the blocking or safe blocking mode. Custom blocking page /usr/share/nginx/html/block.html and the error code 445 returned if the request is originated from any blacklisted IP address.","title":"Path to the HTM or HTML file with the blocking page and error code"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#nginx-configuration-file","text":"wallarm_block_page & /usr/share/nginx/html/wallarm_blocked.html response_code = 445 type = attack ; wallarm_block_page & /usr/share/nginx/html/block.html response_code = 445 type = acl_ip,acl_source ; To apply the settings to the Docker container, the NGINX configuration file with appropriate settings should be mounted to the container. If configuring the custom blocking page, this page should also be mounted to the container. Running the container mounting the configuration file \u2192 To apply the settings to Wallarm sidecar container, the directive should be passed in Wallarm ConfigMap (see the instructions for Kubernetes deployment based on Helm charts or Manifests ).","title":"NGINX configuration file"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#ingress-annotations","text":"Before adding the Ingress annotation: Create ConfigMap from the file block.html . Mount created ConfigMap to the pod with Wallarm Ingress controller. For this, please update the Deployment object relevant for Wallarm Ingress controller following the instructions . Directory for mounted ConfigMap Since existing files in the directory used to mount ConfigMap can be deleted, it is recommended to create a new directory for the files mounted via ConfigMap. Ingress annotations: kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = \"&/usr/share/nginx/html/wallarm_blocked.html response_code=445 type=attack;&/usr/share/nginx/html/block.html response_code=445 type=acl_ip,acl_source\"","title":"Ingress annotations"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#url-for-the-client-redirection","text":"This example shows settings to redirect the client to the page host/err445 if the WAF node blocks the request originated from blacklisted countries or data centers.","title":"URL for the client redirection"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#nginx-configuration-file_1","text":"wallarm_block_page /err445 type = acl_source ; To apply the settings to the Docker container, the NGINX configuration file with appropriate settings should be mounted to the container. Running the container mounting the configuration file \u2192 To apply the settings to Wallarm sidecar container, the directive should be passed in Wallarm ConfigMap (see the instructions for Kubernetes deployment based on Helm charts or Manifests ).","title":"NGINX configuration file"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#ingress-annotations_1","text":"kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = \"/err445 type=acl_source\"","title":"Ingress annotations"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#named-nginx-location","text":"This example shows settings to return to the client the message The page is blocked and the error code 445 regardless of the reason for request blocking (blocking or safe blocking mode, origin blacklisted as a single IP / subnet / country / data center).","title":"Named NGINX location"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#nginx-configuration-file_2","text":"wallarm_block_page @block type = attack,acl_ip,acl_source ; location @block { return 445 'The page is blocked' ; } To apply the settings to the Docker container, the NGINX configuration file with appropriate settings should be mounted to the container. Running the container mounting the configuration file \u2192 To apply the settings to Wallarm sidecar container, the directive should be passed in Wallarm ConfigMap (see the instructions for Kubernetes deployment based on Helm charts or Manifests ).","title":"NGINX configuration file"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#ingress-annotations_2","text":"kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/server-snippet = \"location @block {return 445 'The page is blocked';}\" kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = \"@block type=attack,acl_ip,acl_source\"","title":"Ingress annotations"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#variable-and-error-code","text":"This configuration is returned to the client if the request is originated from the source blacklisted as a single IP or subnet. The WAF node returns the code 445 and the blocking page with the content that depends on the User-Agent header value: By default, the default Wallarm blocking page /usr/share/nginx/html/wallarm_blocked.html is returned. Since NGINX variables are used in the blocking page code, this page should be initialized via the directive wallarm_block_page_add_dynamic_path . For users of Firefox \u2014 /usr/share/nginx/html/block_page_firefox.html : You are blocked! IP ${ remote_addr } Blocked on ${ time_iso8601 } UUID ${ request_id } Since NGINX variables are used in the blocking page code, this page should be initialized via the directive wallarm_block_page_add_dynamic_path . For users of Chrome \u2014 /usr/share/nginx/html/block_page_chrome.html : You are blocked! Since NGINX variables are NOT used in the blocking page code, this page should NOT be initialized.","title":"Variable and error code"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#nginx-configuration-file_3","text":"wallarm_block_page_add_dynamic_path /usr/share/nginx/html/block_page_firefox.html /usr/share/nginx/html/wallarm_blocked.html ; map $http_user_agent $block_page { \"~Firefox\" & /usr/share/nginx/html/block_page_firefox.html ; \"~Chrome\" & /usr/share/nginx/html/block_page_chrome.html ; default & /usr/share/nginx/html/wallarm_blocked.html ; } wallarm_block_page $block_page response_code = 445 type = acl_ip ; To apply the settings to the Docker container, the NGINX configuration file with appropriate settings should be mounted to the container. If configuring the custom blocking page, this page should also be mounted to the container. Running the container mounting the configuration file \u2192 To apply the settings to Wallarm sidecar container, the directives should be passed in Wallarm ConfigMap (see the instructions for Kubernetes deployment based on Helm charts or Manifests ).","title":"NGINX configuration file"},{"location":"admin-en/configuration-guides/configure-block-page-and-code/#ingress-controller","text":"Add the following parameter to the config object in values.yaml of the cloned Wallarm Helm chart repository : config: { http-snippet: 'wallarm_block_page_add_dynamic_path /usr/test-block-page/blocked.html /usr/share/nginx/html/wallarm_blocked.html; map $http_user_agent $block_page { \"~Firefox\" &/usr/share/nginx/html/block_page_firefox.html; \"~Chrome\" &/usr/share/nginx/html/block_page_chrome.html; default &/usr/share/nginx/html/wallarm_blocked.html;}' } Execute the command helm install as described in step 4 of the installation instructions . Create ConfigMap from the files block_page_firefox.html and block_page_chrome.html . Mount created ConfigMap to the pod with Wallarm Ingress controller. For this, please update the Deployment object relevant for Wallarm Ingress controller following the instructions . Directory for mounted ConfigMap Since existing files in the directory used to mount ConfigMap can be deleted, it is recommended to create a new directory for the files mounted via ConfigMap. Add the following annotation to the Ingress: kubectl annotate ingress <INGRESS_NAME> nginx.ingress.kubernetes.io/wallarm-block-page = '$block_page response_code=445 type=acl_ip'","title":"Ingress controller"},{"location":"admin-en/configuration-guides/protecting-against-bruteforce/","text":"Configuration of brute force protection \u00b6 There are the following classes of brute\u2011force attacks: Passwords brute\u2011forcing Session identifiers brute\u2011forcing Forced browsing (dirbust) Credential stuffing Behavioral attacks are characterized by a large number of requests with different forced parameter values sent to a typical URL for a limited timeframe. Detailed brute force description \u2192 Brute force protection restrictions When searching for brute\u2011force attack signs, Wallarm WAF analyzes only HTTP requests that do not contain signs of other attack types. For example, the requests are not considered to be a part of brute-force attack in the following cases: These requests contain signs of input validation attacks . These requests match the regular expression specified in the rule Define a request as an attack based on a regular expression . These instructions provide steps to configure brute force protection. Configuration steps \u00b6 Add the module Brute-force protection to the Wallarm WAF subscription plan. To add the module, please send a request to sales@wallarm.com . If the WAF node is deployed behind a proxy server or load balancer, then configure displaying of a real IP address of the client. Configure the trigger Mark as brute force/dirbust . Test the configuration of brute force protection. Configuring the trigger to identify brute force \u00b6 Open the Wallarm Console \u2192 section Triggers and open the window for trigger creation. Select the condition Number of requests and set the request number threshold or leave the default threshold value. If the threshold is exceeded, then the requests will be marked as brute\u2011force attack. If the threshold is exceeded and the code 404 is returned in the response to all requests, then the requests will be marked as dirbust (forced browsing) attack. Set the URL to filter all incoming requests by this URL and activate the trigger: If you configure password brute\u2011forcing protection, then set the URL used for authentication. If you configure dirbust protection, then set the URL of resource file directory. URL can be set in one of the following ways: Via the rule defining attack counter for any port listening for incoming requests. The created counter should be selected in the trigger filter Counter name . Via the URL trigger filter if the protected resource listens for incoming requests on port other than 80 or 443. The value should correspond to the format host:port/path (for example: example.com:8888/api/frontend/login ). If required, set other trigger filters: Application the requests are addressed to. One or more IP the requests are sent from. Select trigger reactions: Mark as brute force/dirbust to mark requests sent after the threshold was exceeded as the brute\u2011force or dirbust attack. Requests will be marked as an attack but will not be blocked. To block requests, it is required to select one more reaction Blacklist IP address . Blacklist IP address and the period for IP address blocking to add IP addresses from which malicious requests were originated to the blacklist. All requests sent after the threshold was exceeded will be blocked by the WAF node. Example of a configured rule defining attack counter and trigger: Description of the provided example and other trigger examples used for brute force protection is available within this link . You can configure several triggers for brute force protection. Testing the configuration of brute force protection \u00b6 Send the number of requests that exceeds the configured threshold to the protected URL. For example, 50 requests to example.com/api/frontend/login : for (( i = 0 ; $i < 51 ; i++ )) ; do curl https://example.com/api/frontend/login ; done Open the Wallarm Console \u2192 section Blacklist and check that IP address from which the requests were originated is blocked for the period configured in the trigger. Open the section Events and check that requests are displayed in the list as the brute\u2011force or dirbust attack. The number of displayed requests corresponds to the number of requests sent after the trigger threshold was exceeded ( more details on detecting behavioral attacks ). If this number is higher than 5, request sampling is applied and request details are displayed only for the first 5 hits ( more details on requests sampling ). To search for attacks, you can use the filters, for example: dirbust for dirbust attacks, brute for brute\u2011force attacks. All filters are described in the instructions on search using . Demo videos \u00b6","title":"Brute force protection"},{"location":"admin-en/configuration-guides/protecting-against-bruteforce/#configuration-of-brute-force-protection","text":"There are the following classes of brute\u2011force attacks: Passwords brute\u2011forcing Session identifiers brute\u2011forcing Forced browsing (dirbust) Credential stuffing Behavioral attacks are characterized by a large number of requests with different forced parameter values sent to a typical URL for a limited timeframe. Detailed brute force description \u2192 Brute force protection restrictions When searching for brute\u2011force attack signs, Wallarm WAF analyzes only HTTP requests that do not contain signs of other attack types. For example, the requests are not considered to be a part of brute-force attack in the following cases: These requests contain signs of input validation attacks . These requests match the regular expression specified in the rule Define a request as an attack based on a regular expression . These instructions provide steps to configure brute force protection.","title":"Configuration of brute force protection"},{"location":"admin-en/configuration-guides/protecting-against-bruteforce/#configuration-steps","text":"Add the module Brute-force protection to the Wallarm WAF subscription plan. To add the module, please send a request to sales@wallarm.com . If the WAF node is deployed behind a proxy server or load balancer, then configure displaying of a real IP address of the client. Configure the trigger Mark as brute force/dirbust . Test the configuration of brute force protection.","title":"Configuration steps"},{"location":"admin-en/configuration-guides/protecting-against-bruteforce/#configuring-the-trigger-to-identify-brute-force","text":"Open the Wallarm Console \u2192 section Triggers and open the window for trigger creation. Select the condition Number of requests and set the request number threshold or leave the default threshold value. If the threshold is exceeded, then the requests will be marked as brute\u2011force attack. If the threshold is exceeded and the code 404 is returned in the response to all requests, then the requests will be marked as dirbust (forced browsing) attack. Set the URL to filter all incoming requests by this URL and activate the trigger: If you configure password brute\u2011forcing protection, then set the URL used for authentication. If you configure dirbust protection, then set the URL of resource file directory. URL can be set in one of the following ways: Via the rule defining attack counter for any port listening for incoming requests. The created counter should be selected in the trigger filter Counter name . Via the URL trigger filter if the protected resource listens for incoming requests on port other than 80 or 443. The value should correspond to the format host:port/path (for example: example.com:8888/api/frontend/login ). If required, set other trigger filters: Application the requests are addressed to. One or more IP the requests are sent from. Select trigger reactions: Mark as brute force/dirbust to mark requests sent after the threshold was exceeded as the brute\u2011force or dirbust attack. Requests will be marked as an attack but will not be blocked. To block requests, it is required to select one more reaction Blacklist IP address . Blacklist IP address and the period for IP address blocking to add IP addresses from which malicious requests were originated to the blacklist. All requests sent after the threshold was exceeded will be blocked by the WAF node. Example of a configured rule defining attack counter and trigger: Description of the provided example and other trigger examples used for brute force protection is available within this link . You can configure several triggers for brute force protection.","title":"Configuring the trigger to identify brute force"},{"location":"admin-en/configuration-guides/protecting-against-bruteforce/#testing-the-configuration-of-brute-force-protection","text":"Send the number of requests that exceeds the configured threshold to the protected URL. For example, 50 requests to example.com/api/frontend/login : for (( i = 0 ; $i < 51 ; i++ )) ; do curl https://example.com/api/frontend/login ; done Open the Wallarm Console \u2192 section Blacklist and check that IP address from which the requests were originated is blocked for the period configured in the trigger. Open the section Events and check that requests are displayed in the list as the brute\u2011force or dirbust attack. The number of displayed requests corresponds to the number of requests sent after the trigger threshold was exceeded ( more details on detecting behavioral attacks ). If this number is higher than 5, request sampling is applied and request details are displayed only for the first 5 hits ( more details on requests sampling ). To search for attacks, you can use the filters, for example: dirbust for dirbust attacks, brute for brute\u2011force attacks. All filters are described in the instructions on search using .","title":"Testing the configuration of brute force protection"},{"location":"admin-en/configuration-guides/protecting-against-bruteforce/#demo-videos","text":"","title":"Demo videos"},{"location":"admin-en/configuration-guides/envoy/fine-tuning/","text":"Configuration options for the Envoy\u2011based WAF node \u00b6 Envoy uses pluggable filters defined in the Envoy configuration file to process incoming requests. These filters describe the actions to be performed on the request. For example, an envoy.http_connection_manager filter is used to proxy HTTP requests. This filter has its own set of HTTP filters that can be applied to the request. The Wallarm WAF module is designed as an Envoy HTTP filter. The module's general settings are placed in a section dedicated to the wallarm HTTP filter: listeners: - address: filter_chains: - filters: - name: envoy.http_connection_manager typed_config: http_filters: - name: wallarm typed_config: \"@type\": type.googleapis.com/wallarm.Wallarm <the Wallarm WAF module configuration> ... Enable request body processing In order to enable the Wallarm WAF module to process an HTTP request body, the buffer filter must be placed before the WAF node in the Envoy HTTP filter chain. For example: http_filters: - name: envoy.buffer typed_config: \"@type\": type.googleapis.com/envoy.config.filter.http.buffer.v2.Buffer max_request_bytes: <maximum request size (in bytes)> - name: wallarm typed_config: \"@type\": type.googleapis.com/wallarm.Wallarm <the Wallarm module configuration> ... If the incoming request size exceeds the value of the max_request_bytes parameter, then this request will be dropped and Envoy will return the 413 response code (\u201cPayload Too Large\u201d). Request filtering settings \u00b6 The tsets section of the file contains the parameters related to request filtering settings: tsets: - name: ts0 pdb: /etc/wallarm/proton.db lom: /etc/wallarm/lom key: /etc/wallarm/license.key ts_request_memory_limit: 0 ... - name: tsN: ... The ts0 ... tsN entries are one or more parameter groups. The groups can have any name (so that they can be referred to later via the ts parameter in the conf section). At least one group should be present in the WAF node configuration (e.g., with the name ts0 ). This section has no default values. You need to explicitly specify values in the config file. Definition level This section can be defined on the WAF node level only. Parameter Description Default value pdb Path to the proton.db file. This file contains the global settings for request filtering, which do not depend on the application structure. /etc/wallarm/proton.db lom Path to the LOM file that contains information about the protected application and the WAF node settings. /etc/wallarm/lom key Path to the file with the Wallarm license key. /etc/wallarm/license.key ts_request_memory_limit Limit for the maximum amount of memory that can be used by one instance of proton.db and LOM. If the memory limit is exceeded while processing some request, the user will get the 500 error. The following suffixes can be used in this parameter: k or K for kilobytes m or M for megabytes g or G for gigabyte Value of 0 turns the limit off. 0 Postanalytics module settings \u00b6 The tarantool section of the WAF node contains the parameters related to the postanalytics module: tarantool: server: - uri: localhost:3313 max_packets: 512 max_packets_mem: 0 reconnect_interval: 1 The server entry is a parameter group that describes the settings for the Tarantool server. Definition level This section can be defined on the WAF node level only. Parameter Description Default value uri String with the credentials used to connect to the Tarantool server. The string format is IP address or domain name:port . localhost:3313 max_packets Limit of the number of serialized requests to be sent to Tarantool. To remove the limit, set 0 as the parameter value. 512 max_packets_mem Limit of the total volume (in bytes) of serialized requests to be sent to Tarantool. 0 (the volume is not limited) reconnect_interval Interval (in seconds) between attempts to reconnect to the Tarantool. A value of 0 means that the WAF node will try to reconnect to the server as quickly as possible if the server renders unavailable (not recommended). 1 Basic settings \u00b6 The conf section of the Wallarm WAF contains the parameters that influence WAF node's basic operations: conf: ts: ts0 mode: \"monitoring\" mode_allow_override: \"off\" instance: 42 process_time_limit: 1000 process_time_limit_block: \"attack\" request_memory_limit: 104857600 wallarm_status: \"off\" wallarm_status_format: \"json\" Definition level For more flexible protection level, this section can be overridden on the route or virtual host level: on the route level: routes: - match: typed_per_filter_config: wallarm: \"@type\": type.googleapis.com/wallarm.WallarmConf <the section parameters> on the virtual host level: virtual_hosts: - name: <the name of the virtual host> typed_per_filter_config: wallarm: \"@type\": type.googleapis.com/wallarm.WallarmConf <the section parameters> The parameters in the conf section overridden on the route level have priority over the parameters in the section defined on the virtual host level which in turn have higher priority than the parameters listed in the section on the WAF node level. Parameter Description Default value ts One of the parameter groups that is defined in the tsets section. This parameter group sets the request filtering rules to be used. If this parameter is omitted from the conf section of the WAF node, then it should be present in the conf section overridden on the route or virtual host level. - mode WAF node mode: block to block malicious requests monitoring to analyze but not block requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing Detailed description of filtration modes \u2192 block mode_allow_override Allows overriding the WAF node mode that is set via the mode parameter with the LOM rules: off : the LOM rule set is ignored strict : LOM can only strengthen the operation mode. on : it is possible to both strengthen and soften the operation mode For example, if the mode parameter is set to the monitoring value and the mode_allow_override parameter is set to the strict value, then it will be possible to block some requests ( block ) but not to fully disable the WAF node ( off ). off instance Application identifier. It is used to visually separate the data of different applications on the dashboard . Only numeric values are allowed. The application identifiers are used solely for the convenience of data presentation. To correctly separate the data of different applications, the same application identifiers must be set in the Wallarm Console. Any WAF node will filter traffic for any number of applications without additional configuration. - process_time_limit Limit on the processing time of a single request (in milliseconds). If the request cannot be processed in the defined amount of time, then an error message is recorded to the log file and the request is marked as an overlimit_res attack. 1000 process_time_limit_block Action to take when the request processing time exceeds the limit set via the process_time_limit parameter: off : the requests are always ignored on : the requests are always blocked attack : depends on the attack blocking mode set via the mode parameter: off : the requests are not processed monitoring : the requests are ignored block : the requests are blocked attack wallarm_status Whether to enable the WAF node statistics service . false wallarm_status_format Format of the WAF node statistics : json or prometheus . json disable_acl Allows disabling analysis of requests origins. If disabled ( on ), the WAF node does not download IP lists from the Wallarm Cloud and skips request source IPs analysis. off","title":"Envoy directives"},{"location":"admin-en/configuration-guides/envoy/fine-tuning/#configuration-options-for-the-envoybased-waf-node","text":"Envoy uses pluggable filters defined in the Envoy configuration file to process incoming requests. These filters describe the actions to be performed on the request. For example, an envoy.http_connection_manager filter is used to proxy HTTP requests. This filter has its own set of HTTP filters that can be applied to the request. The Wallarm WAF module is designed as an Envoy HTTP filter. The module's general settings are placed in a section dedicated to the wallarm HTTP filter: listeners: - address: filter_chains: - filters: - name: envoy.http_connection_manager typed_config: http_filters: - name: wallarm typed_config: \"@type\": type.googleapis.com/wallarm.Wallarm <the Wallarm WAF module configuration> ... Enable request body processing In order to enable the Wallarm WAF module to process an HTTP request body, the buffer filter must be placed before the WAF node in the Envoy HTTP filter chain. For example: http_filters: - name: envoy.buffer typed_config: \"@type\": type.googleapis.com/envoy.config.filter.http.buffer.v2.Buffer max_request_bytes: <maximum request size (in bytes)> - name: wallarm typed_config: \"@type\": type.googleapis.com/wallarm.Wallarm <the Wallarm module configuration> ... If the incoming request size exceeds the value of the max_request_bytes parameter, then this request will be dropped and Envoy will return the 413 response code (\u201cPayload Too Large\u201d).","title":"Configuration options for the Envoy\u2011based WAF node"},{"location":"admin-en/configuration-guides/envoy/fine-tuning/#request-filtering-settings","text":"The tsets section of the file contains the parameters related to request filtering settings: tsets: - name: ts0 pdb: /etc/wallarm/proton.db lom: /etc/wallarm/lom key: /etc/wallarm/license.key ts_request_memory_limit: 0 ... - name: tsN: ... The ts0 ... tsN entries are one or more parameter groups. The groups can have any name (so that they can be referred to later via the ts parameter in the conf section). At least one group should be present in the WAF node configuration (e.g., with the name ts0 ). This section has no default values. You need to explicitly specify values in the config file. Definition level This section can be defined on the WAF node level only. Parameter Description Default value pdb Path to the proton.db file. This file contains the global settings for request filtering, which do not depend on the application structure. /etc/wallarm/proton.db lom Path to the LOM file that contains information about the protected application and the WAF node settings. /etc/wallarm/lom key Path to the file with the Wallarm license key. /etc/wallarm/license.key ts_request_memory_limit Limit for the maximum amount of memory that can be used by one instance of proton.db and LOM. If the memory limit is exceeded while processing some request, the user will get the 500 error. The following suffixes can be used in this parameter: k or K for kilobytes m or M for megabytes g or G for gigabyte Value of 0 turns the limit off. 0","title":"Request filtering settings"},{"location":"admin-en/configuration-guides/envoy/fine-tuning/#postanalytics-module-settings","text":"The tarantool section of the WAF node contains the parameters related to the postanalytics module: tarantool: server: - uri: localhost:3313 max_packets: 512 max_packets_mem: 0 reconnect_interval: 1 The server entry is a parameter group that describes the settings for the Tarantool server. Definition level This section can be defined on the WAF node level only. Parameter Description Default value uri String with the credentials used to connect to the Tarantool server. The string format is IP address or domain name:port . localhost:3313 max_packets Limit of the number of serialized requests to be sent to Tarantool. To remove the limit, set 0 as the parameter value. 512 max_packets_mem Limit of the total volume (in bytes) of serialized requests to be sent to Tarantool. 0 (the volume is not limited) reconnect_interval Interval (in seconds) between attempts to reconnect to the Tarantool. A value of 0 means that the WAF node will try to reconnect to the server as quickly as possible if the server renders unavailable (not recommended). 1","title":"Postanalytics module settings"},{"location":"admin-en/configuration-guides/envoy/fine-tuning/#basic-settings","text":"The conf section of the Wallarm WAF contains the parameters that influence WAF node's basic operations: conf: ts: ts0 mode: \"monitoring\" mode_allow_override: \"off\" instance: 42 process_time_limit: 1000 process_time_limit_block: \"attack\" request_memory_limit: 104857600 wallarm_status: \"off\" wallarm_status_format: \"json\" Definition level For more flexible protection level, this section can be overridden on the route or virtual host level: on the route level: routes: - match: typed_per_filter_config: wallarm: \"@type\": type.googleapis.com/wallarm.WallarmConf <the section parameters> on the virtual host level: virtual_hosts: - name: <the name of the virtual host> typed_per_filter_config: wallarm: \"@type\": type.googleapis.com/wallarm.WallarmConf <the section parameters> The parameters in the conf section overridden on the route level have priority over the parameters in the section defined on the virtual host level which in turn have higher priority than the parameters listed in the section on the WAF node level. Parameter Description Default value ts One of the parameter groups that is defined in the tsets section. This parameter group sets the request filtering rules to be used. If this parameter is omitted from the conf section of the WAF node, then it should be present in the conf section overridden on the route or virtual host level. - mode WAF node mode: block to block malicious requests monitoring to analyze but not block requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing Detailed description of filtration modes \u2192 block mode_allow_override Allows overriding the WAF node mode that is set via the mode parameter with the LOM rules: off : the LOM rule set is ignored strict : LOM can only strengthen the operation mode. on : it is possible to both strengthen and soften the operation mode For example, if the mode parameter is set to the monitoring value and the mode_allow_override parameter is set to the strict value, then it will be possible to block some requests ( block ) but not to fully disable the WAF node ( off ). off instance Application identifier. It is used to visually separate the data of different applications on the dashboard . Only numeric values are allowed. The application identifiers are used solely for the convenience of data presentation. To correctly separate the data of different applications, the same application identifiers must be set in the Wallarm Console. Any WAF node will filter traffic for any number of applications without additional configuration. - process_time_limit Limit on the processing time of a single request (in milliseconds). If the request cannot be processed in the defined amount of time, then an error message is recorded to the log file and the request is marked as an overlimit_res attack. 1000 process_time_limit_block Action to take when the request processing time exceeds the limit set via the process_time_limit parameter: off : the requests are always ignored on : the requests are always blocked attack : depends on the attack blocking mode set via the mode parameter: off : the requests are not processed monitoring : the requests are ignored block : the requests are blocked attack wallarm_status Whether to enable the WAF node statistics service . false wallarm_status_format Format of the WAF node statistics : json or prometheus . json disable_acl Allows disabling analysis of requests origins. If disabled ( on ), the WAF node does not download IP lists from the Wallarm Cloud and skips request source IPs analysis. off","title":"Basic settings"},{"location":"admin-en/configuration-guides/sso/change-sso-provider/","text":"Changing the Configured SSO Authentication \u00b6 You can edit , disable or remove configured SSO authentication. Attention: SSO will be disabled for all users Note that when you disable or remove SSO authentication, it will be disabled for all users. Users will be notified that SSO authentication is disabled and the password needs to be restored. Editing \u00b6 To edit configured SSO authentication: Go to Settings \u2192 Integration in Wallarm UI. Select the Edit option in configured SSO provider menu. Update SSO provider details and Save changes . Disabling \u00b6 To disable SSO, go to Settings \u2192 Integration . Click on the block of the corresponding SSO provider and then on the Disable button. In the pop-up window, it is required to confirm the disabling of the SSO provider, as well as the disabling of the SSO authentication of all users. Click Yes, disable . After confirmation, the SSO provider will be disconnected, but its settings will be saved and you can enable this provider again in the future. In addition, after disabling, you will be able to connect another SSO provider (another service as an identity provider). Removing \u00b6 Attention: About removing the SSO provider Compared to disabling, removing the SSO provider will cause the loss of all its settings without the possibility of recovery. If you need to reconnect your provider, you will need to reconfigure it. Removing the SSO provider is similar to disabling it. Go to Settings \u2192 Integration . Click on the block of the corresponding SSO provider and then on the Remove button. In the pop-up window, it is required to confirm the removing of the provider, as well as to disable SSO authentication of all users. Click Yes, remove . After confirmation, the selected SSO provider will be removed and will no longer be available. Also, you will be able to connect to another SSO provider.","title":"Change sso provider"},{"location":"admin-en/configuration-guides/sso/change-sso-provider/#changing-the-configured-sso-authentication","text":"You can edit , disable or remove configured SSO authentication. Attention: SSO will be disabled for all users Note that when you disable or remove SSO authentication, it will be disabled for all users. Users will be notified that SSO authentication is disabled and the password needs to be restored.","title":"Changing the Configured SSO Authentication"},{"location":"admin-en/configuration-guides/sso/change-sso-provider/#editing","text":"To edit configured SSO authentication: Go to Settings \u2192 Integration in Wallarm UI. Select the Edit option in configured SSO provider menu. Update SSO provider details and Save changes .","title":"Editing"},{"location":"admin-en/configuration-guides/sso/change-sso-provider/#disabling","text":"To disable SSO, go to Settings \u2192 Integration . Click on the block of the corresponding SSO provider and then on the Disable button. In the pop-up window, it is required to confirm the disabling of the SSO provider, as well as the disabling of the SSO authentication of all users. Click Yes, disable . After confirmation, the SSO provider will be disconnected, but its settings will be saved and you can enable this provider again in the future. In addition, after disabling, you will be able to connect another SSO provider (another service as an identity provider).","title":"Disabling"},{"location":"admin-en/configuration-guides/sso/change-sso-provider/#removing","text":"Attention: About removing the SSO provider Compared to disabling, removing the SSO provider will cause the loss of all its settings without the possibility of recovery. If you need to reconnect your provider, you will need to reconfigure it. Removing the SSO provider is similar to disabling it. Go to Settings \u2192 Integration . Click on the block of the corresponding SSO provider and then on the Remove button. In the pop-up window, it is required to confirm the removing of the provider, as well as to disable SSO authentication of all users. Click Yes, remove . After confirmation, the selected SSO provider will be removed and will no longer be available. Also, you will be able to connect to another SSO provider.","title":"Removing"},{"location":"admin-en/configuration-guides/sso/employ-user-auth/","text":"Configuring SSO Authentication for Users \u00b6 You can enable or disable SSO authentication to Wallarm portal users. Enabling SSO Authentication for Users \u00b6 Warning When enabling SSO authentication for users with any non-administrator roles ( Admin or Super Admin ), a login/password log in mechanism and the two-factor authentication will not be available. When SSO authentication is enabled, the user's password is erased and two-factor authentication is disabled. The Admin and Super Admin roles can use login/password pair, two-factor authentication, and SSO authentication simultaneously. It is assumed that you have already given the required group of users access to the configured Wallarm application on the Okta or G Suite side. To enable SSO authentication for Wallarm users go to Settings \u2192 Users . Find the desired user and open the user action menu by clicking the button on the right of the user's record. Click Enable SSO login . In the pop-up window, you will be prompted to send a notification to the user that SSO authentication is enabled. Click the Send notification button. If the notification is not required, click Cancel . After that, the user can authenticate through the identity provider. Disabling SSO Authentication for Users \u00b6 To disable SSO authentication for Wallarm users, go to Settings \u2192 Users . Find the desired user and open the user action menu by clicking the button on the right of the user's record. Click Disable SSO . After that, the user will be notified by an email that the login using SSO is disabled with a suggestion (link) to restore the password to log in with the login/password pair. In addition, two-factor authentication becomes available to the user.","title":"Employ user auth"},{"location":"admin-en/configuration-guides/sso/employ-user-auth/#configuring-sso-authentication-for-users","text":"You can enable or disable SSO authentication to Wallarm portal users.","title":"Configuring SSO Authentication for Users"},{"location":"admin-en/configuration-guides/sso/employ-user-auth/#enabling-sso-authentication-for-users","text":"Warning When enabling SSO authentication for users with any non-administrator roles ( Admin or Super Admin ), a login/password log in mechanism and the two-factor authentication will not be available. When SSO authentication is enabled, the user's password is erased and two-factor authentication is disabled. The Admin and Super Admin roles can use login/password pair, two-factor authentication, and SSO authentication simultaneously. It is assumed that you have already given the required group of users access to the configured Wallarm application on the Okta or G Suite side. To enable SSO authentication for Wallarm users go to Settings \u2192 Users . Find the desired user and open the user action menu by clicking the button on the right of the user's record. Click Enable SSO login . In the pop-up window, you will be prompted to send a notification to the user that SSO authentication is enabled. Click the Send notification button. If the notification is not required, click Cancel . After that, the user can authenticate through the identity provider.","title":"Enabling SSO Authentication for Users"},{"location":"admin-en/configuration-guides/sso/employ-user-auth/#disabling-sso-authentication-for-users","text":"To disable SSO authentication for Wallarm users, go to Settings \u2192 Users . Find the desired user and open the user action menu by clicking the button on the right of the user's record. Click Disable SSO . After that, the user will be notified by an email that the login using SSO is disabled with a suggestion (link) to restore the password to log in with the login/password pair. In addition, two-factor authentication becomes available to the user.","title":"Disabling SSO Authentication for Users"},{"location":"admin-en/configuration-guides/sso/intro/","text":"Overview of integration with the SAML SSO solution \u00b6 You can use Single Sign\u2011On (SSO) technology to authenticate your company's users to the Wallarm portal if your company already uses a SAML SSO solution. Wallarm can be integrated with any solution that supports the SAML standard. The SSO guides describe integration using Okta or Google Suite (G Suite) as an example. The documents related to the configuration and operation of Wallarm with SSO assume the following: Wallarm acts as a service provider (SP). Google or Okta acts as an identity provider (IdP). More information about roles in SAML SSO can be found here ( PDF ). Enabling the SSO service By default, SSO connection on Wallarm is not available without activating the appropriate service. To activate the SSO service, please contact your account manager. If no SSO service is activated, then SSO-related blocks will not be visible in the Integrations tab of the Settings section on the Wallarm portal.","title":"Intro"},{"location":"admin-en/configuration-guides/sso/intro/#overview-of-integration-with-the-saml-sso-solution","text":"You can use Single Sign\u2011On (SSO) technology to authenticate your company's users to the Wallarm portal if your company already uses a SAML SSO solution. Wallarm can be integrated with any solution that supports the SAML standard. The SSO guides describe integration using Okta or Google Suite (G Suite) as an example. The documents related to the configuration and operation of Wallarm with SSO assume the following: Wallarm acts as a service provider (SP). Google or Okta acts as an identity provider (IdP). More information about roles in SAML SSO can be found here ( PDF ). Enabling the SSO service By default, SSO connection on Wallarm is not available without activating the appropriate service. To activate the SSO service, please contact your account manager. If no SSO service is activated, then SSO-related blocks will not be visible in the Integrations tab of the Settings section on the Wallarm portal.","title":"Overview of integration with the SAML SSO solution"},{"location":"admin-en/configuration-guides/sso/gsuite/allow-access-to-wl/","text":"Step 4: Allowing Access to the Wallarm Application on the G Suite Side \u00b6 To authenticate through G Suite, an account must be created on the G Suite side, and the user must have access rights to the Wallarm application. The required sequence of actions for granting access rights is described below. Go to the G Suite\u2019s user management section by clicking on the Users block. Make sure that the user you are going to give access to the application via SSO authentication is in your organization's user list. Go to the SAML applications section by clicking on the SAML apps menu item as shown below. Enter the settings of the desired application and make sure that the status of the application is \u201cON for everyone.\u201d If the status of the application is \u201cOFF for everyone,\u201d click the Edit service button. Select the \u201cON for everyone\u201d status and click Save . After that you will receive a message that the status of the service has been updated. The Wallarm application is now available for SSO authentication to all users of your organization in G Suite. Setup Complete \u00b6 This completes the configuration of the G Suite\u2011based SSO, and now you can start configuring the user specific SSO authentication on the Wallarm side.","title":"Allow access to wl"},{"location":"admin-en/configuration-guides/sso/gsuite/allow-access-to-wl/#step-4-allowing-access-to-the-wallarm-application-on-the-g-suite-side","text":"To authenticate through G Suite, an account must be created on the G Suite side, and the user must have access rights to the Wallarm application. The required sequence of actions for granting access rights is described below. Go to the G Suite\u2019s user management section by clicking on the Users block. Make sure that the user you are going to give access to the application via SSO authentication is in your organization's user list. Go to the SAML applications section by clicking on the SAML apps menu item as shown below. Enter the settings of the desired application and make sure that the status of the application is \u201cON for everyone.\u201d If the status of the application is \u201cOFF for everyone,\u201d click the Edit service button. Select the \u201cON for everyone\u201d status and click Save . After that you will receive a message that the status of the service has been updated. The Wallarm application is now available for SSO authentication to all users of your organization in G Suite.","title":"Step 4: Allowing Access to the Wallarm Application on the G Suite Side"},{"location":"admin-en/configuration-guides/sso/gsuite/allow-access-to-wl/#setup-complete","text":"This completes the configuration of the G Suite\u2011based SSO, and now you can start configuring the user specific SSO authentication on the Wallarm side.","title":"Setup Complete"},{"location":"admin-en/configuration-guides/sso/gsuite/metadata-transfer/","text":"Step 3: Transferring G Suite Metadata to the Wallarm Setup Wizard \u00b6 Return to the SSO Wallarm setup wizard and click Next to proceed to the next setup step. At this stage, you need to provide the metadata generated by the G Suite service to the Wallarm SSO setup wizard. There are two ways to transfer metadata: Upload an XML file with metadata in the Wallarm setup wizard. Copy and paste the required parameters into the Wallarm setup wizard manually. Uploading Metadata Using an XML File \u00b6 If you saved the metadata of G Suite as an XML file when configuring the application in G Suite earlier (in Step 2 ), click the Upload button and select the desired file. You can also do this by dragging the file from your file manager to the \u201cXML\u201d icon. After uploading the file, click Next to go to the next step. Copying Parameters Manually \u00b6 If you have copied the provided identity provider parameters when configuring the application in G Suite, click the Enter manually link to enter the copied parameters manually and fill out the form. Insert the parameters generated by G Suite into the fields of the Wallarm setup wizard as follows: SSO URL \u2192 Identity provider SSO URL Entity ID \u2192 Identity provider issuer Certificate \u2192 X.509 Certificate Click Next to go to the next step. If you want to return to the previous step, click Back . Completing SSO Wizard \u00b6 On the final step of the Wallarm setup wizard, a test connection to the G Suite service will be performed automatically and the SSO provider will be checked. After successful completion of the test (if all the necessary parameters are filled in correctly), the setup wizard will inform you that the G Suite service is connected as an identity provider and you can start connecting the SSO mechanism to authenticate your users. Finish configuring SSO by clicking the Finish button or go to the user page to configure SSO by clicking the corresponding button. After completing the SSO configuration wizard, on the Integration tab you will see that the G Suite service is connected as an identity provider and that no other SSO providers are available. Now, navigate to the next step of the SSO configuration process.","title":"Metadata transfer"},{"location":"admin-en/configuration-guides/sso/gsuite/metadata-transfer/#step-3-transferring-g-suite-metadata-to-the-wallarm-setup-wizard","text":"Return to the SSO Wallarm setup wizard and click Next to proceed to the next setup step. At this stage, you need to provide the metadata generated by the G Suite service to the Wallarm SSO setup wizard. There are two ways to transfer metadata: Upload an XML file with metadata in the Wallarm setup wizard. Copy and paste the required parameters into the Wallarm setup wizard manually.","title":"Step 3: Transferring G Suite Metadata to the Wallarm Setup Wizard"},{"location":"admin-en/configuration-guides/sso/gsuite/metadata-transfer/#uploading-metadata-using-an-xml-file","text":"If you saved the metadata of G Suite as an XML file when configuring the application in G Suite earlier (in Step 2 ), click the Upload button and select the desired file. You can also do this by dragging the file from your file manager to the \u201cXML\u201d icon. After uploading the file, click Next to go to the next step.","title":"Uploading Metadata Using an XML File"},{"location":"admin-en/configuration-guides/sso/gsuite/metadata-transfer/#copying-parameters-manually","text":"If you have copied the provided identity provider parameters when configuring the application in G Suite, click the Enter manually link to enter the copied parameters manually and fill out the form. Insert the parameters generated by G Suite into the fields of the Wallarm setup wizard as follows: SSO URL \u2192 Identity provider SSO URL Entity ID \u2192 Identity provider issuer Certificate \u2192 X.509 Certificate Click Next to go to the next step. If you want to return to the previous step, click Back .","title":"Copying Parameters Manually"},{"location":"admin-en/configuration-guides/sso/gsuite/metadata-transfer/#completing-sso-wizard","text":"On the final step of the Wallarm setup wizard, a test connection to the G Suite service will be performed automatically and the SSO provider will be checked. After successful completion of the test (if all the necessary parameters are filled in correctly), the setup wizard will inform you that the G Suite service is connected as an identity provider and you can start connecting the SSO mechanism to authenticate your users. Finish configuring SSO by clicking the Finish button or go to the user page to configure SSO by clicking the corresponding button. After completing the SSO configuration wizard, on the Integration tab you will see that the G Suite service is connected as an identity provider and that no other SSO providers are available. Now, navigate to the next step of the SSO configuration process.","title":"Completing SSO Wizard"},{"location":"admin-en/configuration-guides/sso/gsuite/overview/","text":"Connecting SSO with G Suite \u00b6 This guide will cover the process of connecting the G Suite (Google) service as an identity provider to Wallarm, which acts as the service provider. Note By default, SSO connection on Wallarm is not available without activating the appropriate service. To activate the SSO service, please contact your account manager. After activating the service you will be able to perform the following SSO connection procedure, and the SSO-related blocks will be visible in the \u201cIntegrations\u201d tab. In addition, you need accounts with administration rights both for Wallarm and G Suite. The process of connecting SSO with G Suite comprises the following steps: Generating Parameters on the Wallarm Side. Creating and Configuring an Application in G Suite. Transferring G Suite Metadata to the Wallarm Setup Wizard. Allowing Access to the Wallarm Application on the G Suite Side After that, configure SSO authentication for Wallarm users.","title":"Overview"},{"location":"admin-en/configuration-guides/sso/gsuite/overview/#connecting-sso-with-g-suite","text":"This guide will cover the process of connecting the G Suite (Google) service as an identity provider to Wallarm, which acts as the service provider. Note By default, SSO connection on Wallarm is not available without activating the appropriate service. To activate the SSO service, please contact your account manager. After activating the service you will be able to perform the following SSO connection procedure, and the SSO-related blocks will be visible in the \u201cIntegrations\u201d tab. In addition, you need accounts with administration rights both for Wallarm and G Suite. The process of connecting SSO with G Suite comprises the following steps: Generating Parameters on the Wallarm Side. Creating and Configuring an Application in G Suite. Transferring G Suite Metadata to the Wallarm Setup Wizard. Allowing Access to the Wallarm Application on the G Suite Side After that, configure SSO authentication for Wallarm users.","title":"Connecting SSO with G Suite"},{"location":"admin-en/configuration-guides/sso/gsuite/setup-idp/","text":"Step 2: Creating and Configuring an Application in G Suite \u00b6 Prerequisites The following values are used as demonstration values in this guide: WallarmApp as a value for the Application Name parameter (in G Suite). https://sso.online.wallarm.com/acs as a value for the ACS URL parameter (in G Suite). https://sso.online.wallarm.com/entity-id as a value for the Entity ID parameter (in G Suite). Warning Ensure that you replace the sample values for the ACS URL and Entity ID parameters with the real ones obtained in the previous step . Log in to the Google admin console . Click on the Apps block. Click on the SAML apps block. Add a new application by clicking the Add a service/App to your domain link or the \u201c+\u201d button at the bottom right. Click on the Setup my own custom app button. You will be provided with information (metadata) by G Suite as your identity provider: * SSO URL * Entity ID Certificate (X.509) Metadata is a set of parameters describing the identity provider's properties (similar to those generated for the service provider in Step 1 ) that are required to configure SSO. You can transfer them to the SSO Wallarm setup wizard in two ways: Copy each parameter and download the certificate, and then paste (upload) it into the corresponding fields of the Wallarm setup wizard. Download an XML file with metadata and upload it on the Wallarm side. Save the metadata in any way you like and go to the next step of configuring the application by clicking Next . Entering the identity provider metadata on the Wallarm side will be described in Step 3 . The next stage of configuring the application is to provide the service provider's (Wallarm) metadata. Required fields: ACS URL corresponds to the Assertion Consumer Service URL parameter generated on the Wallarm side. Entity ID corresponds to the Wallarm Entity ID parameter generated on the Wallarm side. Fill in the remaining parameters if required. Click Next . At the final stage of configuring the application, you will be prompted to provide mappings between service provider's attributes to the available user profile fields. Wallarm (as a service provider) requires you to create an attribute mapping. Click Add new mapping and then map the email attribute to the \u201cPrimary Email\u201d user profile field (in the \u201cBasic Information\u201d group). Click Finish . After that, you will be informed in the pop-up window that the provided information is saved and, in order to complete the SAML SSO configuration, you will need to upload the data about the identity provider (Google) in the admin panel of the service provider (Wallarm). Click Ok . After that, you will be redirected to the page of the created application. Once the application is created, it is disabled for all your organizations in G Suite. To activate the SSO for this application, click the Edit Service button. Select ON for everyone for the Service status parameter and click Save . Now you can continue configuring the SSO on the Wallarm side.","title":"Setup idp"},{"location":"admin-en/configuration-guides/sso/gsuite/setup-idp/#step-2-creating-and-configuring-an-application-in-g-suite","text":"Prerequisites The following values are used as demonstration values in this guide: WallarmApp as a value for the Application Name parameter (in G Suite). https://sso.online.wallarm.com/acs as a value for the ACS URL parameter (in G Suite). https://sso.online.wallarm.com/entity-id as a value for the Entity ID parameter (in G Suite). Warning Ensure that you replace the sample values for the ACS URL and Entity ID parameters with the real ones obtained in the previous step . Log in to the Google admin console . Click on the Apps block. Click on the SAML apps block. Add a new application by clicking the Add a service/App to your domain link or the \u201c+\u201d button at the bottom right. Click on the Setup my own custom app button. You will be provided with information (metadata) by G Suite as your identity provider: * SSO URL * Entity ID Certificate (X.509) Metadata is a set of parameters describing the identity provider's properties (similar to those generated for the service provider in Step 1 ) that are required to configure SSO. You can transfer them to the SSO Wallarm setup wizard in two ways: Copy each parameter and download the certificate, and then paste (upload) it into the corresponding fields of the Wallarm setup wizard. Download an XML file with metadata and upload it on the Wallarm side. Save the metadata in any way you like and go to the next step of configuring the application by clicking Next . Entering the identity provider metadata on the Wallarm side will be described in Step 3 . The next stage of configuring the application is to provide the service provider's (Wallarm) metadata. Required fields: ACS URL corresponds to the Assertion Consumer Service URL parameter generated on the Wallarm side. Entity ID corresponds to the Wallarm Entity ID parameter generated on the Wallarm side. Fill in the remaining parameters if required. Click Next . At the final stage of configuring the application, you will be prompted to provide mappings between service provider's attributes to the available user profile fields. Wallarm (as a service provider) requires you to create an attribute mapping. Click Add new mapping and then map the email attribute to the \u201cPrimary Email\u201d user profile field (in the \u201cBasic Information\u201d group). Click Finish . After that, you will be informed in the pop-up window that the provided information is saved and, in order to complete the SAML SSO configuration, you will need to upload the data about the identity provider (Google) in the admin panel of the service provider (Wallarm). Click Ok . After that, you will be redirected to the page of the created application. Once the application is created, it is disabled for all your organizations in G Suite. To activate the SSO for this application, click the Edit Service button. Select ON for everyone for the Service status parameter and click Save . Now you can continue configuring the SSO on the Wallarm side.","title":"Step 2: Creating and Configuring an Application in G Suite"},{"location":"admin-en/configuration-guides/sso/gsuite/setup-sp/","text":"Step 1: Generating Parameters on the Wallarm Side (G Suite) \u00b6 Log in to the Wallarm portal using your Admin account and go to the Settings \u2192 Integration tab. In the SSO/SAML Authentication section, click on the Google SSO block. This will bring up the SSO configuration wizard. At the first step of the wizard you will be presented with a form with the parameters (service provider's metadata) that should be passed to the G Suite service: Wallarm Entity ID is a unique application identifier generated by the Wallarm application for the identity provider. Assertion Consumer Service URL (ACS URL) is the address on the Wallarm side of the application on which identity provider sends requests with the SamlResponse parameter. The generated parameters will need to be entered into the corresponding fields on the G Suite service side (see Step 2 ).","title":"Setup sp"},{"location":"admin-en/configuration-guides/sso/gsuite/setup-sp/#step-1-generating-parameters-on-the-wallarm-side-g-suite","text":"Log in to the Wallarm portal using your Admin account and go to the Settings \u2192 Integration tab. In the SSO/SAML Authentication section, click on the Google SSO block. This will bring up the SSO configuration wizard. At the first step of the wizard you will be presented with a form with the parameters (service provider's metadata) that should be passed to the G Suite service: Wallarm Entity ID is a unique application identifier generated by the Wallarm application for the identity provider. Assertion Consumer Service URL (ACS URL) is the address on the Wallarm side of the application on which identity provider sends requests with the SamlResponse parameter. The generated parameters will need to be entered into the corresponding fields on the G Suite service side (see Step 2 ).","title":"Step 1: Generating Parameters on the Wallarm Side (G Suite)"},{"location":"admin-en/configuration-guides/sso/okta/allow-access-to-wl/","text":"Step 4: Allowing Access to the Wallarm Application on the Okta Side \u00b6 To authenticate through Okta, an account must be created on the Okta side and the user must have access rights to the Wallarm application. The required sequence of actions for granting access rights is described below. Click the Admin button at the top right of the Okta portal. In the Dashboard section, click the Assign Applications link. You will be prompted to assign the applications to the right users in order to give these users access to the selected applications. To do this, tick the checkboxes beside the required applications and users and click Next . Next, you will be prompted to check and confirm the application assignments. If all is correct, confirm the assignments by clicking the Confirm Assignments button. After that, you can go to the application settings page on the Assignments tab. Here you will be able to see a list of users who have access to the application for which SSO is configured. The access rights to the Wallarm application are now set up. Now, users assigned to the application can access the application using SSO authentication through the Okta service. Setup Complete \u00b6 This completes the configuration of the Okta\u2011based SSO, and now you can start configuring the user specific SSO authentication on the Wallarm side.","title":"Allow access to wl"},{"location":"admin-en/configuration-guides/sso/okta/allow-access-to-wl/#step-4-allowing-access-to-the-wallarm-application-on-the-okta-side","text":"To authenticate through Okta, an account must be created on the Okta side and the user must have access rights to the Wallarm application. The required sequence of actions for granting access rights is described below. Click the Admin button at the top right of the Okta portal. In the Dashboard section, click the Assign Applications link. You will be prompted to assign the applications to the right users in order to give these users access to the selected applications. To do this, tick the checkboxes beside the required applications and users and click Next . Next, you will be prompted to check and confirm the application assignments. If all is correct, confirm the assignments by clicking the Confirm Assignments button. After that, you can go to the application settings page on the Assignments tab. Here you will be able to see a list of users who have access to the application for which SSO is configured. The access rights to the Wallarm application are now set up. Now, users assigned to the application can access the application using SSO authentication through the Okta service.","title":"Step 4: Allowing Access to the Wallarm Application on the Okta Side"},{"location":"admin-en/configuration-guides/sso/okta/allow-access-to-wl/#setup-complete","text":"This completes the configuration of the Okta\u2011based SSO, and now you can start configuring the user specific SSO authentication on the Wallarm side.","title":"Setup Complete"},{"location":"admin-en/configuration-guides/sso/okta/metadata-transfer/","text":"Step 3: Transferring Okta Metadata to the Wallarm Setup Wizard \u00b6 Return to the SSO Wallarm setup wizard and click Next to proceed to the next setup step. At this step, you need to provide the metadata generated by the Okta service. There are two ways to pass the identity provider metadata (in this case Okta) to the Wallarm setup wizard: By uploading an XML file with metadata. Upload the XML file by clicking the Upload button and selecting the appropriate file. You can also do this by dragging the file from your file manager to the \u201cXML\u201d icon field. By entering the metadata manually. Click the Enter manually link and copy the Okta service parameters to the fields of the setup wizard as follows: Identity Provider Single Sign\u2011On URL to the Identity provider SSO URL field. Identity Provider Issuer to the Identity provider issuer field. X.509 Certificate to the X.509 Certificate field. Click Next to go to the next step. If you want to return to the previous step, click Back . Completing SSO Wizard \u00b6 On the final step of the Wallarm setup wizard, a test connection to the Okta service will be performed automatically and the SSO provider will be checked. After successful completion of the test (if all the necessary parameters are filled in correctly), the setup wizard will inform you that the Okta service is connected as an identity provider, and you can start connecting the SSO mechanism to authenticate your users. Finish configuring SSO by clicking the Finish button or going to the user page to configure SSO by clicking the corresponding button. After completing the SSO configuration wizard, on the Integration tab you will see that the Okta service is connected as an identity provider and that no other SSO providers are available. Now, navigate to the next step of the SSO configuration process.","title":"Metadata transfer"},{"location":"admin-en/configuration-guides/sso/okta/metadata-transfer/#step-3-transferring-okta-metadata-to-the-wallarm-setup-wizard","text":"Return to the SSO Wallarm setup wizard and click Next to proceed to the next setup step. At this step, you need to provide the metadata generated by the Okta service. There are two ways to pass the identity provider metadata (in this case Okta) to the Wallarm setup wizard: By uploading an XML file with metadata. Upload the XML file by clicking the Upload button and selecting the appropriate file. You can also do this by dragging the file from your file manager to the \u201cXML\u201d icon field. By entering the metadata manually. Click the Enter manually link and copy the Okta service parameters to the fields of the setup wizard as follows: Identity Provider Single Sign\u2011On URL to the Identity provider SSO URL field. Identity Provider Issuer to the Identity provider issuer field. X.509 Certificate to the X.509 Certificate field. Click Next to go to the next step. If you want to return to the previous step, click Back .","title":"Step 3: Transferring Okta Metadata to the Wallarm Setup Wizard"},{"location":"admin-en/configuration-guides/sso/okta/metadata-transfer/#completing-sso-wizard","text":"On the final step of the Wallarm setup wizard, a test connection to the Okta service will be performed automatically and the SSO provider will be checked. After successful completion of the test (if all the necessary parameters are filled in correctly), the setup wizard will inform you that the Okta service is connected as an identity provider, and you can start connecting the SSO mechanism to authenticate your users. Finish configuring SSO by clicking the Finish button or going to the user page to configure SSO by clicking the corresponding button. After completing the SSO configuration wizard, on the Integration tab you will see that the Okta service is connected as an identity provider and that no other SSO providers are available. Now, navigate to the next step of the SSO configuration process.","title":"Completing SSO Wizard"},{"location":"admin-en/configuration-guides/sso/okta/overview/","text":"Connecting SSO with Okta \u00b6 This guide will cover the process of connecting the Okta service as an identity provider to Wallarm, which acts as the service provider. Note By default, SSO connection on Wallarm is not available without activating the appropriate service. To activate the SSO service, please contact your account manager. After activating the service you will be able to perform the following SSO connection procedure, and the SSO-related blocks will be visible in the \u201cIntegrations\u201d tab. In addition, you need accounts with administration rights both for Wallarm and Okta. The process of connecting SSO with Okta comprises the following steps: Generating Parameters on the Wallarm Side. Creating and Configuring an Application in Okta. Transferring Okta Metadata to the Wallarm Setup Wizard. Allowing Access to the Wallarm Application on the Okta Side After that, configure SSO authentication for Wallarm users.","title":"Overview"},{"location":"admin-en/configuration-guides/sso/okta/overview/#connecting-sso-with-okta","text":"This guide will cover the process of connecting the Okta service as an identity provider to Wallarm, which acts as the service provider. Note By default, SSO connection on Wallarm is not available without activating the appropriate service. To activate the SSO service, please contact your account manager. After activating the service you will be able to perform the following SSO connection procedure, and the SSO-related blocks will be visible in the \u201cIntegrations\u201d tab. In addition, you need accounts with administration rights both for Wallarm and Okta. The process of connecting SSO with Okta comprises the following steps: Generating Parameters on the Wallarm Side. Creating and Configuring an Application in Okta. Transferring Okta Metadata to the Wallarm Setup Wizard. Allowing Access to the Wallarm Application on the Okta Side After that, configure SSO authentication for Wallarm users.","title":"Connecting SSO with Okta"},{"location":"admin-en/configuration-guides/sso/okta/setup-idp/","text":"Step 2: Creating and Configuring an Application in Okta \u00b6 Prerequisites The following values are used as demonstration values in this guide: WallarmApp as a value for the App name parameter (in Okta). https://sso.online.wallarm.com/acs as a value for the Single sign\u2011on URL parameter (in Okta). https://sso.online.wallarm.com/entity-id as a value for the Audience URI parameter (in Okta). Warning Ensure that you replace the sample values for the Single sign\u2011on URL and Audience URI parameters with the real ones obtained in the previous step . Log in to the Okta service (the account must have administrator rights) and click on the Administrator button in the upper right. In the Dashboard section, click the Add Applications button on the right. In the new application section, click the Create New App button on the right. In the pop-up window, set the following options: Platform \u2192 \u201cWeb\u201d. Sign\u2011on method \u2192 \u201cSAML 2.0\u201d. Click the Create button. After that you will be taken to the SAML integration wizard ( Create SAML Integration ). To create and configure SAML integration you will be prompted to complete three stages: General Settings. Configure SAML. Feedback. After that, the metadata needs to be downloaded for the newly created integration. 1. General Settings \u00b6 Enter the name of the application you are creating in the App Name field. Optionally, you can download the logo of the application ( App logo ) and configure application visibility for your users on the Okta homepage and in the Okta mobile application. Click the Next button. 2. Configure SAML \u00b6 At this stage you will need the parameters generated earlier on the Wallarm side: Wallarm Entity ID Assertion Consumer Service URL (ACS URL) Okta parameters This manual describes only the mandatory parameters to be filled in when configuring SSO with Okta. To learn more about the rest of the parameters (including those related to the digital signature and SAML message encryption settings), please refer to the Okta documentation . Fill in the following basic parameters: Single sign\u2011on URL \u2014enter the Assertion Consumer Service URL (ACS URL) value previously obtained on the Wallarm side. Audience URI (SP Entity ID) \u2014enter the value of the Wallarm Entity ID received earlier on the Wallarm side. The remaining parameters for the initial setup can be left as default. Click Next to continue the setup. If you want to return to the previous step, click Previous . 3. Feedback \u00b6 At this stage, you are asked to provide Okta with additional information about the type of your application, whether you are an Okta customer or partner, and other data. It is enough to choose \u201cI'm an Okta customer adding an internal app\u201d for the parameter Are you a customer or partner ? If required, fill in other available parameters. After that, you can finish the SAML integration wizard by clicking the Finish button. To go to the previous step, click the Previous button. After this stage, you will be taken to the settings page of the created application. Now you need to download the metadata for the created integration to continue configuring the SSO provider on the Wallarm side. The metadata is a set of parameters describing the identity provider's properties (such as those generated for the service provider in Step 1 ) required to configure SSO. Downloading Metadata \u00b6 You can download the metadata either as an XML file or \u201cas is\u201d in text form (you will need to enter the metadata manually when configuring it further). To download as an XML file: Click the Identity Provider metadata link on the settings page of the created application: As a result, you will be taken to a new tab on your browser with similar content: Save the content to an XML file (with your browser or other suitable method). To download the metadata \u201cas is\u201d: On the settings page of the created application, click the View Setup instructions button. Copy all the given data. Now you can continue configuring the SSO on the Wallarm side.","title":"Setup idp"},{"location":"admin-en/configuration-guides/sso/okta/setup-idp/#step-2-creating-and-configuring-an-application-in-okta","text":"Prerequisites The following values are used as demonstration values in this guide: WallarmApp as a value for the App name parameter (in Okta). https://sso.online.wallarm.com/acs as a value for the Single sign\u2011on URL parameter (in Okta). https://sso.online.wallarm.com/entity-id as a value for the Audience URI parameter (in Okta). Warning Ensure that you replace the sample values for the Single sign\u2011on URL and Audience URI parameters with the real ones obtained in the previous step . Log in to the Okta service (the account must have administrator rights) and click on the Administrator button in the upper right. In the Dashboard section, click the Add Applications button on the right. In the new application section, click the Create New App button on the right. In the pop-up window, set the following options: Platform \u2192 \u201cWeb\u201d. Sign\u2011on method \u2192 \u201cSAML 2.0\u201d. Click the Create button. After that you will be taken to the SAML integration wizard ( Create SAML Integration ). To create and configure SAML integration you will be prompted to complete three stages: General Settings. Configure SAML. Feedback. After that, the metadata needs to be downloaded for the newly created integration.","title":"Step 2: Creating and Configuring an Application in Okta"},{"location":"admin-en/configuration-guides/sso/okta/setup-idp/#1-general-settings","text":"Enter the name of the application you are creating in the App Name field. Optionally, you can download the logo of the application ( App logo ) and configure application visibility for your users on the Okta homepage and in the Okta mobile application. Click the Next button.","title":"1.  General Settings"},{"location":"admin-en/configuration-guides/sso/okta/setup-idp/#2-configure-saml","text":"At this stage you will need the parameters generated earlier on the Wallarm side: Wallarm Entity ID Assertion Consumer Service URL (ACS URL) Okta parameters This manual describes only the mandatory parameters to be filled in when configuring SSO with Okta. To learn more about the rest of the parameters (including those related to the digital signature and SAML message encryption settings), please refer to the Okta documentation . Fill in the following basic parameters: Single sign\u2011on URL \u2014enter the Assertion Consumer Service URL (ACS URL) value previously obtained on the Wallarm side. Audience URI (SP Entity ID) \u2014enter the value of the Wallarm Entity ID received earlier on the Wallarm side. The remaining parameters for the initial setup can be left as default. Click Next to continue the setup. If you want to return to the previous step, click Previous .","title":"2.  Configure SAML"},{"location":"admin-en/configuration-guides/sso/okta/setup-idp/#3-feedback","text":"At this stage, you are asked to provide Okta with additional information about the type of your application, whether you are an Okta customer or partner, and other data. It is enough to choose \u201cI'm an Okta customer adding an internal app\u201d for the parameter Are you a customer or partner ? If required, fill in other available parameters. After that, you can finish the SAML integration wizard by clicking the Finish button. To go to the previous step, click the Previous button. After this stage, you will be taken to the settings page of the created application. Now you need to download the metadata for the created integration to continue configuring the SSO provider on the Wallarm side. The metadata is a set of parameters describing the identity provider's properties (such as those generated for the service provider in Step 1 ) required to configure SSO.","title":"3.  Feedback"},{"location":"admin-en/configuration-guides/sso/okta/setup-idp/#downloading-metadata","text":"You can download the metadata either as an XML file or \u201cas is\u201d in text form (you will need to enter the metadata manually when configuring it further). To download as an XML file: Click the Identity Provider metadata link on the settings page of the created application: As a result, you will be taken to a new tab on your browser with similar content: Save the content to an XML file (with your browser or other suitable method). To download the metadata \u201cas is\u201d: On the settings page of the created application, click the View Setup instructions button. Copy all the given data. Now you can continue configuring the SSO on the Wallarm side.","title":"Downloading Metadata"},{"location":"admin-en/configuration-guides/sso/okta/setup-sp/","text":"Step 1: Generating Parameters on the Wallarm Side (Okta) \u00b6 Log in to the Wallarm portal using your Admin account and go to the Settings \u2192 Integration tab. In the SSO/SAML Authentication section, click on the Okta SSO block. This will bring up the SSO configuration wizard. At the first step of the wizard you will be presented with a form with the parameters (service provider's metadata) that should be passed to the Okta service: Wallarm Entity ID is a unique application identifier generated by the Wallarm application for the identity provider. Assertion Consumer Service URL (ACS URL) is the address on the Wallarm side of the application on which identity provider sends requests with the SamlResponse parameter. The generated parameters will need to be entered into the corresponding fields on the Okta service side (see Step 2 ).","title":"Setup sp"},{"location":"admin-en/configuration-guides/sso/okta/setup-sp/#step-1-generating-parameters-on-the-wallarm-side-okta","text":"Log in to the Wallarm portal using your Admin account and go to the Settings \u2192 Integration tab. In the SSO/SAML Authentication section, click on the Okta SSO block. This will bring up the SSO configuration wizard. At the first step of the wizard you will be presented with a form with the parameters (service provider's metadata) that should be passed to the Okta service: Wallarm Entity ID is a unique application identifier generated by the Wallarm application for the identity provider. Assertion Consumer Service URL (ACS URL) is the address on the Wallarm side of the application on which identity provider sends requests with the SamlResponse parameter. The generated parameters will need to be entered into the corresponding fields on the Okta service side (see Step 2 ).","title":"Step 1: Generating Parameters on the Wallarm Side (Okta)"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/configure-waf-in-separated-environments/","text":"Recommendations on Configuring the Filter Node for Separated Environments \u00b6 Initial WAF Protection Deployment Process \u00b6 If you perform the initial rollout of WAF protection for environments, it is recommended you use the following approach (you are welcome to adjust it as needed): Learn about available WAF node deployment options here . If necessary, learn about available options to separately manage the WAF node configuration for your environments. You can find this information here . Deploy WAF nodes in your non-production environments with the filtration mode set to monitoring . Learn about how to operate, scale, and monitor the WAF solution; confirm the stability of the new network component. Deploy WAF nodes in your production environment with the filtration mode set to monitoring . Implement proper configuration management and monitoring processes for the new WAF component. Keep the traffic flowing via the WAF nodes in all your environments, including testing and production, for 7-14 days to give the WAF cloud\u2011based backend some time to learn about your application. Enable the blocking filtration mode in all your non-production environments and use automated or manual tests to confirm the protected application is working as expected. Enable the blocking filtration mode in the production environment. Using available methods, confirm that the application is working as expected. Info To set up the filtration mode, please use these instructions . Gradual Rollout of New WAF Changes \u00b6 From time to time changes might be needed in your existing Wallarm WAF infrastructure. Depending on your organization's change management policy, you might be required to test all potentially risky changes in a non-production environment, and then apply the changes in your production environment. The following approaches are recommended to test and gradually change the configuration of different Wallarm WAF components and features: Low-level configuration of Wallarm WAF filtering nodes in all form-factors Configuration of Wallarm WAF node rules Low-level \u0421onfiguration of Wallarm WAF Filtering Nodes in All Form-factors \u00b6 Low-level configuration of filtering nodes is performed via Docker environment variables, provided NGINX configuration file, Kubernetes Ingress controller parameters, etc. The way of configuration depends on the deployment option . Low-level configuration can easily be separately managed for different customer environments using your existing change management processes for infrastructure resources. Configuration of Wallarm WAF Node Rules \u00b6 Since each rule record can be associated with a different set of application instance IDs or HOST request headers, the following options are recommended: First apply a new configuration to a test or development environment, verify the functionality, and then apply the change for the production environment. Use the Define a request as an attack based on a regular expression WAF rule in the Experimental mode. This mode allows the rule to be deployed directly in the production environment without the risk of mistakenly blocking valid end user requests. Use the Set traffic filtration mode rule to control the WAF filtration mode for specific environments and requests. This rule provides additional flexibility in the way WAF protection can be gradually rolled out to protect new end-points and other resources in different environments. By default, the wallarm_mode value is used depending on the wallarm_mode_allow_override setting.","title":"Configure waf in separated environments"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/configure-waf-in-separated-environments/#recommendations-on-configuring-the-filter-node-for-separated-environments","text":"","title":"Recommendations on Configuring the Filter Node for Separated Environments"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/configure-waf-in-separated-environments/#initial-waf-protection-deployment-process","text":"If you perform the initial rollout of WAF protection for environments, it is recommended you use the following approach (you are welcome to adjust it as needed): Learn about available WAF node deployment options here . If necessary, learn about available options to separately manage the WAF node configuration for your environments. You can find this information here . Deploy WAF nodes in your non-production environments with the filtration mode set to monitoring . Learn about how to operate, scale, and monitor the WAF solution; confirm the stability of the new network component. Deploy WAF nodes in your production environment with the filtration mode set to monitoring . Implement proper configuration management and monitoring processes for the new WAF component. Keep the traffic flowing via the WAF nodes in all your environments, including testing and production, for 7-14 days to give the WAF cloud\u2011based backend some time to learn about your application. Enable the blocking filtration mode in all your non-production environments and use automated or manual tests to confirm the protected application is working as expected. Enable the blocking filtration mode in the production environment. Using available methods, confirm that the application is working as expected. Info To set up the filtration mode, please use these instructions .","title":"Initial WAF Protection Deployment Process"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/configure-waf-in-separated-environments/#gradual-rollout-of-new-waf-changes","text":"From time to time changes might be needed in your existing Wallarm WAF infrastructure. Depending on your organization's change management policy, you might be required to test all potentially risky changes in a non-production environment, and then apply the changes in your production environment. The following approaches are recommended to test and gradually change the configuration of different Wallarm WAF components and features: Low-level configuration of Wallarm WAF filtering nodes in all form-factors Configuration of Wallarm WAF node rules","title":"Gradual Rollout of New WAF Changes"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/configure-waf-in-separated-environments/#low-level-onfiguration-of-wallarm-waf-filtering-nodes-in-all-form-factors","text":"Low-level configuration of filtering nodes is performed via Docker environment variables, provided NGINX configuration file, Kubernetes Ingress controller parameters, etc. The way of configuration depends on the deployment option . Low-level configuration can easily be separately managed for different customer environments using your existing change management processes for infrastructure resources.","title":"Low-level \u0421onfiguration of Wallarm WAF Filtering Nodes in All Form-factors"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/configure-waf-in-separated-environments/#configuration-of-wallarm-waf-node-rules","text":"Since each rule record can be associated with a different set of application instance IDs or HOST request headers, the following options are recommended: First apply a new configuration to a test or development environment, verify the functionality, and then apply the change for the production environment. Use the Define a request as an attack based on a regular expression WAF rule in the Experimental mode. This mode allows the rule to be deployed directly in the production environment without the risk of mistakenly blocking valid end user requests. Use the Set traffic filtration mode rule to control the WAF filtration mode for specific environments and requests. This rule provides additional flexibility in the way WAF protection can be gradually rolled out to protect new end-points and other resources in different environments. By default, the wallarm_mode value is used depending on the wallarm_mode_allow_override setting.","title":"Configuration of Wallarm WAF Node Rules"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/how-waf-in-separated-environments-works/","text":"How WAF Filter Node Works in Separated Environments \u00b6 The application may be deployed to a few different environments: production, staging, testing, development, etc. These instructions provides information about suggested ways to manage a filter node for different environments. What is an Environment \u00b6 The definition of an environment may differ from company to company, and for the purpose of this instruction, the definition below is used. An environment is an isolated set or subset of computing resources serving different purposes (like production, staging, testing, development, etc) and managed using the same or different set of policies (in terms of network/software configurations, software versions, monitoring, change management, etc) by the same or different teams (SRE, QA, Development, etc) of a company. From the best practices perspective, it is recommended to keep the WAF configuration synchronized across all environments used in a single product vertical (development, testing, staging and production stages). Relevant Wallarm Features \u00b6 There are three main features that allow you to manage different filter node configurations for different environments and perform a gradual rollout of filter node changes: Resource identification Separate Wallarm accounts and sub-accounts Filter node operation mode Resource Identification \u00b6 There are two ways to configure the filter node for a particular environment using identification: Wallarm unique IDs for each environment, different URL domain names of environments (if it's already configured in your architecture). Environment Identification by ID \u00b6 The Applications concept allows you to assign different IDs to different protected environments, and manage filter node rules separately for each environment. When configuring a filter node you can add Wallarm IDs for your environments using the Applications concept. To set up IDs: Add environment names and its IDs in your Wallarm account > Settings > Applications section. Specify ID configuration in a filter node: using the wallarm_instance directive for Linux\u2011based, Kubernetes sidecar and Docker\u2011based deployments; using the nginx.ingress.kubernetes.io/wallarm-instance annotation for Kubernetes NGINX Ingress controller deployments. Now, when creating a new filter node rule it is possible to specify that the rule will be assigned to a set of specific instance IDs. Without the attribute, a new rule will be automatically applied to all protected resources in a Wallarm account. Environment Identification by Domain \u00b6 If every environment is using different URL domain names passed in the HOST HTTP request header then it is possible to use the domain names as unique identifiers of each environment. To use the feature, please add proper HOST header pointer for each configured filter node rule. In the following example the rule will be triggered only for requests with the HOST header equal to dev.domain.com : Separate Wallarm Accounts and Sub-accounts \u00b6 One easy option to isolate the filter node configuration of different environments is to use separate Wallarm accounts for each environment or group of environments. This best practice is recommended by many cloud service vendors, including Amazon AWS. To simplify the management of several Wallarm accounts, it is possible to create a logical master Wallarm account and assign other used Wallarm accounts as sub-accounts to the master account. This way a single set of console UI and API credentials can be used to manage all Wallarm accounts belonging to your organization. To activate a master account and sub-accounts, please contact Wallarm's Technical Support team. The feature requires a separate Wallarm enterprise license. Known limitations All WAF nodes connected to the same Wallarm account will receive the same set of WAF rules. You still can apply different rules for different applications by using proper application instance IDs or unique HTTP request headers . If the WAF node decides to automatically block an IP address (for example, because of three or more detected attack vectors from the IP address) the system will block the IP for all application instances in a Wallarm account.","title":"How waf in separated environments works"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/how-waf-in-separated-environments-works/#how-waf-filter-node-works-in-separated-environments","text":"The application may be deployed to a few different environments: production, staging, testing, development, etc. These instructions provides information about suggested ways to manage a filter node for different environments.","title":"How WAF Filter Node Works in Separated Environments"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/how-waf-in-separated-environments-works/#what-is-an-environment","text":"The definition of an environment may differ from company to company, and for the purpose of this instruction, the definition below is used. An environment is an isolated set or subset of computing resources serving different purposes (like production, staging, testing, development, etc) and managed using the same or different set of policies (in terms of network/software configurations, software versions, monitoring, change management, etc) by the same or different teams (SRE, QA, Development, etc) of a company. From the best practices perspective, it is recommended to keep the WAF configuration synchronized across all environments used in a single product vertical (development, testing, staging and production stages).","title":"What is an Environment"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/how-waf-in-separated-environments-works/#relevant-wallarm-features","text":"There are three main features that allow you to manage different filter node configurations for different environments and perform a gradual rollout of filter node changes: Resource identification Separate Wallarm accounts and sub-accounts Filter node operation mode","title":"Relevant Wallarm Features"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/how-waf-in-separated-environments-works/#resource-identification","text":"There are two ways to configure the filter node for a particular environment using identification: Wallarm unique IDs for each environment, different URL domain names of environments (if it's already configured in your architecture).","title":"Resource Identification"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/how-waf-in-separated-environments-works/#environment-identification-by-id","text":"The Applications concept allows you to assign different IDs to different protected environments, and manage filter node rules separately for each environment. When configuring a filter node you can add Wallarm IDs for your environments using the Applications concept. To set up IDs: Add environment names and its IDs in your Wallarm account > Settings > Applications section. Specify ID configuration in a filter node: using the wallarm_instance directive for Linux\u2011based, Kubernetes sidecar and Docker\u2011based deployments; using the nginx.ingress.kubernetes.io/wallarm-instance annotation for Kubernetes NGINX Ingress controller deployments. Now, when creating a new filter node rule it is possible to specify that the rule will be assigned to a set of specific instance IDs. Without the attribute, a new rule will be automatically applied to all protected resources in a Wallarm account.","title":"Environment Identification by ID"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/how-waf-in-separated-environments-works/#environment-identification-by-domain","text":"If every environment is using different URL domain names passed in the HOST HTTP request header then it is possible to use the domain names as unique identifiers of each environment. To use the feature, please add proper HOST header pointer for each configured filter node rule. In the following example the rule will be triggered only for requests with the HOST header equal to dev.domain.com :","title":"Environment Identification by Domain"},{"location":"admin-en/configuration-guides/waf-in-separated-environments/how-waf-in-separated-environments-works/#separate-wallarm-accounts-and-sub-accounts","text":"One easy option to isolate the filter node configuration of different environments is to use separate Wallarm accounts for each environment or group of environments. This best practice is recommended by many cloud service vendors, including Amazon AWS. To simplify the management of several Wallarm accounts, it is possible to create a logical master Wallarm account and assign other used Wallarm accounts as sub-accounts to the master account. This way a single set of console UI and API credentials can be used to manage all Wallarm accounts belonging to your organization. To activate a master account and sub-accounts, please contact Wallarm's Technical Support team. The feature requires a separate Wallarm enterprise license. Known limitations All WAF nodes connected to the same Wallarm account will receive the same set of WAF rules. You still can apply different rules for different applications by using proper application instance IDs or unique HTTP request headers . If the WAF node decides to automatically block an IP address (for example, because of three or more detected attack vectors from the IP address) the system will block the IP for all application instances in a Wallarm account.","title":"Separate Wallarm Accounts and Sub-accounts"},{"location":"admin-en/configuration-guides/wallarm-ingress-controller/best-practices/high-availability-considerations/","text":"High Availability Considerations \u00b6 Info Wallarm\u2019s version of the Kubernetes Ingress controller is based on the community-supported NGINX Ingress controller for Kubernetes , so the majority of recommendations found in the official Ingress controller documentation and on the public Internet are also applicable to Wallarm\u2019s Ingress controller. Recommended reading: Collection of articles about the Kubernetes Ingress controller Official NGINX Ingress Controller Guide The following recommendations are relevant for missing-critical (production) environments. Use more than one Ingress controller pod instances. The behavior is controlled using the attribute controller.replicaCount in the values.yaml file. For example: controller: replicaCount: 2 Force the Kubernetes cluster to place Ingress controller pods on different nodes: this will increase the Ingress service's resilience in case of a node failure. This behavior is controlled using the Kubernetes pod anti-affinity feature, which is configured in the values.yaml file. For example: controller: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx-ingress topologyKey: \"kubernetes.io/hostname\" In clusters that are subject to unexpected traffic spikes or other conditions that may justify the use of Kubernetes's horizontal pod autoscaling (HPA) feature it can enabled in the values.yaml file using the following example: controller: autoscaling: enabled: true minReplicas: 1 maxReplicas: 11 targetCPUUtilizationPercentage: 50 targetMemoryUtilizationPercentage: 50 Run at least two instances of Wallarm's postanalytics service based on the Tarantool database. These pods include ingress-controller-wallarm-tarantool in the name. The behavior is controlled in the file values.yaml using the attribute controller.wallarm.tarantool.replicaCount . For example: controller: wallarm: tarantool: replicaCount: 2","title":"High availability considerations"},{"location":"admin-en/configuration-guides/wallarm-ingress-controller/best-practices/high-availability-considerations/#high-availability-considerations","text":"Info Wallarm\u2019s version of the Kubernetes Ingress controller is based on the community-supported NGINX Ingress controller for Kubernetes , so the majority of recommendations found in the official Ingress controller documentation and on the public Internet are also applicable to Wallarm\u2019s Ingress controller. Recommended reading: Collection of articles about the Kubernetes Ingress controller Official NGINX Ingress Controller Guide The following recommendations are relevant for missing-critical (production) environments. Use more than one Ingress controller pod instances. The behavior is controlled using the attribute controller.replicaCount in the values.yaml file. For example: controller: replicaCount: 2 Force the Kubernetes cluster to place Ingress controller pods on different nodes: this will increase the Ingress service's resilience in case of a node failure. This behavior is controlled using the Kubernetes pod anti-affinity feature, which is configured in the values.yaml file. For example: controller: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx-ingress topologyKey: \"kubernetes.io/hostname\" In clusters that are subject to unexpected traffic spikes or other conditions that may justify the use of Kubernetes's horizontal pod autoscaling (HPA) feature it can enabled in the values.yaml file using the following example: controller: autoscaling: enabled: true minReplicas: 1 maxReplicas: 11 targetCPUUtilizationPercentage: 50 targetMemoryUtilizationPercentage: 50 Run at least two instances of Wallarm's postanalytics service based on the Tarantool database. These pods include ingress-controller-wallarm-tarantool in the name. The behavior is controlled in the file values.yaml using the attribute controller.wallarm.tarantool.replicaCount . For example: controller: wallarm: tarantool: replicaCount: 2","title":"High Availability Considerations"},{"location":"admin-en/configuration-guides/wallarm-ingress-controller/best-practices/ingress-controller-monitoring/","text":"Ingress Controller Monitoring \u00b6 Info Wallarm\u2019s version of the Kubernetes Ingress controller is based on the community-supported NGINX Ingress controller for Kubernetes , so the majority of recommendations found in the official Ingress controller documentation and on the public Internet are also applicable to Wallarm\u2019s Ingress controller. Recommended reading: Collection of articles about the Kubernetes Ingress controller Official NGINX Ingress Controller Guide The general aspects of NGINX Ingress controller monitoring are already well covered on the Internet. Wallarm provides an additional set of monitoring metrics that should be enabled and monitored in a mission-critical environment. The controller.wallarm.metrics attribute of values.yaml enables the /wallarm-metrics metrics endpoint of the Ingress controller: controller: wallarm: metrics: enabled: true service: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: /wallarm-metrics prometheus.io/port: \"18080\" The following is a list of Wallarm-specific metrics in Prometheus format available via the newly exposed endpoint: # HELP nginx_wallarm_requests requests count # TYPE nginx_wallarm_requests gauge nginx_wallarm_requests 5 # HELP nginx_wallarm_attacks attack requests count # TYPE nginx_wallarm_attacks gauge nginx_wallarm_attacks 5 # HELP nginx_wallarm_blocked blocked requests count # TYPE nginx_wallarm_blocked gauge nginx_wallarm_blocked 5 # HELP nginx_wallarm_abnormal abnormal requests count # TYPE nginx_wallarm_abnormal gauge nginx_wallarm_abnormal 5 # HELP nginx_wallarm_tnt_errors tarantool write errors count # TYPE nginx_wallarm_tnt_errors gauge nginx_wallarm_tnt_errors 0 # HELP nginx_wallarm_api_errors API write errors count # TYPE nginx_wallarm_api_errors gauge nginx_wallarm_api_errors 0 # HELP nginx_wallarm_requests_lost lost requests count # TYPE nginx_wallarm_requests_lost gauge nginx_wallarm_requests_lost 0 # HELP nginx_wallarm_overlimits_time overlimits_time count # TYPE nginx_wallarm_overlimits_time gauge nginx_wallarm_overlimits_time 0 # HELP nginx_wallarm_segfaults segmentation faults count # TYPE nginx_wallarm_segfaults gauge nginx_wallarm_segfaults 0 # HELP nginx_wallarm_memfaults vmem limit reached events count # TYPE nginx_wallarm_memfaults gauge nginx_wallarm_memfaults 0 # HELP nginx_wallarm_softmemfaults request memory limit reached events count # TYPE nginx_wallarm_softmemfaults gauge nginx_wallarm_softmemfaults 0 # HELP nginx_wallarm_proton_errors libproton non-memory related libproton faults events count # TYPE nginx_wallarm_proton_errors gauge nginx_wallarm_proton_errors 0 # HELP nginx_wallarm_time_detect_seconds time spent for detection # TYPE nginx_wallarm_time_detect_seconds gauge nginx_wallarm_time_detect_seconds 0 # HELP nginx_wallarm_db_id proton.db file id # TYPE nginx_wallarm_db_id gauge nginx_wallarm_db_id 9 # HELP nginx_wallarm_lom_id LOM file id # TYPE nginx_wallarm_lom_id gauge nginx_wallarm_lom_id 38 # HELP nginx_wallarm_proton_instances proton instances count # TYPE nginx_wallarm_proton_instances gauge nginx_wallarm_proton_instances{status=\"success\"} 4 nginx_wallarm_proton_instances{status=\"fallback\"} 0 nginx_wallarm_proton_instances{status=\"failed\"} 0 # HELP nginx_wallarm_stalled_worker_time_seconds time a worker stalled in libproton # TYPE nginx_wallarm_stalled_worker_time_seconds gauge Detailed information about monitoring setup and the list of available metrics is provided in this documentation .","title":"Ingress controller monitoring"},{"location":"admin-en/configuration-guides/wallarm-ingress-controller/best-practices/ingress-controller-monitoring/#ingress-controller-monitoring","text":"Info Wallarm\u2019s version of the Kubernetes Ingress controller is based on the community-supported NGINX Ingress controller for Kubernetes , so the majority of recommendations found in the official Ingress controller documentation and on the public Internet are also applicable to Wallarm\u2019s Ingress controller. Recommended reading: Collection of articles about the Kubernetes Ingress controller Official NGINX Ingress Controller Guide The general aspects of NGINX Ingress controller monitoring are already well covered on the Internet. Wallarm provides an additional set of monitoring metrics that should be enabled and monitored in a mission-critical environment. The controller.wallarm.metrics attribute of values.yaml enables the /wallarm-metrics metrics endpoint of the Ingress controller: controller: wallarm: metrics: enabled: true service: annotations: prometheus.io/scrape: \"true\" prometheus.io/path: /wallarm-metrics prometheus.io/port: \"18080\" The following is a list of Wallarm-specific metrics in Prometheus format available via the newly exposed endpoint: # HELP nginx_wallarm_requests requests count # TYPE nginx_wallarm_requests gauge nginx_wallarm_requests 5 # HELP nginx_wallarm_attacks attack requests count # TYPE nginx_wallarm_attacks gauge nginx_wallarm_attacks 5 # HELP nginx_wallarm_blocked blocked requests count # TYPE nginx_wallarm_blocked gauge nginx_wallarm_blocked 5 # HELP nginx_wallarm_abnormal abnormal requests count # TYPE nginx_wallarm_abnormal gauge nginx_wallarm_abnormal 5 # HELP nginx_wallarm_tnt_errors tarantool write errors count # TYPE nginx_wallarm_tnt_errors gauge nginx_wallarm_tnt_errors 0 # HELP nginx_wallarm_api_errors API write errors count # TYPE nginx_wallarm_api_errors gauge nginx_wallarm_api_errors 0 # HELP nginx_wallarm_requests_lost lost requests count # TYPE nginx_wallarm_requests_lost gauge nginx_wallarm_requests_lost 0 # HELP nginx_wallarm_overlimits_time overlimits_time count # TYPE nginx_wallarm_overlimits_time gauge nginx_wallarm_overlimits_time 0 # HELP nginx_wallarm_segfaults segmentation faults count # TYPE nginx_wallarm_segfaults gauge nginx_wallarm_segfaults 0 # HELP nginx_wallarm_memfaults vmem limit reached events count # TYPE nginx_wallarm_memfaults gauge nginx_wallarm_memfaults 0 # HELP nginx_wallarm_softmemfaults request memory limit reached events count # TYPE nginx_wallarm_softmemfaults gauge nginx_wallarm_softmemfaults 0 # HELP nginx_wallarm_proton_errors libproton non-memory related libproton faults events count # TYPE nginx_wallarm_proton_errors gauge nginx_wallarm_proton_errors 0 # HELP nginx_wallarm_time_detect_seconds time spent for detection # TYPE nginx_wallarm_time_detect_seconds gauge nginx_wallarm_time_detect_seconds 0 # HELP nginx_wallarm_db_id proton.db file id # TYPE nginx_wallarm_db_id gauge nginx_wallarm_db_id 9 # HELP nginx_wallarm_lom_id LOM file id # TYPE nginx_wallarm_lom_id gauge nginx_wallarm_lom_id 38 # HELP nginx_wallarm_proton_instances proton instances count # TYPE nginx_wallarm_proton_instances gauge nginx_wallarm_proton_instances{status=\"success\"} 4 nginx_wallarm_proton_instances{status=\"fallback\"} 0 nginx_wallarm_proton_instances{status=\"failed\"} 0 # HELP nginx_wallarm_stalled_worker_time_seconds time a worker stalled in libproton # TYPE nginx_wallarm_stalled_worker_time_seconds gauge Detailed information about monitoring setup and the list of available metrics is provided in this documentation .","title":"Ingress Controller Monitoring"},{"location":"admin-en/configuration-guides/wallarm-ingress-controller/best-practices/report-public-user-ip/","text":"Proper Reporting of End User Public IP Address \u00b6 Info Wallarm\u2019s version of the Kubernetes Ingress controller is based on the community-supported NGINX Ingress controller for Kubernetes , so the majority of recommendations found in the official Ingress controller documentation and on the public Internet are also applicable to Wallarm\u2019s Ingress controller. Recommended reading: Collection of articles about the Kubernetes Ingress controller Official NGINX Ingress Controller Guide By default, the Ingress controller assumes that it is directly exposed to the Internet and that the IP addresses of connecting clients are their actual IPs. When passing client requests to upstream services, the controller will automatically add the HTTP request header X-FORWARDED-FOR , which contains the IP addresses of connecting clients. In situations when a controller is placed behind a load balancer (for example, AWS ELB or Google Network Load Balancer) there are two ways for the Ingress controller to report proper end user IP addresses. These ways are provided below. Enable Passing the Real Client IP on the Network Layer \u00b6 This feature is highly dependent on the cloud platform being used; in majority of cases it can be activated by setting the values.yaml file attribute controller.service.externalTrafficPolicy to the value Local : controller: service: externalTrafficPolicy: \"Local\" Enable Ingress Controller to Take the Value from the X-FORWARDED-FOR HTTP Request Header \u00b6 This option is more relevant when a customer is using an external CDN service like Cloudflare or Fastly. To set up: Make sure that the load balancer is passing the real client IP in an HTTP request header with the name X-FORWARDED-FOR . Open the values.yaml Helm chart file and set the controller.config.use-forwarded-headers attribute to true : controller: config: use-forwarded-headers: \"true\"","title":"Report public user ip"},{"location":"admin-en/configuration-guides/wallarm-ingress-controller/best-practices/report-public-user-ip/#proper-reporting-of-end-user-public-ip-address","text":"Info Wallarm\u2019s version of the Kubernetes Ingress controller is based on the community-supported NGINX Ingress controller for Kubernetes , so the majority of recommendations found in the official Ingress controller documentation and on the public Internet are also applicable to Wallarm\u2019s Ingress controller. Recommended reading: Collection of articles about the Kubernetes Ingress controller Official NGINX Ingress Controller Guide By default, the Ingress controller assumes that it is directly exposed to the Internet and that the IP addresses of connecting clients are their actual IPs. When passing client requests to upstream services, the controller will automatically add the HTTP request header X-FORWARDED-FOR , which contains the IP addresses of connecting clients. In situations when a controller is placed behind a load balancer (for example, AWS ELB or Google Network Load Balancer) there are two ways for the Ingress controller to report proper end user IP addresses. These ways are provided below.","title":"Proper Reporting of End User Public IP Address"},{"location":"admin-en/configuration-guides/wallarm-ingress-controller/best-practices/report-public-user-ip/#enable-passing-the-real-client-ip-on-the-network-layer","text":"This feature is highly dependent on the cloud platform being used; in majority of cases it can be activated by setting the values.yaml file attribute controller.service.externalTrafficPolicy to the value Local : controller: service: externalTrafficPolicy: \"Local\"","title":"Enable Passing the Real Client IP on the Network Layer"},{"location":"admin-en/configuration-guides/wallarm-ingress-controller/best-practices/report-public-user-ip/#enable-ingress-controller-to-take-the-value-from-the-x-forwarded-for-http-request-header","text":"This option is more relevant when a customer is using an external CDN service like Cloudflare or Fastly. To set up: Make sure that the load balancer is passing the real client IP in an HTTP request header with the name X-FORWARDED-FOR . Open the values.yaml Helm chart file and set the controller.config.use-forwarded-headers attribute to true : controller: config: use-forwarded-headers: \"true\"","title":"Enable Ingress Controller to Take the Value from the X-FORWARDED-FOR HTTP Request Header"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/","text":"Installing on Yandex.Cloud \u00b6 These instructions describe how to configure a virtual machine with Wallarm WAF on Yandex.Cloud. Requirements \u00b6 Yandex.Cloud configuration \u00b6 Before starting WAF node installation, please check that you meet all these requirements: Have access to the Yandex.Cloud management console Have a payment account in the status of ACTIVE or TRIAL_ACTIVE displayed on the billing page Created folder. By default, the folder default will be created. To create a new folder, please follow these instructions Created 2048\u2011bit RSA key pair for SSH connection. To create a key pair, please follow these instructions Wallarm WAF configuration \u00b6 Before starting WAF node installation, please check that you meet all these requirements: Have access to the account with the Administrator role and two\u2011factor authentication disabled in Wallarm Console in the EU Cloud or US Cloud Have access to https://api.wallarm.com:444 when working with the EU Wallarm Cloud or https://us1.api.wallarm.com:444 when working with the US Wallarm Cloud. Please ensure the access is not blocked by a firewall Execute all commands as a superuser (e.g. root ) Installation \u00b6 1. Create a virtual machine with the WAF node \u00b6 If Wallarm WAF is already deployed If you launch Wallarm WAF instead of the already existing Wallarm WAF or need to duplicate the deployment in the same environment, please keep the same WAF version as currently used or update the version of all deployments to the latest. To check the launched version, connect to the running instance and execute the following command: apt list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update all Wallarm WAF instances to the latest version. If the version 2.16.x or lower is installed, then please update all Wallarm WAF instances to the latest version. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Log in to the management console and select the folder where the virtual machine will be created. Select Compute Cloud from the list of services. Click the Create VM button. Select Wallarm WAF from the list of images in the Image/boot disk selection \u2192 Cloud Marketplace block. Configure other virtual machine parameters following these instructions . 2. Connect to the virtual machine with the WAF node \u00b6 Ensure the virtual machine is in the RUNNING status. Connect to the virtual machine via SSH following these instructions . 3. Connect the WAF node to Wallarm Cloud \u00b6 Connect the WAF node to the Wallarm Cloud using the cloud WAF node token or Wallarm Console account login and password . Using the cloud WAF node token \u00b6 Open the Wallarm Console \u2192 Nodes section and create a cloud WAF node. Open the card of the created cloud WAF node and copy the Node token . On the virtual machine, run the addcloudnode script: EU Cloud sudo /usr/share/wallarm-common/addcloudnode US Cloud sudo /usr/share/wallarm-common/addcloudnode -H us1.api.wallarm.com Paste the copied token value. The WAF node will now synchronize with the cloud every 2\u20114 minutes according to the default synchronization configuration. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 Using the administrator login and password \u00b6 On the virtual machine, run the addnode script: EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Provide your Wallarm administrator account's login and password. Enter the name of the new WAF node or press Enter to use the virtual machine name. The WAF node will now synchronize with the cloud every 2\u20114 minutes according to the default synchronization configuration. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 4. Update Wallarm WAF configuration \u00b6 Main configuration files of NGINX and Wallarm WAF node are located in the directories: /etc/nginx/nginx.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings The file is used for settings applied to all domains. To apply different settings to different domain groups, use the file nginx.conf or create new configuration files for each domain group (for example, example.com.conf and test.com.conf ). More detailed information about NGINX configuration files is available in the official NGINX documentation . /etc/nginx/conf.d/wallarm-status.conf with WAF node monitoring settings. A detailed description is available within this link /etc/default/wallarm-tarantool with the Tarantool database settings Request filtration mode \u00b6 By default, the WAF node is in the status monitoring and searches attack signs in requests but does not block detected attacks. We recommend keeping the traffic flowing via the WAF node in the monitoring mode for several days after the WAF node deployment and only then enable the block mode. Learn recommendations on the WAF node operation mode setup \u2192 To change the monitoring mode to block : Open the file /etc/nginx/conf.d/wallarm.conf : sudo vim /etc/nginx/conf.d/wallarm.conf Comment out the line wallarm_mode monitoring; . Open the file /etc/nginx/nginx.conf : sudo vim /etc/nginx/nginx.conf Add the line wallarm_mode block; to the http , server or location block: Example of the file /etc/nginx/nginx.conf user www-data ; worker_processes auto ; pid /run/nginx.pid ; include /etc/nginx/modules-enabled/*.conf ; events { worker_connections 768 ; # multi_accept on; } http { wallarm_mode block ; sendfile on ; tcp_nopush on ; tcp_nodelay on ; keepalive_timeout 65 ; types_hash_max_size 2048 ; # server_tokens off; # server_names_hash_bucket_size 64; # server_name_in_redirect off; include /etc/nginx/mime.types ; default_type application/octet-stream ; ## # SSL Settings ## ssl_protocols TLSv1 TLSv1.1 TLSv1.2 ; # Dropping SSLv3, ref: POODLE ssl_prefer_server_ciphers on ; ## # Logging Settings ## access_log /var/log/nginx/access.log ; error_log /var/log/nginx/error.log ; ## # Gzip Settings ## gzip on ; # gzip_vary on; # gzip_proxied any; # gzip_comp_level 6; # gzip_buffers 16 8k; # gzip_http_version 1.1; # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf ; include /etc/nginx/sites-enabled/* ; } Memory \u00b6 The WAF node uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 40% of the total server memory. To allocate memory for Tarantool: Open the Tarantool configuration file in the editing mode: sudo vim /etc/default/wallarm-tarantool Specify memory size in GB in the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 To apply changes, restart Tarantool: sudo systemctl restart wallarm-tarantool Other configurations \u00b6 To update other NGINX and Wallarm WAF configurations, use the NGINX documentation and the list of available Wallarm WAF directives . 5. Restart NGINX \u00b6 To apply changes, restart NGINX: sudo systemctl restart nginx 6. Test Wallarm WAF operation \u00b6 Send the request with test SQLI and XSS attacks to the external IP address: curl http://84.201.148.210/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Settings customization \u00b6 The WAF node with default settings is deployed in Yandex.Cloud. To customize Wallarm WAF settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Double\u2011detection of attacks with libdetection","title":"Yandex.Cloud Marketplace image deployment"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#installing-on-yandexcloud","text":"These instructions describe how to configure a virtual machine with Wallarm WAF on Yandex.Cloud.","title":"Installing on Yandex.Cloud"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#requirements","text":"","title":"Requirements"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#yandexcloud-configuration","text":"Before starting WAF node installation, please check that you meet all these requirements: Have access to the Yandex.Cloud management console Have a payment account in the status of ACTIVE or TRIAL_ACTIVE displayed on the billing page Created folder. By default, the folder default will be created. To create a new folder, please follow these instructions Created 2048\u2011bit RSA key pair for SSH connection. To create a key pair, please follow these instructions","title":"Yandex.Cloud configuration"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#wallarm-waf-configuration","text":"Before starting WAF node installation, please check that you meet all these requirements: Have access to the account with the Administrator role and two\u2011factor authentication disabled in Wallarm Console in the EU Cloud or US Cloud Have access to https://api.wallarm.com:444 when working with the EU Wallarm Cloud or https://us1.api.wallarm.com:444 when working with the US Wallarm Cloud. Please ensure the access is not blocked by a firewall Execute all commands as a superuser (e.g. root )","title":"Wallarm WAF configuration"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#installation","text":"","title":"Installation"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#1-create-a-virtual-machine-with-the-waf-node","text":"If Wallarm WAF is already deployed If you launch Wallarm WAF instead of the already existing Wallarm WAF or need to duplicate the deployment in the same environment, please keep the same WAF version as currently used or update the version of all deployments to the latest. To check the launched version, connect to the running instance and execute the following command: apt list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update all Wallarm WAF instances to the latest version. If the version 2.16.x or lower is installed, then please update all Wallarm WAF instances to the latest version. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Log in to the management console and select the folder where the virtual machine will be created. Select Compute Cloud from the list of services. Click the Create VM button. Select Wallarm WAF from the list of images in the Image/boot disk selection \u2192 Cloud Marketplace block. Configure other virtual machine parameters following these instructions .","title":"1. Create a virtual machine with the WAF node"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#2-connect-to-the-virtual-machine-with-the-waf-node","text":"Ensure the virtual machine is in the RUNNING status. Connect to the virtual machine via SSH following these instructions .","title":"2. Connect to the virtual machine with the WAF node"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#3-connect-the-waf-node-to-wallarm-cloud","text":"Connect the WAF node to the Wallarm Cloud using the cloud WAF node token or Wallarm Console account login and password .","title":"3. Connect the WAF node to Wallarm Cloud"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#using-the-cloud-waf-node-token","text":"Open the Wallarm Console \u2192 Nodes section and create a cloud WAF node. Open the card of the created cloud WAF node and copy the Node token . On the virtual machine, run the addcloudnode script: EU Cloud sudo /usr/share/wallarm-common/addcloudnode US Cloud sudo /usr/share/wallarm-common/addcloudnode -H us1.api.wallarm.com Paste the copied token value. The WAF node will now synchronize with the cloud every 2\u20114 minutes according to the default synchronization configuration. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192","title":"Using the cloud WAF node token"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#using-the-administrator-login-and-password","text":"On the virtual machine, run the addnode script: EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Provide your Wallarm administrator account's login and password. Enter the name of the new WAF node or press Enter to use the virtual machine name. The WAF node will now synchronize with the cloud every 2\u20114 minutes according to the default synchronization configuration. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192","title":"Using the administrator login and password"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#4-update-wallarm-waf-configuration","text":"Main configuration files of NGINX and Wallarm WAF node are located in the directories: /etc/nginx/nginx.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings The file is used for settings applied to all domains. To apply different settings to different domain groups, use the file nginx.conf or create new configuration files for each domain group (for example, example.com.conf and test.com.conf ). More detailed information about NGINX configuration files is available in the official NGINX documentation . /etc/nginx/conf.d/wallarm-status.conf with WAF node monitoring settings. A detailed description is available within this link /etc/default/wallarm-tarantool with the Tarantool database settings","title":"4. Update Wallarm WAF configuration"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#request-filtration-mode","text":"By default, the WAF node is in the status monitoring and searches attack signs in requests but does not block detected attacks. We recommend keeping the traffic flowing via the WAF node in the monitoring mode for several days after the WAF node deployment and only then enable the block mode. Learn recommendations on the WAF node operation mode setup \u2192 To change the monitoring mode to block : Open the file /etc/nginx/conf.d/wallarm.conf : sudo vim /etc/nginx/conf.d/wallarm.conf Comment out the line wallarm_mode monitoring; . Open the file /etc/nginx/nginx.conf : sudo vim /etc/nginx/nginx.conf Add the line wallarm_mode block; to the http , server or location block: Example of the file /etc/nginx/nginx.conf user www-data ; worker_processes auto ; pid /run/nginx.pid ; include /etc/nginx/modules-enabled/*.conf ; events { worker_connections 768 ; # multi_accept on; } http { wallarm_mode block ; sendfile on ; tcp_nopush on ; tcp_nodelay on ; keepalive_timeout 65 ; types_hash_max_size 2048 ; # server_tokens off; # server_names_hash_bucket_size 64; # server_name_in_redirect off; include /etc/nginx/mime.types ; default_type application/octet-stream ; ## # SSL Settings ## ssl_protocols TLSv1 TLSv1.1 TLSv1.2 ; # Dropping SSLv3, ref: POODLE ssl_prefer_server_ciphers on ; ## # Logging Settings ## access_log /var/log/nginx/access.log ; error_log /var/log/nginx/error.log ; ## # Gzip Settings ## gzip on ; # gzip_vary on; # gzip_proxied any; # gzip_comp_level 6; # gzip_buffers 16 8k; # gzip_http_version 1.1; # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; ## # Virtual Host Configs ## include /etc/nginx/conf.d/*.conf ; include /etc/nginx/sites-enabled/* ; }","title":"Request filtration mode"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#memory","text":"The WAF node uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 40% of the total server memory. To allocate memory for Tarantool: Open the Tarantool configuration file in the editing mode: sudo vim /etc/default/wallarm-tarantool Specify memory size in GB in the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 To apply changes, restart Tarantool: sudo systemctl restart wallarm-tarantool","title":"Memory"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#other-configurations","text":"To update other NGINX and Wallarm WAF configurations, use the NGINX documentation and the list of available Wallarm WAF directives .","title":"Other configurations"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#5-restart-nginx","text":"To apply changes, restart NGINX: sudo systemctl restart nginx","title":"5. Restart NGINX"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#6-test-wallarm-waf-operation","text":"Send the request with test SQLI and XSS attacks to the external IP address: curl http://84.201.148.210/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"6. Test Wallarm WAF operation"},{"location":"admin-en/installation-guides/install-in-yandex-cloud/#settings-customization","text":"The WAF node with default settings is deployed in Yandex.Cloud. To customize Wallarm WAF settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Double\u2011detection of attacks with libdetection","title":"Settings customization"},{"location":"admin-en/installation-guides/amazon-cloud/autoscaling-group-guide/","text":"Setting Up Filter Node Auto Scaling \u00b6 Required rights Before setting up auto scaling, make sure that your Amazon AWS account is granted with one of the following rights: AutoScalingFullAccess AutoScalingConsoleFullAccess To set up filter node auto scaling, proceed with the following steps: Creating a Launch Template Creating an Auto Scaling Group 1. Creating a Launch Template \u00b6 A Launch Template defines the instance type to be used during the deployment of an Amazon Machine Image (AMI) and sets up some of the general virtual machine parameters. Create a Launch Template by doing the following steps: Navigate to the Launch Templates tab on the Amazon EC2 dashboard and click the Create launch template button. Enter the template name into the Launch template name field. Select the previously created Amazon Machine Image. To do this, click the Search for AMI link and select the required image from the My AMIs catalog. Select the instance type to launch a filter node\u2019s virtual machine on from the Instance type list. Select the proper instance type Select the same instance type that you used when you initially configured the filter node or a more powerful one. Using a less powerful instance type may lead to issues in filter node operation. Select the name of the previously created pair of SSH keys to access the filter node from the Key pair name list. Select the previously created Security Group from the Security Groups list. Click the Create launch template button. Wait until the template creation process is finished. After creating the Launch Template, you can proceed with the creation of an Auto Scaling Group. 2. Creating an Auto Scaling Group \u00b6 Selecting an auto scaling method This section describes the process of creating an Auto Scaling Group using the EC2 Auto Scaling method. You can also use the AWS Auto Scaling method. To see a detailed FAQ about auto scaling methods from Amazon, proceed to this link . To create an Auto Scaling Group, do the following steps: Navigate to the Auto Scaling Groups tab on the Amazon EC2 dashboard and click the Create Auto Scaling Group button. Select the Launch Template option, then select the previously created Launch Template from the list and click the Next Step button. Enter the desired Auto Scaling Group name into the Group name field. Select the \u201cLatest\u201d version of the Launch Template from the Launch Template Version list. Select the instance type required for the Auto Scaling Group by picking one of the Fleet Composition options. If you followed this guide when creating a Launch Template and an instance type to launch virtual machines on was specified, then you can use the Adhere to the launch template option. Select the proper instance type You can also select the Combine purchase options and instances option if no instance type is specified in your Launch Template or if you want to select multiple different instance types for auto scaling. Select the same instance type that you used when you initially configured the filter node or a more powerful one. Using a less powerful instance type may lead to issues in filter node operation. Enter the initial Auto Scaling Group size into the Group size field (e.g., two instances). Select the correct VPC from the Network drop-down list. Select the correct subnets from the Subnets drop-down list. Provide the filter node with an internet connection The filter node requires access to the Wallarm API server for proper operation. The choice of the Wallarm API server depends on the Wallarm Cloud you are using: If you are using the EU cloud, your node needs to be granted access to https://api.wallarm.com:444 . If you are using the US cloud, your node needs to be granted access to https://us1.api.wallarm.com:444 . Make sure that you choose the correct VPC and subnets and configure a security group in a way that does not prevent the filter node\u2019s access to Wallarm API servers. Navigate to the Configure scaling policies page by clicking the Next: Configure scaling policies button. Select the \u201cUse scaling policies to adjust the capacity of this group\u201d option to enable auto scaling. Enter the minimum and the maximum Auto Scaling Group size. Auto Scaling Group size Note that the minimum Auto Scaling Group size can be less than the initial group size specified in the sixth step. Enable the step-by-step policies configuration mode by selecting the \u201cScale the Auto Scaling group using step or simple scaling policies\u201d option. Configure the group size increase policy using the Increase Group Size parameter group. If necessary, specify the group size increase policy name using the Name parameter. Select the event from the Execute policy when to specify the event that will trigger the increase of the group size. If you did not create any events earlier, click the Add Alarm button to create an event. You can set up an event name, a metric to monitor, and notifications about event occurrences. Roles required for configuring notifications Your Amazon AWS account needs the \u201cAutoScalingNotificationAccessRole\u201d for notifications configuration. Example You can set up triggering of an event with the name \u201cHigh CPU utilization\u201d upon reaching a 60% average processor load within five minutes: Available standard metrics of Amazon cloud CPU Utilization (in percentages) Disk Reads (in bytes) Disk Writes (in bytes) Disk Read Operations count Disk Write Operations count Network In (in bytes) Network Out (in bytes) Click the Create Alarm button to create an event. Select the action to be taken in the case the \u201cHigh CPU Utilization\u201d event triggers. For example, you may configure an auto scaling policy to add (using the Add action) one instance when the event is triggered. The event may trigger early if resource consumption leaps occur after adding a new instance. To avoid this, you can set up a warm-up period in seconds using the Instances need X seconds to warm up parameter. No events will be triggered during this period of time. Similarly, use the Decrease Group Size parameter group to configure the group size decrease policy. If necessary, configure notifications and tags for the Auto Scaling Group or proceed to the review of the changes by clicking the Review button. Make sure all of the parameters are correctly specified and then launch the Auto Scaling Group creation process by clicking the Create Auto Scaling group button. The specified number of instances will be launched automatically upon the successful Auto Scaling Group creation. You can check that the Auto Scaling Group has been created correctly by viewing the number of launched instances in the group and comparing this data with the number of filter nodes connected to the Wallarm cloud. You can do this using the Wallarm website. For example, if two instances with filter nodes are concurrently operating, the Wallarm website will display this number for the corresponding cloud node on the Nodes tab. You can now proceed with the creation and configuration of a load balancer.","title":"Autoscaling group guide"},{"location":"admin-en/installation-guides/amazon-cloud/autoscaling-group-guide/#setting-up-filter-node-auto-scaling","text":"Required rights Before setting up auto scaling, make sure that your Amazon AWS account is granted with one of the following rights: AutoScalingFullAccess AutoScalingConsoleFullAccess To set up filter node auto scaling, proceed with the following steps: Creating a Launch Template Creating an Auto Scaling Group","title":"Setting Up Filter Node Auto Scaling"},{"location":"admin-en/installation-guides/amazon-cloud/autoscaling-group-guide/#1-creating-a-launch-template","text":"A Launch Template defines the instance type to be used during the deployment of an Amazon Machine Image (AMI) and sets up some of the general virtual machine parameters. Create a Launch Template by doing the following steps: Navigate to the Launch Templates tab on the Amazon EC2 dashboard and click the Create launch template button. Enter the template name into the Launch template name field. Select the previously created Amazon Machine Image. To do this, click the Search for AMI link and select the required image from the My AMIs catalog. Select the instance type to launch a filter node\u2019s virtual machine on from the Instance type list. Select the proper instance type Select the same instance type that you used when you initially configured the filter node or a more powerful one. Using a less powerful instance type may lead to issues in filter node operation. Select the name of the previously created pair of SSH keys to access the filter node from the Key pair name list. Select the previously created Security Group from the Security Groups list. Click the Create launch template button. Wait until the template creation process is finished. After creating the Launch Template, you can proceed with the creation of an Auto Scaling Group.","title":"1.  Creating a Launch Template"},{"location":"admin-en/installation-guides/amazon-cloud/autoscaling-group-guide/#2-creating-an-auto-scaling-group","text":"Selecting an auto scaling method This section describes the process of creating an Auto Scaling Group using the EC2 Auto Scaling method. You can also use the AWS Auto Scaling method. To see a detailed FAQ about auto scaling methods from Amazon, proceed to this link . To create an Auto Scaling Group, do the following steps: Navigate to the Auto Scaling Groups tab on the Amazon EC2 dashboard and click the Create Auto Scaling Group button. Select the Launch Template option, then select the previously created Launch Template from the list and click the Next Step button. Enter the desired Auto Scaling Group name into the Group name field. Select the \u201cLatest\u201d version of the Launch Template from the Launch Template Version list. Select the instance type required for the Auto Scaling Group by picking one of the Fleet Composition options. If you followed this guide when creating a Launch Template and an instance type to launch virtual machines on was specified, then you can use the Adhere to the launch template option. Select the proper instance type You can also select the Combine purchase options and instances option if no instance type is specified in your Launch Template or if you want to select multiple different instance types for auto scaling. Select the same instance type that you used when you initially configured the filter node or a more powerful one. Using a less powerful instance type may lead to issues in filter node operation. Enter the initial Auto Scaling Group size into the Group size field (e.g., two instances). Select the correct VPC from the Network drop-down list. Select the correct subnets from the Subnets drop-down list. Provide the filter node with an internet connection The filter node requires access to the Wallarm API server for proper operation. The choice of the Wallarm API server depends on the Wallarm Cloud you are using: If you are using the EU cloud, your node needs to be granted access to https://api.wallarm.com:444 . If you are using the US cloud, your node needs to be granted access to https://us1.api.wallarm.com:444 . Make sure that you choose the correct VPC and subnets and configure a security group in a way that does not prevent the filter node\u2019s access to Wallarm API servers. Navigate to the Configure scaling policies page by clicking the Next: Configure scaling policies button. Select the \u201cUse scaling policies to adjust the capacity of this group\u201d option to enable auto scaling. Enter the minimum and the maximum Auto Scaling Group size. Auto Scaling Group size Note that the minimum Auto Scaling Group size can be less than the initial group size specified in the sixth step. Enable the step-by-step policies configuration mode by selecting the \u201cScale the Auto Scaling group using step or simple scaling policies\u201d option. Configure the group size increase policy using the Increase Group Size parameter group. If necessary, specify the group size increase policy name using the Name parameter. Select the event from the Execute policy when to specify the event that will trigger the increase of the group size. If you did not create any events earlier, click the Add Alarm button to create an event. You can set up an event name, a metric to monitor, and notifications about event occurrences. Roles required for configuring notifications Your Amazon AWS account needs the \u201cAutoScalingNotificationAccessRole\u201d for notifications configuration. Example You can set up triggering of an event with the name \u201cHigh CPU utilization\u201d upon reaching a 60% average processor load within five minutes: Available standard metrics of Amazon cloud CPU Utilization (in percentages) Disk Reads (in bytes) Disk Writes (in bytes) Disk Read Operations count Disk Write Operations count Network In (in bytes) Network Out (in bytes) Click the Create Alarm button to create an event. Select the action to be taken in the case the \u201cHigh CPU Utilization\u201d event triggers. For example, you may configure an auto scaling policy to add (using the Add action) one instance when the event is triggered. The event may trigger early if resource consumption leaps occur after adding a new instance. To avoid this, you can set up a warm-up period in seconds using the Instances need X seconds to warm up parameter. No events will be triggered during this period of time. Similarly, use the Decrease Group Size parameter group to configure the group size decrease policy. If necessary, configure notifications and tags for the Auto Scaling Group or proceed to the review of the changes by clicking the Review button. Make sure all of the parameters are correctly specified and then launch the Auto Scaling Group creation process by clicking the Create Auto Scaling group button. The specified number of instances will be launched automatically upon the successful Auto Scaling Group creation. You can check that the Auto Scaling Group has been created correctly by viewing the number of launched instances in the group and comparing this data with the number of filter nodes connected to the Wallarm cloud. You can do this using the Wallarm website. For example, if two instances with filter nodes are concurrently operating, the Wallarm website will display this number for the corresponding cloud node on the Nodes tab. You can now proceed with the creation and configuration of a load balancer.","title":"2.  Creating an Auto Scaling Group"},{"location":"admin-en/installation-guides/amazon-cloud/autoscaling-overview/","text":"Overview of the WAF node Auto Scaling Configuration on AWS \u00b6 You can set up Wallarm filter node auto scaling to make sure that filter nodes are capable of handling traffic fluctuations, if there are any. Enabling auto scaling allows processing the incoming requests to the application using the filter nodes even when traffic soars significantly. The Amazon cloud supports the following auto scaling methods: AWS Autoscaling: The new auto scaling technology on the basis of the metrics that are collected by AWS. To see detailed information about AWS Auto Scaling, proceed to this link . EC2 Autoscaling: The legacy auto scaling technology that allows creating custom variables for defining the scaling rules. To see detailed information about EC2 Auto Scaling, proceed to this link . Information about auto scaling methods To see a detailed FAQ about auto scaling methods provided by Amazon, proceed to this link . This guide explains how to configure auto scaling of the filter nodes using EC2 Auto Scaling, but you can also use AWS Auto Scaling if needed. Prerequisites A virtual machine image (Amazon Machine Image, AMI) with the Wallarm filter node is required for setting up auto scaling. To see detailed information about creating an AMI with the filter node, proceed with this link . Private SSH key Make sure you have access to the private SSH key (stored in PEM format) that you created earlier to connect to the filter node. To enable filter node auto scaling in the Amazon cloud, do the following steps: Set up filter node auto scaling Create a Launch Template Create an Auto Scaling Group Set up incoming requests balancing Create a load balancer Set up an Auto Scaling Group for using the created balancer","title":"Configuring autocaling on AWS"},{"location":"admin-en/installation-guides/amazon-cloud/autoscaling-overview/#overview-of-the-waf-node-auto-scaling-configuration-on-aws","text":"You can set up Wallarm filter node auto scaling to make sure that filter nodes are capable of handling traffic fluctuations, if there are any. Enabling auto scaling allows processing the incoming requests to the application using the filter nodes even when traffic soars significantly. The Amazon cloud supports the following auto scaling methods: AWS Autoscaling: The new auto scaling technology on the basis of the metrics that are collected by AWS. To see detailed information about AWS Auto Scaling, proceed to this link . EC2 Autoscaling: The legacy auto scaling technology that allows creating custom variables for defining the scaling rules. To see detailed information about EC2 Auto Scaling, proceed to this link . Information about auto scaling methods To see a detailed FAQ about auto scaling methods provided by Amazon, proceed to this link . This guide explains how to configure auto scaling of the filter nodes using EC2 Auto Scaling, but you can also use AWS Auto Scaling if needed. Prerequisites A virtual machine image (Amazon Machine Image, AMI) with the Wallarm filter node is required for setting up auto scaling. To see detailed information about creating an AMI with the filter node, proceed with this link . Private SSH key Make sure you have access to the private SSH key (stored in PEM format) that you created earlier to connect to the filter node. To enable filter node auto scaling in the Amazon cloud, do the following steps: Set up filter node auto scaling Create a Launch Template Create an Auto Scaling Group Set up incoming requests balancing Create a load balancer Set up an Auto Scaling Group for using the created balancer","title":"Overview of the WAF node Auto Scaling Configuration on AWS"},{"location":"admin-en/installation-guides/amazon-cloud/create-image/","text":"Creating an AMI with the Wallarm Filter Node \u00b6 You can set up auto scaling for the Wallarm filter nodes deployed on the Amazon cloud. This function requires preliminarily prepared virtual machine images. This document describes the procedure of preparing an Amazon Machine Image (AMI) with the Wallarm filter node installed. AMI is required for the filter node auto scaling setup. To see detailed information about setting up auto scaling, proceed to this link . To create an AMI with the Wallarm filter node, perform the following procedures: Creating and configuring the filter node instance in the Amazon cloud ; Creating an AMI on the basis of the configured filter node instance . 1. Creating and Configuring the Wallarm Filter Node Instance in the Amazon Cloud \u00b6 Before creating an AMI you need to perform an initial configuration of a single Wallarm filter node. To configure a filter node, do the following: Create a filter node instance in the Amazon cloud. Private SSH key Make sure you have access to the private SSH key (stored in PEM format) that you created earlier to connect to the filter node. Provide the filter node with an internet connection The filter node requires access to the Wallarm API server for proper operation. The choice of the Wallarm API server depends on the Wallarm Cloud you are using: If you are using the EU cloud, your node needs to be granted access to https://api.wallarm.com:444 . If you are using the US cloud, your node needs to be granted access to https://us1.api.wallarm.com:444 . Make sure that you choose the correct VPC and subnets and configure a security group in a way that does not prevent the filter node from accessing Wallarm API servers. Connect the filter node to the Wallarm cloud. Use a token to connect to the Wallarm cloud Please note that you need to connect the filter node to the Wallarm cloud using a token. Multiple filter nodes are allowed to connect to the Wallarm cloud using the same token. Thus, upon filter nodes\u2019 auto scaling, you will not need to manually connect each of the filter nodes to the Wallarm cloud. Configure the filter node to act as a reverse proxy for your web application. Make sure that the filter node is configured correctly and protects your web application against malicious requests. After you have finished configuring the filter node, turn the virtual machine off by completing the following actions: Navigate to the Instances tab on the Amazon EC2 dashboard. Select your configured filter node instance. Select \u201cInstance State\u201d and then \u201cStop\u201d in the Actions drop-down menu. Turning off with the poweroff command You may also turn the virtual machine off by connecting to it via the SSH protocol and running the following command: poweroff 2. Creating an Amazon Machine Image \u00b6 You can now create a virtual machine image based on the configured filter node instance. To create an image, perform the following steps: Proceed to the Instances tab on the Amazon EC2 dashboard. Select your configured filter node instance. Launch the image creation wizard by selecting \u201cImage\u201d and then \u201cCreate Image\u201d in the Actions drop-down menu. The Create Image form will appear. Enter the image name into the Image name field. You can leave the remaining fields unaltered. Click the Create Image button to launch the virtual machine image creation process. When the image creation process is finished, the corresponding message is displayed. Navigate to the \u201cAMIs\u201d tab on the Amazon EC2 dashboard to make sure that the image was successfully created and has the \u201cAvailable\u201d status. Image visibility Because the prepared image contains settings that are specific to your application and the Wallarm token, it is not recommended to change the image visibility setting and make it public (by default, AMIs are created with the \u201cPrivate\u201d visibility setting). Now you can set up the auto scaling of Wallarm filter nodes in the Amazon cloud using the prepared image.","title":"Create image"},{"location":"admin-en/installation-guides/amazon-cloud/create-image/#creating-an-ami-with-the-wallarm-filter-node","text":"You can set up auto scaling for the Wallarm filter nodes deployed on the Amazon cloud. This function requires preliminarily prepared virtual machine images. This document describes the procedure of preparing an Amazon Machine Image (AMI) with the Wallarm filter node installed. AMI is required for the filter node auto scaling setup. To see detailed information about setting up auto scaling, proceed to this link . To create an AMI with the Wallarm filter node, perform the following procedures: Creating and configuring the filter node instance in the Amazon cloud ; Creating an AMI on the basis of the configured filter node instance .","title":"Creating an AMI with the Wallarm Filter Node"},{"location":"admin-en/installation-guides/amazon-cloud/create-image/#1-creating-and-configuring-the-wallarm-filter-node-instance-in-the-amazon-cloud","text":"Before creating an AMI you need to perform an initial configuration of a single Wallarm filter node. To configure a filter node, do the following: Create a filter node instance in the Amazon cloud. Private SSH key Make sure you have access to the private SSH key (stored in PEM format) that you created earlier to connect to the filter node. Provide the filter node with an internet connection The filter node requires access to the Wallarm API server for proper operation. The choice of the Wallarm API server depends on the Wallarm Cloud you are using: If you are using the EU cloud, your node needs to be granted access to https://api.wallarm.com:444 . If you are using the US cloud, your node needs to be granted access to https://us1.api.wallarm.com:444 . Make sure that you choose the correct VPC and subnets and configure a security group in a way that does not prevent the filter node from accessing Wallarm API servers. Connect the filter node to the Wallarm cloud. Use a token to connect to the Wallarm cloud Please note that you need to connect the filter node to the Wallarm cloud using a token. Multiple filter nodes are allowed to connect to the Wallarm cloud using the same token. Thus, upon filter nodes\u2019 auto scaling, you will not need to manually connect each of the filter nodes to the Wallarm cloud. Configure the filter node to act as a reverse proxy for your web application. Make sure that the filter node is configured correctly and protects your web application against malicious requests. After you have finished configuring the filter node, turn the virtual machine off by completing the following actions: Navigate to the Instances tab on the Amazon EC2 dashboard. Select your configured filter node instance. Select \u201cInstance State\u201d and then \u201cStop\u201d in the Actions drop-down menu. Turning off with the poweroff command You may also turn the virtual machine off by connecting to it via the SSH protocol and running the following command: poweroff","title":"1.  Creating and Configuring the Wallarm Filter Node Instance in the Amazon Cloud"},{"location":"admin-en/installation-guides/amazon-cloud/create-image/#2-creating-an-amazon-machine-image","text":"You can now create a virtual machine image based on the configured filter node instance. To create an image, perform the following steps: Proceed to the Instances tab on the Amazon EC2 dashboard. Select your configured filter node instance. Launch the image creation wizard by selecting \u201cImage\u201d and then \u201cCreate Image\u201d in the Actions drop-down menu. The Create Image form will appear. Enter the image name into the Image name field. You can leave the remaining fields unaltered. Click the Create Image button to launch the virtual machine image creation process. When the image creation process is finished, the corresponding message is displayed. Navigate to the \u201cAMIs\u201d tab on the Amazon EC2 dashboard to make sure that the image was successfully created and has the \u201cAvailable\u201d status. Image visibility Because the prepared image contains settings that are specific to your application and the Wallarm token, it is not recommended to change the image visibility setting and make it public (by default, AMIs are created with the \u201cPrivate\u201d visibility setting). Now you can set up the auto scaling of Wallarm filter nodes in the Amazon cloud using the prepared image.","title":"2.  Creating an Amazon Machine Image"},{"location":"admin-en/installation-guides/amazon-cloud/load-balancing-guide/","text":"Creating a Load Balancer on AWS \u00b6 Now, once you have a configured filter node Auto Scaling Group, you need to create and configure a Load Balancer that distributes incoming HTTP and HTTPS connections among several filter nodes from the Auto Scaling Group. Load Balancer creation process includes the following steps: Creating a Load Balancer Setting Up an Auto Scaling Group for Using the Created Balancer 1. Creating a Load Balancer \u00b6 You can configure the following types of Load Balancers in the Amazon cloud: Classic Load Balancer Network Load Balancer Application Load Balancer Load Balancers differences To see detailed information about the differences between the Load Balancers, proceed to this link . This document demonstrates configuring and using the Network Load Balancer that distributes traffic at the transport level of the OSI/ISO network model. Create a Load Balancer by completing the following actions: Navigate to the Load Balancers tab on the Amazon EC2 dashboard and click the Create Load Balancer button. Create a Network Load Balancer by clicking the corresponding Create button. Configure the basic Load Balancer parameters: The name of the balancer (the Name parameter). The type of balancer (the \u201cScheme\u201d parameter). Select the \u201cinternet-facing\u201d type for the balancer to be available on the internet. Specify ports for the balancer to listen to using the Listeners parameter group. Specify the required VPC and Availability Zones in which the balancer should be working. Check the Auto Scaling Group availability Make sure you selected the VPC and Availability Zones that contain the previously created Auto Scaling Group for the load balancer to operate properly. Proceed to the next step by clicking the Next: Configure Security Settings button. Configure the security parameters if necessary. Continue to the next step by clicking the Next: Configure Routing button. Configure the routing of the incoming requests to the filter nodes in the Auto Scaling Group. Create a new target group and specify its name in the Name field. The Load Balancer will route incoming requests to the instances located in the specified target group (e.g., \u201cdemo-target\u201d). Configure the protocol and port to be used for request routing. Specify the TCP protocol and the 80 and 443 (if you have HTTPS traffic) ports for the filter node. If necessary, configure the availability checks using the Health Checks parameter group. Proceed to the next step by clicking the Next: Register Targets button. This step requires no actions. Switch to the next step by clicking the Next: Review button. Make sure that all of the parameters are specified correctly, and launch the Load Balancer creation process by clicking the Create button. Wait until the Load Balancer is initialized After the Load Balancer is created, some time must pass for it to be ready to receive traffic. 2. Setting Up an Auto Scaling Group for Using the Created Balancer \u00b6 Configure your Auto Scaling Group for using the Load Balancer you created earlier. This will allow the balancer to route traffic to the filter node instances that are launched in the group. To do this, complete the following actions: Navigate to the Auto Scaling Groups tab on the Amazon EC2 dashboard and select the Auto Scaling Group created earlier . Open the group configuration editing dialog by selecting \u201cEdit\u201d in the Actions dropdown menu. Select the \u201cdemo-target\u201d target group created when setting up the Load Balancer in the Target groups drop-down list. Apply the changes by clicking the Save button. Now the dynamically scaling set of the Wallarm filter nodes will process the incoming traffic to your application. To check the deployed filter nodes\u2019 operation, perform the following steps: Make sure that your application is accessible through the Load Balancer and the Wallarm filter nodes by referring to the balancer IP address or domain name using the browser. Make sure that the Wallarm services protect your application by performing a test attack .","title":"Load balancing guide"},{"location":"admin-en/installation-guides/amazon-cloud/load-balancing-guide/#creating-a-load-balancer-on-aws","text":"Now, once you have a configured filter node Auto Scaling Group, you need to create and configure a Load Balancer that distributes incoming HTTP and HTTPS connections among several filter nodes from the Auto Scaling Group. Load Balancer creation process includes the following steps: Creating a Load Balancer Setting Up an Auto Scaling Group for Using the Created Balancer","title":"Creating a Load Balancer on AWS"},{"location":"admin-en/installation-guides/amazon-cloud/load-balancing-guide/#1-creating-a-load-balancer","text":"You can configure the following types of Load Balancers in the Amazon cloud: Classic Load Balancer Network Load Balancer Application Load Balancer Load Balancers differences To see detailed information about the differences between the Load Balancers, proceed to this link . This document demonstrates configuring and using the Network Load Balancer that distributes traffic at the transport level of the OSI/ISO network model. Create a Load Balancer by completing the following actions: Navigate to the Load Balancers tab on the Amazon EC2 dashboard and click the Create Load Balancer button. Create a Network Load Balancer by clicking the corresponding Create button. Configure the basic Load Balancer parameters: The name of the balancer (the Name parameter). The type of balancer (the \u201cScheme\u201d parameter). Select the \u201cinternet-facing\u201d type for the balancer to be available on the internet. Specify ports for the balancer to listen to using the Listeners parameter group. Specify the required VPC and Availability Zones in which the balancer should be working. Check the Auto Scaling Group availability Make sure you selected the VPC and Availability Zones that contain the previously created Auto Scaling Group for the load balancer to operate properly. Proceed to the next step by clicking the Next: Configure Security Settings button. Configure the security parameters if necessary. Continue to the next step by clicking the Next: Configure Routing button. Configure the routing of the incoming requests to the filter nodes in the Auto Scaling Group. Create a new target group and specify its name in the Name field. The Load Balancer will route incoming requests to the instances located in the specified target group (e.g., \u201cdemo-target\u201d). Configure the protocol and port to be used for request routing. Specify the TCP protocol and the 80 and 443 (if you have HTTPS traffic) ports for the filter node. If necessary, configure the availability checks using the Health Checks parameter group. Proceed to the next step by clicking the Next: Register Targets button. This step requires no actions. Switch to the next step by clicking the Next: Review button. Make sure that all of the parameters are specified correctly, and launch the Load Balancer creation process by clicking the Create button. Wait until the Load Balancer is initialized After the Load Balancer is created, some time must pass for it to be ready to receive traffic.","title":"1.  Creating a Load Balancer"},{"location":"admin-en/installation-guides/amazon-cloud/load-balancing-guide/#2-setting-up-an-auto-scaling-group-for-using-the-created-balancer","text":"Configure your Auto Scaling Group for using the Load Balancer you created earlier. This will allow the balancer to route traffic to the filter node instances that are launched in the group. To do this, complete the following actions: Navigate to the Auto Scaling Groups tab on the Amazon EC2 dashboard and select the Auto Scaling Group created earlier . Open the group configuration editing dialog by selecting \u201cEdit\u201d in the Actions dropdown menu. Select the \u201cdemo-target\u201d target group created when setting up the Load Balancer in the Target groups drop-down list. Apply the changes by clicking the Save button. Now the dynamically scaling set of the Wallarm filter nodes will process the incoming traffic to your application. To check the deployed filter nodes\u2019 operation, perform the following steps: Make sure that your application is accessible through the Load Balancer and the Wallarm filter nodes by referring to the balancer IP address or domain name using the browser. Make sure that the Wallarm services protect your application by performing a test attack .","title":"2.  Setting Up an Auto Scaling Group for Using the Created Balancer"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-code-desc/","text":"Description of Example Terraform Code \u00b6 The Terraform code should be pretty self-explanatory. Only parts performing the deployment and autoscaling of Wallarm WAF nodes are provided below. Configuration of Wallarm WAF Node Deployment \u00b6 Deployment settings are performed in the wallarm_launch_config object of the main.tf file. In the current example the code performs the following operations on a freshly started WAF node: Create local files /etc/nginx/key.pem and /etc/nginx/cert.pem holding a self-signed SSL certificate and its private key. In production use, the files should be replaced with real SSL certificate and private key data. Create local file /etc/nginx/sites-available/default with the configuration of web resources to be protected. In this example the system will define for the following properties: HTTP and HTTPS server configuration blocks as the default proxy servers for all incoming requests, a health check endpoint defined as /healthcheck and it always returns an HTTP status code 200, proxy all incoming HTTP and HTTPS requests to the DNS name of the Wordpress ELB instance as defined in Terraform variable ${aws_elb.wp_elb.dns_name} . Run a set of commands to configure the WAF node (described in the runcmd block): add the new node to the Wallarm cloud, test the configuration and start a local NGINX instance. Configuration of Wallarm WAF Node Autoscaling \u00b6 Autoscaling settings are performed in the following object of the main.tf file: resource \"aws_autoscaling_group\" \"wallarm_waf_asg\" { lifecycle { create_before_destroy = true } name = \"tf-wallarm-demo-waf-asg-${aws_launch_configuration.wallarm_launch_config.name}\" launch_configuration = \"${aws_launch_configuration.wallarm_launch_config.name}\" min_size = \"2\" max_size = \"5\" min_elb_capacity = \"2\" availability_zones = [var.az_a] vpc_zone_identifier = [\"${aws_subnet.public_a.id}\"] target_group_arns = [ \"${aws_lb_target_group.wallarm_asg_target_http.arn}\", \"${aws_lb_target_group.wallarm_asg_target_https.arn}\" ] enabled_metrics = [ \"GroupMinSize\", \"GroupMaxSize\", \"GroupDesiredCapacity\", \"GroupInServiceInstances\", \"GroupTotalInstances\" ] metrics_granularity = \"1Minute\" tag { key = \"Name\" value = \"tf-wallarm-demo-waf-node\" propagate_at_launch = true } } Enabled CloudWatch metrics ( enabled_metrics statement) are required for automatic scaling of the ASG size depending on CPU load. You can omit this part if you plan to use a fixed amount of WAF nodes in the ASG. Autoscaling policies wallarm_policy_up and wallarm_policy_down (and associated CloudWatch metric alarms) are defining CPU usage thresholds and periods which will trigger ASG up or down scaling activities. Configuration statement lifecycle { create_before_destroy = true } defines that should there be a need to introduce a change in the WAF cluster's configuration Terraform will first create a new set of Launch Configuration and ASG objects, verify that new WAF nodes have been recognized by the associated load balancer as healthy (statement min_elb_capacity ), and only after that remove old Launch Configuration and ASG resources. The approach guarantees that new WAF configuration changes can be rollout out with no interruption to the traffic flow.","title":"Description of Example Terraform Code"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-code-desc/#description-of-example-terraform-code","text":"The Terraform code should be pretty self-explanatory. Only parts performing the deployment and autoscaling of Wallarm WAF nodes are provided below.","title":"Description of Example Terraform Code"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-code-desc/#configuration-of-wallarm-waf-node-deployment","text":"Deployment settings are performed in the wallarm_launch_config object of the main.tf file. In the current example the code performs the following operations on a freshly started WAF node: Create local files /etc/nginx/key.pem and /etc/nginx/cert.pem holding a self-signed SSL certificate and its private key. In production use, the files should be replaced with real SSL certificate and private key data. Create local file /etc/nginx/sites-available/default with the configuration of web resources to be protected. In this example the system will define for the following properties: HTTP and HTTPS server configuration blocks as the default proxy servers for all incoming requests, a health check endpoint defined as /healthcheck and it always returns an HTTP status code 200, proxy all incoming HTTP and HTTPS requests to the DNS name of the Wordpress ELB instance as defined in Terraform variable ${aws_elb.wp_elb.dns_name} . Run a set of commands to configure the WAF node (described in the runcmd block): add the new node to the Wallarm cloud, test the configuration and start a local NGINX instance.","title":"Configuration of Wallarm WAF Node Deployment"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-code-desc/#configuration-of-wallarm-waf-node-autoscaling","text":"Autoscaling settings are performed in the following object of the main.tf file: resource \"aws_autoscaling_group\" \"wallarm_waf_asg\" { lifecycle { create_before_destroy = true } name = \"tf-wallarm-demo-waf-asg-${aws_launch_configuration.wallarm_launch_config.name}\" launch_configuration = \"${aws_launch_configuration.wallarm_launch_config.name}\" min_size = \"2\" max_size = \"5\" min_elb_capacity = \"2\" availability_zones = [var.az_a] vpc_zone_identifier = [\"${aws_subnet.public_a.id}\"] target_group_arns = [ \"${aws_lb_target_group.wallarm_asg_target_http.arn}\", \"${aws_lb_target_group.wallarm_asg_target_https.arn}\" ] enabled_metrics = [ \"GroupMinSize\", \"GroupMaxSize\", \"GroupDesiredCapacity\", \"GroupInServiceInstances\", \"GroupTotalInstances\" ] metrics_granularity = \"1Minute\" tag { key = \"Name\" value = \"tf-wallarm-demo-waf-node\" propagate_at_launch = true } } Enabled CloudWatch metrics ( enabled_metrics statement) are required for automatic scaling of the ASG size depending on CPU load. You can omit this part if you plan to use a fixed amount of WAF nodes in the ASG. Autoscaling policies wallarm_policy_up and wallarm_policy_down (and associated CloudWatch metric alarms) are defining CPU usage thresholds and periods which will trigger ASG up or down scaling activities. Configuration statement lifecycle { create_before_destroy = true } defines that should there be a need to introduce a change in the WAF cluster's configuration Terraform will first create a new set of Launch Configuration and ASG objects, verify that new WAF nodes have been recognized by the associated load balancer as healthy (statement min_elb_capacity ), and only after that remove old Launch Configuration and ASG resources. The approach guarantees that new WAF configuration changes can be rollout out with no interruption to the traffic flow.","title":"Configuration of Wallarm WAF Node Autoscaling"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-intro/","text":"Deploying WAF Node Using Terraform \u00b6 Terraform is a tool to describe an infrastructure via configuration files and run a single application or your entire datacenter on the basis of these configuration files. To get more information, please use Terraform official documentation . Wallarm WAF node can be also deployed via Terraform. These instructions provide an example of Terraform code which can be easily used to deploy a cluster of Wallarm WAF node in AWS public cloud. The example code deploys a complete set of required AWS resources like VPC, subnets, routing tables, Security Groups, SSH public keys, etc, including a simple Wordpress application to be protected by the Wallarm WAF node. Recommended reading Managing auto scaling groups and load balancers to better understand provided Terraform code Cloud config examples to better understand provided Terraform code Used Resources \u00b6 The example uses the following resources: An official Wallarm WAF Node AMI available on the AWS Marketplace AWS Autoscaling Group (ASG) feature for automatic scaling of cluster size up or down depending on CPU load of active WAF nodes AWS CloudWatch metrics and alerts to monitor the CPU usage of active WAF nodes AWS NLB load balancer instance to monitor the availability of registered WAF nodes and distribute incoming web requests among healthy instances While the document is based on the Official Wallarm Node AMI the same approach can be used for a custom Wallarm WAF node AMI built by you. Deployed Network Architecture \u00b6 The provided Terraform code deploys the following network architecture: A new VPC called tf-wallarm-demo and associated resources like Public Subnets, Routing Table, Internet Gateway. An ASG managing a cluster of Wallarm WAF nodes. The ASG will use AWS's User Data feature to automatically configure new WAF nodes. The implemented provisioning process is mostly following the manual configuration process described on this page . An NLB instance facing the Internet and accepting incoming requests to ports 80/TCP and 443/TCP; the requests are passed to the Wallarm WAF nodes. The NLB instance will not terminate SSL sessions (assuming that the WAF nodes will provide the functionality). An ASG running a sample Wordpress application, and an associated ELB instance. The WAF nodes are automatically configured to proxy incoming requests to the DNS name of the Wordpress ELB instance.","title":"Introduction to using Terraform for the node deployment"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-intro/#deploying-waf-node-using-terraform","text":"Terraform is a tool to describe an infrastructure via configuration files and run a single application or your entire datacenter on the basis of these configuration files. To get more information, please use Terraform official documentation . Wallarm WAF node can be also deployed via Terraform. These instructions provide an example of Terraform code which can be easily used to deploy a cluster of Wallarm WAF node in AWS public cloud. The example code deploys a complete set of required AWS resources like VPC, subnets, routing tables, Security Groups, SSH public keys, etc, including a simple Wordpress application to be protected by the Wallarm WAF node. Recommended reading Managing auto scaling groups and load balancers to better understand provided Terraform code Cloud config examples to better understand provided Terraform code","title":"Deploying WAF Node Using Terraform"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-intro/#used-resources","text":"The example uses the following resources: An official Wallarm WAF Node AMI available on the AWS Marketplace AWS Autoscaling Group (ASG) feature for automatic scaling of cluster size up or down depending on CPU load of active WAF nodes AWS CloudWatch metrics and alerts to monitor the CPU usage of active WAF nodes AWS NLB load balancer instance to monitor the availability of registered WAF nodes and distribute incoming web requests among healthy instances While the document is based on the Official Wallarm Node AMI the same approach can be used for a custom Wallarm WAF node AMI built by you.","title":"Used Resources"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-intro/#deployed-network-architecture","text":"The provided Terraform code deploys the following network architecture: A new VPC called tf-wallarm-demo and associated resources like Public Subnets, Routing Table, Internet Gateway. An ASG managing a cluster of Wallarm WAF nodes. The ASG will use AWS's User Data feature to automatically configure new WAF nodes. The implemented provisioning process is mostly following the manual configuration process described on this page . An NLB instance facing the Internet and accepting incoming requests to ports 80/TCP and 443/TCP; the requests are passed to the Wallarm WAF nodes. The NLB instance will not terminate SSL sessions (assuming that the WAF nodes will provide the functionality). An ASG running a sample Wordpress application, and an associated ELB instance. The WAF nodes are automatically configured to proxy incoming requests to the DNS name of the Wordpress ELB instance.","title":"Deployed Network Architecture"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-quick-start/","text":"Quick Start with Provided Example \u00b6 Prerequisites \u00b6 Wallarm account in the EU cloud or US cloud Username and password of the user with the Deploy role added to your Wallarm account. To add a new user, please follow the instructions AWS account and user with the admin permissions Accepted Terms for the WordPress Certified by Bitnami and Automattic and Wallarm Node (AI\u2011based NG-WAF instance) by Wallarm products on AWS Marketplace Installed terraform CLI tools version 0.12.18 or later Installed jq CLI tools Installed git CLI tools Installed aws CLI tools Quick Start \u00b6 Download Terraform code example. Prepare Terraform environment and variables. Deploy described stack. Test WAF node operation. Step 1: Downloading Terraform Code Example \u00b6 Terraform code used in this example can be cloned from the GitHub repository using the following command: git clone https://github.com/wallarm/terraform-example.git Configuration files are located in the terraform folder of the repository: variables.tf is used to define necessary Terraform variables which describe the managed environment main.tf holds the Terraform code which performed the actual AWS provisioning Step 2: Preparing Terraform Environment and Variables \u00b6 Set environment variables with credentials for the Wallarm user with the Deploy role: export TF_VAR_deploy_username='DEPLOY_USERNAME' export TF_VAR_deploy_password='DEPLOY_PASSWORD' DEPLOY_USERNAME is the username of the user with the Deploy role DEPLOY_PASSWORD is the password of the user with the Deploy role Set environment variables with your AWS access keys : export AWS_ACCESS_KEY_ID='YOUR_ACCESS_KEY_ID' export AWS_SECRET_ACCESS_KEY='YOUR_SECRET_ACCESS_KEY' YOUR_ACCESS_KEY_ID is your access key ID YOUR_SECRET_ACCESS_KEY is your secret access key (Optional) Specify your public SSH key in the key_pair variable in the variables.tf file, if you plan to access the employed EC2 instances using SSH. (Optional) Specify the api.wallarm.com API endpoint in the wallarm_api_domain variable in the variables.tf file, if you use the EU cloud . If you use the US cloud , please leave an existing value. (Optional) Set AWS region data in the variables listed below in the variables.tf file. The provided example is configured for AWS region us-west-1 (North California). aws_region (you can find the list of AWS regions here ) az_a az_b wallarm_node_ami_id with the used AWS EC2 Wallarm WAF node image ID got by the command below. Please replace REGION_CODE by aws-region value: aws ec2 describe-images --filters \"Name=name,Values=*Wallarm Node-3.0*\" --region REGION_CODE | jq -r '.Images[] | \"\\(.ImageId)\"' wordpress_ami_id with the used AWS EC2 Wordpress image ID got by the command below. Please replace REGION_CODE by aws-region value: aws ec2 describe-images --filters \"Name=name,Values=*bitnami-wordpress-5.3.2-3-linux-ubuntu-16.04*\" --region REGION_CODE | jq -r '.Images[] | \"\\(.ImageId)\"' Step 3: Deploying Described Stack \u00b6 Go to the terraform folder of the cloned repository: cd terraform-example/terraform Deploy the whole stack using the following commands: terraform init terraform plan terraform apply After a successful run, Terraform will print out a DNS name of the deployed NLB instance. For example: Apply complete! Resources: 4 added, 2 changed, 4 destroyed. Outputs: waf_nlb_dns_name = [ \"tf-wallarm-demo-asg-nlb-7b32738728e6ea44.elb.us-east-1.amazonaws.com\", ] The DNS name can be used to access the freshly installed Wordpress service with Wallarm WAF cluster deployed in front of it. Step 4: Testing WAF Node Operation \u00b6 The WAF cluster is configured with a self-signed SSL certificate so it should be possible to access the same DNS name using HTTPS protocol but the browser will show a security warning. You can simulate a web attack by adding /?id='or+1=1--a-<script>prompt(1)</script>' to the web request - the request should be blocked by the WAF with response code 403: A few minutes after simulating a web attack it should be possible to see two blocked attacks - SQLI and XSS - in your Wallarm account > Events : WAF node deployment settings are performed in the wallarm_launch_config object of the main.tf file. To change settings to your own, please use directive description available by the link . Info To remove the demonstration environment, please use the terraform destroy command.","title":"Quick Start with Provided Example"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-quick-start/#quick-start-with-provided-example","text":"","title":"Quick Start with Provided Example"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-quick-start/#prerequisites","text":"Wallarm account in the EU cloud or US cloud Username and password of the user with the Deploy role added to your Wallarm account. To add a new user, please follow the instructions AWS account and user with the admin permissions Accepted Terms for the WordPress Certified by Bitnami and Automattic and Wallarm Node (AI\u2011based NG-WAF instance) by Wallarm products on AWS Marketplace Installed terraform CLI tools version 0.12.18 or later Installed jq CLI tools Installed git CLI tools Installed aws CLI tools","title":"Prerequisites"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-quick-start/#quick-start","text":"Download Terraform code example. Prepare Terraform environment and variables. Deploy described stack. Test WAF node operation.","title":"Quick Start"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-quick-start/#step-1-downloading-terraform-code-example","text":"Terraform code used in this example can be cloned from the GitHub repository using the following command: git clone https://github.com/wallarm/terraform-example.git Configuration files are located in the terraform folder of the repository: variables.tf is used to define necessary Terraform variables which describe the managed environment main.tf holds the Terraform code which performed the actual AWS provisioning","title":"Step 1: Downloading Terraform Code Example"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-quick-start/#step-2-preparing-terraform-environment-and-variables","text":"Set environment variables with credentials for the Wallarm user with the Deploy role: export TF_VAR_deploy_username='DEPLOY_USERNAME' export TF_VAR_deploy_password='DEPLOY_PASSWORD' DEPLOY_USERNAME is the username of the user with the Deploy role DEPLOY_PASSWORD is the password of the user with the Deploy role Set environment variables with your AWS access keys : export AWS_ACCESS_KEY_ID='YOUR_ACCESS_KEY_ID' export AWS_SECRET_ACCESS_KEY='YOUR_SECRET_ACCESS_KEY' YOUR_ACCESS_KEY_ID is your access key ID YOUR_SECRET_ACCESS_KEY is your secret access key (Optional) Specify your public SSH key in the key_pair variable in the variables.tf file, if you plan to access the employed EC2 instances using SSH. (Optional) Specify the api.wallarm.com API endpoint in the wallarm_api_domain variable in the variables.tf file, if you use the EU cloud . If you use the US cloud , please leave an existing value. (Optional) Set AWS region data in the variables listed below in the variables.tf file. The provided example is configured for AWS region us-west-1 (North California). aws_region (you can find the list of AWS regions here ) az_a az_b wallarm_node_ami_id with the used AWS EC2 Wallarm WAF node image ID got by the command below. Please replace REGION_CODE by aws-region value: aws ec2 describe-images --filters \"Name=name,Values=*Wallarm Node-3.0*\" --region REGION_CODE | jq -r '.Images[] | \"\\(.ImageId)\"' wordpress_ami_id with the used AWS EC2 Wordpress image ID got by the command below. Please replace REGION_CODE by aws-region value: aws ec2 describe-images --filters \"Name=name,Values=*bitnami-wordpress-5.3.2-3-linux-ubuntu-16.04*\" --region REGION_CODE | jq -r '.Images[] | \"\\(.ImageId)\"'","title":"Step 2: Preparing Terraform Environment and Variables"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-quick-start/#step-3-deploying-described-stack","text":"Go to the terraform folder of the cloned repository: cd terraform-example/terraform Deploy the whole stack using the following commands: terraform init terraform plan terraform apply After a successful run, Terraform will print out a DNS name of the deployed NLB instance. For example: Apply complete! Resources: 4 added, 2 changed, 4 destroyed. Outputs: waf_nlb_dns_name = [ \"tf-wallarm-demo-asg-nlb-7b32738728e6ea44.elb.us-east-1.amazonaws.com\", ] The DNS name can be used to access the freshly installed Wordpress service with Wallarm WAF cluster deployed in front of it.","title":"Step 3: Deploying Described Stack"},{"location":"admin-en/installation-guides/amazon-cloud/deploy-waf-via-terraform/deploy-waf-via-terraform-quick-start/#step-4-testing-waf-node-operation","text":"The WAF cluster is configured with a self-signed SSL certificate so it should be possible to access the same DNS name using HTTPS protocol but the browser will show a security warning. You can simulate a web attack by adding /?id='or+1=1--a-<script>prompt(1)</script>' to the web request - the request should be blocked by the WAF with response code 403: A few minutes after simulating a web attack it should be possible to see two blocked attacks - SQLI and XSS - in your Wallarm account > Events : WAF node deployment settings are performed in the wallarm_launch_config object of the main.tf file. To change settings to your own, please use directive description available by the link . Info To remove the demonstration environment, please use the terraform destroy command.","title":"Step 4: Testing WAF Node Operation"},{"location":"admin-en/installation-guides/envoy/envoy-docker/","text":"Running Docker Envoy\u2011based image \u00b6 Image overview \u00b6 These instructions describe the steps to run the WAF Docker image based on Envoy 1.15.0 . The image contains all systems required for correct WAF operation: Envoy proxy services with embedded Wallarm WAF module Tarantool modules for postanalytics Other services and scripts Wallarm WAF module is designed as an Envoy HTTP filter for requests proxying. Supported configuration parameters Please note that the most directives for the NGINX\u2011based WAF node configuration are not supported for the Envoy\u2011based WAF node configuration. See the list of parameters available for the Envoy\u2011based WAF node configuration \u2192 If the Wallarm WAF image is already deployed in your environment If you deploy the Wallarm WAF image instead of the already deployed image or need to duplicate the deployment, please keep the same WAF version as currently used or update the version of all images to the latest. To check the installed version, run the following command in the container: yum list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update the packages to the latest version in all deployments. If the version 2.16.x is installed, then please update the packages to the latest version in all deployments. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Requirements \u00b6 Access to the account with the Deploy or Administrator role and two\u2011factor authentication disabled in the Wallarm Console in the EU Cloud or US Cloud Access to https://api.wallarm.com:444 if working with EU Wallarm Cloud or https://us1.api.wallarm.com:444 if working with US Wallarm Cloud. Please ensure the access is not blocked by a firewall Options for running the container \u00b6 The WAF node configuration parameters can be passed to the docker run command in the following ways: In the environment variables . This option allows for configuration of only basic WAF node parameters, the most parameters cannot be changed through environment variables. In the mounted configuration file . This option allows for configuration of all the WAF node parameters . Run the container passing the environment variables \u00b6 You can pass the following basic WAF node settings to the container via the option -e : Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes ENVOY_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No To run the image, use the command: EU Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e ENVOY_BACKEND = 'example.com' -e TARANTOOL_MEMORY_GB = 16 -p 80 :80 wallarm/envoy:3.0.0-1 US Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e ENVOY_BACKEND = 'example.com' -e WALLARM_API_HOST = 'us1.api.wallarm.com' -e TARANTOOL_MEMORY_GB = 16 -p 80 :80 wallarm/envoy:3.0.0-1 The command does the following: Automatically creates new WAF node in the Wallarm Cloud. Created WAF node will be displayed in the Wallarm Console \u2192 Nodes . Creates the file envoy.yaml with minimal Envoy configuration in the /etc/envoy container directory. Creates files with WAF node credentials to access the Wallarm Cloud in the /etc/wallarm container directory: node.yaml with WAF node UUID and secret key license.key with Wallarm license key Protects the resource http://ENVOY_BACKEND:80 . Run the container mounting envoy.yaml \u00b6 You can mount the prepared file envoy.yaml to the Docker container via the -v option. The file must contain the following settings: WAF node settings as described in the instructions Envoy settings as described in the Envoy instructions To run the image: Pass required environment variables to the container via the -e option: Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Mount the directory with the configuration file envoy.yaml to the /etc/envoy container directory via the -v option. EU Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -v /configs/envoy.yaml:/etc/envoy/envoy.yaml -p 80 :80 wallarm/envoy:3.0.0-1 US Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v /configs/envoy.yaml:/etc/envoy/envoy.yaml -p 80 :80 wallarm/envoy:3.0.0-1 The command does the following: Automatically creates new WAF node in the Wallarm Cloud. Created WAF node will be displayed in the Wallarm Console \u2192 Nodes . Mounts the file envoy.yaml into the /etc/envoy container directory. Creates files with WAF node credentials to access the Wallarm Cloud in the /etc/wallarm container directory: node.yaml with WAF node UUID and secret key license.key with Wallarm license key Protects the resource http://ENVOY_BACKEND:80 . Configuration of log rotation (optional) \u00b6 The log file rotation is preconfigured and enabled by default. You can adjust the rotation settings if necessary. These settings are located in the /etc/logrotate.d directory of the container. Testing WAF node operation \u00b6 Send the request with test SQLI and XSS attacks to the protected resource address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"Running Docker Envoy\u2011based image"},{"location":"admin-en/installation-guides/envoy/envoy-docker/#running-docker-envoybased-image","text":"","title":"Running Docker Envoy\u2011based image"},{"location":"admin-en/installation-guides/envoy/envoy-docker/#image-overview","text":"These instructions describe the steps to run the WAF Docker image based on Envoy 1.15.0 . The image contains all systems required for correct WAF operation: Envoy proxy services with embedded Wallarm WAF module Tarantool modules for postanalytics Other services and scripts Wallarm WAF module is designed as an Envoy HTTP filter for requests proxying. Supported configuration parameters Please note that the most directives for the NGINX\u2011based WAF node configuration are not supported for the Envoy\u2011based WAF node configuration. See the list of parameters available for the Envoy\u2011based WAF node configuration \u2192 If the Wallarm WAF image is already deployed in your environment If you deploy the Wallarm WAF image instead of the already deployed image or need to duplicate the deployment, please keep the same WAF version as currently used or update the version of all images to the latest. To check the installed version, run the following command in the container: yum list wallarm-node If the version 3.0.x is installed, then follow the current instructions. If the version 2.18.x is installed, then follow the instructions for 2.18 or update the packages to the latest version in all deployments. If the version 2.16.x is installed, then please update the packages to the latest version in all deployments. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"Image overview"},{"location":"admin-en/installation-guides/envoy/envoy-docker/#requirements","text":"Access to the account with the Deploy or Administrator role and two\u2011factor authentication disabled in the Wallarm Console in the EU Cloud or US Cloud Access to https://api.wallarm.com:444 if working with EU Wallarm Cloud or https://us1.api.wallarm.com:444 if working with US Wallarm Cloud. Please ensure the access is not blocked by a firewall","title":"Requirements"},{"location":"admin-en/installation-guides/envoy/envoy-docker/#options-for-running-the-container","text":"The WAF node configuration parameters can be passed to the docker run command in the following ways: In the environment variables . This option allows for configuration of only basic WAF node parameters, the most parameters cannot be changed through environment variables. In the mounted configuration file . This option allows for configuration of all the WAF node parameters .","title":"Options for running the container"},{"location":"admin-en/installation-guides/envoy/envoy-docker/#run-the-container-passing-the-environment-variables","text":"You can pass the following basic WAF node settings to the container via the option -e : Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes ENVOY_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No To run the image, use the command: EU Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e ENVOY_BACKEND = 'example.com' -e TARANTOOL_MEMORY_GB = 16 -p 80 :80 wallarm/envoy:3.0.0-1 US Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e ENVOY_BACKEND = 'example.com' -e WALLARM_API_HOST = 'us1.api.wallarm.com' -e TARANTOOL_MEMORY_GB = 16 -p 80 :80 wallarm/envoy:3.0.0-1 The command does the following: Automatically creates new WAF node in the Wallarm Cloud. Created WAF node will be displayed in the Wallarm Console \u2192 Nodes . Creates the file envoy.yaml with minimal Envoy configuration in the /etc/envoy container directory. Creates files with WAF node credentials to access the Wallarm Cloud in the /etc/wallarm container directory: node.yaml with WAF node UUID and secret key license.key with Wallarm license key Protects the resource http://ENVOY_BACKEND:80 .","title":"Run the container passing the environment variables"},{"location":"admin-en/installation-guides/envoy/envoy-docker/#run-the-container-mounting-envoyyaml","text":"You can mount the prepared file envoy.yaml to the Docker container via the -v option. The file must contain the following settings: WAF node settings as described in the instructions Envoy settings as described in the Envoy instructions To run the image: Pass required environment variables to the container via the -e option: Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Mount the directory with the configuration file envoy.yaml to the /etc/envoy container directory via the -v option. EU Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -v /configs/envoy.yaml:/etc/envoy/envoy.yaml -p 80 :80 wallarm/envoy:3.0.0-1 US Cloud docker run -d -e DEPLOY_USER = 'deploy@example.com' -e DEPLOY_PASSWORD = 'very_secret' -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v /configs/envoy.yaml:/etc/envoy/envoy.yaml -p 80 :80 wallarm/envoy:3.0.0-1 The command does the following: Automatically creates new WAF node in the Wallarm Cloud. Created WAF node will be displayed in the Wallarm Console \u2192 Nodes . Mounts the file envoy.yaml into the /etc/envoy container directory. Creates files with WAF node credentials to access the Wallarm Cloud in the /etc/wallarm container directory: node.yaml with WAF node UUID and secret key license.key with Wallarm license key Protects the resource http://ENVOY_BACKEND:80 .","title":"Run the container mounting envoy.yaml"},{"location":"admin-en/installation-guides/envoy/envoy-docker/#configuration-of-log-rotation-optional","text":"The log file rotation is preconfigured and enabled by default. You can adjust the rotation settings if necessary. These settings are located in the /etc/logrotate.d directory of the container.","title":"Configuration of log rotation (optional)"},{"location":"admin-en/installation-guides/envoy/envoy-docker/#testing-waf-node-operation","text":"Send the request with test SQLI and XSS attacks to the protected resource address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"Testing WAF node operation"},{"location":"admin-en/installation-guides/google-cloud/autoscaling-overview/","text":"Setting Up Filter Node Auto Scaling on the Google Cloud Platform: Overview \u00b6 You can set up Wallarm filter node auto scaling on the Google Cloud Platform (GCP) to make sure that filter nodes are capable of handling traffic fluctuations (if there are any). Enabling auto scaling allows the processing of incoming requests to the application using the filter nodes even when traffic soars significantly. Prerequisites Setting up auto scaling requires the image of the virtual machine with the Wallarm filter node. For detailed information about creating an image of the virtual machine with the Wallarm filter node on GCP, proceed to this link . Connecting to the instance via a custom private key If during base instance creation process you have enabled connection to the instance via a custom SSH key pair, make sure you have access to the private key from this key pair. To auto scale filter nodes on the Google Cloud Platform, perform the following steps: Set up filter node auto scaling: Create a filter node instance template ; Create a managed instance group with auto scaling enabled ; Set up incoming requests balancing . Required rights Before setting up auto scaling, make sure that your GCP account has the Compute Admin role.","title":"Configuring autocaling on GCP"},{"location":"admin-en/installation-guides/google-cloud/autoscaling-overview/#setting-up-filter-node-auto-scaling-on-the-google-cloud-platform-overview","text":"You can set up Wallarm filter node auto scaling on the Google Cloud Platform (GCP) to make sure that filter nodes are capable of handling traffic fluctuations (if there are any). Enabling auto scaling allows the processing of incoming requests to the application using the filter nodes even when traffic soars significantly. Prerequisites Setting up auto scaling requires the image of the virtual machine with the Wallarm filter node. For detailed information about creating an image of the virtual machine with the Wallarm filter node on GCP, proceed to this link . Connecting to the instance via a custom private key If during base instance creation process you have enabled connection to the instance via a custom SSH key pair, make sure you have access to the private key from this key pair. To auto scale filter nodes on the Google Cloud Platform, perform the following steps: Set up filter node auto scaling: Create a filter node instance template ; Create a managed instance group with auto scaling enabled ; Set up incoming requests balancing . Required rights Before setting up auto scaling, make sure that your GCP account has the Compute Admin role.","title":"Setting Up Filter Node Auto Scaling on the Google Cloud Platform: Overview"},{"location":"admin-en/installation-guides/google-cloud/create-image/","text":"Creating an Image with the Wallarm Filter Node on the Google Cloud Platform \u00b6 To set up auto scaling of the Wallarm filter nodes deployed on the Google Cloud Platform (GCP) you first need virtual machine images. This document describes the procedure for preparing an image of the virtual machine with the Wallarm filter node installed. For detailed information about setting up auto scaling, proceed to this link . To create an image with the Wallarm filter node on GCP, perform the following procedures: Creating and configuring the filter node instance on the Google Cloud Platform . Creating a virtual machine image on the basis of the configured filter node instance . 1. Creating and Configuring the Filter Node Instance on the Google Cloud Platform \u00b6 Before creating an image, you need to perform an initial configuration of a single Wallarm filter node. To configure a filter node, do the following: Create and configure a filter node instance on GCP. Provide the filter node with an internet connection The filter node requires access to a Wallarm API server for proper operation. The choice of Wallarm API server depends on the Wallarm Cloud you are using: If you are using the EU cloud, your node needs to be granted access to https://api.wallarm.com:444 . If you are using the US cloud, your node needs to be granted access to https://us1.api.wallarm.com:444 . Connecting to the instance via a custom private key If during base instance creation process you have enabled connection to the instance via a custom SSH key pair, make sure you have access to the private key from this key pair. Connect the filter node to the Wallarm cloud. Use a token to connect to the Wallarm cloud Please note that you need to connect the filter node to the Wallarm cloud using a token. Multiple filter nodes are allowed to connect to the Wallarm cloud using the same token. Thus, you will not need to manually connect each of the filter nodes to the Wallarm cloud when they auto-scale. Configure the filter node to act as a reverse proxy for your web application. Make sure that the filter node is configured correctly and protects your web application against malicious requests. After you have finished configuring the filter node, turn the virtual machine off by completing the following actions: Navigate to the VM Instances page in the Compute Engine section of the menu. Open the drop-down menu by clicking the menu button on the right of the Connect column. Select \u201cStop\u201d in the drop-down menu. Turning off using the poweroff command You may also turn the virtual machine off by connecting to it via the SSH protocol and running the following command: poweroff 2. Creating a Virtual Machine Image \u00b6 You can now create a virtual machine image based on the configured filter node instance. To create an image, perform the following steps: Navigate to the Images page in the Compute Engine section of the menu and click the Create image button. Enter the image name into the Name field. Select \u201cDisk\u201d from the Source drop-down list. Select the name of the previously created virtual machine instance from the Source disk drop-down list. Click the Create button to launch the virtual machine image creation process. Once the image creation process is finished, you will be directed to a page that contains the list of available images. Make sure that the image was successfully created and is present in the list. Now you can set up the auto scaling of Wallarm filter nodes on the Google Cloud Platform using the prepared image.","title":"Create image"},{"location":"admin-en/installation-guides/google-cloud/create-image/#creating-an-image-with-the-wallarm-filter-node-on-the-google-cloud-platform","text":"To set up auto scaling of the Wallarm filter nodes deployed on the Google Cloud Platform (GCP) you first need virtual machine images. This document describes the procedure for preparing an image of the virtual machine with the Wallarm filter node installed. For detailed information about setting up auto scaling, proceed to this link . To create an image with the Wallarm filter node on GCP, perform the following procedures: Creating and configuring the filter node instance on the Google Cloud Platform . Creating a virtual machine image on the basis of the configured filter node instance .","title":"Creating an Image with the Wallarm Filter Node on the Google Cloud Platform"},{"location":"admin-en/installation-guides/google-cloud/create-image/#1-creating-and-configuring-the-filter-node-instance-on-the-google-cloud-platform","text":"Before creating an image, you need to perform an initial configuration of a single Wallarm filter node. To configure a filter node, do the following: Create and configure a filter node instance on GCP. Provide the filter node with an internet connection The filter node requires access to a Wallarm API server for proper operation. The choice of Wallarm API server depends on the Wallarm Cloud you are using: If you are using the EU cloud, your node needs to be granted access to https://api.wallarm.com:444 . If you are using the US cloud, your node needs to be granted access to https://us1.api.wallarm.com:444 . Connecting to the instance via a custom private key If during base instance creation process you have enabled connection to the instance via a custom SSH key pair, make sure you have access to the private key from this key pair. Connect the filter node to the Wallarm cloud. Use a token to connect to the Wallarm cloud Please note that you need to connect the filter node to the Wallarm cloud using a token. Multiple filter nodes are allowed to connect to the Wallarm cloud using the same token. Thus, you will not need to manually connect each of the filter nodes to the Wallarm cloud when they auto-scale. Configure the filter node to act as a reverse proxy for your web application. Make sure that the filter node is configured correctly and protects your web application against malicious requests. After you have finished configuring the filter node, turn the virtual machine off by completing the following actions: Navigate to the VM Instances page in the Compute Engine section of the menu. Open the drop-down menu by clicking the menu button on the right of the Connect column. Select \u201cStop\u201d in the drop-down menu. Turning off using the poweroff command You may also turn the virtual machine off by connecting to it via the SSH protocol and running the following command: poweroff","title":"1.  Creating and Configuring the Filter Node Instance on the Google Cloud Platform"},{"location":"admin-en/installation-guides/google-cloud/create-image/#2-creating-a-virtual-machine-image","text":"You can now create a virtual machine image based on the configured filter node instance. To create an image, perform the following steps: Navigate to the Images page in the Compute Engine section of the menu and click the Create image button. Enter the image name into the Name field. Select \u201cDisk\u201d from the Source drop-down list. Select the name of the previously created virtual machine instance from the Source disk drop-down list. Click the Create button to launch the virtual machine image creation process. Once the image creation process is finished, you will be directed to a page that contains the list of available images. Make sure that the image was successfully created and is present in the list. Now you can set up the auto scaling of Wallarm filter nodes on the Google Cloud Platform using the prepared image.","title":"2.  Creating a Virtual Machine Image"},{"location":"admin-en/installation-guides/google-cloud/creating-autoscaling-group/","text":"Creating a Managed Instance Group with Enabled Auto Scaling \u00b6 To create a managed instance group and configure its auto scaling, perform the following steps: Navigate to the \u201cInstance groups\u201d page in the \u201cCompute Engine\u201d section of the menu and click the \u201cCreate instance group\u201d button. Enter the instance group name into the \u201cName\u201d field. Select \u201cManaged instance group\u201d in the \u201cGroup type\u201d setting. Enable auto scaling for the instance group by selecting the \u201cOn\u201d option from the \u201cAutoscaling\u201d drop-down list. Select the required scaling policy from the \u201cAutoscaling policy\u201d drop-down list. Scaling policies contain rules for increasing and decreasing the size of the instance group. The system determines when it should add or remove an instance from the group to keep the metric on which the policy is based at the target level defined by the user. You can select one of the following policies: CPU Usage: The size of the group is controlled to keep the average processor load of the virtual machines in the group at the required level ( CPU usage policy documentation ). HTTP Load Balancing Usage: The size of the group is controlled to keep the load of the HTTP traffic balancer at the required level ( HTTP load balancing usage policy documentation ). Stackdriver Monitoring Metric: The size of the group is controlled to keep the selected metric from the Stackdriver Monitoring instrument at the required level ( Stackdriver Monitoring Metric policy documentation ). Multiple Metrics: The decision to change the size of the group is made on the basis of multiple metrics ( multiple metrics policy documentation ). This guide uses the \u201cCPU usage\u201d policy to demonstrate the principles of working with the auto scaling mechanism. To apply this policy, specify the required average processors' load level in the \u201cTarget CPU usage\u201d field (in percentages). Example The following configuration describes the control of the instance group size to keep the average virtual machine processors' load at the 60 percent level. Specify the minimum instance group size in the \u201cMinimum number of instances\u201d field (e.g., two instances). Specify the maximum instance group size in the \u201cMaximum number of instances\u201d field (e.g., 10 instances). Specify the period of time during which the metric values should not be recorded from the newly added instance in the \u201cCool down period\u201d field (e.g., 60 seconds). This may be necessary if you see resource consumption leaps after adding a new instance. Cooldown period requirements The cooldown period must be longer than the time required for instance initialization. Make sure all of the parameters of the instance group are configured correctly and then click the \u201cCreate\u201d button. The specified number of instances will automatically launch upon the successful creation of the auto scaling group. You can check that the auto scaling group was created correctly by viewing the number of launched instances in the group and comparing this data point with the number of filter nodes connected to the Wallarm cloud. You can do this using the Wallarm website. For example, if two instances with filter nodes are concurrently operating, the Wallarm website will display this number for the corresponding cloud node on the Nodes tab. You can now proceed with the creation and configuration of a load balancer .","title":"Creating autoscaling group"},{"location":"admin-en/installation-guides/google-cloud/creating-autoscaling-group/#creating-a-managed-instance-group-with-enabled-auto-scaling","text":"To create a managed instance group and configure its auto scaling, perform the following steps: Navigate to the \u201cInstance groups\u201d page in the \u201cCompute Engine\u201d section of the menu and click the \u201cCreate instance group\u201d button. Enter the instance group name into the \u201cName\u201d field. Select \u201cManaged instance group\u201d in the \u201cGroup type\u201d setting. Enable auto scaling for the instance group by selecting the \u201cOn\u201d option from the \u201cAutoscaling\u201d drop-down list. Select the required scaling policy from the \u201cAutoscaling policy\u201d drop-down list. Scaling policies contain rules for increasing and decreasing the size of the instance group. The system determines when it should add or remove an instance from the group to keep the metric on which the policy is based at the target level defined by the user. You can select one of the following policies: CPU Usage: The size of the group is controlled to keep the average processor load of the virtual machines in the group at the required level ( CPU usage policy documentation ). HTTP Load Balancing Usage: The size of the group is controlled to keep the load of the HTTP traffic balancer at the required level ( HTTP load balancing usage policy documentation ). Stackdriver Monitoring Metric: The size of the group is controlled to keep the selected metric from the Stackdriver Monitoring instrument at the required level ( Stackdriver Monitoring Metric policy documentation ). Multiple Metrics: The decision to change the size of the group is made on the basis of multiple metrics ( multiple metrics policy documentation ). This guide uses the \u201cCPU usage\u201d policy to demonstrate the principles of working with the auto scaling mechanism. To apply this policy, specify the required average processors' load level in the \u201cTarget CPU usage\u201d field (in percentages). Example The following configuration describes the control of the instance group size to keep the average virtual machine processors' load at the 60 percent level. Specify the minimum instance group size in the \u201cMinimum number of instances\u201d field (e.g., two instances). Specify the maximum instance group size in the \u201cMaximum number of instances\u201d field (e.g., 10 instances). Specify the period of time during which the metric values should not be recorded from the newly added instance in the \u201cCool down period\u201d field (e.g., 60 seconds). This may be necessary if you see resource consumption leaps after adding a new instance. Cooldown period requirements The cooldown period must be longer than the time required for instance initialization. Make sure all of the parameters of the instance group are configured correctly and then click the \u201cCreate\u201d button. The specified number of instances will automatically launch upon the successful creation of the auto scaling group. You can check that the auto scaling group was created correctly by viewing the number of launched instances in the group and comparing this data point with the number of filter nodes connected to the Wallarm cloud. You can do this using the Wallarm website. For example, if two instances with filter nodes are concurrently operating, the Wallarm website will display this number for the corresponding cloud node on the Nodes tab. You can now proceed with the creation and configuration of a load balancer .","title":"Creating a Managed Instance Group with Enabled Auto Scaling"},{"location":"admin-en/installation-guides/google-cloud/creating-instance-template/","text":"Creating a Filter Node Instance Template \u00b6 A filter node instance template will be used later as the base when creating a managed instance group. To create a filter node instance template, perform the following: Navigate to the \u201cInstance templates\u201d page in the \u201cCompute Engine\u201d section of the menu and click the \u201cCreate instance template\u201d button. Enter the template name into the \u201cName\u201d field. Select the virtual machine type to be used to launch a virtual machine with the filter node on from the \u201cMachine type\u201d field. Select the proper instance type Select the same instance type that you used when you initially configured the filter node (or a more powerful one). Using a less powerful instance type may lead to issues in filter node operation. Click the \u201cChange\u201d button in the \u201cBoot disk\u201d setting. In the window that appears, navigate to the \u201cCustom images\u201d tab and select the name of the project where you created your virtual machine image from the \u201cShow images from\u201d drop-down list. Select the previously created image from the list of available images of the project and click the \u201cSelect\u201d button. For the instances based on the template to be identical to the basic instance, configure all of the remaining parameters in the same way as you configured the parameters when creating your base instance . Configuring the firewall Make sure that the firewall does not block HTTP traffic to the created template. To enable HTTP traffic, select the \u201cAllow HTTP traffic\u201d checkbox. Connecting to the instance via a custom private key If during base instance creation process you have enabled connection to the instance via a custom SSH key pair, make sure you have access to the private key from this key pair. Click the \u201cCreate\u201d button and wait until the template creation process is finished. After creating the instance template, you can proceed with the creation of a managed instance group with enabled auto scaling.","title":"Creating instance template"},{"location":"admin-en/installation-guides/google-cloud/creating-instance-template/#creating-a-filter-node-instance-template","text":"A filter node instance template will be used later as the base when creating a managed instance group. To create a filter node instance template, perform the following: Navigate to the \u201cInstance templates\u201d page in the \u201cCompute Engine\u201d section of the menu and click the \u201cCreate instance template\u201d button. Enter the template name into the \u201cName\u201d field. Select the virtual machine type to be used to launch a virtual machine with the filter node on from the \u201cMachine type\u201d field. Select the proper instance type Select the same instance type that you used when you initially configured the filter node (or a more powerful one). Using a less powerful instance type may lead to issues in filter node operation. Click the \u201cChange\u201d button in the \u201cBoot disk\u201d setting. In the window that appears, navigate to the \u201cCustom images\u201d tab and select the name of the project where you created your virtual machine image from the \u201cShow images from\u201d drop-down list. Select the previously created image from the list of available images of the project and click the \u201cSelect\u201d button. For the instances based on the template to be identical to the basic instance, configure all of the remaining parameters in the same way as you configured the parameters when creating your base instance . Configuring the firewall Make sure that the firewall does not block HTTP traffic to the created template. To enable HTTP traffic, select the \u201cAllow HTTP traffic\u201d checkbox. Connecting to the instance via a custom private key If during base instance creation process you have enabled connection to the instance via a custom SSH key pair, make sure you have access to the private key from this key pair. Click the \u201cCreate\u201d button and wait until the template creation process is finished. After creating the instance template, you can proceed with the creation of a managed instance group with enabled auto scaling.","title":"Creating a Filter Node Instance Template"},{"location":"admin-en/installation-guides/google-cloud/load-balancing-guide/","text":"Setting up Incoming Request Balancing on GCP \u00b6 Now that you have a configured managed instance group with enabled auto scaling, you need to create and configure a Load Balancer that distributes incoming HTTP and HTTPS connections between several filter nodes from the instance group. You can configure the following types of Load Balancers on the Google Cloud Platform: HTTP(S) Load Balancer, TCP Load Balancer, UDP Load Balancer. The differences between Load Balancers For detailed information about the differences between Load Balancers, proceed to this link . This document demonstrates how to configure and use the TCP Load Balancer that distributes traffic at the transport level of the OSI/ISO network model. Create a TCP Load Balancer for your instance group by completing the following actions: Navigate to the \u201cLoad balancing\u201d page in the \u201cNetwork services\u201d section of the menu and click the \u201cCreate load balancer\u201d button. Click the \u201cStart configuration\u201d button on the \u201cTCP load balancing\u201d card. Select the required options in the following settings: Select the \u201cFrom Internet to my VMs\u201d option in the \u201cInternet facing or internal only\u201d setting so that the load balancer will control incoming requests from clients to your server. Select the \u201cSingle region only\u201d option in the \u201cMultiple regions or single region\u201d setting. Traffic balancing for resources located in different regions This guide describes the configuration of the load balancer for one instance group located in a single region. In the case of balancing traffic for several resources located in multiple regions, select the \u201cMultiple regions (or not sure yet)\u201d option. Click the \u201cContinue\u201d button. Enter the load balancer name into the \u201cName\u201d field. Click the \u201cBackend configuration\u201d to use the created instance group as the backend to which the load balancer will route the incoming requests. Fill in the form with the following data: Select the region where the instance group is located from the \u201cRegion\u201d drop-down list. Navigate to the \u201cSelect existing instance groups\u201d tab in the \u201cBackends\u201d setting and select the name of the instance group from the \u201cAdd an instance group\u201d drop-down list. If necessary, specify the backup pool by selecting the \u201cCreate a backup pool\u201d option from the \u201cBackup Pool\u201d drop-down list. Using a backup pool A backup pool processes the requests if the instance group selected in the previous setting is unavailable. For detailed information about configuring a backup pool, proceed to this link . This document does not describe the backup pool configuration. If necessary, configure the group instances availability checkup by selecting the \u201cCreate a health check\u201d option in the \u201cHealth check\u201d drop-down list. For detailed information about the machine availability checkup, proceed to this link . The availability checkup The availability checkup is not configured in the scope of this document. Thus, here the \u201cNo health check\u201d option is selected in the \u201cHealth check\u201d drop-down list. If necessary, configure the method of choosing an instance for request processing by selecting the corresponding option in the \u201cSession affinity\u201d drop-down list. Detailed information about selecting an instance for request processing is available at this link . Configuring a method of choosing an instance The method of choosing an instance for request processing is not in the scope of this document. Thus, here the \u201cNone\u201d option is selected in the \u201cSession affinity\u201d drop-down list. Click the \u201cFrontend configuration\u201d button to specify the IP addresses and ports to which clients will send their requests. Fill in the form for new IP addresses and ports creation with the required data: If necessary, enter the new IP address and port pair's name into the \u201cName\u201d field. Select the required network service tier in the \u201cNetwork Service Tier\u201d setting. For detailed information about network service tiers, proceed to this link ; Select the IP address where the load balancer will receive requests from the \u201cIP\u201d drop-down list. Select the \u201cEphemeral\u201d option if you want the load balancer to obtain a new IP address upon each virtual machine startup. Select the \u201cCreate IP address\u201d option to generate a static IP address for your load balancer. In the form that appears, enter the name of the new IP address into the \u201cName\u201d field and click the \u201cReserve\u201d button. Enter the port where the load balancer will receive requests in the \u201cPort\u201d field. Choosing the port In this document, port 80 is specified for receiving requests via the HTTP protocol. Click the \u201cDone\u201d button to create the configured IP address and port pair. Required frontend ports In this document, the balancer is configured for receiving requests via the HTTP protocol. If your instance group receives requests via the HTTPS protocol, create another IP address and port pair that specifies port 443 . Click the \u201cCreate\u201d button to create the configured load balancer. Wait until the load balancer creation process is finished and the load balancer connects to the instance group that you created earlier. Because the created TCP balancer uses the Backend service (which works together with the backend created for your instance group), the instance group requires no configuration modifications for the balancer to connect to it. Now the dynamically scaling set of the Wallarm filter nodes will process the incoming traffic to your application. To check the deployed filter nodes\u2019 operation, perform the following steps: Make sure that your application is accessible through the load balancer and the Wallarm filter nodes by referring to the balancer IP address or domain name using your browser. Make sure that the Wallarm services protect your application by performing a test attack .","title":"Load balancing guide"},{"location":"admin-en/installation-guides/google-cloud/load-balancing-guide/#setting-up-incoming-request-balancing-on-gcp","text":"Now that you have a configured managed instance group with enabled auto scaling, you need to create and configure a Load Balancer that distributes incoming HTTP and HTTPS connections between several filter nodes from the instance group. You can configure the following types of Load Balancers on the Google Cloud Platform: HTTP(S) Load Balancer, TCP Load Balancer, UDP Load Balancer. The differences between Load Balancers For detailed information about the differences between Load Balancers, proceed to this link . This document demonstrates how to configure and use the TCP Load Balancer that distributes traffic at the transport level of the OSI/ISO network model. Create a TCP Load Balancer for your instance group by completing the following actions: Navigate to the \u201cLoad balancing\u201d page in the \u201cNetwork services\u201d section of the menu and click the \u201cCreate load balancer\u201d button. Click the \u201cStart configuration\u201d button on the \u201cTCP load balancing\u201d card. Select the required options in the following settings: Select the \u201cFrom Internet to my VMs\u201d option in the \u201cInternet facing or internal only\u201d setting so that the load balancer will control incoming requests from clients to your server. Select the \u201cSingle region only\u201d option in the \u201cMultiple regions or single region\u201d setting. Traffic balancing for resources located in different regions This guide describes the configuration of the load balancer for one instance group located in a single region. In the case of balancing traffic for several resources located in multiple regions, select the \u201cMultiple regions (or not sure yet)\u201d option. Click the \u201cContinue\u201d button. Enter the load balancer name into the \u201cName\u201d field. Click the \u201cBackend configuration\u201d to use the created instance group as the backend to which the load balancer will route the incoming requests. Fill in the form with the following data: Select the region where the instance group is located from the \u201cRegion\u201d drop-down list. Navigate to the \u201cSelect existing instance groups\u201d tab in the \u201cBackends\u201d setting and select the name of the instance group from the \u201cAdd an instance group\u201d drop-down list. If necessary, specify the backup pool by selecting the \u201cCreate a backup pool\u201d option from the \u201cBackup Pool\u201d drop-down list. Using a backup pool A backup pool processes the requests if the instance group selected in the previous setting is unavailable. For detailed information about configuring a backup pool, proceed to this link . This document does not describe the backup pool configuration. If necessary, configure the group instances availability checkup by selecting the \u201cCreate a health check\u201d option in the \u201cHealth check\u201d drop-down list. For detailed information about the machine availability checkup, proceed to this link . The availability checkup The availability checkup is not configured in the scope of this document. Thus, here the \u201cNo health check\u201d option is selected in the \u201cHealth check\u201d drop-down list. If necessary, configure the method of choosing an instance for request processing by selecting the corresponding option in the \u201cSession affinity\u201d drop-down list. Detailed information about selecting an instance for request processing is available at this link . Configuring a method of choosing an instance The method of choosing an instance for request processing is not in the scope of this document. Thus, here the \u201cNone\u201d option is selected in the \u201cSession affinity\u201d drop-down list. Click the \u201cFrontend configuration\u201d button to specify the IP addresses and ports to which clients will send their requests. Fill in the form for new IP addresses and ports creation with the required data: If necessary, enter the new IP address and port pair's name into the \u201cName\u201d field. Select the required network service tier in the \u201cNetwork Service Tier\u201d setting. For detailed information about network service tiers, proceed to this link ; Select the IP address where the load balancer will receive requests from the \u201cIP\u201d drop-down list. Select the \u201cEphemeral\u201d option if you want the load balancer to obtain a new IP address upon each virtual machine startup. Select the \u201cCreate IP address\u201d option to generate a static IP address for your load balancer. In the form that appears, enter the name of the new IP address into the \u201cName\u201d field and click the \u201cReserve\u201d button. Enter the port where the load balancer will receive requests in the \u201cPort\u201d field. Choosing the port In this document, port 80 is specified for receiving requests via the HTTP protocol. Click the \u201cDone\u201d button to create the configured IP address and port pair. Required frontend ports In this document, the balancer is configured for receiving requests via the HTTP protocol. If your instance group receives requests via the HTTPS protocol, create another IP address and port pair that specifies port 443 . Click the \u201cCreate\u201d button to create the configured load balancer. Wait until the load balancer creation process is finished and the load balancer connects to the instance group that you created earlier. Because the created TCP balancer uses the Backend service (which works together with the backend created for your instance group), the instance group requires no configuration modifications for the balancer to connect to it. Now the dynamically scaling set of the Wallarm filter nodes will process the incoming traffic to your application. To check the deployed filter nodes\u2019 operation, perform the following steps: Make sure that your application is accessible through the load balancer and the Wallarm filter nodes by referring to the balancer IP address or domain name using your browser. Make sure that the Wallarm services protect your application by performing a test attack .","title":"Setting up Incoming Request Balancing on GCP"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/","text":"Kubernetes Deployment Based on Helm Charts \u00b6 Prerequisites \u00b6 Local or cloud (EKS, GKE, AKE, etc) cluster running any version of Kubernetes Application packaged as a Helm chart Pod exposed to the public Internet or other potential sources of malicious web and API attacks Kubernetes ingress controller or external load balancer (like AWS ELB or ALB) to add the HTTP request header X-Forwarded-For , which contains the real public IP address of the connecting client Wallarm account in the EU cloud or US cloud Username and password of the user with the Deploy role added to your Wallarm account. To add a new user, please follow these instructions Installation \u00b6 Create Wallarm ConfigMap. Update the definition of the Deployment object in Kubernetes. Update the definition of the Service object in Kubernetes. Update the Helm chart configuration file. Test the Wallarm sidecar container. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of already existing Wallarm WAF or need to duplicate the installation in the same environment, please keep the same WAF version as currently used or update the version of all installations to the latest. The version of deployed Wallarm WAF image is specified in the Helm chart configuration file \u2192 wallarm.image.tag . If the version 3.0.x is specified, follow the current instructions. If the version 2.18.x is specified, follow the instructions for 2.18 or increase the version of the image to 3.0.0-2 in all deployments and follow the current instructions. If the version 2.16.x or lower is specified, please increase the version of the image to 3.0.0-2 in all deployments and follow the current instructions. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Step 1: Creating Wallarm ConfigMap \u00b6 Go to the Helm chart directory > the templates folder and create a wallarm-sidecar-configmap.yaml template with the following content: apiVersion: v1 kind: ConfigMap metadata: name: wallarm-sidecar-nginx-conf data: default: | server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; server_name localhost; root /usr/share/nginx/html; index index.html index.htm; wallarm_mode {{ .Values.wallarm.mode | quote }}; # wallarm_instance 1; set_real_ip_from 0.0.0.0/0; real_ip_header X-Forwarded-For; location / { proxy_pass http://localhost:{{ .Values.wallarm.app_container_port }}; include proxy_params; } } Step 2: Updating the Deployment Object in Kubernetes \u00b6 Return to the Helm chart directory > the templates folder and open the template defining the Deployment object for the application. A complex application can have several Deployment objects for different components of the application - please find an object which defines pods which are actually exposed to the Internet. For example: apiVersion: apps/v1 kind: Deployment metadata: name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: # Definition of your main app container - name: myapp image: <Image> resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: # Port on which the application container accepts incoming requests - containerPort: 8080 Copy the following elements to the template: the checksum/config annotation to the spec.template.metadata.annotations section to update the running pods after a change in the previously created ConfigMap object, the wallarm sidecar container definition to the spec.template.spec.containers section, the wallarm-nginx-conf volume definition to the spec.template.spec.volumes section. An example of the template with added elements is provided below. Elements for copying are indicated by the Wallarm element comment. apiVersion: apps/v1 kind: Deployment metadata: annotations: # Wallarm element: annotation to update running pods after changing Wallarm ConfigMap checksum/config: '{{ include (print $.Template.BasePath \"/wallarm-sidecar-configmap.yaml\") . | sha256sum }}' name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: # Wallarm element: definition of Wallarm sidecar container - name: wallarm image: {{ .Values.wallarm.image.repository }}:{{ .Values.wallarm.image.tag }} imagePullPolicy: {{ .Values.wallarm.image.pullPolicy | quote }} env: - name: WALLARM_API_HOST value: {{ .Values.wallarm.wallarm_host_api | quote }} - name: DEPLOY_USER value: {{ .Values.wallarm.deploy_username | quote }} - name: DEPLOY_PASSWORD value: {{ .Values.wallarm.deploy_password | quote }} - name: DEPLOY_FORCE value: \"true\" - name: TARANTOOL_MEMORY_GB value: {{ .Values.wallarm.tarantool_memory_gb | quote }} ports: - name: http # Port on which the Wallarm sidecar container accepts requests # from the Service object containerPort: 80 volumeMounts: - mountPath: /etc/nginx/sites-enabled readOnly: true name: wallarm-nginx-conf # Definition of your main app container - name: myapp image: <Image> resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: # Port on which the application container accepts incoming requests - containerPort: 8080 volumes: # Wallarm element: definition of the wallarm-nginx-conf volume - name: wallarm-nginx-conf configMap: name: wallarm-sidecar-nginx-conf items: - key: default path: default Update the ports.containerPort value in sidecar container definition following the code comments. Step 3: Updating the Service Object in Kubernetes \u00b6 Return to the Helm chart directory > the templates folder and open the template defining the Service object that points to Deployment modified in the previous step. For example: apiVersion: v1 kind: Service metadata: name: myapp spec: selector: app: myapp ports: - port: {{ .Values.service.port }} # Wallarm sidecar container port; # the value must be identical to ports.containerPort # in definition of Wallarm sidecar container targetPort: 8080 Change the ports.targetPort value to point to the Wallarm sidecar container port ( ports.containerPort defined in the Wallarm sidecar container). For example: ... - port: {{ .Values.service.port }} # Wallarm sidecar container port; # the value must be identical to ports.containerPort # in definition of Wallarm sidecar container targetPort: 80 Step 4: Updating the Helm Chart Configuration File \u00b6 Return to the Helm chart directory and open the values.yaml file. Copy the wallarm object definition provided below to values.yaml and update parameter values following the code comments. wallarm: image: repository: wallarm/node tag: 3.0.0-2 pullPolicy: Always # Wallarm API endpoint: # \"api.wallarm.com\" for the EU cloud # \"us1.api.wallarm.com\" for the US cloud wallarm_host_api: \"api.wallarm.com\" # Username of the user with the Deploy role deploy_username: \"username\" # Password of the user with the Deploy role deploy_password: \"password\" # Port on which the container accepts incoming requests, # the value must be identical to ports.containerPort # in definition of your main app container app_container_port: 80 # Request filtration mode: # \"off\" to disable request processing # \"monitoring\" to process but not block requests # \"safe_blocking\" to block malicious requests originated from greylisted IPs # \"block\" to process all requests and block the malicious ones mode: \"block\" # Amount of memory in GB for request analytics data, # recommended value is 75% of the total server memory tarantool_memory_gb: 2 Make sure the values.yaml file is valid using the following command: helm lint Deploy the modified Helm chart in the Kubernetes cluster using the following command: helm upgrade <RELEASE> <CHART> <RELEASE> is the name of an existing Helm chart, <CHART> is the path to the Helm chart directory. NetworkPolicy Object in Kubernetes If the application also uses the NetworkPolicy object it should be updated to reflect the Wallarm sidecar container port specified above. Step 5: Testing the Wallarm Sidecar Container \u00b6 Get the list of pods using the following command: kubectl get pods The number of containers in the pod should increase, and the status of the pod should be \"Running\". NAME READY STATUS RESTARTS AGE mychart-856f957bbd-cr4kt 2/2 Running 0 3m48s Go to your Wallarm account > Nodes via the link below and make sure that a new node is displayed. This created node is used to filter requests to your application. https://my.wallarm.com/nodes/ for the EU cloud https://us1.my.wallarm.com/nodes/ for the US cloud Send a malicious test attack request to the application as described in these instructions . Go to your Wallarm account > Events via the link below and make sure that an attack is displayed in the list: https://my.wallarm.com/events/ for the EU cloud https://us1.my.wallarm.com/events/ for the US cloud","title":"Kubernetes Deployment Based on Helm Charts"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/#kubernetes-deployment-based-on-helm-charts","text":"","title":"Kubernetes Deployment Based on Helm Charts"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/#prerequisites","text":"Local or cloud (EKS, GKE, AKE, etc) cluster running any version of Kubernetes Application packaged as a Helm chart Pod exposed to the public Internet or other potential sources of malicious web and API attacks Kubernetes ingress controller or external load balancer (like AWS ELB or ALB) to add the HTTP request header X-Forwarded-For , which contains the real public IP address of the connecting client Wallarm account in the EU cloud or US cloud Username and password of the user with the Deploy role added to your Wallarm account. To add a new user, please follow these instructions","title":"Prerequisites"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/#installation","text":"Create Wallarm ConfigMap. Update the definition of the Deployment object in Kubernetes. Update the definition of the Service object in Kubernetes. Update the Helm chart configuration file. Test the Wallarm sidecar container. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of already existing Wallarm WAF or need to duplicate the installation in the same environment, please keep the same WAF version as currently used or update the version of all installations to the latest. The version of deployed Wallarm WAF image is specified in the Helm chart configuration file \u2192 wallarm.image.tag . If the version 3.0.x is specified, follow the current instructions. If the version 2.18.x is specified, follow the instructions for 2.18 or increase the version of the image to 3.0.0-2 in all deployments and follow the current instructions. If the version 2.16.x or lower is specified, please increase the version of the image to 3.0.0-2 in all deployments and follow the current instructions. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"Installation"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/#step-1-creating-wallarm-configmap","text":"Go to the Helm chart directory > the templates folder and create a wallarm-sidecar-configmap.yaml template with the following content: apiVersion: v1 kind: ConfigMap metadata: name: wallarm-sidecar-nginx-conf data: default: | server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; server_name localhost; root /usr/share/nginx/html; index index.html index.htm; wallarm_mode {{ .Values.wallarm.mode | quote }}; # wallarm_instance 1; set_real_ip_from 0.0.0.0/0; real_ip_header X-Forwarded-For; location / { proxy_pass http://localhost:{{ .Values.wallarm.app_container_port }}; include proxy_params; } }","title":"Step 1: Creating Wallarm ConfigMap"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/#step-2-updating-the-deployment-object-in-kubernetes","text":"Return to the Helm chart directory > the templates folder and open the template defining the Deployment object for the application. A complex application can have several Deployment objects for different components of the application - please find an object which defines pods which are actually exposed to the Internet. For example: apiVersion: apps/v1 kind: Deployment metadata: name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: # Definition of your main app container - name: myapp image: <Image> resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: # Port on which the application container accepts incoming requests - containerPort: 8080 Copy the following elements to the template: the checksum/config annotation to the spec.template.metadata.annotations section to update the running pods after a change in the previously created ConfigMap object, the wallarm sidecar container definition to the spec.template.spec.containers section, the wallarm-nginx-conf volume definition to the spec.template.spec.volumes section. An example of the template with added elements is provided below. Elements for copying are indicated by the Wallarm element comment. apiVersion: apps/v1 kind: Deployment metadata: annotations: # Wallarm element: annotation to update running pods after changing Wallarm ConfigMap checksum/config: '{{ include (print $.Template.BasePath \"/wallarm-sidecar-configmap.yaml\") . | sha256sum }}' name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: # Wallarm element: definition of Wallarm sidecar container - name: wallarm image: {{ .Values.wallarm.image.repository }}:{{ .Values.wallarm.image.tag }} imagePullPolicy: {{ .Values.wallarm.image.pullPolicy | quote }} env: - name: WALLARM_API_HOST value: {{ .Values.wallarm.wallarm_host_api | quote }} - name: DEPLOY_USER value: {{ .Values.wallarm.deploy_username | quote }} - name: DEPLOY_PASSWORD value: {{ .Values.wallarm.deploy_password | quote }} - name: DEPLOY_FORCE value: \"true\" - name: TARANTOOL_MEMORY_GB value: {{ .Values.wallarm.tarantool_memory_gb | quote }} ports: - name: http # Port on which the Wallarm sidecar container accepts requests # from the Service object containerPort: 80 volumeMounts: - mountPath: /etc/nginx/sites-enabled readOnly: true name: wallarm-nginx-conf # Definition of your main app container - name: myapp image: <Image> resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: # Port on which the application container accepts incoming requests - containerPort: 8080 volumes: # Wallarm element: definition of the wallarm-nginx-conf volume - name: wallarm-nginx-conf configMap: name: wallarm-sidecar-nginx-conf items: - key: default path: default Update the ports.containerPort value in sidecar container definition following the code comments.","title":"Step 2: Updating the Deployment Object in Kubernetes"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/#step-3-updating-the-service-object-in-kubernetes","text":"Return to the Helm chart directory > the templates folder and open the template defining the Service object that points to Deployment modified in the previous step. For example: apiVersion: v1 kind: Service metadata: name: myapp spec: selector: app: myapp ports: - port: {{ .Values.service.port }} # Wallarm sidecar container port; # the value must be identical to ports.containerPort # in definition of Wallarm sidecar container targetPort: 8080 Change the ports.targetPort value to point to the Wallarm sidecar container port ( ports.containerPort defined in the Wallarm sidecar container). For example: ... - port: {{ .Values.service.port }} # Wallarm sidecar container port; # the value must be identical to ports.containerPort # in definition of Wallarm sidecar container targetPort: 80","title":"Step 3: Updating the Service Object in Kubernetes"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/#step-4-updating-the-helm-chart-configuration-file","text":"Return to the Helm chart directory and open the values.yaml file. Copy the wallarm object definition provided below to values.yaml and update parameter values following the code comments. wallarm: image: repository: wallarm/node tag: 3.0.0-2 pullPolicy: Always # Wallarm API endpoint: # \"api.wallarm.com\" for the EU cloud # \"us1.api.wallarm.com\" for the US cloud wallarm_host_api: \"api.wallarm.com\" # Username of the user with the Deploy role deploy_username: \"username\" # Password of the user with the Deploy role deploy_password: \"password\" # Port on which the container accepts incoming requests, # the value must be identical to ports.containerPort # in definition of your main app container app_container_port: 80 # Request filtration mode: # \"off\" to disable request processing # \"monitoring\" to process but not block requests # \"safe_blocking\" to block malicious requests originated from greylisted IPs # \"block\" to process all requests and block the malicious ones mode: \"block\" # Amount of memory in GB for request analytics data, # recommended value is 75% of the total server memory tarantool_memory_gb: 2 Make sure the values.yaml file is valid using the following command: helm lint Deploy the modified Helm chart in the Kubernetes cluster using the following command: helm upgrade <RELEASE> <CHART> <RELEASE> is the name of an existing Helm chart, <CHART> is the path to the Helm chart directory. NetworkPolicy Object in Kubernetes If the application also uses the NetworkPolicy object it should be updated to reflect the Wallarm sidecar container port specified above.","title":"Step 4: Updating the Helm Chart Configuration File"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-helm/#step-5-testing-the-wallarm-sidecar-container","text":"Get the list of pods using the following command: kubectl get pods The number of containers in the pod should increase, and the status of the pod should be \"Running\". NAME READY STATUS RESTARTS AGE mychart-856f957bbd-cr4kt 2/2 Running 0 3m48s Go to your Wallarm account > Nodes via the link below and make sure that a new node is displayed. This created node is used to filter requests to your application. https://my.wallarm.com/nodes/ for the EU cloud https://us1.my.wallarm.com/nodes/ for the US cloud Send a malicious test attack request to the application as described in these instructions . Go to your Wallarm account > Events via the link below and make sure that an attack is displayed in the list: https://my.wallarm.com/events/ for the EU cloud https://us1.my.wallarm.com/events/ for the US cloud","title":"Step 5: Testing the Wallarm Sidecar Container"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/","text":"Kubernetes Deployment Based on Manifests \u00b6 Prerequisites \u00b6 Local or cloud (EKS, GKE, AKE, etc) cluster running any version of Kubernetes Application defined in plain Kubernetes manifest files Pod exposed to the public Internet or other potential sources of malicious web and API attacks Kubernetes ingress controller or external load balancer (like AWS ELB or ALB) to add the HTTP request header X-Forwarded-For , which contains the real public IP address of the connecting client Wallarm account in the EU cloud or US cloud Username and password of the user with the Deploy role added to your Wallarm account. To add a new user, please follow these instructions Installation \u00b6 Create Wallarm ConfigMap. Update the definition of the Deployment object in Kubernetes. Update the definition of the Service object in Kubernetes. Deploy the manifest to the Kubernetes cluster. Test the Wallarm sidecar container. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of already existing Wallarm WAF or need to duplicate the installation in the same environment, please keep the same WAF version as currently used or update the version of all installations to the latest. The version of deployed Wallarm WAF image is specified in the Deployment template \u2192 spec.template.spec.containers section \u2192 image of the Wallarm container. If the version 3.0.x is specified, follow the current instructions. If the version 2.18.x is specified, follow the instructions for 2.18 or increase the version of the image to 3.0.0-2 in all deployments and follow the current instructions. If the version 2.16.x or lower is specified, please increase the version of the image to 3.0.0-2 in all deployments and follow the current instructions. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Step 1: Creating Wallarm ConfigMap \u00b6 Create a new manifest file or add a new object to the existing manifest for a new Kubernetes ConfigMap object that will hold the NGINX configuration file for the Wallarm sidecar container: apiVersion: v1 kind: ConfigMap metadata: name: wallarm-sidecar-nginx-conf data: default: | server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; server_name localhost; root /usr/share/nginx/html; index index.html index.htm; # Please replace <WALLARM_MODE> below by the request filtration mode: # off to disable request processing # monitoring to process but not block requests # safe_blocking to block only those malicious requests originated from greylisted IPs # block to process all requests and block the malicious ones wallarm_mode <WALLARM_MODE>; # wallarm_instance 1; set_real_ip_from 0.0.0.0/0; real_ip_header X-Forwarded-For; location / { # Please replace <APP_CONTAINER_PORT> below by the port number # on which the container accepts incoming requests, # the value must be identical to ports.containerPort # in definition of your main app container proxy_pass http://localhost:<APP_CONTAINER_PORT>; include proxy_params; } } Update parameter values following the code comments. Step 2: Updating the Deployment Object in Kubernetes \u00b6 Go to the Kubernetes manifests and open the template that defines the Deployment object for the application. A complex application can have several Deployment objects for different components of the application - please find an object which defines pods which are actually exposed to the Internet. For example: apiVersion: apps/v1 kind: Deployment metadata: name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: # Definition of your main app container - name: myapp image: <Image> resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: # Port on which the application container accepts incoming requests - containerPort: 8080 Copy the following elements to the template: the wallarm sidecar container definition to the spec.template.spec.containers section, the wallarm-nginx-conf volume definition to the spec.template.spec.volumes section. An example of the template with added elements is provided below. Elements for copying are indicated by the Wallarm element comment. apiVersion: apps/v1 kind: Deployment metadata: name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: # Wallarm element: definition of Wallarm sidecar container - name: wallarm image: wallarm/node:3.0.0-2 imagePullPolicy: Always env: # Wallarm API endpoint: # \"api.wallarm.com\" for the EU cloud # \"us1.api.wallarm.com\" for the US cloud - name: WALLARM_API_HOST value: \"api.wallarm.com\" # Username of the user with the Deploy role - name: DEPLOY_USER value: \"username\" # Password of the user with the Deploy role - name: DEPLOY_PASSWORD value: \"password\" - name: DEPLOY_FORCE value: \"true\" # Amount of memory in GB for request analytics data, # recommended value is 75% of the total server memory - name: TARANTOOL_MEMORY_GB value: \"2\" ports: - name: http # Port on which the Wallarm sidecar container accepts requests # from the Service object containerPort: 80 volumeMounts: - mountPath: /etc/nginx/sites-enabled readOnly: true name: wallarm-nginx-conf # Definition of your main app container - name: myapp image: <Image> resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: # Port on which the application container accepts incoming requests - containerPort: 8080 volumes: # Wallarm element: definition of the wallarm-nginx-conf volume - name: wallarm-nginx-conf configMap: name: wallarm-sidecar-nginx-conf items: - key: default path: default Update parameter values following the code comments. Step 3: Updating the Service Object in Kubernetes \u00b6 Return to the Kubernetes manifests and open the template that defines the Service object that points to Deployment modified in the previous step. For example: apiVersion: v1 kind: Service metadata: name: myapp labels: run: myapp spec: type: NodePort ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: run: myapp Change the ports.targetPort value to point to the Wallarm sidecar container port ( ports.containerPort defined in the Wallarm sidecar container). For example: ... ports: - port: 80 targetPort: 80 protocol: TCP name: http selector: run: myapp Step 4: Deploying the Manifest to the Kubernetes Cluster \u00b6 Update or deploy the new application manifest in the Kubernetes cluster. NetworkPolicy Object in Kubernetes If the application also uses the NetworkPolicy object it should be updated to reflect the Wallarm sidecar container port specified above. Step 5: Testing the Wallarm Sidecar Container \u00b6 Get the list of pods using the following command: kubectl get pods The number of containers in the pod should increase, and the status of the pod should be \"Running\". NAME READY STATUS RESTARTS AGE mychart-856f957bbd-cr4kt 2/2 Running 0 3m48s Go to your Wallarm account > Nodes via the link below and make sure that a new node is displayed. This created node is used to filter requests to your application. https://my.wallarm.com/nodes/ for the EU cloud https://us1.my.wallarm.com/nodes/ for the US cloud Send a malicious test attack request to the application as described in these instructions . Go to your Wallarm account > Events via the link below and make sure that an attack is displayed in the list: https://my.wallarm.com/events/ for the EU cloud https://us1.my.wallarm.com/events/ for the US cloud","title":"Kubernetes Deployment Based on Manifests"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/#kubernetes-deployment-based-on-manifests","text":"","title":"Kubernetes Deployment Based on Manifests"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/#prerequisites","text":"Local or cloud (EKS, GKE, AKE, etc) cluster running any version of Kubernetes Application defined in plain Kubernetes manifest files Pod exposed to the public Internet or other potential sources of malicious web and API attacks Kubernetes ingress controller or external load balancer (like AWS ELB or ALB) to add the HTTP request header X-Forwarded-For , which contains the real public IP address of the connecting client Wallarm account in the EU cloud or US cloud Username and password of the user with the Deploy role added to your Wallarm account. To add a new user, please follow these instructions","title":"Prerequisites"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/#installation","text":"Create Wallarm ConfigMap. Update the definition of the Deployment object in Kubernetes. Update the definition of the Service object in Kubernetes. Deploy the manifest to the Kubernetes cluster. Test the Wallarm sidecar container. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of already existing Wallarm WAF or need to duplicate the installation in the same environment, please keep the same WAF version as currently used or update the version of all installations to the latest. The version of deployed Wallarm WAF image is specified in the Deployment template \u2192 spec.template.spec.containers section \u2192 image of the Wallarm container. If the version 3.0.x is specified, follow the current instructions. If the version 2.18.x is specified, follow the instructions for 2.18 or increase the version of the image to 3.0.0-2 in all deployments and follow the current instructions. If the version 2.16.x or lower is specified, please increase the version of the image to 3.0.0-2 in all deployments and follow the current instructions. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"Installation"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/#step-1-creating-wallarm-configmap","text":"Create a new manifest file or add a new object to the existing manifest for a new Kubernetes ConfigMap object that will hold the NGINX configuration file for the Wallarm sidecar container: apiVersion: v1 kind: ConfigMap metadata: name: wallarm-sidecar-nginx-conf data: default: | server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; server_name localhost; root /usr/share/nginx/html; index index.html index.htm; # Please replace <WALLARM_MODE> below by the request filtration mode: # off to disable request processing # monitoring to process but not block requests # safe_blocking to block only those malicious requests originated from greylisted IPs # block to process all requests and block the malicious ones wallarm_mode <WALLARM_MODE>; # wallarm_instance 1; set_real_ip_from 0.0.0.0/0; real_ip_header X-Forwarded-For; location / { # Please replace <APP_CONTAINER_PORT> below by the port number # on which the container accepts incoming requests, # the value must be identical to ports.containerPort # in definition of your main app container proxy_pass http://localhost:<APP_CONTAINER_PORT>; include proxy_params; } } Update parameter values following the code comments.","title":"Step 1: Creating Wallarm ConfigMap"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/#step-2-updating-the-deployment-object-in-kubernetes","text":"Go to the Kubernetes manifests and open the template that defines the Deployment object for the application. A complex application can have several Deployment objects for different components of the application - please find an object which defines pods which are actually exposed to the Internet. For example: apiVersion: apps/v1 kind: Deployment metadata: name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: # Definition of your main app container - name: myapp image: <Image> resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: # Port on which the application container accepts incoming requests - containerPort: 8080 Copy the following elements to the template: the wallarm sidecar container definition to the spec.template.spec.containers section, the wallarm-nginx-conf volume definition to the spec.template.spec.volumes section. An example of the template with added elements is provided below. Elements for copying are indicated by the Wallarm element comment. apiVersion: apps/v1 kind: Deployment metadata: name: myapp spec: selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: # Wallarm element: definition of Wallarm sidecar container - name: wallarm image: wallarm/node:3.0.0-2 imagePullPolicy: Always env: # Wallarm API endpoint: # \"api.wallarm.com\" for the EU cloud # \"us1.api.wallarm.com\" for the US cloud - name: WALLARM_API_HOST value: \"api.wallarm.com\" # Username of the user with the Deploy role - name: DEPLOY_USER value: \"username\" # Password of the user with the Deploy role - name: DEPLOY_PASSWORD value: \"password\" - name: DEPLOY_FORCE value: \"true\" # Amount of memory in GB for request analytics data, # recommended value is 75% of the total server memory - name: TARANTOOL_MEMORY_GB value: \"2\" ports: - name: http # Port on which the Wallarm sidecar container accepts requests # from the Service object containerPort: 80 volumeMounts: - mountPath: /etc/nginx/sites-enabled readOnly: true name: wallarm-nginx-conf # Definition of your main app container - name: myapp image: <Image> resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: # Port on which the application container accepts incoming requests - containerPort: 8080 volumes: # Wallarm element: definition of the wallarm-nginx-conf volume - name: wallarm-nginx-conf configMap: name: wallarm-sidecar-nginx-conf items: - key: default path: default Update parameter values following the code comments.","title":"Step 2: Updating the Deployment Object in Kubernetes"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/#step-3-updating-the-service-object-in-kubernetes","text":"Return to the Kubernetes manifests and open the template that defines the Service object that points to Deployment modified in the previous step. For example: apiVersion: v1 kind: Service metadata: name: myapp labels: run: myapp spec: type: NodePort ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: run: myapp Change the ports.targetPort value to point to the Wallarm sidecar container port ( ports.containerPort defined in the Wallarm sidecar container). For example: ... ports: - port: 80 targetPort: 80 protocol: TCP name: http selector: run: myapp","title":"Step 3: Updating the Service Object in Kubernetes"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/#step-4-deploying-the-manifest-to-the-kubernetes-cluster","text":"Update or deploy the new application manifest in the Kubernetes cluster. NetworkPolicy Object in Kubernetes If the application also uses the NetworkPolicy object it should be updated to reflect the Wallarm sidecar container port specified above.","title":"Step 4: Deploying the Manifest to the Kubernetes Cluster"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container-manifest/#step-5-testing-the-wallarm-sidecar-container","text":"Get the list of pods using the following command: kubectl get pods The number of containers in the pod should increase, and the status of the pod should be \"Running\". NAME READY STATUS RESTARTS AGE mychart-856f957bbd-cr4kt 2/2 Running 0 3m48s Go to your Wallarm account > Nodes via the link below and make sure that a new node is displayed. This created node is used to filter requests to your application. https://my.wallarm.com/nodes/ for the EU cloud https://us1.my.wallarm.com/nodes/ for the US cloud Send a malicious test attack request to the application as described in these instructions . Go to your Wallarm account > Events via the link below and make sure that an attack is displayed in the list: https://my.wallarm.com/events/ for the EU cloud https://us1.my.wallarm.com/events/ for the US cloud","title":"Step 5: Testing the Wallarm Sidecar Container"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container/","text":"How Wallarm Sidecar Container Works \u00b6 Wallarm WAF node installs as a sidecar container to the same pod as the main application container. The WAF node filters incoming requests and forwards valid requests to the application container. Kubernetes runs the sidecar container alongside the main container image. The sidecar container also shares the same lifecycle as the main application container, being created and retired alongside it. See also Types of containers in a Kubernetes pod Traffic Flow \u00b6 Normally, Kubernetes directly exposes a Service object with the ClusterIP or NodePort type to the Internet or other Kubernetes applications. The following are examples of traffic flow for architecture with such a Service object, both with and without a Wallarm sidecar container. Scheme of the traffic flow without the Wallarm sidecar container \u00b6 An application container accepts incoming requests on port 8080/TCP , and the Service object forwards incoming requests to the same port ( 8080/TCP ) on all healthy pods of the application (Kubernetes Deployment object). Scheme of the traffic flow with the Wallarm sidecar container \u00b6 An application container accepts incoming requests on port 8080/TCP and the Service object forwards incoming requests to another port (for example, 80/TCP ) on the Wallarm sidecar container. The Wallarm sidecar container filters requests and forwards the valid ones to the 8080/TCP port on all healthy pods of the application (Kubernetes Deployment object). When a Wallarm WAF node sidecar container is added to a Kubernetes pod it is necessary to change the flow of HTTP requests hitting the pod. A detailed description of changing this flow is provided in the instructions. Wallarm Sidecar Container Installation \u00b6 The method of sidecar container installation depends on the Kubernetes application deployment options. Please select your option below and follow the instructions: Kubernetes deployment based on Helm Charts Kubernetes deployment based on manifests Demo videos \u00b6","title":"How It Works"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container/#how-wallarm-sidecar-container-works","text":"Wallarm WAF node installs as a sidecar container to the same pod as the main application container. The WAF node filters incoming requests and forwards valid requests to the application container. Kubernetes runs the sidecar container alongside the main container image. The sidecar container also shares the same lifecycle as the main application container, being created and retired alongside it. See also Types of containers in a Kubernetes pod","title":"How Wallarm Sidecar Container Works"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container/#traffic-flow","text":"Normally, Kubernetes directly exposes a Service object with the ClusterIP or NodePort type to the Internet or other Kubernetes applications. The following are examples of traffic flow for architecture with such a Service object, both with and without a Wallarm sidecar container.","title":"Traffic Flow"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container/#scheme-of-the-traffic-flow-without-the-wallarm-sidecar-container","text":"An application container accepts incoming requests on port 8080/TCP , and the Service object forwards incoming requests to the same port ( 8080/TCP ) on all healthy pods of the application (Kubernetes Deployment object).","title":"Scheme of the traffic flow without the Wallarm sidecar container"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container/#scheme-of-the-traffic-flow-with-the-wallarm-sidecar-container","text":"An application container accepts incoming requests on port 8080/TCP and the Service object forwards incoming requests to another port (for example, 80/TCP ) on the Wallarm sidecar container. The Wallarm sidecar container filters requests and forwards the valid ones to the 8080/TCP port on all healthy pods of the application (Kubernetes Deployment object). When a Wallarm WAF node sidecar container is added to a Kubernetes pod it is necessary to change the flow of HTTP requests hitting the pod. A detailed description of changing this flow is provided in the instructions.","title":"Scheme of the traffic flow with the Wallarm sidecar container"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container/#wallarm-sidecar-container-installation","text":"The method of sidecar container installation depends on the Kubernetes application deployment options. Please select your option below and follow the instructions: Kubernetes deployment based on Helm Charts Kubernetes deployment based on manifests","title":"Wallarm Sidecar Container Installation"},{"location":"admin-en/installation-guides/kubernetes/wallarm-sidecar-container/#demo-videos","text":"","title":"Demo videos"},{"location":"admin-en/integration-guides/repo-mirroring/centos/how-to-mirror-repo-artifactory/","text":"How to Mirror the Wallarm Repository for CentOS \u00b6 You can create and use a local copy (also known as a mirror ) of the Wallarm repository to be sure that all filter nodes in your infrastructure are deployed from a single source and have the same version number. This document will guide you through the process of mirroring the Wallarm repository for a CentOS 7 server via the JFrog Artifactory repository manager. Prerequisites Make sure that the following conditions are met prior to taking any further steps: You have these components installed on your server: CentOS 7 operating system yum-utils and epel-release packages JFrog Artifactory software capable of creating RPM repositories ( installation instructions ) Learn more about JFrog Artifactory editions and features here . JFrog Artifactory is up and running. The server has internet access. Wallarm repository mirroring comprises Creating a local copy of the Wallarm repository Creating a local RPM repository in JFrog Artifactory Importing the local copy of the Wallarm repository into JFrog Artifactory 1. Creating a Local Copy of the Wallarm Repository \u00b6 To create a local copy of the Wallarm repository, do the following: Add the Wallarm repository by executing the following command: sudo rpm --install https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Navigate to a temporary directory (e.g., /tmp ) and synchronize the Wallarm repository to this directory by executing the following command: reposync -r wallarm-node -p . If the reposync command finishes successfully, then the Wallarm packages will be placed in the wallarm-node/Packages subdirectory of your temporary directory (e.g., /tmp/wallarm-node/Packages ). 2. Creating a Local RPM Repository in JFrog Artifactory \u00b6 To create a local RPM repository in JFrog Artifactory, do the following: Navigate to the JFrog Artifactory web UI via either the domain name or IP address (e.g., http://jfrog.example.local:8081/artifactory ). Log in to the web UI with the administrator account. Click the Admin menu entry, then the Local link in the Repositories section. Click the New button to create a new local repository. Select the \u201cRPM\u201d package type. Fill the repository name in the Repository Key field. This name should be unique in JFrog Artifactory. We recommend choosing a name that complies with the Artifactory repositories naming best practices (e.g., wallarm-centos-upload-local ). Select the \u201cmaven-2-default\u201d layout from the Repository Layout drop-down list. You can leave other settings unchanged. Click the Save & Finish button to create the local Artifactory repository. Now, the newly created repository should be displayed in the local repository list. To finish mirroring the Wallarm repository, import synchronized packages into the local Artifactory repository. 3. Importing the Local Copy of the Wallarm Repository into JFrog Artifactory \u00b6 To import the Wallarm packages into the Artifactory local RPM repository, do the following: Log in to the JFrog Artifactory web UI with the administrator account. Click the Admin menu entry, then the Repositories link in the Import & Export section. In the Import Repository from Path section, select the local repository you created earlier from the Repository from Path drop-down list. Click the Browse button and select the directory with the Wallarm packages you created earlier . Click the Import button to import the Wallarm packages from the directory. Click the Artifacts menu entry, and make sure that the imported Wallarm packages are present in the desired local repository. Now you can deploy Wallarm filter nodes using the local mirror of the Wallarm repository.","title":"How to Mirror the Wallarm Repository for CentOS"},{"location":"admin-en/integration-guides/repo-mirroring/centos/how-to-mirror-repo-artifactory/#how-to-mirror-the-wallarm-repository-for-centos","text":"You can create and use a local copy (also known as a mirror ) of the Wallarm repository to be sure that all filter nodes in your infrastructure are deployed from a single source and have the same version number. This document will guide you through the process of mirroring the Wallarm repository for a CentOS 7 server via the JFrog Artifactory repository manager. Prerequisites Make sure that the following conditions are met prior to taking any further steps: You have these components installed on your server: CentOS 7 operating system yum-utils and epel-release packages JFrog Artifactory software capable of creating RPM repositories ( installation instructions ) Learn more about JFrog Artifactory editions and features here . JFrog Artifactory is up and running. The server has internet access. Wallarm repository mirroring comprises Creating a local copy of the Wallarm repository Creating a local RPM repository in JFrog Artifactory Importing the local copy of the Wallarm repository into JFrog Artifactory","title":"How to Mirror the Wallarm Repository for CentOS"},{"location":"admin-en/integration-guides/repo-mirroring/centos/how-to-mirror-repo-artifactory/#1-creating-a-local-copy-of-the-wallarm-repository","text":"To create a local copy of the Wallarm repository, do the following: Add the Wallarm repository by executing the following command: sudo rpm --install https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Navigate to a temporary directory (e.g., /tmp ) and synchronize the Wallarm repository to this directory by executing the following command: reposync -r wallarm-node -p . If the reposync command finishes successfully, then the Wallarm packages will be placed in the wallarm-node/Packages subdirectory of your temporary directory (e.g., /tmp/wallarm-node/Packages ).","title":"1.  Creating a Local Copy of the Wallarm Repository"},{"location":"admin-en/integration-guides/repo-mirroring/centos/how-to-mirror-repo-artifactory/#2-creating-a-local-rpm-repository-in-jfrog-artifactory","text":"To create a local RPM repository in JFrog Artifactory, do the following: Navigate to the JFrog Artifactory web UI via either the domain name or IP address (e.g., http://jfrog.example.local:8081/artifactory ). Log in to the web UI with the administrator account. Click the Admin menu entry, then the Local link in the Repositories section. Click the New button to create a new local repository. Select the \u201cRPM\u201d package type. Fill the repository name in the Repository Key field. This name should be unique in JFrog Artifactory. We recommend choosing a name that complies with the Artifactory repositories naming best practices (e.g., wallarm-centos-upload-local ). Select the \u201cmaven-2-default\u201d layout from the Repository Layout drop-down list. You can leave other settings unchanged. Click the Save & Finish button to create the local Artifactory repository. Now, the newly created repository should be displayed in the local repository list. To finish mirroring the Wallarm repository, import synchronized packages into the local Artifactory repository.","title":"2.  Creating a Local RPM Repository in JFrog Artifactory"},{"location":"admin-en/integration-guides/repo-mirroring/centos/how-to-mirror-repo-artifactory/#3-importing-the-local-copy-of-the-wallarm-repository-into-jfrog-artifactory","text":"To import the Wallarm packages into the Artifactory local RPM repository, do the following: Log in to the JFrog Artifactory web UI with the administrator account. Click the Admin menu entry, then the Repositories link in the Import & Export section. In the Import Repository from Path section, select the local repository you created earlier from the Repository from Path drop-down list. Click the Browse button and select the directory with the Wallarm packages you created earlier . Click the Import button to import the Wallarm packages from the directory. Click the Artifacts menu entry, and make sure that the imported Wallarm packages are present in the desired local repository. Now you can deploy Wallarm filter nodes using the local mirror of the Wallarm repository.","title":"3.  Importing the Local Copy of the Wallarm Repository into JFrog Artifactory"},{"location":"admin-en/integration-guides/repo-mirroring/centos/how-to-use-mirrored-repo/","text":"How to Install Wallarm Packages from the Local JFrog Artifactory Repository for CentOS \u00b6 To install Wallarm packages from the JFrog Artifactory repository on a host dedicated to a filter node, perform the following actions on this host: Navigate to the JFrog Artifactory web UI via either the domain name or IP address (e.g., http://jfrog.example.local:8081/artifactory ). Log in to the web UI with a user account. Click the Artifacts menu entry and select a repository containing the Wallarm packages. Click the Set Me Up link. A pop-up window will appear. Type your user account\u2019s password in the Type Password field and press Enter . Now, the instructions in this window will contain your credentials. Scroll down to the yum configuration example and click the Copy Snippet to Clipboard button to copy this example to the clipboard. Create a yum configuration file (e.g., /etc/yum.repos.d/artifactory.repo ) and paste the copied snippet into it. Important! Make sure to remove the <PATH_TO_REPODATA_FOLDER> fragment from the baseurl parameter so that the baseurl points to the root of the repository. An example of the /etc/yum.repos.d/artifactory.repo file for the wallarm-centos-upload-local sample repository: [ Artifactory ] name = Artifactory baseurl = http://user:password@jfrog.example.local:8081/artifactory/wallarm-centos-upload-local/ enabled = 1 gpgcheck = 0 #Optional - if you have GPG signing keys installed, use the below flags to verify the repository metadata signature: #gpgkey=http://user:password@jfrog.example.local:8081/artifactory/wallarm-centos-upload-local/<PATH_TO_REPODATA_FOLDER>/repomd.xml.key #repo_gpgcheck=1 Install the epel-release package on the host: sudo yum install epel-release Now you can follow any installation instructions for CentOS. You will need to skip the step where the repository is added because you have set up a local repository instead.","title":"How to Install Wallarm Packages from the Local JFrog Artifactory Repository for CentOS"},{"location":"admin-en/integration-guides/repo-mirroring/centos/how-to-use-mirrored-repo/#how-to-install-wallarm-packages-from-the-local-jfrog-artifactory-repository-for-centos","text":"To install Wallarm packages from the JFrog Artifactory repository on a host dedicated to a filter node, perform the following actions on this host: Navigate to the JFrog Artifactory web UI via either the domain name or IP address (e.g., http://jfrog.example.local:8081/artifactory ). Log in to the web UI with a user account. Click the Artifacts menu entry and select a repository containing the Wallarm packages. Click the Set Me Up link. A pop-up window will appear. Type your user account\u2019s password in the Type Password field and press Enter . Now, the instructions in this window will contain your credentials. Scroll down to the yum configuration example and click the Copy Snippet to Clipboard button to copy this example to the clipboard. Create a yum configuration file (e.g., /etc/yum.repos.d/artifactory.repo ) and paste the copied snippet into it. Important! Make sure to remove the <PATH_TO_REPODATA_FOLDER> fragment from the baseurl parameter so that the baseurl points to the root of the repository. An example of the /etc/yum.repos.d/artifactory.repo file for the wallarm-centos-upload-local sample repository: [ Artifactory ] name = Artifactory baseurl = http://user:password@jfrog.example.local:8081/artifactory/wallarm-centos-upload-local/ enabled = 1 gpgcheck = 0 #Optional - if you have GPG signing keys installed, use the below flags to verify the repository metadata signature: #gpgkey=http://user:password@jfrog.example.local:8081/artifactory/wallarm-centos-upload-local/<PATH_TO_REPODATA_FOLDER>/repomd.xml.key #repo_gpgcheck=1 Install the epel-release package on the host: sudo yum install epel-release Now you can follow any installation instructions for CentOS. You will need to skip the step where the repository is added because you have set up a local repository instead.","title":"How to Install Wallarm Packages from the Local JFrog Artifactory Repository for CentOS"},{"location":"admin-en/monitoring/available-metrics/","text":"Available Metrics \u00b6 Metric Format Types of Wallarm Metrics NGINX Metrics and NGINX Wallarm Module Metrics Postanalytics Module Metrics Metric Format \u00b6 The collectd metrics have the following view: host/plugin[-plugin_instance]/type[-type_instance] A detailed description of the metric format is available at this link . Note In the list of available metrics below, the host name (the host/ part) is omitted. When using the collectd_nagios utility, the host name must be omitted. It is set separately using the -H parameter ( more about using this utility ). Types of Wallarm Metrics \u00b6 The allowed types of Wallarm metrics are described below. The type is stored in the type metric parameter. gauge is a numerical representation of the measured value. The value can both increase and decrease. derive is the rate of change of the measured value since the previous measurement (derived value). The value can both increase and decrease. counter is similar to the gauge metric. The value can only increase. NGINX Metrics and NGINX Wallarm Module Metrics \u00b6 Number of Requests \u00b6 The number of requests processed by the filter node since installation. Metric: curl_json-wallarm_nginx/gauge-requests Metric value: 0 for the off mode >0 for the monitoring / block mode Rate of change: curl_json-wallarm_nginx/derive-requests Troubleshooting recommendations: Check if the filter node settings are correct. Check the filter node operation as described in the instructions . The value should increase by 1 after sending one test attack. Number of Attacks \u00b6 The number of attacks detected by the filter node since installation. Metric: curl_json-wallarm_nginx/gauge-attacks Metric value: 0 for the off mode >0 for the monitoring / block mode Rate of change: curl_json-wallarm_nginx/derive-attacks Troubleshooting recommendations: Check if the filter node settings are correct. Check the filter node operation as described in the instructions . The value should increase by 1 after sending one test attack. Number of Blocked Requests \u00b6 The number of requests blocked by the filter node since installation. This metric is collected if the filter node is in the block mode . Metric: curl_json-wallarm_nginx/gauge-blocked Metric value: 0 for the off / monitoring mode >0 for the block mode Rate of change: curl_json-wallarm_nginx/derive-blocked Troubleshooting recommendations: Check if the filter node settings are correct and make sure the filter node is in the block mode. Check the filter node operation as described in the instructions . The value should increase by 1 after sending one test attack. Number of Abnormal Requests \u00b6 The number of requests that were considered abnormal for the application. Temporarily, the metric collects all requests processed by the filter node. Metric: curl_json-wallarm_nginx/gauge-abnormal Metric value: temporarily equal to gauge-requests Rate of change: curl_json-wallarm_nginx/derive-abnormal Troubleshooting recommendations: temporarily does not matter Number of Lost Requests \u00b6 The number of requests not analyzed by the postanalytics module and not passed to Wallarm API. Blocking rules are applied to these requests, but requests are not visible in your Wallarm account and are not taken into account when analyzing next requests. The number is the sum of tnt_errors and api_errors . Metric: curl_json-wallarm_nginx/gauge-requests_lost Metric value: 0 , the sum of tnt_errors and api_errors Rate of change: curl_json-wallarm_nginx/derive-requests_lost Troubleshooting recommendations: follow the instructions for tnt_errors and api_errors Number of Requests not Analyzed by the Postanalytics Module \u00b6 The number of requests not analyzed by the postanalytics module. This metric is collected if sending requests to the postanalytics module is configured ( wallarm_upstream_backend tarantool ). Blocking rules are applied to these requests, but requests are not visible in your Wallarm account and are not taken into account when analyzing next requests. Metric: curl_json-wallarm_nginx/gauge-tnt_errors Metric value: 0 Rate of change: curl_json-wallarm_nginx/derive-tnt_errors Troubleshooting recommendations: Get the NGINX and Tarantool logs and analyze errors if any. Check if the Tarantool server address ( wallarm_tarantool_upstream ) is correct. Check that enough memory is allocated for Tarantool ( wallarm_ts_request_memory_limit ). Contact the Wallarm support team and provide the data above if the issue is not resolved. Number of Requests not Passed to the Wallarm API \u00b6 The number of requests not passed to Wallarm API. This metric is collected if passing requests to Wallarm API is configured ( wallarm_upstream_backend api ). Blocking rules are applied to these requests, but requests are not visible in your Wallarm account and not taken into account when analyzing next requests. Metric: curl_json-wallarm_nginx/gauge-api_errors Metric value: 0 Rate of change: curl_json-wallarm_nginx/derive-api_errors Troubleshooting recommendations: Get the NGINX and Tarantool logs and analyze errors if any. Check if the Wallarm API settings ( wallarm_api_conf ) are correct. Check that enough memory is allocated for Tarantool ( wallarm_ts_request_memory_limit ). Contact the Wallarm support team and provide the data above if the issue was not resolved. Number of Issues Completed NGINX Worker Process Abnormally \u00b6 A number of issues have led to abnormal completion of the NGINX worker process. The most common reason for abnormal completion is a critical error in NGINX. Metric: curl_json-wallarm_nginx/gauge-segfaults Metric value: 0 Rate of change: curl_json-wallarm_nginx/derive-segfaults Troubleshooting recommendations: Collect data about the current state using the /usr/share/wallarm-common/collect-info.sh script. Provide the generated file to the Wallarm support team for investigation. Number of Situations Exceeding the Virtual Memory Limit \u00b6 The number of situations when the virtual memory limit was exceeded. Metric: curl_json-wallarm_nginx/gauge-memfaults if the limit in your system was exceeded curl_json-wallarm_nginx/gauge-softmemfaults if the limit for proton.db +lom was exceeded ( wallarm_ts_request_memory_limit ) Metric value: 0 Rate of change: curl_json-wallarm_nginx/derive-memfaults for curl_json-wallarm_nginx/gauge-memfaults curl_json-wallarm_nginx/derive-softmemfaults for curl_json-wallarm_nginx/gauge-softmemfaults Troubleshooting recommendations: Collect data about the current state using the /usr/share/wallarm-common/collect-info.sh script. Provide the generated file to the Wallarm support team for investigation. Request Analysis Time (in Seconds) \u00b6 Time spent by the filter node analyzing requests since installation. Metric: curl_json-wallarm_nginx/gauge-time_detect Metric value: >0 Rate of change: curl_json-wallarm_nginx/derive-time_detect Troubleshooting recommendations: Check if the filter node settings are correct. Check the filter node operation as described in the instructions . The value should increase by 1 after sending one test attack. Version of proton.db \u00b6 The version of proton.db in use. Metric: curl_json-wallarm_nginx/gauge-db_id Metric value: no limits Version of LOM \u00b6 The version of LOM in use. Metric: curl_json-wallarm_nginx/gauge-lom_id Metric value: no limits proton.db and LOM Pairs \u00b6 Number of proton.db and LOM Pairs \u00b6 The number of proton.db and LOM pairs in use. Metric: curl_json-wallarm_nginx/gauge-proton_instances-total Metric value: >0 Troubleshooting recommendations: Check if the filter node settings are correct. Check if the path to the proton.db file is specified correctly ( wallarm_global_trainingset_path ). Check if the path to the LOM file is specified correctly ( wallarm_local_trainingset_path ). Number of Successfully Downloaded proton.db and LOM Pairs \u00b6 The number of proton.db and LOM pairs that were successfully downloaded and read. Metric: curl_json-wallarm_nginx/gauge-proton_instances-success Metric value: is equal to proton_instances-total Troubleshooting recommendations: Check if the filter node settings are correct. Check if the path to the proton.db file is specified correctly ( wallarm_global_trainingset_path ). Check if the path to the LOM file is specified correctly ( wallarm_local_trainingset_path ). Number of proton.db and LOM Pairs Downloaded from the Last Saved Files \u00b6 The number of proton.db and LOM pairs downloaded from the last saved files. These files store the last successfully downloaded pairs. If pairs were updated but not downloaded, the data from the last saved files is used. Metric: curl_json-wallarm_nginx/gauge-proton_instances-fallback Metric value: >0 Troubleshooting recommendations: Check if the filter node settings are correct. Check if the path to the proton.db file is specified correctly ( wallarm_global_trainingset_path ). Check if the path to the LOM file is specified correctly ( wallarm_local_trainingset_path ). Number of Inactive proton.db and LOM Pairs \u00b6 The number of connected proton.db and LOM pairs that could not be read. Metric: curl_json-wallarm_nginx/gauge-proton_instances-failed Metric value: 0 Troubleshooting recommendations: Check if the filter node settings are correct. Check if the path to the proton.db file is specified correctly ( wallarm_global_trainingset_path ). Check if the path to the LOM file is specified correctly ( wallarm_local_trainingset_path ). Postanalytics Module Metrics \u00b6 Identifier of the Last Processed Request \u00b6 ID of the last processed request. The value can both increase and decrease. Metric: wallarm-tarantool/counter-last_request_id if the value increased wallarm-tarantool/gauge-last_request_id if the value increased or decreased Metric value: no limits Troubleshooting recommendations: if there are incoming requests but the value does not change, check if the filter node settings are correct Deleted Requests \u00b6 Indication of Deleted Requests \u00b6 The flag signaling that requests with attacks have been deleted from the postanalytics module but not sent to the cloud . Metric: wallarm-tarantool/gauge-export_drops_flag Metric value: 0 if requests are not deleted 1 if requests are deleted (not enough memory, please follow the instructions below) Troubleshooting recommendations: Increase the wallarm_ts_request_memory_limit value. Install the postanalytics module in a separate server pool following these instructions . Number of Deleted Requests \u00b6 The number of requests with attacks that were deleted from the postanalytics module but were not sent to the cloud . The number of attacks in the request does not affect the value. The metric is collected if wallarm-tarantool/gauge-export_drops_flag: 1 . It is recommended to use the wallarm-tarantool/gauge-export_drops_flag metric when configuring monitoring notifications. Metric: wallarm-tarantool/gauge-export_drops Metric value: 0 Rate of change: wallarm-tarantool/derive-export_drops Troubleshooting recommendations: Increase the wallarm_ts_request_memory_limit value. Install the postanalytics module in a separate server pool following the instructions . Request Export Delay (in Seconds) \u00b6 The delay between the recording of a request by the postanalytics module and downloading of the information about detected attacks to the Wallarm cloud. Metric: wallarm-tarantool/gauge-export_delay Metric value: optimal if <60 warning if >60 critical if >300 Troubleshooting recommendations: Read logs from the /var/log/wallarm/export-attacks.log file and analyze errors. An increased value can be caused by low network throughput from the filter node to Wallarm\u2019s API service. Check that enough memory is allocated for Tarantool ( wallarm_ts_request_memory_limit ). The tnt_errors metric also increases when allocated memory is exceeded. Time of Storing Requests in the Postanalytics Module (in Seconds) \u00b6 Time that the postanalytics module stores requests. The value depends on the amount of memory allocated to the postanalytics module and on the size and properties of the processed HTTP requests. The shorter the interval, the worse the detection algorithms work\u2014because they rely on historical data. As a result, if the intervals are too short, an attacker can perform brute force attacks faster and without being noticed. In this case, less data will be obtained on the attacker's behavior history. Metric: wallarm-tarantool/gauge-timeframe_size Metric value: optimal if >900 warning if <900 critical if <300 Troubleshooting recommendations: Increase the wallarm_ts_request_memory_limit value. Install the postanalytics module in a separate server pool following the instructions .","title":"Available Metrics"},{"location":"admin-en/monitoring/available-metrics/#available-metrics","text":"Metric Format Types of Wallarm Metrics NGINX Metrics and NGINX Wallarm Module Metrics Postanalytics Module Metrics","title":"Available Metrics"},{"location":"admin-en/monitoring/available-metrics/#metric-format","text":"The collectd metrics have the following view: host/plugin[-plugin_instance]/type[-type_instance] A detailed description of the metric format is available at this link . Note In the list of available metrics below, the host name (the host/ part) is omitted. When using the collectd_nagios utility, the host name must be omitted. It is set separately using the -H parameter ( more about using this utility ).","title":"Metric Format"},{"location":"admin-en/monitoring/available-metrics/#types-of-wallarm-metrics","text":"The allowed types of Wallarm metrics are described below. The type is stored in the type metric parameter. gauge is a numerical representation of the measured value. The value can both increase and decrease. derive is the rate of change of the measured value since the previous measurement (derived value). The value can both increase and decrease. counter is similar to the gauge metric. The value can only increase.","title":"Types of Wallarm Metrics"},{"location":"admin-en/monitoring/available-metrics/#nginx-metrics-and-nginx-wallarm-module-metrics","text":"","title":"NGINX Metrics and NGINX Wallarm Module Metrics"},{"location":"admin-en/monitoring/available-metrics/#number-of-requests","text":"The number of requests processed by the filter node since installation. Metric: curl_json-wallarm_nginx/gauge-requests Metric value: 0 for the off mode >0 for the monitoring / block mode Rate of change: curl_json-wallarm_nginx/derive-requests Troubleshooting recommendations: Check if the filter node settings are correct. Check the filter node operation as described in the instructions . The value should increase by 1 after sending one test attack.","title":"Number of Requests"},{"location":"admin-en/monitoring/available-metrics/#number-of-attacks","text":"The number of attacks detected by the filter node since installation. Metric: curl_json-wallarm_nginx/gauge-attacks Metric value: 0 for the off mode >0 for the monitoring / block mode Rate of change: curl_json-wallarm_nginx/derive-attacks Troubleshooting recommendations: Check if the filter node settings are correct. Check the filter node operation as described in the instructions . The value should increase by 1 after sending one test attack.","title":"Number of Attacks"},{"location":"admin-en/monitoring/available-metrics/#number-of-blocked-requests","text":"The number of requests blocked by the filter node since installation. This metric is collected if the filter node is in the block mode . Metric: curl_json-wallarm_nginx/gauge-blocked Metric value: 0 for the off / monitoring mode >0 for the block mode Rate of change: curl_json-wallarm_nginx/derive-blocked Troubleshooting recommendations: Check if the filter node settings are correct and make sure the filter node is in the block mode. Check the filter node operation as described in the instructions . The value should increase by 1 after sending one test attack.","title":"Number of Blocked Requests"},{"location":"admin-en/monitoring/available-metrics/#number-of-abnormal-requests","text":"The number of requests that were considered abnormal for the application. Temporarily, the metric collects all requests processed by the filter node. Metric: curl_json-wallarm_nginx/gauge-abnormal Metric value: temporarily equal to gauge-requests Rate of change: curl_json-wallarm_nginx/derive-abnormal Troubleshooting recommendations: temporarily does not matter","title":"Number of Abnormal Requests"},{"location":"admin-en/monitoring/available-metrics/#number-of-lost-requests","text":"The number of requests not analyzed by the postanalytics module and not passed to Wallarm API. Blocking rules are applied to these requests, but requests are not visible in your Wallarm account and are not taken into account when analyzing next requests. The number is the sum of tnt_errors and api_errors . Metric: curl_json-wallarm_nginx/gauge-requests_lost Metric value: 0 , the sum of tnt_errors and api_errors Rate of change: curl_json-wallarm_nginx/derive-requests_lost Troubleshooting recommendations: follow the instructions for tnt_errors and api_errors","title":"Number of Lost Requests"},{"location":"admin-en/monitoring/available-metrics/#number-of-requests-not-analyzed-by-the-postanalytics-module","text":"The number of requests not analyzed by the postanalytics module. This metric is collected if sending requests to the postanalytics module is configured ( wallarm_upstream_backend tarantool ). Blocking rules are applied to these requests, but requests are not visible in your Wallarm account and are not taken into account when analyzing next requests. Metric: curl_json-wallarm_nginx/gauge-tnt_errors Metric value: 0 Rate of change: curl_json-wallarm_nginx/derive-tnt_errors Troubleshooting recommendations: Get the NGINX and Tarantool logs and analyze errors if any. Check if the Tarantool server address ( wallarm_tarantool_upstream ) is correct. Check that enough memory is allocated for Tarantool ( wallarm_ts_request_memory_limit ). Contact the Wallarm support team and provide the data above if the issue is not resolved.","title":"Number of Requests not Analyzed by the Postanalytics Module"},{"location":"admin-en/monitoring/available-metrics/#number-of-requests-not-passed-to-the-wallarm-api","text":"The number of requests not passed to Wallarm API. This metric is collected if passing requests to Wallarm API is configured ( wallarm_upstream_backend api ). Blocking rules are applied to these requests, but requests are not visible in your Wallarm account and not taken into account when analyzing next requests. Metric: curl_json-wallarm_nginx/gauge-api_errors Metric value: 0 Rate of change: curl_json-wallarm_nginx/derive-api_errors Troubleshooting recommendations: Get the NGINX and Tarantool logs and analyze errors if any. Check if the Wallarm API settings ( wallarm_api_conf ) are correct. Check that enough memory is allocated for Tarantool ( wallarm_ts_request_memory_limit ). Contact the Wallarm support team and provide the data above if the issue was not resolved.","title":"Number of Requests not Passed to the Wallarm API"},{"location":"admin-en/monitoring/available-metrics/#number-of-issues-completed-nginx-worker-process-abnormally","text":"A number of issues have led to abnormal completion of the NGINX worker process. The most common reason for abnormal completion is a critical error in NGINX. Metric: curl_json-wallarm_nginx/gauge-segfaults Metric value: 0 Rate of change: curl_json-wallarm_nginx/derive-segfaults Troubleshooting recommendations: Collect data about the current state using the /usr/share/wallarm-common/collect-info.sh script. Provide the generated file to the Wallarm support team for investigation.","title":"Number of Issues Completed NGINX Worker Process Abnormally"},{"location":"admin-en/monitoring/available-metrics/#number-of-situations-exceeding-the-virtual-memory-limit","text":"The number of situations when the virtual memory limit was exceeded. Metric: curl_json-wallarm_nginx/gauge-memfaults if the limit in your system was exceeded curl_json-wallarm_nginx/gauge-softmemfaults if the limit for proton.db +lom was exceeded ( wallarm_ts_request_memory_limit ) Metric value: 0 Rate of change: curl_json-wallarm_nginx/derive-memfaults for curl_json-wallarm_nginx/gauge-memfaults curl_json-wallarm_nginx/derive-softmemfaults for curl_json-wallarm_nginx/gauge-softmemfaults Troubleshooting recommendations: Collect data about the current state using the /usr/share/wallarm-common/collect-info.sh script. Provide the generated file to the Wallarm support team for investigation.","title":"Number of Situations Exceeding the Virtual Memory Limit"},{"location":"admin-en/monitoring/available-metrics/#request-analysis-time-in-seconds","text":"Time spent by the filter node analyzing requests since installation. Metric: curl_json-wallarm_nginx/gauge-time_detect Metric value: >0 Rate of change: curl_json-wallarm_nginx/derive-time_detect Troubleshooting recommendations: Check if the filter node settings are correct. Check the filter node operation as described in the instructions . The value should increase by 1 after sending one test attack.","title":"Request Analysis Time (in Seconds)"},{"location":"admin-en/monitoring/available-metrics/#version-of-protondb","text":"The version of proton.db in use. Metric: curl_json-wallarm_nginx/gauge-db_id Metric value: no limits","title":"Version of proton.db"},{"location":"admin-en/monitoring/available-metrics/#version-of-lom","text":"The version of LOM in use. Metric: curl_json-wallarm_nginx/gauge-lom_id Metric value: no limits","title":"Version of LOM"},{"location":"admin-en/monitoring/available-metrics/#protondb-and-lom-pairs","text":"","title":"proton.db and LOM Pairs"},{"location":"admin-en/monitoring/available-metrics/#number-of-protondb-and-lom-pairs","text":"The number of proton.db and LOM pairs in use. Metric: curl_json-wallarm_nginx/gauge-proton_instances-total Metric value: >0 Troubleshooting recommendations: Check if the filter node settings are correct. Check if the path to the proton.db file is specified correctly ( wallarm_global_trainingset_path ). Check if the path to the LOM file is specified correctly ( wallarm_local_trainingset_path ).","title":"Number of proton.db and LOM Pairs"},{"location":"admin-en/monitoring/available-metrics/#number-of-successfully-downloaded-protondb-and-lom-pairs","text":"The number of proton.db and LOM pairs that were successfully downloaded and read. Metric: curl_json-wallarm_nginx/gauge-proton_instances-success Metric value: is equal to proton_instances-total Troubleshooting recommendations: Check if the filter node settings are correct. Check if the path to the proton.db file is specified correctly ( wallarm_global_trainingset_path ). Check if the path to the LOM file is specified correctly ( wallarm_local_trainingset_path ).","title":"Number of Successfully Downloaded proton.db and LOM Pairs"},{"location":"admin-en/monitoring/available-metrics/#number-of-protondb-and-lom-pairs-downloaded-from-the-last-saved-files","text":"The number of proton.db and LOM pairs downloaded from the last saved files. These files store the last successfully downloaded pairs. If pairs were updated but not downloaded, the data from the last saved files is used. Metric: curl_json-wallarm_nginx/gauge-proton_instances-fallback Metric value: >0 Troubleshooting recommendations: Check if the filter node settings are correct. Check if the path to the proton.db file is specified correctly ( wallarm_global_trainingset_path ). Check if the path to the LOM file is specified correctly ( wallarm_local_trainingset_path ).","title":"Number of proton.db and LOM Pairs Downloaded from the Last Saved Files"},{"location":"admin-en/monitoring/available-metrics/#number-of-inactive-protondb-and-lom-pairs","text":"The number of connected proton.db and LOM pairs that could not be read. Metric: curl_json-wallarm_nginx/gauge-proton_instances-failed Metric value: 0 Troubleshooting recommendations: Check if the filter node settings are correct. Check if the path to the proton.db file is specified correctly ( wallarm_global_trainingset_path ). Check if the path to the LOM file is specified correctly ( wallarm_local_trainingset_path ).","title":"Number of Inactive proton.db and LOM Pairs"},{"location":"admin-en/monitoring/available-metrics/#postanalytics-module-metrics","text":"","title":"Postanalytics Module Metrics"},{"location":"admin-en/monitoring/available-metrics/#identifier-of-the-last-processed-request","text":"ID of the last processed request. The value can both increase and decrease. Metric: wallarm-tarantool/counter-last_request_id if the value increased wallarm-tarantool/gauge-last_request_id if the value increased or decreased Metric value: no limits Troubleshooting recommendations: if there are incoming requests but the value does not change, check if the filter node settings are correct","title":"Identifier of the Last Processed Request"},{"location":"admin-en/monitoring/available-metrics/#deleted-requests","text":"","title":"Deleted Requests"},{"location":"admin-en/monitoring/available-metrics/#indication-of-deleted-requests","text":"The flag signaling that requests with attacks have been deleted from the postanalytics module but not sent to the cloud . Metric: wallarm-tarantool/gauge-export_drops_flag Metric value: 0 if requests are not deleted 1 if requests are deleted (not enough memory, please follow the instructions below) Troubleshooting recommendations: Increase the wallarm_ts_request_memory_limit value. Install the postanalytics module in a separate server pool following these instructions .","title":"Indication of Deleted Requests"},{"location":"admin-en/monitoring/available-metrics/#number-of-deleted-requests","text":"The number of requests with attacks that were deleted from the postanalytics module but were not sent to the cloud . The number of attacks in the request does not affect the value. The metric is collected if wallarm-tarantool/gauge-export_drops_flag: 1 . It is recommended to use the wallarm-tarantool/gauge-export_drops_flag metric when configuring monitoring notifications. Metric: wallarm-tarantool/gauge-export_drops Metric value: 0 Rate of change: wallarm-tarantool/derive-export_drops Troubleshooting recommendations: Increase the wallarm_ts_request_memory_limit value. Install the postanalytics module in a separate server pool following the instructions .","title":"Number of Deleted Requests"},{"location":"admin-en/monitoring/available-metrics/#request-export-delay-in-seconds","text":"The delay between the recording of a request by the postanalytics module and downloading of the information about detected attacks to the Wallarm cloud. Metric: wallarm-tarantool/gauge-export_delay Metric value: optimal if <60 warning if >60 critical if >300 Troubleshooting recommendations: Read logs from the /var/log/wallarm/export-attacks.log file and analyze errors. An increased value can be caused by low network throughput from the filter node to Wallarm\u2019s API service. Check that enough memory is allocated for Tarantool ( wallarm_ts_request_memory_limit ). The tnt_errors metric also increases when allocated memory is exceeded.","title":"Request Export Delay (in Seconds)"},{"location":"admin-en/monitoring/available-metrics/#time-of-storing-requests-in-the-postanalytics-module-in-seconds","text":"Time that the postanalytics module stores requests. The value depends on the amount of memory allocated to the postanalytics module and on the size and properties of the processed HTTP requests. The shorter the interval, the worse the detection algorithms work\u2014because they rely on historical data. As a result, if the intervals are too short, an attacker can perform brute force attacks faster and without being noticed. In this case, less data will be obtained on the attacker's behavior history. Metric: wallarm-tarantool/gauge-timeframe_size Metric value: optimal if >900 warning if <900 critical if <300 Troubleshooting recommendations: Increase the wallarm_ts_request_memory_limit value. Install the postanalytics module in a separate server pool following the instructions .","title":"Time of Storing Requests in the Postanalytics Module (in Seconds)"},{"location":"admin-en/monitoring/collectd-nagios/","text":"Exporting Metrics to Nagios via the collectd-nagios Utility \u00b6 This document provides an example of exporting filter node metrics to the Nagios monitoring system (the Nagios Core edition is suggested; however, this document is suitable for any Nagios edition) using the collectd-nagios utility. Assumptions and requirements The collectd service must be configured for working via a Unix domain socket (see here for details). It is assumed that you already have the Nagios Core edition installed. If not, install Nagios Core (for example, follow these instructions ). You can use another edition of Nagios if necessary (for example, Nagios XI). The \u201cNagios\u201d term will be used hereinafter to refer to any edition of Nagios, unless stated otherwise. You must have the ability to connect to the filter node and the Nagios host (for example, via the SSH protocol), and work under the root account or another account with superuser rights. The Nagios Remote Plugin Executor service (which will be referred to as NRPE throughout this example) must be installed on the filter node. Example Workflow \u00b6 Example of metric This example shows how to work with the single curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the filter node. The following deployment scheme is used in this document: The Wallarm filter node is deployed on a host accessible via the 10.0.30.5 IP address and the node.example.local fully qualified domain name. Nagios is installed on a separate host accessible via the 10.0.30.30 IP address. To execute commands on a remote host, the NRPE plugin is used. The plugin comprises The nrpe service that is installed on the monitored host alongside the filter node. It listens on the 5666/TCP standard NRPE port. The check_nrpe NRPE Nagios plugin that is installed on the Nagios host and allows Nagios to execute commands on the remote host where the nrpe service is installed. NRPE will be used to call the collectd_nagios utility that provides the collectd metrics in a Nagios\u2011compatible format. Configuring Metrics Export to Nagios \u00b6 A note on this installation example This document describes how to install and configure the NRPE plugin when Nagios is already installed with default parameters (it is assumed that Nagios is installed in the /usr/local/nagios directory, and uses the nagios user to operate). If you are doing a non-default installation of the plugin or Nagios, adjust the corresponding commands and instructions from the document as needed. To configure metrics export from the filter node to Nagios, follow these steps: 1. Configure NRPE to Communicate with the Nagios Host \u00b6 To do this, on a filter node host: Open the NRPE configuration file (default: /usr/local/nagios/etc/nrpe.cfg ). Add the IP address or fully qualified domain name of the Nagios server to the allowed_hosts directive in this file. For example, if the Nagios host uses the 10.0.30.30 IP address: allowed_hosts=127.0.0.1,10.0.30.30 Restart the NRPE service by executing the appropriate command: Ubuntu 14.04 sudo service nrpe restart Other supported distributions sudo systemctl restart nrpe 2. Install the Nagios NRPE Plugin on the Nagios Host \u00b6 To do this, on the Nagios host, take the following steps: Download and unzip the source files for the NRPE plugin, and install the necessary utilities to build and install the plugin (see the NRPE documentation for details). Go to the directory with the plugin source code, build from sources, then install the plugin. The minimal steps to take are: ./configure make all make install-plugin 3. Make Sure the NRPE Nagios Plugin Successfully Interacts with the NRPE Service \u00b6 To do this, execute the following command on the Nagios host: /usr/local/nagios/libexec/check_nrpe -H node.example.local If NRPE is operating normally, the command\u2019s output should contain an NRPE version (e.g., NRPE v3.2.1 ). 4. Define the check_nrpe Command to Run the NRPE Nagios Plugin with a Single Argument on the Nagios Host \u00b6 To do this, add to the /usr/local/nagios/etc/objects/commands.cfg file the following lines: define command{ command_name check_nrpe command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$ } 5. Install the collectd_nagios Utility on the Filter Node Host \u00b6 Execute one of the following commands: DEB-based distributions sudo apt install --no-install-recommends collectd-utils RPM-based distributions sudo yum install collectd-utils 6. Configure the collectd-nagios Utility to Run with Elevated Privileges on Behalf of the nagios User \u00b6 To do this, perform the following steps on the filter node host: Using the visudo utility, add the following line to the /etc/sudoers file: nagios ALL=(ALL:ALL) NOPASSWD:/usr/bin/collectd-nagios This allows the nagios user to run the collectd-nagios utility with superuser privileges using sudo without the need to provide any passwords. Running collectd-nagios with superuser privileges The utility must be run with superuser privileges because it uses the collectd Unix domain socket to receive data. Only a superuser can access this socket. Make sure that the nagios user can receive metric values from collectd by executing the following test command: sudo -u nagios sudo /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H node.example.local This command allows the nagios user to get the value of the curl_json-wallarm_nginx/gauge-attacks metric (the number of recorded attacks) for the node.example.local host. Example of command output: OKAY: 0 critical, 0 warning, 1 okay | value=0.000000;;;; Add a prefix to the NRPE service configuration file so that it will be able to execute commands using the sudo utility: command_prefix=/usr/bin/sudo 7. Add Commands to the NRPE Service Configuration File on the Filter Node to Get the Required Metrics \u00b6 For example, to create a command named check_wallarm_nginx_attacks that will receive the curl_json-wallarm_nginx/gauge-attacks metric for the filter node with the node.example.local fully qualified domain name, add the following line to the NRPE service\u2019s configuration file: command[check_wallarm_nginx_attacks]=/usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H node.example.local How to set threshold values for a metric If necessary, you can specify a range of values for which the collectd-nagios utility will return the WARNING or CRITICAL status by using the corresponding -w and -c options (detailed information is available in the utility documentation ). After you have added all necessary commands to the NRPE service configuration file, restart the service by executing the appropriate command: Ubuntu 14.04 sudo service nrpe restart Other supported distributions sudo systemctl restart nrpe 8. On the Nagios Host, Use the Configuration Files to Specify the Filter Node Host and to Define the Services to Monitor \u00b6 Services and Metrics This document assumes that one Nagios service is equivalent to one metric. For example, this can be done as follows: Create a /usr/local/nagios/etc/objects/nodes.cfg file with the following contents: define host{ use linux-server host_name node.example.local address 10.0.30.5 } define service { use generic-service host_name node.example.local check_command check_nrpe!check_wallarm_nginx_attacks max_check_attempts 5 service_description wallarm_nginx_attacks } This file defines the node.example.local host with the 10.0.30.5 IP address and the command to check the status of the wallarm_nginx_attacks service, which means receiving the curl_json-wallarm_nginx/gauge-attacks metric from the filter node (see the description of the check_wallarm_nginx_attacks command). Add the following line to the Nagios configuration file (by default, /usr/local/nagios/etc/nagios.cfg ): cfg_file=/usr/local/nagios/etc/objects/nodes.cfg This is necessary for Nagios to start using the data from the nodes.cfg file on the next start. Restart the Nagios service by running the appropriate command: Ubuntu 14.04 sudo service nagios restart Other supported distributions sudo systemctl restart nagios Setup is Complete \u00b6 Nagios is now monitoring the service associated with the specific metric of the filter node. If necessary, you can define other commands and services to check the metrics you are interested in. Information about NRPE Sources of additional information about NRPE: README of the NRPE on GitHub; NRPE documentation ( PDF ).","title":"Exporting Metrics to Nagios via the `collectd-nagios` Utility"},{"location":"admin-en/monitoring/collectd-nagios/#exporting-metrics-to-nagios-via-the-collectd-nagios-utility","text":"This document provides an example of exporting filter node metrics to the Nagios monitoring system (the Nagios Core edition is suggested; however, this document is suitable for any Nagios edition) using the collectd-nagios utility. Assumptions and requirements The collectd service must be configured for working via a Unix domain socket (see here for details). It is assumed that you already have the Nagios Core edition installed. If not, install Nagios Core (for example, follow these instructions ). You can use another edition of Nagios if necessary (for example, Nagios XI). The \u201cNagios\u201d term will be used hereinafter to refer to any edition of Nagios, unless stated otherwise. You must have the ability to connect to the filter node and the Nagios host (for example, via the SSH protocol), and work under the root account or another account with superuser rights. The Nagios Remote Plugin Executor service (which will be referred to as NRPE throughout this example) must be installed on the filter node.","title":"Exporting Metrics to Nagios via the collectd-nagios Utility"},{"location":"admin-en/monitoring/collectd-nagios/#example-workflow","text":"Example of metric This example shows how to work with the single curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the filter node. The following deployment scheme is used in this document: The Wallarm filter node is deployed on a host accessible via the 10.0.30.5 IP address and the node.example.local fully qualified domain name. Nagios is installed on a separate host accessible via the 10.0.30.30 IP address. To execute commands on a remote host, the NRPE plugin is used. The plugin comprises The nrpe service that is installed on the monitored host alongside the filter node. It listens on the 5666/TCP standard NRPE port. The check_nrpe NRPE Nagios plugin that is installed on the Nagios host and allows Nagios to execute commands on the remote host where the nrpe service is installed. NRPE will be used to call the collectd_nagios utility that provides the collectd metrics in a Nagios\u2011compatible format.","title":"Example Workflow"},{"location":"admin-en/monitoring/collectd-nagios/#configuring-metrics-export-to-nagios","text":"A note on this installation example This document describes how to install and configure the NRPE plugin when Nagios is already installed with default parameters (it is assumed that Nagios is installed in the /usr/local/nagios directory, and uses the nagios user to operate). If you are doing a non-default installation of the plugin or Nagios, adjust the corresponding commands and instructions from the document as needed. To configure metrics export from the filter node to Nagios, follow these steps:","title":"Configuring Metrics Export to Nagios"},{"location":"admin-en/monitoring/collectd-nagios/#1-configure-nrpe-to-communicate-with-the-nagios-host","text":"To do this, on a filter node host: Open the NRPE configuration file (default: /usr/local/nagios/etc/nrpe.cfg ). Add the IP address or fully qualified domain name of the Nagios server to the allowed_hosts directive in this file. For example, if the Nagios host uses the 10.0.30.30 IP address: allowed_hosts=127.0.0.1,10.0.30.30 Restart the NRPE service by executing the appropriate command: Ubuntu 14.04 sudo service nrpe restart Other supported distributions sudo systemctl restart nrpe","title":"1.  Configure NRPE to Communicate with the Nagios Host"},{"location":"admin-en/monitoring/collectd-nagios/#2-install-the-nagios-nrpe-plugin-on-the-nagios-host","text":"To do this, on the Nagios host, take the following steps: Download and unzip the source files for the NRPE plugin, and install the necessary utilities to build and install the plugin (see the NRPE documentation for details). Go to the directory with the plugin source code, build from sources, then install the plugin. The minimal steps to take are: ./configure make all make install-plugin","title":"2.  Install the Nagios NRPE Plugin on the Nagios Host"},{"location":"admin-en/monitoring/collectd-nagios/#3-make-sure-the-nrpe-nagios-plugin-successfully-interacts-with-the-nrpe-service","text":"To do this, execute the following command on the Nagios host: /usr/local/nagios/libexec/check_nrpe -H node.example.local If NRPE is operating normally, the command\u2019s output should contain an NRPE version (e.g., NRPE v3.2.1 ).","title":"3.  Make Sure the NRPE Nagios Plugin Successfully Interacts with the NRPE Service"},{"location":"admin-en/monitoring/collectd-nagios/#4-define-the-check_nrpe-command-to-run-the-nrpe-nagios-plugin-with-a-single-argument-on-the-nagios-host","text":"To do this, add to the /usr/local/nagios/etc/objects/commands.cfg file the following lines: define command{ command_name check_nrpe command_line $USER1$/check_nrpe -H $HOSTADDRESS$ -c $ARG1$ }","title":"4.  Define the check_nrpe Command to Run the NRPE Nagios Plugin with a Single Argument on the Nagios Host"},{"location":"admin-en/monitoring/collectd-nagios/#5-install-the-collectd_nagios-utility-on-the-filter-node-host","text":"Execute one of the following commands: DEB-based distributions sudo apt install --no-install-recommends collectd-utils RPM-based distributions sudo yum install collectd-utils","title":"5. Install the collectd_nagios Utility on the Filter Node Host"},{"location":"admin-en/monitoring/collectd-nagios/#6-configure-the-collectd-nagios-utility-to-run-with-elevated-privileges-on-behalf-of-the-nagios-user","text":"To do this, perform the following steps on the filter node host: Using the visudo utility, add the following line to the /etc/sudoers file: nagios ALL=(ALL:ALL) NOPASSWD:/usr/bin/collectd-nagios This allows the nagios user to run the collectd-nagios utility with superuser privileges using sudo without the need to provide any passwords. Running collectd-nagios with superuser privileges The utility must be run with superuser privileges because it uses the collectd Unix domain socket to receive data. Only a superuser can access this socket. Make sure that the nagios user can receive metric values from collectd by executing the following test command: sudo -u nagios sudo /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H node.example.local This command allows the nagios user to get the value of the curl_json-wallarm_nginx/gauge-attacks metric (the number of recorded attacks) for the node.example.local host. Example of command output: OKAY: 0 critical, 0 warning, 1 okay | value=0.000000;;;; Add a prefix to the NRPE service configuration file so that it will be able to execute commands using the sudo utility: command_prefix=/usr/bin/sudo","title":"6.  Configure the collectd-nagios Utility to Run with Elevated Privileges on Behalf of the nagios User"},{"location":"admin-en/monitoring/collectd-nagios/#7-add-commands-to-the-nrpe-service-configuration-file-on-the-filter-node-to-get-the-required-metrics","text":"For example, to create a command named check_wallarm_nginx_attacks that will receive the curl_json-wallarm_nginx/gauge-attacks metric for the filter node with the node.example.local fully qualified domain name, add the following line to the NRPE service\u2019s configuration file: command[check_wallarm_nginx_attacks]=/usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H node.example.local How to set threshold values for a metric If necessary, you can specify a range of values for which the collectd-nagios utility will return the WARNING or CRITICAL status by using the corresponding -w and -c options (detailed information is available in the utility documentation ). After you have added all necessary commands to the NRPE service configuration file, restart the service by executing the appropriate command: Ubuntu 14.04 sudo service nrpe restart Other supported distributions sudo systemctl restart nrpe","title":"7.  Add Commands to the NRPE Service Configuration File on the Filter Node to Get the Required Metrics"},{"location":"admin-en/monitoring/collectd-nagios/#8-on-the-nagios-host-use-the-configuration-files-to-specify-the-filter-node-host-and-to-define-the-services-to-monitor","text":"Services and Metrics This document assumes that one Nagios service is equivalent to one metric. For example, this can be done as follows: Create a /usr/local/nagios/etc/objects/nodes.cfg file with the following contents: define host{ use linux-server host_name node.example.local address 10.0.30.5 } define service { use generic-service host_name node.example.local check_command check_nrpe!check_wallarm_nginx_attacks max_check_attempts 5 service_description wallarm_nginx_attacks } This file defines the node.example.local host with the 10.0.30.5 IP address and the command to check the status of the wallarm_nginx_attacks service, which means receiving the curl_json-wallarm_nginx/gauge-attacks metric from the filter node (see the description of the check_wallarm_nginx_attacks command). Add the following line to the Nagios configuration file (by default, /usr/local/nagios/etc/nagios.cfg ): cfg_file=/usr/local/nagios/etc/objects/nodes.cfg This is necessary for Nagios to start using the data from the nodes.cfg file on the next start. Restart the Nagios service by running the appropriate command: Ubuntu 14.04 sudo service nagios restart Other supported distributions sudo systemctl restart nagios","title":"8.  On the Nagios Host, Use the Configuration Files to Specify the Filter Node Host and to Define the Services to Monitor"},{"location":"admin-en/monitoring/collectd-nagios/#setup-is-complete","text":"Nagios is now monitoring the service associated with the specific metric of the filter node. If necessary, you can define other commands and services to check the metrics you are interested in. Information about NRPE Sources of additional information about NRPE: README of the NRPE on GitHub; NRPE documentation ( PDF ).","title":"Setup is Complete"},{"location":"admin-en/monitoring/collectd-zabbix/","text":"Exporting Metrics to Zabbix via the collectd-nagios Utility \u00b6 This document provides an example of exporting filter node metrics to the Zabbix monitoring system using the collectd-nagios utility. Example Workflow \u00b6 Example of metric This example shows how to work with the single curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the filter node. The following deployment scheme is used in this document: The Wallarm filter node is deployed on a host accessible via the 10.0.30.5 IP address and the node.example.local fully qualified domain name. The host has the Zabbix agent 4.0 LTS deployed that Downloads the filter node metrics using the collectd-nagios utility. Listens to incoming connections on the 10050/TCP port (thus passive checks will take place with the use of Zabbix Appliance). Passes metric values to Zabbix Appliance. On a dedicated host with the 10.0.30.30 IP address (hereinafter referred to as the Docker host), the Zabbix Appliance 4.0 LTS is deployed in the form of a Docker container. The Zabbix Appliance includes A Zabbix server that periodically polls the Zabbix agent installed on the filter node host to get information about changes to any monitored metrics). The Zabbix server management web interface, available on the 80/TCP port. Configuring Metrics Export to Zabbix \u00b6 Prerequisites It is assumed that The collectd service has been configured for working via a Unix domain socket (see here for details). Docker Community Edition is already installed on the 10.0.30.30 Docker host. The node.example.local filter node is already deployed, configured, available for further configuration (for example, via the SSH protocol), and working. Deploying Zabbix \u00b6 To deploy the Zabbix Appliance 4.0 LTS, execute the following command on the Docker host: docker run --name zabbix-appliance -p 80 :80 -d zabbix/zabbix-appliance:alpine-4.0-latest Now you have a working Zabbix monitoring system. Deploying the Zabbix Agent \u00b6 Install the Zabbix Agent 4.0 LTS on a host with the filter node: Connect to the filter node (for example, using the SSH protocol). Make sure you are running as root or another account with superuser privileges. Connect the Zabbix repositories (use the \u201cInstall Zabbix repository\u201d entry of the instructions for your operating system). Install the Zabbix agent by executing the appropriate command: DEB-based distributions sudo apt install zabbix-agent RPM-based distributions sudo yum install zabbix-agent Configure the Zabbix Agent to work with the Zabbix Appliance. To do this, make the following changes to the /etc/zabbix/zabbix_agentd.conf configuration file: Server=10.0.30.30 # Zabbix IP address Hostname=node.example.local # FQDN of the host with the filter node Configuring Metrics Collection Using the Zabbix Agent \u00b6 Connect to the filter node (for example, using the SSH protocol) and configure the collection of metrics using the Zabbix agent. To do this, perform the following steps on the host with the filter node: 1. Install the collectd_nagios utility \u00b6 Execute the appropriate command: DEB-based distributions sudo apt install --no-install-recommends collectd-utils RPM-based distributions sudo yum install collectd-utils 2. Configure the collectd-nagios utility to run with elevated privileges on behalf of the zabbix user \u00b6 Use the visudo utility to add the following line to the /etc/sudoers file: zabbix ALL=(ALL:ALL) NOPASSWD:/usr/bin/collectd-nagios This allows the zabbix user to run the collectd-nagios utility with superuser privileges using the sudo utility without the need to provide a password. Running collectd-nagios with superuser privileges The utility must be run with superuser privileges because it uses the collectd Unix domain socket to receive data. Only a superuser can access this socket. As an alternative to adding the zabbix user to the sudoers list, you can configure the Zabbix agent to run as root (this may pose a security risk, so this is not recommended). This can be achieved by enabling the AllowRoot option in the agent configuration file. 3. Make sure that the zabbix user can receive metric values from collectd \u00b6 Run the following test command on the filter node: sudo -u zabbix sudo /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H node.example.local This command invokes the zabbix user to get the value of the curl_json-wallarm_nginx/gauge-attacks metric for the node.example.local host with the filter node. Example of the command output: OKAY: 0 critical, 0 warning, 1 okay | value=0.000000;;;; 4. Add custom parameters to the Zabbix agent configuration file on the filter node host to get the metrics you need \u00b6 For example, to create a custom parameter wallarm_nginx-gauge-attacks that corresponds to the curl_json-wallarm_nginx/gauge-attacks metric for a filter node with the fully qualified domain name node.example.local , add the following line to the configuration file: Extracting a metric value To extract the value of a metric that goes after value= in the output of the collectd-nagios utility (e.g., OKAY: 0 critical, 0 warning, 1 okay | value=0.000000;;;; ), this output is piped to the sed utility that executes the sed script to strip off unnecessary characters. See the sed documentation for more information on the syntax of its scripts. 5. After all the necessary commands have been added to the Zabbix agent configuration file, restart the agent \u00b6 Ubuntu 14.04 sudo service zabbix-agent restart Other supported distributions sudo systemctl restart zabbix-agent Setup Complete \u00b6 Now you can monitor user parameters related to Wallarm-specific metrics with Zabbix.","title":"Exporting Metrics to Zabbix via the `collectd-nagios` Utility"},{"location":"admin-en/monitoring/collectd-zabbix/#exporting-metrics-to-zabbix-via-the-collectd-nagios-utility","text":"This document provides an example of exporting filter node metrics to the Zabbix monitoring system using the collectd-nagios utility.","title":"Exporting Metrics to Zabbix via the collectd-nagios Utility"},{"location":"admin-en/monitoring/collectd-zabbix/#example-workflow","text":"Example of metric This example shows how to work with the single curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the filter node. The following deployment scheme is used in this document: The Wallarm filter node is deployed on a host accessible via the 10.0.30.5 IP address and the node.example.local fully qualified domain name. The host has the Zabbix agent 4.0 LTS deployed that Downloads the filter node metrics using the collectd-nagios utility. Listens to incoming connections on the 10050/TCP port (thus passive checks will take place with the use of Zabbix Appliance). Passes metric values to Zabbix Appliance. On a dedicated host with the 10.0.30.30 IP address (hereinafter referred to as the Docker host), the Zabbix Appliance 4.0 LTS is deployed in the form of a Docker container. The Zabbix Appliance includes A Zabbix server that periodically polls the Zabbix agent installed on the filter node host to get information about changes to any monitored metrics). The Zabbix server management web interface, available on the 80/TCP port.","title":"Example Workflow"},{"location":"admin-en/monitoring/collectd-zabbix/#configuring-metrics-export-to-zabbix","text":"Prerequisites It is assumed that The collectd service has been configured for working via a Unix domain socket (see here for details). Docker Community Edition is already installed on the 10.0.30.30 Docker host. The node.example.local filter node is already deployed, configured, available for further configuration (for example, via the SSH protocol), and working.","title":"Configuring Metrics Export to Zabbix"},{"location":"admin-en/monitoring/collectd-zabbix/#deploying-zabbix","text":"To deploy the Zabbix Appliance 4.0 LTS, execute the following command on the Docker host: docker run --name zabbix-appliance -p 80 :80 -d zabbix/zabbix-appliance:alpine-4.0-latest Now you have a working Zabbix monitoring system.","title":"Deploying Zabbix"},{"location":"admin-en/monitoring/collectd-zabbix/#deploying-the-zabbix-agent","text":"Install the Zabbix Agent 4.0 LTS on a host with the filter node: Connect to the filter node (for example, using the SSH protocol). Make sure you are running as root or another account with superuser privileges. Connect the Zabbix repositories (use the \u201cInstall Zabbix repository\u201d entry of the instructions for your operating system). Install the Zabbix agent by executing the appropriate command: DEB-based distributions sudo apt install zabbix-agent RPM-based distributions sudo yum install zabbix-agent Configure the Zabbix Agent to work with the Zabbix Appliance. To do this, make the following changes to the /etc/zabbix/zabbix_agentd.conf configuration file: Server=10.0.30.30 # Zabbix IP address Hostname=node.example.local # FQDN of the host with the filter node","title":"Deploying the Zabbix Agent"},{"location":"admin-en/monitoring/collectd-zabbix/#configuring-metrics-collection-using-the-zabbix-agent","text":"Connect to the filter node (for example, using the SSH protocol) and configure the collection of metrics using the Zabbix agent. To do this, perform the following steps on the host with the filter node:","title":"Configuring Metrics Collection Using the Zabbix Agent"},{"location":"admin-en/monitoring/collectd-zabbix/#1-install-the-collectd_nagios-utility","text":"Execute the appropriate command: DEB-based distributions sudo apt install --no-install-recommends collectd-utils RPM-based distributions sudo yum install collectd-utils","title":"1.  Install the collectd_nagios utility"},{"location":"admin-en/monitoring/collectd-zabbix/#2-configure-the-collectd-nagios-utility-to-run-with-elevated-privileges-on-behalf-of-the-zabbix-user","text":"Use the visudo utility to add the following line to the /etc/sudoers file: zabbix ALL=(ALL:ALL) NOPASSWD:/usr/bin/collectd-nagios This allows the zabbix user to run the collectd-nagios utility with superuser privileges using the sudo utility without the need to provide a password. Running collectd-nagios with superuser privileges The utility must be run with superuser privileges because it uses the collectd Unix domain socket to receive data. Only a superuser can access this socket. As an alternative to adding the zabbix user to the sudoers list, you can configure the Zabbix agent to run as root (this may pose a security risk, so this is not recommended). This can be achieved by enabling the AllowRoot option in the agent configuration file.","title":"2.  Configure the collectd-nagios utility to run with elevated privileges on behalf of the zabbix user"},{"location":"admin-en/monitoring/collectd-zabbix/#3-make-sure-that-the-zabbix-user-can-receive-metric-values-from-collectd","text":"Run the following test command on the filter node: sudo -u zabbix sudo /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H node.example.local This command invokes the zabbix user to get the value of the curl_json-wallarm_nginx/gauge-attacks metric for the node.example.local host with the filter node. Example of the command output: OKAY: 0 critical, 0 warning, 1 okay | value=0.000000;;;;","title":"3.  Make sure that the zabbix user can receive metric values from collectd"},{"location":"admin-en/monitoring/collectd-zabbix/#4-add-custom-parameters-to-the-zabbix-agent-configuration-file-on-the-filter-node-host-to-get-the-metrics-you-need","text":"For example, to create a custom parameter wallarm_nginx-gauge-attacks that corresponds to the curl_json-wallarm_nginx/gauge-attacks metric for a filter node with the fully qualified domain name node.example.local , add the following line to the configuration file: Extracting a metric value To extract the value of a metric that goes after value= in the output of the collectd-nagios utility (e.g., OKAY: 0 critical, 0 warning, 1 okay | value=0.000000;;;; ), this output is piped to the sed utility that executes the sed script to strip off unnecessary characters. See the sed documentation for more information on the syntax of its scripts.","title":"4.  Add custom parameters to the Zabbix agent configuration file on the filter node host to get the metrics you need"},{"location":"admin-en/monitoring/collectd-zabbix/#5-after-all-the-necessary-commands-have-been-added-to-the-zabbix-agent-configuration-file-restart-the-agent","text":"Ubuntu 14.04 sudo service zabbix-agent restart Other supported distributions sudo systemctl restart zabbix-agent","title":"5.  After all the necessary commands have been added to the Zabbix agent configuration file, restart the agent"},{"location":"admin-en/monitoring/collectd-zabbix/#setup-complete","text":"Now you can monitor user parameters related to Wallarm-specific metrics with Zabbix.","title":"Setup Complete"},{"location":"admin-en/monitoring/fetching-metrics/","text":"How to Fetch Metrics \u00b6 Exporting Metrics Directly From collectd \u00b6 You can export the metrics collected by collectd directly to the tools that support working with collectd data streams. Prerequisites All further steps must be performed as a superuser (e.g., root ). Exporting Metrics via the collectd Network Plugin \u00b6 Configure and connect the network plugin to collectd : In the /etc/collectd/collectd.conf.d/ directory, create a file with the .conf extension (e.g., export-via-network.conf ) and the following content: LoadPlugin network <Plugin \"network\"> Server \"Server IPv4/v6 address or FQDN\" \"Server port\" </Plugin> As stated in this file, the plugin will be loaded upon starting collectd , operate in the client mode, and send the filter node\u2019s metrics data to the specified server. Configure a server that will receive data from the collectd client. The necessary configuration steps depend on the selected server (see examples for collectd and InfluxDB ). Working with the Network Plugin The network plugin works over UDP (see the plugin documentation ). Make sure that the server allows communication over UDP for metrics collection to be operational. Restart the collectd service by executing the appropriate command: Ubuntu 14.04 LTS sudo service collectd restart Other supported distributions sudo systemctl restart collectd Example Read an example of exporting metrics to InfluxDB via the Network plugin with subsequent visualization of the metrics in Grafana. Exporting Metrics via the collectd Write Plugins \u00b6 To configure export of metrics via the collectd write plugins , refer to the documentation of the corresponding plugin. Example To get basic information about using write plugins, read an example of exporting metrics to Graphite with subsequent visualization of the metrics in Grafana. Exporting Metrics Using the collectd-nagios Utility \u00b6 To export metrics using this method: Install the collectd-nagios utility on a host with a filter node by running the appropriate command (for a filter node installed on Linux): DEB-based distributions sudo apt install --no-install-recommends collectd-utils RPM-based distributions sudo yum install collectd-utils Docker image The filter node Docker image ships with a preinstalled collectd-nagios utility. Make sure that you can run this utility with elevated privileges either on behalf of a superuser (for example, root ) or as a regular user. In the latter case, add the user to the sudoers file with the NOPASSWD directive, and use the sudo utility. Working with the Docker container When executing the collectd-nagios utility in a Docker container with the filter node, elevation of privileges is not required. Connect and configure the UnixSock plugin to transmit the collectd metrics via a Unix domain socket. To do this, create the file /etc/collectd/collectd.conf.d/unixsock.conf with the following content: LoadPlugin unixsock <Plugin unixsock> SocketFile \"/var/run/collectd-unixsock\" SocketGroup \"root\" SocketPerms \"0770\" DeleteSocket true </Plugin> Restart the collectd service by executing the appropriate command: Ubuntu 14.04 LTS sudo service collectd restart Other supported distributions sudo systemctl restart collectd Get the value of the necessary metric by running the appropriate command: Linux /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n <metric name without host name> -H <FQDN of the host with the filter node on which the utility is running> Docker docker exec <container name> /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n <metric name without host name> -H <container ID> Getting the Docker container's ID You can find the value of the container identifier by running the docker ps command (see the \u201cCONTAINER ID\u201d column). Setting Thresholds for the collectd-nagios Utility If necessary, you can specify a range of values for which the collectd-nagios utility will return the WARNING or CRITICAL status by using the corresponding -w and -c options (detailed information is available in the utility documentation ). Examples of using the utility: To get the value of the curl_json-wallarm_nginx/gauge-attacks metric (at the time collectd-nagios was called) on the Linux host node.example.local with the filter node, run the following command: /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H node.example.local To get the value of the curl_json-wallarm_nginx/gauge-attacks metric (at the time collectd-nagios was called) for the filter node running in the Docker container with the wallarm-node name and the 95d278317794 identifier, run the following command: docker exec wallarm-node /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H 95d278317794 More examples To get basic information about using the collectd-nagios utility, read examples of exporting metrics to the Nagios monitoring system and to the Zabbix monitoring system . Sending Notifications from collectd \u00b6 Notifications are configured in the following file: DEB-based distributions /etc/collectd/conf.d/traps.conf RPM-based distributions /etc/collectd.d/traps.conf A general description of how notifications work is available here . More detailed information about how to set up notifications is available here . Possible methods of sending notifications: NSCA and NSCA-ng SNMP TRAP email messages custom scripts","title":"How to Fetch Metrics"},{"location":"admin-en/monitoring/fetching-metrics/#how-to-fetch-metrics","text":"","title":"How to Fetch Metrics"},{"location":"admin-en/monitoring/fetching-metrics/#exporting-metrics-directly-from-collectd","text":"You can export the metrics collected by collectd directly to the tools that support working with collectd data streams. Prerequisites All further steps must be performed as a superuser (e.g., root ).","title":"Exporting Metrics Directly From collectd"},{"location":"admin-en/monitoring/fetching-metrics/#exporting-metrics-via-the-collectd-network-plugin","text":"Configure and connect the network plugin to collectd : In the /etc/collectd/collectd.conf.d/ directory, create a file with the .conf extension (e.g., export-via-network.conf ) and the following content: LoadPlugin network <Plugin \"network\"> Server \"Server IPv4/v6 address or FQDN\" \"Server port\" </Plugin> As stated in this file, the plugin will be loaded upon starting collectd , operate in the client mode, and send the filter node\u2019s metrics data to the specified server. Configure a server that will receive data from the collectd client. The necessary configuration steps depend on the selected server (see examples for collectd and InfluxDB ). Working with the Network Plugin The network plugin works over UDP (see the plugin documentation ). Make sure that the server allows communication over UDP for metrics collection to be operational. Restart the collectd service by executing the appropriate command: Ubuntu 14.04 LTS sudo service collectd restart Other supported distributions sudo systemctl restart collectd Example Read an example of exporting metrics to InfluxDB via the Network plugin with subsequent visualization of the metrics in Grafana.","title":"Exporting Metrics via the collectd Network Plugin"},{"location":"admin-en/monitoring/fetching-metrics/#exporting-metrics-via-the-collectd-write-plugins","text":"To configure export of metrics via the collectd write plugins , refer to the documentation of the corresponding plugin. Example To get basic information about using write plugins, read an example of exporting metrics to Graphite with subsequent visualization of the metrics in Grafana.","title":"Exporting Metrics via the collectd Write Plugins"},{"location":"admin-en/monitoring/fetching-metrics/#exporting-metrics-using-the-collectd-nagios-utility","text":"To export metrics using this method: Install the collectd-nagios utility on a host with a filter node by running the appropriate command (for a filter node installed on Linux): DEB-based distributions sudo apt install --no-install-recommends collectd-utils RPM-based distributions sudo yum install collectd-utils Docker image The filter node Docker image ships with a preinstalled collectd-nagios utility. Make sure that you can run this utility with elevated privileges either on behalf of a superuser (for example, root ) or as a regular user. In the latter case, add the user to the sudoers file with the NOPASSWD directive, and use the sudo utility. Working with the Docker container When executing the collectd-nagios utility in a Docker container with the filter node, elevation of privileges is not required. Connect and configure the UnixSock plugin to transmit the collectd metrics via a Unix domain socket. To do this, create the file /etc/collectd/collectd.conf.d/unixsock.conf with the following content: LoadPlugin unixsock <Plugin unixsock> SocketFile \"/var/run/collectd-unixsock\" SocketGroup \"root\" SocketPerms \"0770\" DeleteSocket true </Plugin> Restart the collectd service by executing the appropriate command: Ubuntu 14.04 LTS sudo service collectd restart Other supported distributions sudo systemctl restart collectd Get the value of the necessary metric by running the appropriate command: Linux /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n <metric name without host name> -H <FQDN of the host with the filter node on which the utility is running> Docker docker exec <container name> /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n <metric name without host name> -H <container ID> Getting the Docker container's ID You can find the value of the container identifier by running the docker ps command (see the \u201cCONTAINER ID\u201d column). Setting Thresholds for the collectd-nagios Utility If necessary, you can specify a range of values for which the collectd-nagios utility will return the WARNING or CRITICAL status by using the corresponding -w and -c options (detailed information is available in the utility documentation ). Examples of using the utility: To get the value of the curl_json-wallarm_nginx/gauge-attacks metric (at the time collectd-nagios was called) on the Linux host node.example.local with the filter node, run the following command: /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H node.example.local To get the value of the curl_json-wallarm_nginx/gauge-attacks metric (at the time collectd-nagios was called) for the filter node running in the Docker container with the wallarm-node name and the 95d278317794 identifier, run the following command: docker exec wallarm-node /usr/bin/collectd-nagios -s /var/run/collectd-unixsock -n curl_json-wallarm_nginx/gauge-attacks -H 95d278317794 More examples To get basic information about using the collectd-nagios utility, read examples of exporting metrics to the Nagios monitoring system and to the Zabbix monitoring system .","title":"Exporting Metrics Using the collectd-nagios Utility"},{"location":"admin-en/monitoring/fetching-metrics/#sending-notifications-from-collectd","text":"Notifications are configured in the following file: DEB-based distributions /etc/collectd/conf.d/traps.conf RPM-based distributions /etc/collectd.d/traps.conf A general description of how notifications work is available here . More detailed information about how to set up notifications is available here . Possible methods of sending notifications: NSCA and NSCA-ng SNMP TRAP email messages custom scripts","title":"Sending Notifications from collectd"},{"location":"admin-en/monitoring/intro/","text":"Introduction to the WAF node monitoring \u00b6 You can monitor the state of a filter node using the node-provided metrics. These metrics are gathered by the collectd service that is installed on every Wallarm filter node. The collectd service provides several ways to transfer data and can serve as a source of metrics for many monitoring systems, offering you control over the state of the filter nodes. Need for Monitoring \u00b6 Failure or unstable work in the Wallarm module can lead to complete or partial denial of service for user requests to an application protected by a filter node. Failure of or unstable work in the postanalytics module can lead to inaccessibility of the following functionalities: Uploading attack data to the Wallarm cloud. As a result, the attacks will not be displayed on the Wallarm portal. Detecting behavioral attacks (see brute-force attacks ). Getting information about the structure of the protected application. You can monitor both the Wallarm module and the postanalytics module even if the latter is installed separately . Terminology agreement To monitor the Wallarm module and the postanalytics module, the same tools and methods are used; therefore both modules will be referred to as a \u201cfilter node\u201d throughout this guide, unless stated otherwise. All documents describing how to set up monitoring of a filter node are suitable for separately deployed Wallarm modules, separately deployed postanalytics modules, and jointly deployed Wallarm and postanalytics modules. Prerequisites for Monitoring \u00b6 For monitoring to work, it is required that: NGINX returns the statistics to the filter node ( wallarm_status on ), the filtration mode is in the monitoring / block mode . By default, this service is accessible at http://127.0.0.8/wallarm-status . If you configure the statistics service to be available at a non-standard address, you will need to correct the URL parameter accordingly in the collectd configuration file. The location of this file depends on the type of operating system distribution you have: DEB-based distributions /etc/collectd/collectd.conf.d/nginx-wallarm.conf RPM-based distributions /etc/collectd.d/nginx-wallarm.conf If a non-standard IP address or port for Tarantool are used, you will need to correct the Tarantool configuration file accordingly. The location of this file depends on the type of operating system distribution you have: DEB-based distributions /etc/collectd/collectd.conf.d/wallarm-tarantool.conf RPM-based distributions /etc/collectd.d/wallarm-tarantool.conf If SELinux is installed on the filter node host, make sure that SELinux is either configured or disabled . For simplicity, this document assumes that SELinux is disabled. How Metrics Look \u00b6 What the collectd Metrics Look Like \u00b6 A collectd metric identifier has the following format: host/plugin[-plugin_instance]/type[-type_instance] Where host : the host\u2019s Fully Qualified Domain Name (FQDN) for which the metric is obtained plugin : the name of the plugin with which the metric is obtained, -plugin_instance : the instance of the plugin, if one exists, type : the type of the metric value. Allowed types: counter derive gauge Detailed information about value types is available here . -type_instance : an instance of the type, if there is one. Instance type is equivalent to the value for which we want to get the metric. A full description of metric formats is available here . What Wallarm-Specific collectd Metrics Look Like \u00b6 The filter node uses collectd to collect Wallarm-specific metrics. Metrics of NGINX with the Wallarm module have the following format: host/curl_json-wallarm_nginx/type-type_instance Metrics of the postanalytics module have the following format: host/wallarm-tarantool/type-type_instance Metric Examples For a filter node on the node.example.local host: node.example.local/curl_json-wallarm_nginx/gauge-attacks is the metric of the number of recorded attacks; node.example.local/wallarm-tarantool/gauge-export_delay is the metric of the Tarantool export delay in seconds. A complete list of metrics that can be monitored is available here . Methods of Fetching Metrics \u00b6 You can collect metrics from a filter node in several ways: By exporting data directly from the collectd service via the Network plugin for collectd . This plugin enables collectd to download metrics from a filter node to the collectd server or to the InfluxDB database. InfluxDB InfluxDB can be used for the aggregation of metrics from collectd and other data sources with subsequent visualization (for example, a Grafana monitoring system to visualize the metrics stored in the InfluxDB). via one of the write plugins for collectd . For example, you can export collected data to Graphite using the write_graphite plugin. Graphite Graphite can be used as a data source for monitoring and visualization systems (for example, Grafana ). This method is suitable for the following filter node deployment types: in the clouds: Amazon AWS, Google Cloud; on Linux for NGINX/NGINX Plus and Kong platforms. By exporting data via collectd-nagios . This utility receives the value of the given metric from collectd and presents it in a Nagios\u2011compatible format . You can export metrics to Nagios or Zabbix monitoring systems by employing this utility. This method is supported by any Wallarm filter node, no matter how that node is deployed. By sending notifications from collectd when a metric has achieved a predetermined threshold value. This method is supported by any Wallarm filter node, no matter how that node is deployed.","title":"Introduction to monitoring"},{"location":"admin-en/monitoring/intro/#introduction-to-the-waf-node-monitoring","text":"You can monitor the state of a filter node using the node-provided metrics. These metrics are gathered by the collectd service that is installed on every Wallarm filter node. The collectd service provides several ways to transfer data and can serve as a source of metrics for many monitoring systems, offering you control over the state of the filter nodes.","title":"Introduction to the WAF node monitoring"},{"location":"admin-en/monitoring/intro/#need-for-monitoring","text":"Failure or unstable work in the Wallarm module can lead to complete or partial denial of service for user requests to an application protected by a filter node. Failure of or unstable work in the postanalytics module can lead to inaccessibility of the following functionalities: Uploading attack data to the Wallarm cloud. As a result, the attacks will not be displayed on the Wallarm portal. Detecting behavioral attacks (see brute-force attacks ). Getting information about the structure of the protected application. You can monitor both the Wallarm module and the postanalytics module even if the latter is installed separately . Terminology agreement To monitor the Wallarm module and the postanalytics module, the same tools and methods are used; therefore both modules will be referred to as a \u201cfilter node\u201d throughout this guide, unless stated otherwise. All documents describing how to set up monitoring of a filter node are suitable for separately deployed Wallarm modules, separately deployed postanalytics modules, and jointly deployed Wallarm and postanalytics modules.","title":"Need for Monitoring"},{"location":"admin-en/monitoring/intro/#prerequisites-for-monitoring","text":"For monitoring to work, it is required that: NGINX returns the statistics to the filter node ( wallarm_status on ), the filtration mode is in the monitoring / block mode . By default, this service is accessible at http://127.0.0.8/wallarm-status . If you configure the statistics service to be available at a non-standard address, you will need to correct the URL parameter accordingly in the collectd configuration file. The location of this file depends on the type of operating system distribution you have: DEB-based distributions /etc/collectd/collectd.conf.d/nginx-wallarm.conf RPM-based distributions /etc/collectd.d/nginx-wallarm.conf If a non-standard IP address or port for Tarantool are used, you will need to correct the Tarantool configuration file accordingly. The location of this file depends on the type of operating system distribution you have: DEB-based distributions /etc/collectd/collectd.conf.d/wallarm-tarantool.conf RPM-based distributions /etc/collectd.d/wallarm-tarantool.conf If SELinux is installed on the filter node host, make sure that SELinux is either configured or disabled . For simplicity, this document assumes that SELinux is disabled.","title":"Prerequisites for Monitoring"},{"location":"admin-en/monitoring/intro/#how-metrics-look","text":"","title":"How Metrics Look"},{"location":"admin-en/monitoring/intro/#what-the-collectd-metrics-look-like","text":"A collectd metric identifier has the following format: host/plugin[-plugin_instance]/type[-type_instance] Where host : the host\u2019s Fully Qualified Domain Name (FQDN) for which the metric is obtained plugin : the name of the plugin with which the metric is obtained, -plugin_instance : the instance of the plugin, if one exists, type : the type of the metric value. Allowed types: counter derive gauge Detailed information about value types is available here . -type_instance : an instance of the type, if there is one. Instance type is equivalent to the value for which we want to get the metric. A full description of metric formats is available here .","title":"What the collectd Metrics Look Like"},{"location":"admin-en/monitoring/intro/#what-wallarm-specific-collectd-metrics-look-like","text":"The filter node uses collectd to collect Wallarm-specific metrics. Metrics of NGINX with the Wallarm module have the following format: host/curl_json-wallarm_nginx/type-type_instance Metrics of the postanalytics module have the following format: host/wallarm-tarantool/type-type_instance Metric Examples For a filter node on the node.example.local host: node.example.local/curl_json-wallarm_nginx/gauge-attacks is the metric of the number of recorded attacks; node.example.local/wallarm-tarantool/gauge-export_delay is the metric of the Tarantool export delay in seconds. A complete list of metrics that can be monitored is available here .","title":"What Wallarm-Specific collectd Metrics Look Like"},{"location":"admin-en/monitoring/intro/#methods-of-fetching-metrics","text":"You can collect metrics from a filter node in several ways: By exporting data directly from the collectd service via the Network plugin for collectd . This plugin enables collectd to download metrics from a filter node to the collectd server or to the InfluxDB database. InfluxDB InfluxDB can be used for the aggregation of metrics from collectd and other data sources with subsequent visualization (for example, a Grafana monitoring system to visualize the metrics stored in the InfluxDB). via one of the write plugins for collectd . For example, you can export collected data to Graphite using the write_graphite plugin. Graphite Graphite can be used as a data source for monitoring and visualization systems (for example, Grafana ). This method is suitable for the following filter node deployment types: in the clouds: Amazon AWS, Google Cloud; on Linux for NGINX/NGINX Plus and Kong platforms. By exporting data via collectd-nagios . This utility receives the value of the given metric from collectd and presents it in a Nagios\u2011compatible format . You can export metrics to Nagios or Zabbix monitoring systems by employing this utility. This method is supported by any Wallarm filter node, no matter how that node is deployed. By sending notifications from collectd when a metric has achieved a predetermined threshold value. This method is supported by any Wallarm filter node, no matter how that node is deployed.","title":"Methods of Fetching Metrics"},{"location":"admin-en/monitoring/network-plugin-influxdb/","text":"Exporting Metrics to InfluxDB via the collectd Network Plugin \u00b6 This document provides an example of using the Network plugin to export metrics to the InfluxDB temporal database. It will also demonstrate how to visualize the metrics collected in InfluxDB using Grafana. Example Workflow \u00b6 Example of metric This example shows how to work with the single curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the filter node. The following deployment scheme is used in this document: The Wallarm filter node is deployed on a host accessible via the 10.0.30.5 IP address and the node.example.local fully qualified domain name. The network plugin for collectd on the filter node is configured in such a way that all metrics will be sent to the 10.0.30.30 InfluxDB server on port 25826/UDP . Network plugin features Please note that the plugin operates over UDP (see using examples and documentation of the network plugin). Both influxdb and grafana services are deployed as Docker containers on a separate host with the 10.0.30.30 IP address. The influxdb service with the InfluxDB database is configured as follows: A collectd data source has been created (the collectd input according to InfluxDB terminology), which listens on the 25826/UDP port and writes incoming metrics to a database called collectd . Communication with the InfluxDB API occurs via the 8086/TCP port. The service shares a sample-net Docker network with the grafana service. The grafana service with Grafana is configured as follows: The Grafana web console is available at http://10.0.30.30:3000 . The service shares the sample-net Docker network with the influxdb service. Configuring Metrics Export to InfluxDB \u00b6 Prerequisites It is assumed that Docker Community Edition and docker-compose are already installed on the 10.0.30.30 Docker host. The node.example.local filter node is already deployed, configured, available for further configuration (for example, via the SSH protocol), and working. Deploying InfluxDB and Grafana \u00b6 Deploy InfluxDB and Grafana on the Docker host: Create a working directory, for example, /tmp/influxdb-grafana , and navigate to it: mkdir /tmp/influxdb-grafana cd /tmp/influxdb-grafana For the InfluxDB data source to work, you will need a file named types.db that contains the collectd value types. This file describes the dataset specifications used by collectd . Such datasets include definitions of measurable types. Detailed information about this file is available here . Download the types.db file from the GitHub repository of the collectd project and put it in the working directory. Get the basic InfluxDB configuration file by running the following command: docker run --rm influxdb influxd config > influxdb.conf Enable the collectd data source in the influxdb.conf InfluxDB configuration file by changing the value of the enabled parameter in the [[collectd]] section from false to true . Leave other parameters unchanged. The section should look like this: [[collectd]] enabled = true bind-address = \":25826\" database = \"collectd\" retention-policy = \"\" batch-size = 5000 batch-pending = 10 batch-timeout = \"10s\" read-buffer = 0 typesdb = \"/usr/share/collectd/types.db\" security-level = \"none\" auth-file = \"/etc/collectd/auth_file\" parse-multivalue-plugin = \"split\" Create a docker-compose.yaml file in the working directory with the following content: version: \"3\" services: influxdb: image: influxdb container_name: influxdb ports: - 8086:8086 - 25826:25826/udp networks: - sample-net volumes: - ./:/var/lib/influxdb - ./influxdb.conf:/etc/influxdb/influxdb.conf:ro - ./types.db:/usr/share/collectd/types.db:ro grafana: image: grafana/grafana container_name: grafana restart: always ports: - 3000:3000 networks: - sample-net networks: sample-net: According to the settings in volumes: , InfluxDB will use 1. The working directory as storage for the database. 2. The influxdb.conf configuration file that is located in the working directory. 3. The types.db file with the types of measurable values that is located in the working directory. Build the services by executing the docker-compose build command. Run the services by executing the docker-compose up -d influxdb grafana command. Create a database named collectd for the corresponding InfluxDB data source by executing the following command: curl -i -X POST http://10.0.30.30:8086/query --data-urlencode \"q=CREATE DATABASE collectd\" The InfluxDB server should return a response similar to: HTTP/1.1 200 OK Content-Type: application/json Request-Id: 23604241-b086-11e9-8001-0242ac190002 X-Influxdb-Build: OSS X-Influxdb-Version: 1.7.7 X-Request-Id: 23604241-b086-11e9-8001-0242ac190002 Date: Sat, 27 Jul 2019 15:49:37 GMT Transfer-Encoding: chunked {\"results\":[{\"statement_id\":0}]} At this point, InfluxDB should be running, ready to receive metrics from collectd , and Grafana should be ready to monitor and visualize the data stored in InfluxDB. Configuring collectd \u00b6 Configure collectd to export metrics to InfluxDB: Connect to the filter node (for example, by using the SSH protocol). Make sure you are logged in as root or another account with superuser privileges. Create a file named /etc/collectd/collectd.conf.d/export-to-influxdb.conf with the following content: LoadPlugin network <Plugin \"network\"> Server \"10.0.30.30\" \"25826\" </Plugin> The following entities are configured here: The server, to send metrics to ( 10.0.30.30 ) The port that server listens on ( 25826/UDP ) Restart the collectd service by running the appropriate command: Ubuntu 14.04 LTS sudo service collectd restart Other supported distributions sudo systemctl restart collectd Now InfluxDB receives all the metrics of the filter node. You can visualize the metrics you are interested in and monitor them with Grafana .","title":"Exporting Metrics to InfluxDB via the `collectd` Network Plugin"},{"location":"admin-en/monitoring/network-plugin-influxdb/#exporting-metrics-to-influxdb-via-the-collectd-network-plugin","text":"This document provides an example of using the Network plugin to export metrics to the InfluxDB temporal database. It will also demonstrate how to visualize the metrics collected in InfluxDB using Grafana.","title":"Exporting Metrics to InfluxDB via the collectd Network Plugin"},{"location":"admin-en/monitoring/network-plugin-influxdb/#example-workflow","text":"Example of metric This example shows how to work with the single curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the filter node. The following deployment scheme is used in this document: The Wallarm filter node is deployed on a host accessible via the 10.0.30.5 IP address and the node.example.local fully qualified domain name. The network plugin for collectd on the filter node is configured in such a way that all metrics will be sent to the 10.0.30.30 InfluxDB server on port 25826/UDP . Network plugin features Please note that the plugin operates over UDP (see using examples and documentation of the network plugin). Both influxdb and grafana services are deployed as Docker containers on a separate host with the 10.0.30.30 IP address. The influxdb service with the InfluxDB database is configured as follows: A collectd data source has been created (the collectd input according to InfluxDB terminology), which listens on the 25826/UDP port and writes incoming metrics to a database called collectd . Communication with the InfluxDB API occurs via the 8086/TCP port. The service shares a sample-net Docker network with the grafana service. The grafana service with Grafana is configured as follows: The Grafana web console is available at http://10.0.30.30:3000 . The service shares the sample-net Docker network with the influxdb service.","title":"Example Workflow"},{"location":"admin-en/monitoring/network-plugin-influxdb/#configuring-metrics-export-to-influxdb","text":"Prerequisites It is assumed that Docker Community Edition and docker-compose are already installed on the 10.0.30.30 Docker host. The node.example.local filter node is already deployed, configured, available for further configuration (for example, via the SSH protocol), and working.","title":"Configuring Metrics Export to InfluxDB"},{"location":"admin-en/monitoring/network-plugin-influxdb/#deploying-influxdb-and-grafana","text":"Deploy InfluxDB and Grafana on the Docker host: Create a working directory, for example, /tmp/influxdb-grafana , and navigate to it: mkdir /tmp/influxdb-grafana cd /tmp/influxdb-grafana For the InfluxDB data source to work, you will need a file named types.db that contains the collectd value types. This file describes the dataset specifications used by collectd . Such datasets include definitions of measurable types. Detailed information about this file is available here . Download the types.db file from the GitHub repository of the collectd project and put it in the working directory. Get the basic InfluxDB configuration file by running the following command: docker run --rm influxdb influxd config > influxdb.conf Enable the collectd data source in the influxdb.conf InfluxDB configuration file by changing the value of the enabled parameter in the [[collectd]] section from false to true . Leave other parameters unchanged. The section should look like this: [[collectd]] enabled = true bind-address = \":25826\" database = \"collectd\" retention-policy = \"\" batch-size = 5000 batch-pending = 10 batch-timeout = \"10s\" read-buffer = 0 typesdb = \"/usr/share/collectd/types.db\" security-level = \"none\" auth-file = \"/etc/collectd/auth_file\" parse-multivalue-plugin = \"split\" Create a docker-compose.yaml file in the working directory with the following content: version: \"3\" services: influxdb: image: influxdb container_name: influxdb ports: - 8086:8086 - 25826:25826/udp networks: - sample-net volumes: - ./:/var/lib/influxdb - ./influxdb.conf:/etc/influxdb/influxdb.conf:ro - ./types.db:/usr/share/collectd/types.db:ro grafana: image: grafana/grafana container_name: grafana restart: always ports: - 3000:3000 networks: - sample-net networks: sample-net: According to the settings in volumes: , InfluxDB will use 1. The working directory as storage for the database. 2. The influxdb.conf configuration file that is located in the working directory. 3. The types.db file with the types of measurable values that is located in the working directory. Build the services by executing the docker-compose build command. Run the services by executing the docker-compose up -d influxdb grafana command. Create a database named collectd for the corresponding InfluxDB data source by executing the following command: curl -i -X POST http://10.0.30.30:8086/query --data-urlencode \"q=CREATE DATABASE collectd\" The InfluxDB server should return a response similar to: HTTP/1.1 200 OK Content-Type: application/json Request-Id: 23604241-b086-11e9-8001-0242ac190002 X-Influxdb-Build: OSS X-Influxdb-Version: 1.7.7 X-Request-Id: 23604241-b086-11e9-8001-0242ac190002 Date: Sat, 27 Jul 2019 15:49:37 GMT Transfer-Encoding: chunked {\"results\":[{\"statement_id\":0}]} At this point, InfluxDB should be running, ready to receive metrics from collectd , and Grafana should be ready to monitor and visualize the data stored in InfluxDB.","title":"Deploying InfluxDB and Grafana"},{"location":"admin-en/monitoring/network-plugin-influxdb/#configuring-collectd","text":"Configure collectd to export metrics to InfluxDB: Connect to the filter node (for example, by using the SSH protocol). Make sure you are logged in as root or another account with superuser privileges. Create a file named /etc/collectd/collectd.conf.d/export-to-influxdb.conf with the following content: LoadPlugin network <Plugin \"network\"> Server \"10.0.30.30\" \"25826\" </Plugin> The following entities are configured here: The server, to send metrics to ( 10.0.30.30 ) The port that server listens on ( 25826/UDP ) Restart the collectd service by running the appropriate command: Ubuntu 14.04 LTS sudo service collectd restart Other supported distributions sudo systemctl restart collectd Now InfluxDB receives all the metrics of the filter node. You can visualize the metrics you are interested in and monitor them with Grafana .","title":"Configuring collectd"},{"location":"admin-en/monitoring/working-with-grafana/","text":"Working with the Filter Node Metrics in Grafana \u00b6 If you have configured the export of metrics in InfluxDB or Graphite, then you can visualize the metrics with Grafana . A few assumptions This document assumes that you have deployed Grafana alongside InfluxDB or Graphite . The curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the node.example.local filter node, is used as an example. However, you can monitor any supported metric . In your browser, go to http://10.0.30.30:3000 to open the Grafana web console, then log in to the console using the standard username ( admin ) and password ( admin ). In order to monitor a filter node using Grafana, you will need to Connect a data source. Fetch the required metrics from the data source. Set up metric visualization. It is assumed that you are using one of the following data sources: InfluxDB Graphite Connecting a Data Source \u00b6 InfluxDB \u00b6 To connect an InfluxDB server as the data source take the following steps: On the main page of the Grafana console, click the Add data source button. Select \u201cInfluxDB\u201d as the data source type. Fill in the required parameters: Name: InfluxDB URL: http://influxdb:8086 Database: collectd User: root Password: root Click the Save & Test button. Graphite \u00b6 To connect a Graphite server as the data source take the following steps: On the main page of the Grafana console, click the Add data source button. Select \u201cGraphite\u201d as the data source type. Fill in the required parameters: Name: Graphite URL: http://graphite:8080 . Version: select the newest available version from the drop-down list. Click the Save & Test button. Checking a Data Source Status If a data source was connected successfully, the \u201cData source is working\u201d message should appear. Further Actions \u00b6 Perform the following actions to enable Grafana to monitor metrics: Click the Grafana icon in the upper left corner of the console to return to the main page. Create a new dashboard by clicking the New Dashboard button. Then add a query to fetch a metric to the dashboard by clicking the Add Query button. Fetching the Required Metrics from the Data Source \u00b6 InfluxDB \u00b6 To fetch a metric from the InfluxDB data source do the following: Select the newly created \u201cInfluxDB\u201d data source from the Query drop-down list. Design a query to the InfluxDB either by using the graphical query design tool, or by manually filling in a query in plain text (to do this, click the Toggle text edit button, which is highlighted in the screenshot below). The query to fetch the curl_json-wallarm_nginx/gauge-attacks metric is: SELECT value FROM curl_json_value WHERE (host = 'node.example.local' AND instance = 'wallarm_nginx' AND type = 'gauge' AND type_instance = 'attacks') Graphite \u00b6 To fetch a metric from the Graphite data source do the following: Select the newly created \u201cGraphite\u201d data source from the Query drop-down list. Select the elements of the required metric in a sequential manner by clicking the select metric button for the metric\u2019s element in the Series line. The elements of the curl_json-wallarm_nginx/gauge-attacks metric go as follows: The hostname, as it was set in the write_graphite plugin configuration file. The _ character serves as a delimiter by default in this plugin; therefore, the node.example.local domain name will be represented as node_example_local in the query. The name of the collectd plugin that provides a specific value. For this metric, the plugin is curl_json . The name of the plugin instance. For this metric, the name is wallarm_nginx . The type of value. For this metric, the type is gauge . The name of value. For this metric, the name is attacks . Further Actions \u00b6 After the creation of the query, set up a visualization for the corresponding metric. Setting Up Metric Visualization \u00b6 Switch from the Query tab to the Visualization tab, and select the desired visualization for the metric. For the curl_json-wallarm_nginx/gauge-attacks metric, we recommend using the \u201cGauge\u201d visualization: Select the Calc: Last option to display the current metric value. If necessary, you can configure thresholds and other parameters. Further Actions \u00b6 After configuring visualization take the following steps: Complete the query configuration by clicking on the \u201c\u2190\u201d button in the upper left corner of the console. Save any changes that were made to the dashboard. Verify and confirm that Grafana is successfully monitoring the metric. Verifying Monitoring \u00b6 After you have connected one of the data sources and configured the query and visualization for the curl_json-wallarm_nginx/gauge-attacks metric, check the monitoring operation: Enable automatic metric updates at five-second intervals (select a value from the drop-down list in the upper right corner of the Grafana console). Make sure that the current number of attacks on the Grafana dashboard matches the output from wallarm-status on the filter node: Execute the curl http://127.0.0.8/wallarm-status command if the default configuration of the statistics service is in use. Otherwise, see the /etc/nginx/conf.d/wallarm-status.conf configuration file to construct the correct command similar to the one above. {\"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } Perform a test attack on an application protected by the filter node. To do this, you can send a malicious request to the application either with the curl utility or a browser. Example curl -I \u201chttp://node.example.local/?id = 'or+1=1--a-<script>prompt(1)</script>' \u201d Make sure that the attack counter has increased both in the wallarm-status output and on the Grafana dashboard: {\"requests\":64,\"attacks\":16,\"blocked\":0,\"abnormal\":64,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } The Grafana dashboard now displays the curl_json-wallarm_nginx/gauge-attacks metric values for the node.example.local filter node.","title":"Working with the Filter Node Metrics in Grafana"},{"location":"admin-en/monitoring/working-with-grafana/#working-with-the-filter-node-metrics-in-grafana","text":"If you have configured the export of metrics in InfluxDB or Graphite, then you can visualize the metrics with Grafana . A few assumptions This document assumes that you have deployed Grafana alongside InfluxDB or Graphite . The curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the node.example.local filter node, is used as an example. However, you can monitor any supported metric . In your browser, go to http://10.0.30.30:3000 to open the Grafana web console, then log in to the console using the standard username ( admin ) and password ( admin ). In order to monitor a filter node using Grafana, you will need to Connect a data source. Fetch the required metrics from the data source. Set up metric visualization. It is assumed that you are using one of the following data sources: InfluxDB Graphite","title":"Working with the Filter Node Metrics in Grafana"},{"location":"admin-en/monitoring/working-with-grafana/#connecting-a-data-source","text":"","title":"Connecting a Data Source"},{"location":"admin-en/monitoring/working-with-grafana/#influxdb","text":"To connect an InfluxDB server as the data source take the following steps: On the main page of the Grafana console, click the Add data source button. Select \u201cInfluxDB\u201d as the data source type. Fill in the required parameters: Name: InfluxDB URL: http://influxdb:8086 Database: collectd User: root Password: root Click the Save & Test button.","title":"InfluxDB"},{"location":"admin-en/monitoring/working-with-grafana/#graphite","text":"To connect a Graphite server as the data source take the following steps: On the main page of the Grafana console, click the Add data source button. Select \u201cGraphite\u201d as the data source type. Fill in the required parameters: Name: Graphite URL: http://graphite:8080 . Version: select the newest available version from the drop-down list. Click the Save & Test button. Checking a Data Source Status If a data source was connected successfully, the \u201cData source is working\u201d message should appear.","title":"Graphite"},{"location":"admin-en/monitoring/working-with-grafana/#further-actions","text":"Perform the following actions to enable Grafana to monitor metrics: Click the Grafana icon in the upper left corner of the console to return to the main page. Create a new dashboard by clicking the New Dashboard button. Then add a query to fetch a metric to the dashboard by clicking the Add Query button.","title":"Further Actions"},{"location":"admin-en/monitoring/working-with-grafana/#fetching-the-required-metrics-from-the-data-source","text":"","title":"Fetching the Required Metrics from the Data Source"},{"location":"admin-en/monitoring/working-with-grafana/#influxdb_1","text":"To fetch a metric from the InfluxDB data source do the following: Select the newly created \u201cInfluxDB\u201d data source from the Query drop-down list. Design a query to the InfluxDB either by using the graphical query design tool, or by manually filling in a query in plain text (to do this, click the Toggle text edit button, which is highlighted in the screenshot below). The query to fetch the curl_json-wallarm_nginx/gauge-attacks metric is: SELECT value FROM curl_json_value WHERE (host = 'node.example.local' AND instance = 'wallarm_nginx' AND type = 'gauge' AND type_instance = 'attacks')","title":"InfluxDB"},{"location":"admin-en/monitoring/working-with-grafana/#graphite_1","text":"To fetch a metric from the Graphite data source do the following: Select the newly created \u201cGraphite\u201d data source from the Query drop-down list. Select the elements of the required metric in a sequential manner by clicking the select metric button for the metric\u2019s element in the Series line. The elements of the curl_json-wallarm_nginx/gauge-attacks metric go as follows: The hostname, as it was set in the write_graphite plugin configuration file. The _ character serves as a delimiter by default in this plugin; therefore, the node.example.local domain name will be represented as node_example_local in the query. The name of the collectd plugin that provides a specific value. For this metric, the plugin is curl_json . The name of the plugin instance. For this metric, the name is wallarm_nginx . The type of value. For this metric, the type is gauge . The name of value. For this metric, the name is attacks .","title":"Graphite"},{"location":"admin-en/monitoring/working-with-grafana/#further-actions_1","text":"After the creation of the query, set up a visualization for the corresponding metric.","title":"Further Actions"},{"location":"admin-en/monitoring/working-with-grafana/#setting-up-metric-visualization","text":"Switch from the Query tab to the Visualization tab, and select the desired visualization for the metric. For the curl_json-wallarm_nginx/gauge-attacks metric, we recommend using the \u201cGauge\u201d visualization: Select the Calc: Last option to display the current metric value. If necessary, you can configure thresholds and other parameters.","title":"Setting Up Metric Visualization"},{"location":"admin-en/monitoring/working-with-grafana/#further-actions_2","text":"After configuring visualization take the following steps: Complete the query configuration by clicking on the \u201c\u2190\u201d button in the upper left corner of the console. Save any changes that were made to the dashboard. Verify and confirm that Grafana is successfully monitoring the metric.","title":"Further Actions"},{"location":"admin-en/monitoring/working-with-grafana/#verifying-monitoring","text":"After you have connected one of the data sources and configured the query and visualization for the curl_json-wallarm_nginx/gauge-attacks metric, check the monitoring operation: Enable automatic metric updates at five-second intervals (select a value from the drop-down list in the upper right corner of the Grafana console). Make sure that the current number of attacks on the Grafana dashboard matches the output from wallarm-status on the filter node: Execute the curl http://127.0.0.8/wallarm-status command if the default configuration of the statistics service is in use. Otherwise, see the /etc/nginx/conf.d/wallarm-status.conf configuration file to construct the correct command similar to the one above. {\"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } Perform a test attack on an application protected by the filter node. To do this, you can send a malicious request to the application either with the curl utility or a browser. Example curl -I \u201chttp://node.example.local/?id = 'or+1=1--a-<script>prompt(1)</script>' \u201d Make sure that the attack counter has increased both in the wallarm-status output and on the Grafana dashboard: {\"requests\":64,\"attacks\":16,\"blocked\":0,\"abnormal\":64,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } The Grafana dashboard now displays the curl_json-wallarm_nginx/gauge-attacks metric values for the node.example.local filter node.","title":"Verifying Monitoring"},{"location":"admin-en/monitoring/working-with-nagios/","text":"Working with the Filter Node Metrics in Nagios \u00b6 Verify that Nagios is successfully monitoring the status of the previously created service: Log in to the Nagios web interface. Go to the services page by clicking on the \u201cServices\u201d link. Make sure that the wallarm_nginx_attacks service is displayed and has the \u201cOK\u201d status: Forcing service check If the service does not have the \u201cOK\u201d status, you can force a check of the service to confirm its status. To do this, click on the service name in the \u201cService\u201d column, and then run the check by selecting \u201cReschedule the next check of this service\u201d in the \u201cService Commands\u201d list and entering the necessary parameters. View detailed information about the service by clicking on the link with its name in the \u201cStatus\u201d column: Make sure that the metric value displayed in Nagios (the \u201cPerformance Data\u201d row) matches the wallarm-status output on the filter node: Execute the curl http://127.0.0.8/wallarm-status command if the default configuration of the statistics service is in use. Otherwise, see the /etc/nginx/conf.d/wallarm-status.conf configuration file to construct the correct command similar to the one above. {\"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } Perform a test attack on an application protected by the filter node. To do this, you can send a malicious request to the application either with the curl utility or a browser. Example curl -I \u201chttp://node.example.local/?id = 'or+1=1--a-<script>prompt(1)</script>' \u201d Ensure that the \u201cPerformance Data\u201d value in Nagios has increased and matches the value displayed by wallarm-status on the filter node: {\"requests\":64,\"attacks\":16,\"blocked\":0,\"abnormal\":64,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } Now the values of the curl_json-wallarm_nginx/gauge-attacks metric of the filter node are displayed in the service state information in Nagios. Nagios data visualization By default, Nagios Core only supports tracking service status ( OK , WARNING , CRITICAL ). To store and visualize metric values contained in \u201cPerformance Data,\u201d you can use third-party utilities, for example, PHP4Nagios .","title":"Working with the Filter Node Metrics in Nagios"},{"location":"admin-en/monitoring/working-with-nagios/#working-with-the-filter-node-metrics-in-nagios","text":"Verify that Nagios is successfully monitoring the status of the previously created service: Log in to the Nagios web interface. Go to the services page by clicking on the \u201cServices\u201d link. Make sure that the wallarm_nginx_attacks service is displayed and has the \u201cOK\u201d status: Forcing service check If the service does not have the \u201cOK\u201d status, you can force a check of the service to confirm its status. To do this, click on the service name in the \u201cService\u201d column, and then run the check by selecting \u201cReschedule the next check of this service\u201d in the \u201cService Commands\u201d list and entering the necessary parameters. View detailed information about the service by clicking on the link with its name in the \u201cStatus\u201d column: Make sure that the metric value displayed in Nagios (the \u201cPerformance Data\u201d row) matches the wallarm-status output on the filter node: Execute the curl http://127.0.0.8/wallarm-status command if the default configuration of the statistics service is in use. Otherwise, see the /etc/nginx/conf.d/wallarm-status.conf configuration file to construct the correct command similar to the one above. {\"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } Perform a test attack on an application protected by the filter node. To do this, you can send a malicious request to the application either with the curl utility or a browser. Example curl -I \u201chttp://node.example.local/?id = 'or+1=1--a-<script>prompt(1)</script>' \u201d Ensure that the \u201cPerformance Data\u201d value in Nagios has increased and matches the value displayed by wallarm-status on the filter node: {\"requests\":64,\"attacks\":16,\"blocked\":0,\"abnormal\":64,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } Now the values of the curl_json-wallarm_nginx/gauge-attacks metric of the filter node are displayed in the service state information in Nagios. Nagios data visualization By default, Nagios Core only supports tracking service status ( OK , WARNING , CRITICAL ). To store and visualize metric values contained in \u201cPerformance Data,\u201d you can use third-party utilities, for example, PHP4Nagios .","title":"Working with the Filter Node Metrics in Nagios"},{"location":"admin-en/monitoring/working-with-zabbix/","text":"Working with the Filter Node Metrics in Zabbix \u00b6 Go to http://10.0.30.30 to access the Zabbix web interface login page. Log in to the web interface using the standard login ( Admin ) and password ( zabbix ). To monitor the metrics of the node.example.local filter node, perform the following actions: Create a new host by performing the following steps: Go to the Configuration \u2192 Hosts tab and click the Create host button. Fill the fully qualified domain name of the filter node host in the Host name field ( node.example.local ). Select the group you want to place the host into from the Groups field (for example, you can use the predefined \u201cLinux servers\u201d group, or create a dedicated group). Fill the IP address of the filter node host ( 10.0.30.5 ) in the Agent interfaces parameter group. Leave the default port value ( 10050 ) unchanged. Connecting using a domain name If necessary, you can set up a domain name to connect to the Zabbix agent. To do this, change the appropriate settings accordingly. Configure other settings, if necessary. Make sure that the Enabled checkbox is checked. Complete the host creation process by clicking the Add button. Add metrics that should be monitored for the filter node host. To add a single metric, follow the steps below: Click the name of the created host node.example.local in the list of hosts on the Configuration \u2192 Hosts tab. A page with the host data will open. Switch to the Items tab and click the Create item button. Fill a metric name in the Name field (for example, Wallarm NGINX Attacks ). Leave the Type , Host interface , and Type of information parameters unchanged. Enter the key name of the metric in the Key field (as specified in UserParameter= in the Zabbix agent configuration ; for example, wallarm_nginx-gauge-attacks ). If necessary, adjust the update frequency of the metric value and other parameters. Make sure that the Enabled checkbox is checked. Complete the process of adding a metric by clicking the Add button. Configure the visualization of the added metrics: Click the Zabbix logo in the upper left corner of the web interface to access the dashboard. Click the Edit dashboard button to make changes to the dashboard: Add a widget by clicking the Add widget button. Select the required widget type (for example, \u201cPlain Text\u201d) from the Type drop-down list. Fill any suitable name in the Name field. Add the required metric to the Items list (e.g., the newly created Wallarm NGINX Attacks ). Make sure that the Show text as HTML and Dynamic Items checkboxes are checked. Complete the Add widget wizard by clicking the Add button. Save the changes that you made to the dashboard by clicking the Save changes button. Check the monitoring operation: Make sure that the current number of attacks in the Zabbix widget matches the output of wallarm-status on the filter node. Execute the curl http://127.0.0.8/wallarm-status command if the default configuration of the statistics service is in use. Otherwise, see the /etc/nginx/conf.d/wallarm-status.conf configuration file to construct the correct command similar to the one above. {\"requests\":64,\"attacks\":16,\"blocked\":0,\"abnormal\":64,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } Perform a test attack on an application protected by the filter node. To do this, you can send a malicious request to the application either with the curl utility or a browser. Example curl -I \u201chttp://node.example.local/?id = 'or+1=1--a-<script>prompt(1)</script>' \u201d Make sure that the attack counter has increased in both the wallarm-status output and the Zabbix widget: {\"requests\":64,\"attacks\":16,\"blocked\":0,\"abnormal\":64,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } The Zabbix dashboard now displays the curl_json-wallarm_nginx/gauge-attacks metric of the node.example.local filter node.","title":"Working with the Filter Node in Zabbix"},{"location":"admin-en/monitoring/working-with-zabbix/#working-with-the-filter-node-metrics-in-zabbix","text":"Go to http://10.0.30.30 to access the Zabbix web interface login page. Log in to the web interface using the standard login ( Admin ) and password ( zabbix ). To monitor the metrics of the node.example.local filter node, perform the following actions: Create a new host by performing the following steps: Go to the Configuration \u2192 Hosts tab and click the Create host button. Fill the fully qualified domain name of the filter node host in the Host name field ( node.example.local ). Select the group you want to place the host into from the Groups field (for example, you can use the predefined \u201cLinux servers\u201d group, or create a dedicated group). Fill the IP address of the filter node host ( 10.0.30.5 ) in the Agent interfaces parameter group. Leave the default port value ( 10050 ) unchanged. Connecting using a domain name If necessary, you can set up a domain name to connect to the Zabbix agent. To do this, change the appropriate settings accordingly. Configure other settings, if necessary. Make sure that the Enabled checkbox is checked. Complete the host creation process by clicking the Add button. Add metrics that should be monitored for the filter node host. To add a single metric, follow the steps below: Click the name of the created host node.example.local in the list of hosts on the Configuration \u2192 Hosts tab. A page with the host data will open. Switch to the Items tab and click the Create item button. Fill a metric name in the Name field (for example, Wallarm NGINX Attacks ). Leave the Type , Host interface , and Type of information parameters unchanged. Enter the key name of the metric in the Key field (as specified in UserParameter= in the Zabbix agent configuration ; for example, wallarm_nginx-gauge-attacks ). If necessary, adjust the update frequency of the metric value and other parameters. Make sure that the Enabled checkbox is checked. Complete the process of adding a metric by clicking the Add button. Configure the visualization of the added metrics: Click the Zabbix logo in the upper left corner of the web interface to access the dashboard. Click the Edit dashboard button to make changes to the dashboard: Add a widget by clicking the Add widget button. Select the required widget type (for example, \u201cPlain Text\u201d) from the Type drop-down list. Fill any suitable name in the Name field. Add the required metric to the Items list (e.g., the newly created Wallarm NGINX Attacks ). Make sure that the Show text as HTML and Dynamic Items checkboxes are checked. Complete the Add widget wizard by clicking the Add button. Save the changes that you made to the dashboard by clicking the Save changes button. Check the monitoring operation: Make sure that the current number of attacks in the Zabbix widget matches the output of wallarm-status on the filter node. Execute the curl http://127.0.0.8/wallarm-status command if the default configuration of the statistics service is in use. Otherwise, see the /etc/nginx/conf.d/wallarm-status.conf configuration file to construct the correct command similar to the one above. {\"requests\":64,\"attacks\":16,\"blocked\":0,\"abnormal\":64,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } Perform a test attack on an application protected by the filter node. To do this, you can send a malicious request to the application either with the curl utility or a browser. Example curl -I \u201chttp://node.example.local/?id = 'or+1=1--a-<script>prompt(1)</script>' \u201d Make sure that the attack counter has increased in both the wallarm-status output and the Zabbix widget: {\"requests\":64,\"attacks\":16,\"blocked\":0,\"abnormal\":64,\"tnt_errors\":0,\"api_errors\":0,\"requests_lost\":0,\"segfaults\":0,\"memfaults\":0,\"softmemfaults\":0,\"time_detect\":0,\"db_id\":46,\"lom_id\":4,\"proton_instances\": { \"total\":2,\"success\":2,\"fallback\":0,\"failed\":0 },\"stalled_workers_count\":0,\"stalled_workers\":[] } The Zabbix dashboard now displays the curl_json-wallarm_nginx/gauge-attacks metric of the node.example.local filter node.","title":"Working with the Filter Node Metrics in Zabbix"},{"location":"admin-en/monitoring/write-plugin-graphite/","text":"Exporting Metrics to Graphite via the collectd Write Plugin \u00b6 This document provides an example of using the write_graphite write plugin to export metrics to Graphite. Example Workflow \u00b6 Example of metric This example shows how to work with the single curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the filter node. The following deployment scheme is used in this document: Wallarm filter node is deployed on a host accessible via the 10.0.30.5 IP address and the node.example.local fully qualified domain name. The write_graphite plugin for collectd on the filter node is configured as follows: All metrics are sent to the 10.0.30.30 server listening on the 2003/TCP port. Some Wallarm-specific collectd plugins support multiple instances , so the write_graphite plugin contains the SeparateInstances parameter set to true . The true value means that the plugin can work with several instances. A complete list of plugin options is available here . Both graphite and grafana services are deployed as Docker containers on a separate host with the 10.0.30.30 IP address. The graphite service with Graphite is configured as follows: It listens for incoming connections on the 2003/TCP port, to which collectd will send the filter node metrics. It listens for incoming connections on the 8080/TCP port, through which communication with Grafana will occur. The service shares the sample-net Docker network with the grafana service. The grafana service with Grafana is configured as follows: The Grafana web console is available at http://10.0.30.30:3000 . The service shares the sample-net Docker network with the graphite service. Configuring Metrics Export to Graphite \u00b6 Prerequisites It is assumed that Docker Community Edition and docker-compose are already installed on the 10.0.30.30 Docker host. The node.example.local filter node is already deployed, configured, available for further configuration (for example, via the SSH protocol), and working. Deploying Graphite and Grafana \u00b6 Deploy Graphite and Grafana on the Docker host: Create a docker-compose.yaml file with the following content: version: \"3\" services: grafana: image: grafana/grafana container_name: grafana restart: always ports: - 3000:3000 networks: - sample-net graphite: image: graphiteapp/graphite-statsd container_name: graphite restart: always ports: - 8080:8080 - 2003:2003 networks: - sample-net networks: sample-net: Build the services by executing the docker-compose build command. Run the services by executing the docker-compose up -d graphite grafana command. At this point, you should have Graphite running and ready to receive metrics from collectd , and Grafana ready to monitor and visualize the data stored in Graphite. Configuring collectd \u00b6 Configure collectd to download metrics to Graphite: Connect to the filter node (for example, using the SSH protocol). Make sure you are logged in as root or another account with superuser privileges. Create a file named /etc/collectd/collectd.conf.d/export-to-graphite.conf with the following content: LoadPlugin write_graphite <Plugin write_graphite> <Node \"node.example.local\"> Host \"10.0.30.30\" Port \"2003\" Protocol \"tcp\" SeparateInstances true </Node> </Plugin> The following entities are configured here: The host name from which metrics are collected ( node.example.local ). The server to which metrics should be sent ( 10.0.30.30 ). The server port ( 2003 ) and the protocol ( tcp ). The data transfer logic: the data of one instance of the plugin is separated from the data of another instance ( SeparateInstances true ). Restart the collectd service by running the appropriate command: Ubuntu 14.04 LTS sudo service collectd restart Other supported distributions sudo systemctl restart collectd Now Graphite will receive all metrics of the filter node. You can visualize the metrics you are interested in, and monitor them with Grafana .","title":"Exporting Metrics to Graphite via the `collectd` Write Plugin"},{"location":"admin-en/monitoring/write-plugin-graphite/#exporting-metrics-to-graphite-via-the-collectd-write-plugin","text":"This document provides an example of using the write_graphite write plugin to export metrics to Graphite.","title":"Exporting Metrics to Graphite via the collectd Write Plugin"},{"location":"admin-en/monitoring/write-plugin-graphite/#example-workflow","text":"Example of metric This example shows how to work with the single curl_json-wallarm_nginx/gauge-attacks metric, which shows the number of attacks on an application that is protected by the filter node. The following deployment scheme is used in this document: Wallarm filter node is deployed on a host accessible via the 10.0.30.5 IP address and the node.example.local fully qualified domain name. The write_graphite plugin for collectd on the filter node is configured as follows: All metrics are sent to the 10.0.30.30 server listening on the 2003/TCP port. Some Wallarm-specific collectd plugins support multiple instances , so the write_graphite plugin contains the SeparateInstances parameter set to true . The true value means that the plugin can work with several instances. A complete list of plugin options is available here . Both graphite and grafana services are deployed as Docker containers on a separate host with the 10.0.30.30 IP address. The graphite service with Graphite is configured as follows: It listens for incoming connections on the 2003/TCP port, to which collectd will send the filter node metrics. It listens for incoming connections on the 8080/TCP port, through which communication with Grafana will occur. The service shares the sample-net Docker network with the grafana service. The grafana service with Grafana is configured as follows: The Grafana web console is available at http://10.0.30.30:3000 . The service shares the sample-net Docker network with the graphite service.","title":"Example Workflow"},{"location":"admin-en/monitoring/write-plugin-graphite/#configuring-metrics-export-to-graphite","text":"Prerequisites It is assumed that Docker Community Edition and docker-compose are already installed on the 10.0.30.30 Docker host. The node.example.local filter node is already deployed, configured, available for further configuration (for example, via the SSH protocol), and working.","title":"Configuring Metrics Export to Graphite"},{"location":"admin-en/monitoring/write-plugin-graphite/#deploying-graphite-and-grafana","text":"Deploy Graphite and Grafana on the Docker host: Create a docker-compose.yaml file with the following content: version: \"3\" services: grafana: image: grafana/grafana container_name: grafana restart: always ports: - 3000:3000 networks: - sample-net graphite: image: graphiteapp/graphite-statsd container_name: graphite restart: always ports: - 8080:8080 - 2003:2003 networks: - sample-net networks: sample-net: Build the services by executing the docker-compose build command. Run the services by executing the docker-compose up -d graphite grafana command. At this point, you should have Graphite running and ready to receive metrics from collectd , and Grafana ready to monitor and visualize the data stored in Graphite.","title":"Deploying Graphite and Grafana"},{"location":"admin-en/monitoring/write-plugin-graphite/#configuring-collectd","text":"Configure collectd to download metrics to Graphite: Connect to the filter node (for example, using the SSH protocol). Make sure you are logged in as root or another account with superuser privileges. Create a file named /etc/collectd/collectd.conf.d/export-to-graphite.conf with the following content: LoadPlugin write_graphite <Plugin write_graphite> <Node \"node.example.local\"> Host \"10.0.30.30\" Port \"2003\" Protocol \"tcp\" SeparateInstances true </Node> </Plugin> The following entities are configured here: The host name from which metrics are collected ( node.example.local ). The server to which metrics should be sent ( 10.0.30.30 ). The server port ( 2003 ) and the protocol ( tcp ). The data transfer logic: the data of one instance of the plugin is separated from the data of another instance ( SeparateInstances true ). Restart the collectd service by running the appropriate command: Ubuntu 14.04 LTS sudo service collectd restart Other supported distributions sudo systemctl restart collectd Now Graphite will receive all metrics of the filter node. You can visualize the metrics you are interested in, and monitor them with Grafana .","title":"Configuring collectd"},{"location":"admin-en/operation/learn-incoming-request-number/","text":"Learning the number of requests per month handled by the application \u00b6 Wallarm's primary licensing/billing methods are based on the level of requests served by Wallarm WAF nodes deployed in your environment. This document explains how to easily learn the number of requests handled by the application. Teams having an access to the information \u00b6 Normally, the following teams in a company may have an easy access to the information: DevOps Technical Operations Cloud Operations Platform Operations DevSecOps System Administrators Application Administrators NOC Methods to learn the number of requests \u00b6 There are several methods to look for the number of requests handled by the application: AWS customers using ELB or ALB load balancers can use AWS monitoring metrics of load balancers to estimate the level of daily and weekly requests for applications served by the load balancers: For example, if a graph shows that the average request per minute level is 350 and assuming that there are, on average, 730 hours in a month, then the number of monthly requests is 350 * 60 * 730 = 15,330,000 . GCP users of HTTP load balancers can use monitoring metric https/request_count . The metric is not available for Network Load Balancers. Microsoft IIS users can rely on Requests Per Sec metric to average the number of requests per second and calculate the number of requests served by a single IIS server per month. In calculation, assume that there are, on average, 730 * 3,600 seconds per month. Users of Application Performance Monitoring services like New Relic, Datadog, AppDynamics, SignalFX and others can get the information from their APM consoles (just make sure to get an aggregated value for all involved servers in the edge layer, and not just one server). Users of cloud\u2011based infrastructure monitoring systems like Datadog, AWS CloudWatch (and many others) or users of internal monitoring systems like Prometheus or Nagios most likely already monitor the level of requests served on their edge location (load balancers, web servers, application servers), and can use the information to easily estimate the average number of handled requests per month. Another approach is to use the logs of edge load balancers or web servers to count the number of log records in a period of time (ideally - 24 hours) assuming that there is one log record per served requests. For example, this web server rotates the NGINX access log file once a day, with 653,525 requests recorded in the log file: cd /var/log/nginx/ zcat access.log.2.gz | wc -l # 653525 The estimate of requests served by the server in a month is 653,525 * 30 = 19,605,750 . Knowing the total number of used web servers makes it possible to estimate the number of requests handled by the whole application. For pure web applications using Google Analytics or similar user experience tracking and monitoring services, the information about the number of served pages and all embedded objects can be extracted from the services.","title":"Learning the amount of requests per month handled by the application"},{"location":"admin-en/operation/learn-incoming-request-number/#learning-the-number-of-requests-per-month-handled-by-the-application","text":"Wallarm's primary licensing/billing methods are based on the level of requests served by Wallarm WAF nodes deployed in your environment. This document explains how to easily learn the number of requests handled by the application.","title":"Learning the number of requests per month handled by the application"},{"location":"admin-en/operation/learn-incoming-request-number/#teams-having-an-access-to-the-information","text":"Normally, the following teams in a company may have an easy access to the information: DevOps Technical Operations Cloud Operations Platform Operations DevSecOps System Administrators Application Administrators NOC","title":"Teams having an access to the information"},{"location":"admin-en/operation/learn-incoming-request-number/#methods-to-learn-the-number-of-requests","text":"There are several methods to look for the number of requests handled by the application: AWS customers using ELB or ALB load balancers can use AWS monitoring metrics of load balancers to estimate the level of daily and weekly requests for applications served by the load balancers: For example, if a graph shows that the average request per minute level is 350 and assuming that there are, on average, 730 hours in a month, then the number of monthly requests is 350 * 60 * 730 = 15,330,000 . GCP users of HTTP load balancers can use monitoring metric https/request_count . The metric is not available for Network Load Balancers. Microsoft IIS users can rely on Requests Per Sec metric to average the number of requests per second and calculate the number of requests served by a single IIS server per month. In calculation, assume that there are, on average, 730 * 3,600 seconds per month. Users of Application Performance Monitoring services like New Relic, Datadog, AppDynamics, SignalFX and others can get the information from their APM consoles (just make sure to get an aggregated value for all involved servers in the edge layer, and not just one server). Users of cloud\u2011based infrastructure monitoring systems like Datadog, AWS CloudWatch (and many others) or users of internal monitoring systems like Prometheus or Nagios most likely already monitor the level of requests served on their edge location (load balancers, web servers, application servers), and can use the information to easily estimate the average number of handled requests per month. Another approach is to use the logs of edge load balancers or web servers to count the number of log records in a period of time (ideally - 24 hours) assuming that there is one log record per served requests. For example, this web server rotates the NGINX access log file once a day, with 653,525 requests recorded in the log file: cd /var/log/nginx/ zcat access.log.2.gz | wc -l # 653525 The estimate of requests served by the server in a month is 653,525 * 30 = 19,605,750 . Knowing the total number of used web servers makes it possible to estimate the number of requests handled by the whole application. For pure web applications using Google Analytics or similar user experience tracking and monitoring services, the information about the number of served pages and all embedded objects can be extracted from the services.","title":"Methods to learn the number of requests"},{"location":"api/overview/","text":"Wallarm API overview \u00b6 Wallarm API provides interaction between components of the Wallarm system. You can use Wallarm API methods to create, get, or update the following instances: vulnerabilities attacks incidents users clients filter nodes etc. Description of API methods is given in the API Reference by the link: https://apiconsole.eu1.wallarm.com/ for the EU cloud https://apiconsole.us1.wallarm.com/ for the US cloud API endpoint \u00b6 API requests are sent to the following URL: https://api.wallarm.com/ for the EU cloud https://us1.api.wallarm.com/ for the US cloud Authentication of API requests \u00b6 You must be a verified user to make Wallarm API requests. The method of API requests authentication depends on the client sending the request: API Reference UI Your own client API Reference UI \u00b6 A token is used for request authentication. The token is generated after successful authentication in your Wallarm account. Sign in to your Wallarm account using the link: https://my.wallarm.com/ for the EU cloud, https://us1.my.wallarm.com/ for the US cloud. Refresh the API Reference page using the link: https://apiconsole.eu1.wallarm.com/ for the EU cloud, https://apiconsole.us1.wallarm.com/ for the US cloud. Go to the required API method > the Try it out section, input parameter values, and Execute the request. Your own client \u00b6 Your UUID and secret key are used for request authentication. Sign in to your Wallarm account in the EU Cloud or US Cloud \u2192 Settings \u2192 API credentials . Copy the UUID value. Get the Secret key value: If you know the secret key value, then you can continue using the known value. The Wallarm Console displays the encrypted value of your active secret key. If you do not know the secret key value or it was lost, generate the new secret key by the button Renew secret key and copy its value. The secret key value will not be shown again. Reusing the secret key value The button Renew secret key generates the new value of the secret key and invalidates the previous value. To use the secret key securely: Write down the key value in a secure place. The secret key value will not be shown again. Reuse the stored key value in all requests to Wallarm API. If you generated the new key value, make sure the previous value is not used in other API clients. If the previous value is in use, replace it with the newly generated secret value. Send the required API request passing the following values: UUID in the X-WallarmAPI-UUID header parameter Secret key in the X-WallarmAPI-Secret header parameter Wallarm approach to API development and documentation \u00b6 Wallarm API Reference is a single page application (SPA) with all displayed data being dynamically fetched from the API. This design drives Wallarm to use the API-first approach when new data and functionality is initially made available in the public API and as the next step is described in the API Reference. Normally all new functionality is released in parallel in both public API and API Reference, but sometimes new API changes are released ahead of API Reference changes, and some functionality is available via the public API only. Wallarm API Reference is generated from the Swagger file using the Swagger UI tool. API Reference provides an easy way to learn about available API endpoints, methods, and data structures. It also provides a simple way to try all available endpoints.","title":"Wallarm API overview"},{"location":"api/overview/#wallarm-api-overview","text":"Wallarm API provides interaction between components of the Wallarm system. You can use Wallarm API methods to create, get, or update the following instances: vulnerabilities attacks incidents users clients filter nodes etc. Description of API methods is given in the API Reference by the link: https://apiconsole.eu1.wallarm.com/ for the EU cloud https://apiconsole.us1.wallarm.com/ for the US cloud","title":"Wallarm API overview"},{"location":"api/overview/#api-endpoint","text":"API requests are sent to the following URL: https://api.wallarm.com/ for the EU cloud https://us1.api.wallarm.com/ for the US cloud","title":"API endpoint"},{"location":"api/overview/#authentication-of-api-requests","text":"You must be a verified user to make Wallarm API requests. The method of API requests authentication depends on the client sending the request: API Reference UI Your own client","title":"Authentication of API requests"},{"location":"api/overview/#api-reference-ui","text":"A token is used for request authentication. The token is generated after successful authentication in your Wallarm account. Sign in to your Wallarm account using the link: https://my.wallarm.com/ for the EU cloud, https://us1.my.wallarm.com/ for the US cloud. Refresh the API Reference page using the link: https://apiconsole.eu1.wallarm.com/ for the EU cloud, https://apiconsole.us1.wallarm.com/ for the US cloud. Go to the required API method > the Try it out section, input parameter values, and Execute the request.","title":"API Reference UI"},{"location":"api/overview/#your-own-client","text":"Your UUID and secret key are used for request authentication. Sign in to your Wallarm account in the EU Cloud or US Cloud \u2192 Settings \u2192 API credentials . Copy the UUID value. Get the Secret key value: If you know the secret key value, then you can continue using the known value. The Wallarm Console displays the encrypted value of your active secret key. If you do not know the secret key value or it was lost, generate the new secret key by the button Renew secret key and copy its value. The secret key value will not be shown again. Reusing the secret key value The button Renew secret key generates the new value of the secret key and invalidates the previous value. To use the secret key securely: Write down the key value in a secure place. The secret key value will not be shown again. Reuse the stored key value in all requests to Wallarm API. If you generated the new key value, make sure the previous value is not used in other API clients. If the previous value is in use, replace it with the newly generated secret value. Send the required API request passing the following values: UUID in the X-WallarmAPI-UUID header parameter Secret key in the X-WallarmAPI-Secret header parameter","title":"Your own client"},{"location":"api/overview/#wallarm-approach-to-api-development-and-documentation","text":"Wallarm API Reference is a single page application (SPA) with all displayed data being dynamically fetched from the API. This design drives Wallarm to use the API-first approach when new data and functionality is initially made available in the public API and as the next step is described in the API Reference. Normally all new functionality is released in parallel in both public API and API Reference, but sometimes new API changes are released ahead of API Reference changes, and some functionality is available via the public API only. Wallarm API Reference is generated from the Swagger file using the Swagger UI tool. API Reference provides an easy way to learn about available API endpoints, methods, and data structures. It also provides a simple way to try all available endpoints.","title":"Wallarm approach to API development and documentation"},{"location":"api/request-examples/","text":"Wallarm API request examples \u00b6 The following are some examples of Wallarm API use. You can also generate code examples via the API Reference UI for the EU cloud or the US cloud . Experienced users can also use the browser\u2019s Developer console (\u201cNetwork\u201d tab) to quickly learn which API endpoints and requests are used by the UI of your Wallarm account to fetch data from the public API. To find information about how to open the Developer console, you can use the official browser documentation ( Safari , Chrome , Firefox , Vivaldi ). Get the first 50 attacks detected in the last 24 hours \u00b6 Please replace TIMESTAMP with the date 24 hours ago converted to the Unix Timestamp format. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/attack\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID], \\\"time\\\": [[TIMESTAMP, null]] }, \\\"offset\\\": 0, \\\"limit\\\": 50, \\\"order_by\\\": \\\"last_time\\\", \\\"order_desc\\\": true}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/attack\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID], \\\"time\\\": [[TIMESTAMP, null]] }, \\\"offset\\\": 0, \\\"limit\\\": 50, \\\"order_by\\\": \\\"last_time\\\", \\\"order_desc\\\": true}\" Get the first 50 incidents confirmed in the last 24 hours \u00b6 The request is very similar to the previous example for a list of attacks; the \"!vulnid\": null term is added to this request. This term instructs the API to ignore all attacks without specified vulnerability ID, and this is how the system distinguishes between attacks and incidents. Please replace TIMESTAMP with the date 24 hours ago converted to the Unix Timestamp format. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/attack\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID], \\\"\\!vulnid\\\": null, \\\"time\\\": [[TIMESTAMP, null]] }, \\\"offset\\\": 0, \\\"limit\\\": 50, \\\"order_by\\\": \\\"last_time\\\", \\\"order_desc\\\": true}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/attack\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID], \\\"\\!vulnid\\\": null, \\\"time\\\": [[TIMESTAMP, null]] }, \\\"offset\\\": 0, \\\"limit\\\": 50, \\\"order_by\\\": \\\"last_time\\\", \\\"order_desc\\\": true}\" Get the first 50 vulnerabilities in the status \"active\" within the last 24 hours \u00b6 Please replace TIMESTAMP with the date 24 hours ago converted to the Unix Timestamp format. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/vuln\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"limit\\\":50, \\\"offset\\\":0, \\\"filter\\\":{\\\"clientid\\\":[YOUR_CLIENT_ID], \\\"testrun_id\\\":null, \\\"validated\\\":true, \\\"time\\\":[[TIMESTAMP, null]]}}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/vuln\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"limit\\\":50, \\\"offset\\\":0, \\\"filter\\\":{\\\"clientid\\\":[YOUR_CLIENT_ID], \\\"testrun_id\\\":null, \\\"validated\\\":true, \\\"time\\\":[[TIMESTAMP, null]]}}\" Get all configured rules \u00b6 EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\": [YOUR_CLIENT_ID]},\\\"order_by\\\": \\\"updated_at\\\",\\\"order_desc\\\": true,\\\"limit\\\": 1000,\\\"offset\\\": 0}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\": [YOUR_CLIENT_ID]},\\\"order_by\\\": \\\"updated_at\\\",\\\"order_desc\\\": true,\\\"limit\\\": 1000,\\\"offset\\\": 0}\" Get defined conditions for request blocking \u00b6 EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/action\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID] }, \\\"offset\\\": 0, \\\"limit\\\": 1000}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/action\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID] }, \\\"offset\\\": 0, \\\"limit\\\": 1000}\" Get rules attached to a specific condition \u00b6 EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\": [YOUR_CLIENT_ID],\\\"actionid\\\": YOUR_CONDITION_ID},\\\"limit\\\": 1000,\\\"offset\\\": 0}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\": [YOUR_CLIENT_ID],\\\"actionid\\\": YOUR_CONDITION_ID},\\\"limit\\\": 1000,\\\"offset\\\": 0}\" Create the virtual patch to block all requests sent to /my/api/* \u00b6 EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"clientid\\\": YOUR_CLIENT_ID, \\\"type\\\": \\\"vpatch\\\", \\\"action\\\": [ {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"my\\\",\\\"point\\\":[\\\"path\\\",0]}, {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"api\\\",\\\"point\\\":[\\\"path\\\",1]}, {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"endpoint\\\",\\\"point\\\":[\\\"header\\\",\\\"2\\\"]}], \\\"validated\\\": false, \\\"point\\\": [ [ \\\"header\\\", \\\"HOST\\\" ] ], \\\"attack_type\\\": \\\"any\\\"}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"clientid\\\": YOUR_CLIENT_ID, \\\"type\\\": \\\"vpatch\\\", \\\"action\\\": [ {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"my\\\",\\\"point\\\":[\\\"path\\\",0]}, {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"api\\\",\\\"point\\\":[\\\"path\\\",1]}, {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"endpoint\\\",\\\"point\\\":[\\\"header\\\",\\\"2\\\"]}], \\\"validated\\\": false, \\\"point\\\": [ [ \\\"header\\\", \\\"HOST\\\" ] ], \\\"attack_type\\\": \\\"any\\\"}\" Create the virtual patch for a specific application instance ID to block all requests sent to /my/api/* \u00b6 An application ID is specified in the action.value parameter. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"type\\\":\\\"vpatch\\\",\\\"action\\\":[{\\\"point\\\":[\\\"instance\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"-1\\\",\\\"weight\\\":102},{\\\"point\\\":[\\\"path\\\",0],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"my\\\",\\\"weight\\\":72},{\\\"point\\\":[\\\"path\\\",1],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"api\\\",\\\"weight\\\":72},{\\\"point\\\":[\\\"header\\\",\\\"2\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"endpoint\\\",\\\"weight\\\":42}],\\\"clientid\\\":YOUR_CLIENT_ID,\\\"validated\\\":false,\\\"point\\\":[[\\\"header\\\",\\\"HOST\\\"]],\\\"attack_type\\\":\\\"any\\\"}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"type\\\":\\\"vpatch\\\",\\\"action\\\":[{\\\"point\\\":[\\\"instance\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"-1\\\",\\\"weight\\\":102},{\\\"point\\\":[\\\"path\\\",0],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"my\\\",\\\"weight\\\":72},{\\\"point\\\":[\\\"path\\\",1],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"api\\\",\\\"weight\\\":72},{\\\"point\\\":[\\\"header\\\",\\\"2\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"endpoint\\\",\\\"weight\\\":42}],\\\"clientid\\\":YOUR_CLIENT_ID,\\\"validated\\\":false,\\\"point\\\":[[\\\"header\\\",\\\"HOST\\\"]],\\\"attack_type\\\":\\\"any\\\"}\" Create a rule to block all requests with specific values of HOST and X-FORWARDED-FOR request headers \u00b6 For domain MY.DOMAIN.COM the rule will block all requests besides one having IP address 44.33.22.11 in the value of X-FORWARDED-FOR HTTP request header. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"type\\\":\\\"regex\\\",\\\"action\\\":[{\\\"point\\\":[\\\"header\\\",\\\"HOST\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"MY.DOMAIN.NAME\\\"}],\\\"clientid\\\":YOUR_CLIENT_ID,\\\"validated\\\":false,\\\"comment\\\":\\\"comment\\\",\\\"point\\\":[[\\\"header\\\",\\\"X-FORWARDED-FOR\\\"]],\\\"attack_type\\\":\\\"scanner\\\",\\\"regex\\\":\\\"^\\(~\\(44[.]33[.]22[.]11\\)\\) $ \\\"}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"type\\\":\\\"regex\\\",\\\"action\\\":[{\\\"point\\\":[\\\"header\\\",\\\"HOST\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"MY.DOMAIN.NAME\\\"}],\\\"clientid\\\":YOUR_CLIENT_ID,\\\"validated\\\":false,\\\"comment\\\":\\\"comment\\\",\\\"point\\\":[[\\\"header\\\",\\\"X-FORWARDED-FOR\\\"]],\\\"attack_type\\\":\\\"scanner\\\",\\\"regex\\\":\\\"^\\(~\\(44[.]33[.]22[.]11\\)\\) $ \\\"}\" Delete rule by its ID \u00b6 EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint/delete\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\":[YOUR_CLIENT_ID],\\\"id\\\": YOUR_RULE_ID}}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint/delete\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\":[YOUR_CLIENT_ID],\\\"id\\\": YOUR_RULE_ID}}\"","title":"Wallarm API request examples"},{"location":"api/request-examples/#wallarm-api-request-examples","text":"The following are some examples of Wallarm API use. You can also generate code examples via the API Reference UI for the EU cloud or the US cloud . Experienced users can also use the browser\u2019s Developer console (\u201cNetwork\u201d tab) to quickly learn which API endpoints and requests are used by the UI of your Wallarm account to fetch data from the public API. To find information about how to open the Developer console, you can use the official browser documentation ( Safari , Chrome , Firefox , Vivaldi ).","title":"Wallarm API request examples"},{"location":"api/request-examples/#get-the-first-50-attacks-detected-in-the-last-24-hours","text":"Please replace TIMESTAMP with the date 24 hours ago converted to the Unix Timestamp format. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/attack\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID], \\\"time\\\": [[TIMESTAMP, null]] }, \\\"offset\\\": 0, \\\"limit\\\": 50, \\\"order_by\\\": \\\"last_time\\\", \\\"order_desc\\\": true}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/attack\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID], \\\"time\\\": [[TIMESTAMP, null]] }, \\\"offset\\\": 0, \\\"limit\\\": 50, \\\"order_by\\\": \\\"last_time\\\", \\\"order_desc\\\": true}\"","title":"Get the first 50 attacks detected in the last 24 hours"},{"location":"api/request-examples/#get-the-first-50-incidents-confirmed-in-the-last-24-hours","text":"The request is very similar to the previous example for a list of attacks; the \"!vulnid\": null term is added to this request. This term instructs the API to ignore all attacks without specified vulnerability ID, and this is how the system distinguishes between attacks and incidents. Please replace TIMESTAMP with the date 24 hours ago converted to the Unix Timestamp format. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/attack\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID], \\\"\\!vulnid\\\": null, \\\"time\\\": [[TIMESTAMP, null]] }, \\\"offset\\\": 0, \\\"limit\\\": 50, \\\"order_by\\\": \\\"last_time\\\", \\\"order_desc\\\": true}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/attack\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID], \\\"\\!vulnid\\\": null, \\\"time\\\": [[TIMESTAMP, null]] }, \\\"offset\\\": 0, \\\"limit\\\": 50, \\\"order_by\\\": \\\"last_time\\\", \\\"order_desc\\\": true}\"","title":"Get the first 50 incidents confirmed in the last 24 hours"},{"location":"api/request-examples/#get-the-first-50-vulnerabilities-in-the-status-active-within-the-last-24-hours","text":"Please replace TIMESTAMP with the date 24 hours ago converted to the Unix Timestamp format. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/vuln\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"limit\\\":50, \\\"offset\\\":0, \\\"filter\\\":{\\\"clientid\\\":[YOUR_CLIENT_ID], \\\"testrun_id\\\":null, \\\"validated\\\":true, \\\"time\\\":[[TIMESTAMP, null]]}}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/vuln\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"limit\\\":50, \\\"offset\\\":0, \\\"filter\\\":{\\\"clientid\\\":[YOUR_CLIENT_ID], \\\"testrun_id\\\":null, \\\"validated\\\":true, \\\"time\\\":[[TIMESTAMP, null]]}}\"","title":"Get the first 50 vulnerabilities in the status \"active\" within the last 24 hours"},{"location":"api/request-examples/#get-all-configured-rules","text":"EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\": [YOUR_CLIENT_ID]},\\\"order_by\\\": \\\"updated_at\\\",\\\"order_desc\\\": true,\\\"limit\\\": 1000,\\\"offset\\\": 0}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\": [YOUR_CLIENT_ID]},\\\"order_by\\\": \\\"updated_at\\\",\\\"order_desc\\\": true,\\\"limit\\\": 1000,\\\"offset\\\": 0}\"","title":"Get all configured rules"},{"location":"api/request-examples/#get-defined-conditions-for-request-blocking","text":"EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/action\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID] }, \\\"offset\\\": 0, \\\"limit\\\": 1000}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/action\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"filter\\\": { \\\"clientid\\\": [YOUR_CLIENT_ID] }, \\\"offset\\\": 0, \\\"limit\\\": 1000}\"","title":"Get defined conditions for request blocking"},{"location":"api/request-examples/#get-rules-attached-to-a-specific-condition","text":"EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\": [YOUR_CLIENT_ID],\\\"actionid\\\": YOUR_CONDITION_ID},\\\"limit\\\": 1000,\\\"offset\\\": 0}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\": [YOUR_CLIENT_ID],\\\"actionid\\\": YOUR_CONDITION_ID},\\\"limit\\\": 1000,\\\"offset\\\": 0}\"","title":"Get rules attached to a specific condition"},{"location":"api/request-examples/#create-the-virtual-patch-to-block-all-requests-sent-to-myapi","text":"EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"clientid\\\": YOUR_CLIENT_ID, \\\"type\\\": \\\"vpatch\\\", \\\"action\\\": [ {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"my\\\",\\\"point\\\":[\\\"path\\\",0]}, {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"api\\\",\\\"point\\\":[\\\"path\\\",1]}, {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"endpoint\\\",\\\"point\\\":[\\\"header\\\",\\\"2\\\"]}], \\\"validated\\\": false, \\\"point\\\": [ [ \\\"header\\\", \\\"HOST\\\" ] ], \\\"attack_type\\\": \\\"any\\\"}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"clientid\\\": YOUR_CLIENT_ID, \\\"type\\\": \\\"vpatch\\\", \\\"action\\\": [ {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"my\\\",\\\"point\\\":[\\\"path\\\",0]}, {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"api\\\",\\\"point\\\":[\\\"path\\\",1]}, {\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"endpoint\\\",\\\"point\\\":[\\\"header\\\",\\\"2\\\"]}], \\\"validated\\\": false, \\\"point\\\": [ [ \\\"header\\\", \\\"HOST\\\" ] ], \\\"attack_type\\\": \\\"any\\\"}\"","title":"Create the virtual patch to block all requests sent to /my/api/*"},{"location":"api/request-examples/#create-the-virtual-patch-for-a-specific-application-instance-id-to-block-all-requests-sent-to-myapi","text":"An application ID is specified in the action.value parameter. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"type\\\":\\\"vpatch\\\",\\\"action\\\":[{\\\"point\\\":[\\\"instance\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"-1\\\",\\\"weight\\\":102},{\\\"point\\\":[\\\"path\\\",0],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"my\\\",\\\"weight\\\":72},{\\\"point\\\":[\\\"path\\\",1],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"api\\\",\\\"weight\\\":72},{\\\"point\\\":[\\\"header\\\",\\\"2\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"endpoint\\\",\\\"weight\\\":42}],\\\"clientid\\\":YOUR_CLIENT_ID,\\\"validated\\\":false,\\\"point\\\":[[\\\"header\\\",\\\"HOST\\\"]],\\\"attack_type\\\":\\\"any\\\"}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"type\\\":\\\"vpatch\\\",\\\"action\\\":[{\\\"point\\\":[\\\"instance\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"-1\\\",\\\"weight\\\":102},{\\\"point\\\":[\\\"path\\\",0],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"my\\\",\\\"weight\\\":72},{\\\"point\\\":[\\\"path\\\",1],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"api\\\",\\\"weight\\\":72},{\\\"point\\\":[\\\"header\\\",\\\"2\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"endpoint\\\",\\\"weight\\\":42}],\\\"clientid\\\":YOUR_CLIENT_ID,\\\"validated\\\":false,\\\"point\\\":[[\\\"header\\\",\\\"HOST\\\"]],\\\"attack_type\\\":\\\"any\\\"}\"","title":"Create the virtual patch for a specific application instance ID to block all requests sent to /my/api/*"},{"location":"api/request-examples/#create-a-rule-to-block-all-requests-with-specific-values-of-host-and-x-forwarded-for-request-headers","text":"For domain MY.DOMAIN.COM the rule will block all requests besides one having IP address 44.33.22.11 in the value of X-FORWARDED-FOR HTTP request header. EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"type\\\":\\\"regex\\\",\\\"action\\\":[{\\\"point\\\":[\\\"header\\\",\\\"HOST\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"MY.DOMAIN.NAME\\\"}],\\\"clientid\\\":YOUR_CLIENT_ID,\\\"validated\\\":false,\\\"comment\\\":\\\"comment\\\",\\\"point\\\":[[\\\"header\\\",\\\"X-FORWARDED-FOR\\\"]],\\\"attack_type\\\":\\\"scanner\\\",\\\"regex\\\":\\\"^\\(~\\(44[.]33[.]22[.]11\\)\\) $ \\\"}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"type\\\":\\\"regex\\\",\\\"action\\\":[{\\\"point\\\":[\\\"header\\\",\\\"HOST\\\"],\\\"type\\\":\\\"equal\\\",\\\"value\\\":\\\"MY.DOMAIN.NAME\\\"}],\\\"clientid\\\":YOUR_CLIENT_ID,\\\"validated\\\":false,\\\"comment\\\":\\\"comment\\\",\\\"point\\\":[[\\\"header\\\",\\\"X-FORWARDED-FOR\\\"]],\\\"attack_type\\\":\\\"scanner\\\",\\\"regex\\\":\\\"^\\(~\\(44[.]33[.]22[.]11\\)\\) $ \\\"}\"","title":"Create a rule to block all requests with specific values of HOST and X-FORWARDED-FOR request headers"},{"location":"api/request-examples/#delete-rule-by-its-id","text":"EU cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/hint/delete\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\":[YOUR_CLIENT_ID],\\\"id\\\": YOUR_RULE_ID}}\" US cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/hint/delete\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{\\\"filter\\\":{\\\"clientid\\\":[YOUR_CLIENT_ID],\\\"id\\\": YOUR_RULE_ID}}\"","title":"Delete rule by its ID"},{"location":"api-firewall/overview/","text":"Wallarm API Firewall overview \u00b6 Light-weighted Wallarm API Firewall protects your API endpoints in cloud-native environments with API schema validation. Wallarm API Firewall relies on a positive security model allowing calls that match a predefined API specification, while rejecting everything else. API schema validation and positive security model \u00b6 When starting API Firewall, you should provide the OpenAPI 3.0 specification of the application that should be protected with API Firewall. The started API Firewall will operate as a reverse proxy and validate whether requests and responses match the schema defined in the specification. The traffic that does not match the schema will be blocked or logged (depending on the configured API Firewall operation mode). Provided API schema should be described using the OpenAPI 3.0 specification in the YAML or JSON file ( .yaml , .yml , .json file extensions). By allowing you to set the traffic requirements with the OpenAPI 3.0 specification, Wallarm API Firewall relies on a positive security model. Technical characteristics \u00b6 API Firewall works as a reverse proxy with a built-in OpenAPI 3.0 request and response validator. The validator is written in Go and optimized for extreme performance and near-zero added latency.","title":"Wallarm API Firewall overview"},{"location":"api-firewall/overview/#wallarm-api-firewall-overview","text":"Light-weighted Wallarm API Firewall protects your API endpoints in cloud-native environments with API schema validation. Wallarm API Firewall relies on a positive security model allowing calls that match a predefined API specification, while rejecting everything else.","title":"Wallarm API Firewall overview"},{"location":"api-firewall/overview/#api-schema-validation-and-positive-security-model","text":"When starting API Firewall, you should provide the OpenAPI 3.0 specification of the application that should be protected with API Firewall. The started API Firewall will operate as a reverse proxy and validate whether requests and responses match the schema defined in the specification. The traffic that does not match the schema will be blocked or logged (depending on the configured API Firewall operation mode). Provided API schema should be described using the OpenAPI 3.0 specification in the YAML or JSON file ( .yaml , .yml , .json file extensions). By allowing you to set the traffic requirements with the OpenAPI 3.0 specification, Wallarm API Firewall relies on a positive security model.","title":"API schema validation and positive security model"},{"location":"api-firewall/overview/#technical-characteristics","text":"API Firewall works as a reverse proxy with a built-in OpenAPI 3.0 request and response validator. The validator is written in Go and optimized for extreme performance and near-zero added latency.","title":"Technical characteristics"},{"location":"api-firewall/demos/docker-compose/","text":"Running the example application and API Firewall with Docker Compose \u00b6 This document describes the steps to run the demo code that deploys the application httpbin protected by API Firewall using Docker Compose . Used resources \u00b6 The following resources are used in this demo: httpbin Docker image as the example application image API Firewall Docker image The demo code description \u00b6 The demo code published on our GitHub contains the following configuration files: The following OpenAPI 3.0 specifications located in the volumes directory: httpbin.json is the httpbin OpenAPI 2.0 specification converted to the OpenAPI 3.0 specification format. httpbin-with-constraints.json is the httpbin OpenAPI 3.0 specification with additional API restrictions added explicitly. Both these files will be used to test the demo deployment. Makefile is the configuration file defining Docker routines. docker-compose.yml is the file defining the httpbin and API Firewall Docker images configuration. Step 1: Running the demo code \u00b6 To run the demo code: Clone the GitHub repository containing the demo code: git clone https://github.com/wallarm/api-firewall.git Change to the demo/docker-compose directory of the cloned repository: cd api-firewall/demo/docker-compose Run the demo code by using the following command: make start The application httpbin protected by API Firewall will be available at http://localhost:8080 . The application httpbin unprotected by API Firewall will be available at http://localhost:8090 . When testing the demo deployment, you can send requests to the unprotected application to know the difference. Proceed to the demo testing. Step 2: Testing the demo based on the original OpenAPI 3.0 specification \u00b6 By default, this demo is running with the original httpbin OpenAPI 3.0 specification. To test this demo option, you can use the following requests: Check that API Firewall blocks requests sent to unexposed path: curl -sD - http://localhost:8080/unexposed/path Expected response: HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 06 :58:29 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000200000001 Check that API Firewall blocks request with string value passed in the parameter that requires integer data type: curl -sD - http://localhost:8080/cache/arewfser Expected response: HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 06 :58:29 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000200000001 This case demonstrates that API Firewall protects the application from Cache-Poisoned DoS attacks. Step 3: Testing the demo based on the stricter OpenAPI 3.0 specification \u00b6 Firstly, please update the path to the OpenAPI 3.0 specification used in the demo: In the docker-compose.yml file, replace the APIFW_API_SPECS environment variable value with the path to the stricter OpenAPI 3.0 specification ( /opt/resources/httpbin-with-constraints.json ). Restart the demo by using the commands: make stop make start Then, to test this demo option, you can use the following methods: Check that API Firewall blocks requests with the required query parameter int that does not match the following definition: ... { \"in\" : \"query\" , \"name\" : \"int\" , \"schema\" : { \"type\" : \"integer\" , \"minimum\" : 10 , \"maximum\" : 100 }, \"required\" : true }, ... Test the definition by using the following requests: # Request with missed required query parameter curl -sD - http://localhost:8080/get # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :09:08 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000100000001 # Request with the int parameter value which is in a valid range curl -sD - http://localhost:8080/get?int = 15 # Expected response HTTP/1.1 200 OK Server: gunicorn/19.9.0 Date: Mon, 31 May 2021 07 :09:38 GMT Content-Type: application/json Content-Length: 280 Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true Apifw-Request-Id: 0000000300000001 ... # Request with the int parameter value which is out of range curl -sD - http://localhost:8080/get?int = 5 # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :09:27 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000200000001 # Request with the int parameter value which is out of range curl -sD - http://localhost:8080/get?int = 1000 # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :09:53 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000400000001 # Request with the int parameter value which is out of range # POTENTIAL EVIL: 8-byte integer overflow can respond with stack drop curl -sD - http://localhost:8080/get?int = 18446744073710000001 # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :10:04 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000500000001 Check that API Firewall blocks requests with the query parameter str that does not match the following definition: ... { \"in\" : \"query\" , \"name\" : \"str\" , \"schema\" : { \"type\" : \"string\" , \"pattern\" : \"^.{1,10}-\\\\d{1,10}$\" } }, ... Test the definition by using the following requests (the int parameter is still required): # Request with the str parameter value that does not match the defined regular expression curl -sD - \"http://localhost:8080/get?int=15&str=fasxxx.xxxawe-6354\" # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :10:42 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000700000001 # Request with the str parameter value that does not match the defined regular expression curl -sD - \"http://localhost:8080/get?int=15&str=faswerffa-63sss54\" # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :10:42 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000700000001 # Request with the str parameter value that matches the defined regular expression curl -sD - http://localhost:8080/get?int = 15 & str = ri0.2-3ur0-6354 # Expected response HTTP/1.1 200 OK Server: gunicorn/19.9.0 Date: Mon, 31 May 2021 07 :11:03 GMT Content-Type: application/json Content-Length: 331 Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true Apifw-Request-Id: 0000000800000001 ... # Request with the str parameter value that does not match the defined regular expression # POTENTIAL EVIL: SQL Injection curl -sD - 'http://localhost:8080/get?int=15&str=\";SELECT%20*%20FROM%20users.credentials;\"' # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :12:04 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000B00000001 Step 4: Stopping the demo code \u00b6 To stop the demo deployment and clear your environment, use the command: make stop","title":"Running the example application and API Firewall with Docker Compose"},{"location":"api-firewall/demos/docker-compose/#running-the-example-application-and-api-firewall-with-docker-compose","text":"This document describes the steps to run the demo code that deploys the application httpbin protected by API Firewall using Docker Compose .","title":"Running the example application and API Firewall with Docker Compose"},{"location":"api-firewall/demos/docker-compose/#used-resources","text":"The following resources are used in this demo: httpbin Docker image as the example application image API Firewall Docker image","title":"Used resources"},{"location":"api-firewall/demos/docker-compose/#the-demo-code-description","text":"The demo code published on our GitHub contains the following configuration files: The following OpenAPI 3.0 specifications located in the volumes directory: httpbin.json is the httpbin OpenAPI 2.0 specification converted to the OpenAPI 3.0 specification format. httpbin-with-constraints.json is the httpbin OpenAPI 3.0 specification with additional API restrictions added explicitly. Both these files will be used to test the demo deployment. Makefile is the configuration file defining Docker routines. docker-compose.yml is the file defining the httpbin and API Firewall Docker images configuration.","title":"The demo code description"},{"location":"api-firewall/demos/docker-compose/#step-1-running-the-demo-code","text":"To run the demo code: Clone the GitHub repository containing the demo code: git clone https://github.com/wallarm/api-firewall.git Change to the demo/docker-compose directory of the cloned repository: cd api-firewall/demo/docker-compose Run the demo code by using the following command: make start The application httpbin protected by API Firewall will be available at http://localhost:8080 . The application httpbin unprotected by API Firewall will be available at http://localhost:8090 . When testing the demo deployment, you can send requests to the unprotected application to know the difference. Proceed to the demo testing.","title":"Step 1: Running the demo code"},{"location":"api-firewall/demos/docker-compose/#step-2-testing-the-demo-based-on-the-original-openapi-30-specification","text":"By default, this demo is running with the original httpbin OpenAPI 3.0 specification. To test this demo option, you can use the following requests: Check that API Firewall blocks requests sent to unexposed path: curl -sD - http://localhost:8080/unexposed/path Expected response: HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 06 :58:29 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000200000001 Check that API Firewall blocks request with string value passed in the parameter that requires integer data type: curl -sD - http://localhost:8080/cache/arewfser Expected response: HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 06 :58:29 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000200000001 This case demonstrates that API Firewall protects the application from Cache-Poisoned DoS attacks.","title":"Step 2: Testing the demo based on the original OpenAPI 3.0 specification"},{"location":"api-firewall/demos/docker-compose/#step-3-testing-the-demo-based-on-the-stricter-openapi-30-specification","text":"Firstly, please update the path to the OpenAPI 3.0 specification used in the demo: In the docker-compose.yml file, replace the APIFW_API_SPECS environment variable value with the path to the stricter OpenAPI 3.0 specification ( /opt/resources/httpbin-with-constraints.json ). Restart the demo by using the commands: make stop make start Then, to test this demo option, you can use the following methods: Check that API Firewall blocks requests with the required query parameter int that does not match the following definition: ... { \"in\" : \"query\" , \"name\" : \"int\" , \"schema\" : { \"type\" : \"integer\" , \"minimum\" : 10 , \"maximum\" : 100 }, \"required\" : true }, ... Test the definition by using the following requests: # Request with missed required query parameter curl -sD - http://localhost:8080/get # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :09:08 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000100000001 # Request with the int parameter value which is in a valid range curl -sD - http://localhost:8080/get?int = 15 # Expected response HTTP/1.1 200 OK Server: gunicorn/19.9.0 Date: Mon, 31 May 2021 07 :09:38 GMT Content-Type: application/json Content-Length: 280 Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true Apifw-Request-Id: 0000000300000001 ... # Request with the int parameter value which is out of range curl -sD - http://localhost:8080/get?int = 5 # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :09:27 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000200000001 # Request with the int parameter value which is out of range curl -sD - http://localhost:8080/get?int = 1000 # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :09:53 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000400000001 # Request with the int parameter value which is out of range # POTENTIAL EVIL: 8-byte integer overflow can respond with stack drop curl -sD - http://localhost:8080/get?int = 18446744073710000001 # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :10:04 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000500000001 Check that API Firewall blocks requests with the query parameter str that does not match the following definition: ... { \"in\" : \"query\" , \"name\" : \"str\" , \"schema\" : { \"type\" : \"string\" , \"pattern\" : \"^.{1,10}-\\\\d{1,10}$\" } }, ... Test the definition by using the following requests (the int parameter is still required): # Request with the str parameter value that does not match the defined regular expression curl -sD - \"http://localhost:8080/get?int=15&str=fasxxx.xxxawe-6354\" # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :10:42 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000700000001 # Request with the str parameter value that does not match the defined regular expression curl -sD - \"http://localhost:8080/get?int=15&str=faswerffa-63sss54\" # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :10:42 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000700000001 # Request with the str parameter value that matches the defined regular expression curl -sD - http://localhost:8080/get?int = 15 & str = ri0.2-3ur0-6354 # Expected response HTTP/1.1 200 OK Server: gunicorn/19.9.0 Date: Mon, 31 May 2021 07 :11:03 GMT Content-Type: application/json Content-Length: 331 Access-Control-Allow-Origin: * Access-Control-Allow-Credentials: true Apifw-Request-Id: 0000000800000001 ... # Request with the str parameter value that does not match the defined regular expression # POTENTIAL EVIL: SQL Injection curl -sD - 'http://localhost:8080/get?int=15&str=\";SELECT%20*%20FROM%20users.credentials;\"' # Expected response HTTP/1.1 403 Forbidden Date: Mon, 31 May 2021 07 :12:04 GMT Content-Type: text/plain ; charset = utf-8 Content-Length: 0 Apifw-Request-Id: 0000000B00000001","title":"Step 3: Testing the demo based on the stricter OpenAPI 3.0 specification"},{"location":"api-firewall/demos/docker-compose/#step-4-stopping-the-demo-code","text":"To stop the demo deployment and clear your environment, use the command: make stop","title":"Step 4: Stopping the demo code"},{"location":"api-firewall/installation-guides/docker-container/","text":"Running API Firewall on Docker \u00b6 This guide walks through downloading, installing, and starting Wallarm API Firewall on Docker. Requirements \u00b6 Installed and configured Docker OpenAPI 3.0 specification developed for the REST API of the application that should be protected with Wallarm API Firewall Step 1. Create the Docker network \u00b6 To allow the containerized application and API Firewall communication without manual linking, create a separate Docker network by using the command docker network create . The application and API Firewall containers will be linked to this network. For example, to create the Docker network named api-firewall-network : docker network create api-firewall-network Step 2. Start the containerized application \u00b6 Start the containerized application that should be protected with API Firewall by using the command docker run and passing the created network name in the option --network . For example, to start the kennethreitz/httpbin Docker container connected to the api-firewall-network and assigned with the backend network alias on the port 8090: docker run --rm -it --network api-firewall-network \\ --network-alias backend -p 8090 :8090 kennethreitz/httpbin Step 3. Pull the Docker image of API Firewall \u00b6 Pull the following API Firewall Docker image: docker pull wallarm/api-firewall Step 4. Start API Firewall \u00b6 Start the pulled API Firewall image by using the command docker run and passing API Firewall configuration in the environment variables as described below. For example, to start API Firewall connected to the api-firewall-network network and assigned with the api-firewall network alias on the port 8088: docker run --rm -it --network api-firewall-network --network-alias api-firewall \\ -v <HOST_PATH_TO_SPEC>:<CONTAINER_PATH_TO_SPEC> -e APIFW_API_SPECS = <PATH_TO_MOUNTED_SPEC> \\ -e APIFW_URL = <API_FIREWALL_URL> -e APIFW_SERVER_URL = <PROTECTED_APP_URL> \\ -e APIFW_REQUEST_VALIDATION = <REQUEST_VALIDATION_MODE> -e APIFW_RESPONSE_VALIDATION = <RESPONSE_VALIDATION_MODE> \\ -p 8088 :8088 wallarm/api-firewall With the -v option , please mount the OpenAPI 3.0 specification to the API Firewall container directory: HOST_PATH_TO_SPEC : the path to the OpenAPI 3.0 specification for your application REST API located on the host machine. The accepted file formats are YAML and JSON ( .yaml , .yml , .json file extensions). For example: /opt/my-api/openapi3/swagger.json . <CONTAINER_PATH_TO_SPEC> : the path to the container directory to mount the OpenAPI 3.0 specification to. For example: /api-firewall/resources/swagger.json . With the -e option , please set the API Firewall configuration through the following environment variables: Environment variable Description Required? Main settings APIFW_API_SPECS Path to the OpenAPI 3.0 specification mounted to the container. For example: /api-firewall/resources/swagger.json Yes APIFW_URL URL for API Firewall. For example: http://0.0.0.0:8088/ . The port value should correspond to the container port published to the host. If API Firewall listens to the HTTPS protocol, please mount the generated SSL/TLS certificate and private key to the container, and pass to the container the API Firewall SSL/TLS settings described below. Yes APIFW_SERVER_URL URL of the application described in the mounted OpenAPI specification that should be protected with API Firewall. For example: http://backend:80 . Yes APIFW_REQUEST_VALIDATION API Firewall mode when validating requests sent to the application URL: BLOCK to block and log the requests that do not match the schema provided in the mounted OpenAPI 3.0 specification (the 403 Forbidden response will be returned to the blocked requests). LOG_ONLY to log but not block the requests that do not match the schema provided in the mounted OpenAPI 3.0 specification. DISABLE to disable request validation. Yes APIFW_RESPONSE_VALIDATION API Firewall mode when validating application responses to incoming requests: BLOCK to block and log the request if the application response to this request does not match the schema provided in the mounted OpenAPI 3.0 specification. This request will be proxied to the application URL but the client will receive the 403 Forbidden response. LOG_ONLY to log but not block the request if the application response to this request does not match the schema provided in the mounted OpenAPI 3.0 specification. DISABLE to disable request validation. Yes APIFW_LOG_LEVEL API Firewall logging level. Possible values: DEBUG to log events of any type (INFO, ERROR, WARNING, and DEBUG). INFO to log events of the INFO, WARNING, and ERROR types. WARNING to log events of the WARNING and ERROR types. ERROR to log events of only the ERROR type. Default value is DEBUG . Logs on requests and responses that do not match the provied schema have the ERROR type. No API Firewall SSL/TLS settings APIFW_TLS_CERTS_PATH The path to the container directory with the mounted certificate and private key generated for API Firewall. No APIFW_TLS_CERT_FILE The name of the file with the SSL/TLS certificate generated for API Firewall and located in the directory specified in APIFW_TLS_CERTS_PATH . No APIFW_TLS_CERT_KEY The name of the file with the SSL/TLS private key generated for API Firewall and located in the directory specified in APIFW_TLS_CERTS_PATH . No Timeout settings APIFW_READ_TIMEOUT The timeout for API Firewall to read the full request (including body) sent to the application URL. The default value is 5s . No APIFW_WRITE_TIMEOUT The timeout for API Firewall to return the response to the request sent to the application URL. The default value is 5s . No APIFW_SERVER_MAX_CONNS_PER_HOST The maximum number of connections that API Firewall can handle simultaneously. The default value is 512 . No APIFW_SERVER_READ_TIMEOUT The timeout for API Firewall to read the full response (including body) returned to the request by the application. The default value is 5s . No APIFW_SERVER_WRITE_TIMEOUT The timeout for API Firewall to write the full request (including body) to the application. The default value is 5s . No APIFW_SERVER_DIAL_TIMEOUT The timeout for API Firewall to connect to the application. The default value is 200ms . No Step 5. Test API Firewall operation \u00b6 To test API Firewall operation, send the request that does not match the mounted Open API 3.0 specification to the API Firewall Docker container address. For example, you can pass the string value in the parameter that requires the integer value. If the request does not match the providded API schema, the appropriate ERROR message will be added to the API Firewall Docker container logs. Step 6. Enable traffic on API Firewall \u00b6 To finalize the API Firewall configuration, please enable incoming traffic on API Firewall by updating your application deployment scheme configuration. For example, this would require updating the Ingress, NGINX, or load balancer settings.","title":"Running API Firewall on Docker"},{"location":"api-firewall/installation-guides/docker-container/#running-api-firewall-on-docker","text":"This guide walks through downloading, installing, and starting Wallarm API Firewall on Docker.","title":"Running API Firewall on Docker"},{"location":"api-firewall/installation-guides/docker-container/#requirements","text":"Installed and configured Docker OpenAPI 3.0 specification developed for the REST API of the application that should be protected with Wallarm API Firewall","title":"Requirements"},{"location":"api-firewall/installation-guides/docker-container/#step-1-create-the-docker-network","text":"To allow the containerized application and API Firewall communication without manual linking, create a separate Docker network by using the command docker network create . The application and API Firewall containers will be linked to this network. For example, to create the Docker network named api-firewall-network : docker network create api-firewall-network","title":"Step 1. Create the Docker network"},{"location":"api-firewall/installation-guides/docker-container/#step-2-start-the-containerized-application","text":"Start the containerized application that should be protected with API Firewall by using the command docker run and passing the created network name in the option --network . For example, to start the kennethreitz/httpbin Docker container connected to the api-firewall-network and assigned with the backend network alias on the port 8090: docker run --rm -it --network api-firewall-network \\ --network-alias backend -p 8090 :8090 kennethreitz/httpbin","title":"Step 2. Start the containerized application"},{"location":"api-firewall/installation-guides/docker-container/#step-3-pull-the-docker-image-of-api-firewall","text":"Pull the following API Firewall Docker image: docker pull wallarm/api-firewall","title":"Step 3. Pull the Docker image of API Firewall"},{"location":"api-firewall/installation-guides/docker-container/#step-4-start-api-firewall","text":"Start the pulled API Firewall image by using the command docker run and passing API Firewall configuration in the environment variables as described below. For example, to start API Firewall connected to the api-firewall-network network and assigned with the api-firewall network alias on the port 8088: docker run --rm -it --network api-firewall-network --network-alias api-firewall \\ -v <HOST_PATH_TO_SPEC>:<CONTAINER_PATH_TO_SPEC> -e APIFW_API_SPECS = <PATH_TO_MOUNTED_SPEC> \\ -e APIFW_URL = <API_FIREWALL_URL> -e APIFW_SERVER_URL = <PROTECTED_APP_URL> \\ -e APIFW_REQUEST_VALIDATION = <REQUEST_VALIDATION_MODE> -e APIFW_RESPONSE_VALIDATION = <RESPONSE_VALIDATION_MODE> \\ -p 8088 :8088 wallarm/api-firewall With the -v option , please mount the OpenAPI 3.0 specification to the API Firewall container directory: HOST_PATH_TO_SPEC : the path to the OpenAPI 3.0 specification for your application REST API located on the host machine. The accepted file formats are YAML and JSON ( .yaml , .yml , .json file extensions). For example: /opt/my-api/openapi3/swagger.json . <CONTAINER_PATH_TO_SPEC> : the path to the container directory to mount the OpenAPI 3.0 specification to. For example: /api-firewall/resources/swagger.json . With the -e option , please set the API Firewall configuration through the following environment variables: Environment variable Description Required? Main settings APIFW_API_SPECS Path to the OpenAPI 3.0 specification mounted to the container. For example: /api-firewall/resources/swagger.json Yes APIFW_URL URL for API Firewall. For example: http://0.0.0.0:8088/ . The port value should correspond to the container port published to the host. If API Firewall listens to the HTTPS protocol, please mount the generated SSL/TLS certificate and private key to the container, and pass to the container the API Firewall SSL/TLS settings described below. Yes APIFW_SERVER_URL URL of the application described in the mounted OpenAPI specification that should be protected with API Firewall. For example: http://backend:80 . Yes APIFW_REQUEST_VALIDATION API Firewall mode when validating requests sent to the application URL: BLOCK to block and log the requests that do not match the schema provided in the mounted OpenAPI 3.0 specification (the 403 Forbidden response will be returned to the blocked requests). LOG_ONLY to log but not block the requests that do not match the schema provided in the mounted OpenAPI 3.0 specification. DISABLE to disable request validation. Yes APIFW_RESPONSE_VALIDATION API Firewall mode when validating application responses to incoming requests: BLOCK to block and log the request if the application response to this request does not match the schema provided in the mounted OpenAPI 3.0 specification. This request will be proxied to the application URL but the client will receive the 403 Forbidden response. LOG_ONLY to log but not block the request if the application response to this request does not match the schema provided in the mounted OpenAPI 3.0 specification. DISABLE to disable request validation. Yes APIFW_LOG_LEVEL API Firewall logging level. Possible values: DEBUG to log events of any type (INFO, ERROR, WARNING, and DEBUG). INFO to log events of the INFO, WARNING, and ERROR types. WARNING to log events of the WARNING and ERROR types. ERROR to log events of only the ERROR type. Default value is DEBUG . Logs on requests and responses that do not match the provied schema have the ERROR type. No API Firewall SSL/TLS settings APIFW_TLS_CERTS_PATH The path to the container directory with the mounted certificate and private key generated for API Firewall. No APIFW_TLS_CERT_FILE The name of the file with the SSL/TLS certificate generated for API Firewall and located in the directory specified in APIFW_TLS_CERTS_PATH . No APIFW_TLS_CERT_KEY The name of the file with the SSL/TLS private key generated for API Firewall and located in the directory specified in APIFW_TLS_CERTS_PATH . No Timeout settings APIFW_READ_TIMEOUT The timeout for API Firewall to read the full request (including body) sent to the application URL. The default value is 5s . No APIFW_WRITE_TIMEOUT The timeout for API Firewall to return the response to the request sent to the application URL. The default value is 5s . No APIFW_SERVER_MAX_CONNS_PER_HOST The maximum number of connections that API Firewall can handle simultaneously. The default value is 512 . No APIFW_SERVER_READ_TIMEOUT The timeout for API Firewall to read the full response (including body) returned to the request by the application. The default value is 5s . No APIFW_SERVER_WRITE_TIMEOUT The timeout for API Firewall to write the full request (including body) to the application. The default value is 5s . No APIFW_SERVER_DIAL_TIMEOUT The timeout for API Firewall to connect to the application. The default value is 200ms . No","title":"Step 4. Start API Firewall"},{"location":"api-firewall/installation-guides/docker-container/#step-5-test-api-firewall-operation","text":"To test API Firewall operation, send the request that does not match the mounted Open API 3.0 specification to the API Firewall Docker container address. For example, you can pass the string value in the parameter that requires the integer value. If the request does not match the providded API schema, the appropriate ERROR message will be added to the API Firewall Docker container logs.","title":"Step 5. Test API Firewall operation"},{"location":"api-firewall/installation-guides/docker-container/#step-6-enable-traffic-on-api-firewall","text":"To finalize the API Firewall configuration, please enable incoming traffic on API Firewall by updating your application deployment scheme configuration. For example, this would require updating the Ingress, NGINX, or load balancer settings.","title":"Step 6. Enable traffic on API Firewall"},{"location":"demo-videos/fast-overview/","text":"Wallarm FAST overview and integration options \u00b6 FAST overview \u00b6 Related documentation articles Wallarm FAST documentation Integrating FAST with GitLab CI/CD \u00b6 Related documentation articles Available options of FAST integration Examples of FAST integration into CI/CD: GitLab CI/CD CircleCI Jenkins Bamboo Azure DevOps Examples of FAST integration via Wallarm API: CircleCI","title":"Wallarm FAST overview and integration options"},{"location":"demo-videos/fast-overview/#wallarm-fast-overview-and-integration-options","text":"","title":"Wallarm FAST overview and integration options"},{"location":"demo-videos/fast-overview/#fast-overview","text":"Related documentation articles Wallarm FAST documentation","title":"FAST overview"},{"location":"demo-videos/fast-overview/#integrating-fast-with-gitlab-cicd","text":"Related documentation articles Available options of FAST integration Examples of FAST integration into CI/CD: GitLab CI/CD CircleCI Jenkins Bamboo Azure DevOps Examples of FAST integration via Wallarm API: CircleCI","title":"Integrating FAST with GitLab CI/CD"},{"location":"demo-videos/notifications/","text":"Notifications and reactions to events detected by Wallarm WAF \u00b6 Integrating Wallarm WAF with existing DevOps tools \u00b6 Related documentation articles Overview of available integrations and configuration steps Using triggers for custom notifications and reactions \u00b6 Related documentation articles Overview of triggers and configuration steps Examples of configured triggers","title":"Notifications and reactions to events detected by Wallarm WAF"},{"location":"demo-videos/notifications/#notifications-and-reactions-to-events-detected-by-wallarm-waf","text":"","title":"Notifications and reactions to events detected by Wallarm WAF"},{"location":"demo-videos/notifications/#integrating-wallarm-waf-with-existing-devops-tools","text":"Related documentation articles Overview of available integrations and configuration steps","title":"Integrating Wallarm WAF with existing DevOps tools"},{"location":"demo-videos/notifications/#using-triggers-for-custom-notifications-and-reactions","text":"Related documentation articles Overview of triggers and configuration steps Examples of configured triggers","title":"Using triggers for custom notifications and reactions"},{"location":"demo-videos/scanner-overview/","text":"Wallarm Scanner overview \u00b6 Wallarm Scanner overview \u00b6 Related documentation articles Scanner overview Configuring Scanner Configuring Wallarm Scanner \u00b6 Related documentation articles Scanner overview Configuring Scanner","title":"Wallarm Scanner overview"},{"location":"demo-videos/scanner-overview/#wallarm-scanner-overview","text":"","title":"Wallarm Scanner overview"},{"location":"demo-videos/scanner-overview/#wallarm-scanner-overview_1","text":"Related documentation articles Scanner overview Configuring Scanner","title":"Wallarm Scanner overview"},{"location":"demo-videos/scanner-overview/#configuring-wallarm-scanner","text":"Related documentation articles Scanner overview Configuring Scanner","title":"Configuring Wallarm Scanner"},{"location":"demo-videos/waf-events-inspection/","text":"Inspecting events detected by the Wallarm WAF node \u00b6 Viewing statistics on detected events on the dashboards \u00b6 Related documentation articles Dashboards overview Event types overview \u00b6 Related documentation articles Checking events in the Wallarm Console Analyzing attacks detected by the WAF node Analyzing vulnerabilities detected by the WAF node Analyzing attacks detected by the WAF node \u00b6 Related documentation articles Checking events in the Wallarm Console Analyzing attacks detected by the WAF node Analyzing vulnerabilities detected by the WAF node","title":"Inspecting events detected by the Wallarm WAF node"},{"location":"demo-videos/waf-events-inspection/#inspecting-events-detected-by-the-wallarm-waf-node","text":"","title":"Inspecting events detected by the Wallarm WAF node"},{"location":"demo-videos/waf-events-inspection/#viewing-statistics-on-detected-events-on-the-dashboards","text":"Related documentation articles Dashboards overview","title":"Viewing statistics on detected events on the dashboards"},{"location":"demo-videos/waf-events-inspection/#event-types-overview","text":"Related documentation articles Checking events in the Wallarm Console Analyzing attacks detected by the WAF node Analyzing vulnerabilities detected by the WAF node","title":"Event types overview"},{"location":"demo-videos/waf-events-inspection/#analyzing-attacks-detected-by-the-waf-node","text":"Related documentation articles Checking events in the Wallarm Console Analyzing attacks detected by the WAF node Analyzing vulnerabilities detected by the WAF node","title":"Analyzing attacks detected by the WAF node"},{"location":"demo-videos/waf-overview/","text":"Wallarm WAF overview and deployment options \u00b6 30 second Wallarm WAF overview \u00b6 Wallarm WAF architecture \u00b6 Related documentation articles Overview of Wallarm WAF components Protecting applications against OWASP Top 10 \u00b6 Related documentation articles The list of attacks and vulnerabilities that Wallarm WAF detects How Wallarm WAF detects vulnerabilities in applications Protecting applications against brute\u2011force attacks \u00b6 Related documentation articles The list of attacks and vulnerabilities that Wallarm WAF detects Configuration of brute force protection Protecting cloud applications with Wallarm WAF \u00b6 Related documentation articles The list of all plaforms availablle for the WAF node deployment Deploying the WAF node as Kubernetes sidecar container \u00b6 Related documentation articles Overview of the WAF node deployment as Kubernetes sidecar container The list of all plaforms availablle for the WAF node deployment Wallarm Console features overview \u00b6 Related documentation articles Documentation for Wallarm Console sections Using multiple accounts in Wallarm system \u00b6","title":"Wallarm WAF overview and deployment options"},{"location":"demo-videos/waf-overview/#wallarm-waf-overview-and-deployment-options","text":"","title":"Wallarm WAF overview and deployment options"},{"location":"demo-videos/waf-overview/#30-second-wallarm-waf-overview","text":"","title":"30 second Wallarm WAF overview"},{"location":"demo-videos/waf-overview/#wallarm-waf-architecture","text":"Related documentation articles Overview of Wallarm WAF components","title":"Wallarm WAF architecture"},{"location":"demo-videos/waf-overview/#protecting-applications-against-owasp-top-10","text":"Related documentation articles The list of attacks and vulnerabilities that Wallarm WAF detects How Wallarm WAF detects vulnerabilities in applications","title":"Protecting applications against OWASP Top 10"},{"location":"demo-videos/waf-overview/#protecting-applications-against-bruteforce-attacks","text":"Related documentation articles The list of attacks and vulnerabilities that Wallarm WAF detects Configuration of brute force protection","title":"Protecting applications against brute\u2011force attacks"},{"location":"demo-videos/waf-overview/#protecting-cloud-applications-with-wallarm-waf","text":"Related documentation articles The list of all plaforms availablle for the WAF node deployment","title":"Protecting cloud applications with Wallarm WAF"},{"location":"demo-videos/waf-overview/#deploying-the-waf-node-as-kubernetes-sidecar-container","text":"Related documentation articles Overview of the WAF node deployment as Kubernetes sidecar container The list of all plaforms availablle for the WAF node deployment","title":"Deploying the WAF node as Kubernetes sidecar container"},{"location":"demo-videos/waf-overview/#wallarm-console-features-overview","text":"Related documentation articles Documentation for Wallarm Console sections","title":"Wallarm Console features overview"},{"location":"demo-videos/waf-overview/#using-multiple-accounts-in-wallarm-system","text":"","title":"Using multiple accounts in Wallarm system"},{"location":"faq/common-errors-after-waf-installation/","text":"Errors after Wallarm WAF installation \u00b6 File Download Scenarios Fail \u00b6 If your file download scenarios fail after installing a filter node, the issue is in the request size exceeding the limit set in the client_max_body_size directive in the Wallarm configuration file. Change the value in client_max_body_size in the directive location for the address that accepts the file uploads. Changing only the location value protects the main page from getting large requests. Change the value in client_max_body_size : Open for editing the configuration file in the /etc/nginx-wallarm directory. Put in the new value: location /file/upload { client_max_body_size 16m; } /file/upload is the address that accepts the file uploads. Detailed directive description is available in the official NGINX documentation . How to fix the errors \"signature could not be verified for wallarm-node\", \"yum doesn't have enough cached data to continue\"? \u00b6 If GPG keys for Wallarm RPM packages are expired, you may get the following error messages: https://repo.wallarm.com/centos/wallarm-node/7/2.18/x86_64/repodata/repomd.xml: [Errno -1] repomd.xml signature could not be verified for wallarm-node_2.18 One of the configured repositories failed (Wallarm Node for CentOS 7 - 2.18), and yum doesn't have enough cached data to continue. To fix the problem, please follow the steps: Remove the previously added repository using the command: sudo yum remove wallarm-node-repo Add a new repository using the command for appropriate CentOS and WAF node versions: CentOS 7.x \u0438\u043b\u0438 Amazon Linux # WAF node and postanalytics module of the 2.16 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/2.16/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm # WAF node and postanalytics module of the 2.18 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/2.18/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm # WAF node and postanalytics module of the 3.0 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x # WAF node and postanalytics module of the 2.16 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/2.16/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm # WAF node and postanalytics module of the 2.18 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/2.18/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm # WAF node and postanalytics module of the 3.0 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm Why does not the WAF node block attacks when operating in blocking mode ( wallarm_mode block )? \u00b6 Using the wallarm_mode directive is only one of several methods of traffic filtration mode configuration. Some of these configuration methods have a higher priority than the wallarm_mode directive value. If you have configured blocking mode via wallarm_mode block but WAF node does not block attacks, please ensure that filtration mode is not overridden using other configuration methods: Using the rule Set traffic filtration mode In the General section of the Wallarm Console More details on filtration mode configuration methods \u2192","title":"Errors after Wallarm WAF installation"},{"location":"faq/common-errors-after-waf-installation/#errors-after-wallarm-waf-installation","text":"","title":"Errors after Wallarm WAF installation"},{"location":"faq/common-errors-after-waf-installation/#file-download-scenarios-fail","text":"If your file download scenarios fail after installing a filter node, the issue is in the request size exceeding the limit set in the client_max_body_size directive in the Wallarm configuration file. Change the value in client_max_body_size in the directive location for the address that accepts the file uploads. Changing only the location value protects the main page from getting large requests. Change the value in client_max_body_size : Open for editing the configuration file in the /etc/nginx-wallarm directory. Put in the new value: location /file/upload { client_max_body_size 16m; } /file/upload is the address that accepts the file uploads. Detailed directive description is available in the official NGINX documentation .","title":"File Download Scenarios Fail"},{"location":"faq/common-errors-after-waf-installation/#how-to-fix-the-errors-signature-could-not-be-verified-for-wallarm-node-yum-doesnt-have-enough-cached-data-to-continue","text":"If GPG keys for Wallarm RPM packages are expired, you may get the following error messages: https://repo.wallarm.com/centos/wallarm-node/7/2.18/x86_64/repodata/repomd.xml: [Errno -1] repomd.xml signature could not be verified for wallarm-node_2.18 One of the configured repositories failed (Wallarm Node for CentOS 7 - 2.18), and yum doesn't have enough cached data to continue. To fix the problem, please follow the steps: Remove the previously added repository using the command: sudo yum remove wallarm-node-repo Add a new repository using the command for appropriate CentOS and WAF node versions: CentOS 7.x \u0438\u043b\u0438 Amazon Linux # WAF node and postanalytics module of the 2.16 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/2.16/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm # WAF node and postanalytics module of the 2.18 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/2.18/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm # WAF node and postanalytics module of the 3.0 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x # WAF node and postanalytics module of the 2.16 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/2.16/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm # WAF node and postanalytics module of the 2.18 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/2.18/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm # WAF node and postanalytics module of the 3.0 version sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm","title":"How to fix the errors \"signature could not be verified for wallarm-node\", \"yum doesn't have enough cached data to continue\"?"},{"location":"faq/common-errors-after-waf-installation/#why-does-not-the-waf-node-block-attacks-when-operating-in-blocking-mode-wallarm_mode-block","text":"Using the wallarm_mode directive is only one of several methods of traffic filtration mode configuration. Some of these configuration methods have a higher priority than the wallarm_mode directive value. If you have configured blocking mode via wallarm_mode block but WAF node does not block attacks, please ensure that filtration mode is not overridden using other configuration methods: Using the rule Set traffic filtration mode In the General section of the Wallarm Console More details on filtration mode configuration methods \u2192","title":"Why does not the WAF node block attacks when operating in blocking mode (wallarm_mode block)?"},{"location":"faq/waf-aws-via-terraform-installation/","text":"Deploying WAF Node in AWS Using Terraform \u00b6 On the first run terraform apply command fails with message \"Error: Provider produced inconsistent final plan\". What to do? \u00b6 Please try to run terraform apply one more time. This should solve the problem. How to access the created WAF node instances? \u00b6 You can get a remote access to the server using user admin and proper SSH private key. It looks like a WAF node is not getting configured properly. How to debug the instance? \u00b6 Get a remote access to the server using user admin and proper SSH private key. Review cloud-init logs: /var/log/cloud-init.log /var/log/cloud-init-output.log Review running processes using the command ps -ef . Check the NGINX configuration for correctness using command nginx -t . Review NGINX error logs in the file /var/log/nginx/error.log .","title":"Deploying WAF Node in AWS Using Terraform"},{"location":"faq/waf-aws-via-terraform-installation/#deploying-waf-node-in-aws-using-terraform","text":"","title":"Deploying WAF Node in AWS Using Terraform"},{"location":"faq/waf-aws-via-terraform-installation/#on-the-first-run-terraform-apply-command-fails-with-message-error-provider-produced-inconsistent-final-plan-what-to-do","text":"Please try to run terraform apply one more time. This should solve the problem.","title":"On the first run terraform apply command fails with message \"Error: Provider produced inconsistent final plan\". What to do?"},{"location":"faq/waf-aws-via-terraform-installation/#how-to-access-the-created-waf-node-instances","text":"You can get a remote access to the server using user admin and proper SSH private key.","title":"How to access the created WAF node instances?"},{"location":"faq/waf-aws-via-terraform-installation/#it-looks-like-a-waf-node-is-not-getting-configured-properly-how-to-debug-the-instance","text":"Get a remote access to the server using user admin and proper SSH private key. Review cloud-init logs: /var/log/cloud-init.log /var/log/cloud-init-output.log Review running processes using the command ps -ef . Check the NGINX configuration for correctness using command nginx -t . Review NGINX error logs in the file /var/log/nginx/error.log .","title":"It looks like a WAF node is not getting configured properly. How to debug the instance?"},{"location":"faq/waf-ingress-installation/","text":"Installing Wallarm WAF Ingress controller \u00b6 How to check which clients' IP addresses are detected/used by the Ingress controller? \u00b6 Take a look at the controller container\u2019s log and find records about handled requests. In the default logging format the first reported field is the detected client IP address. 25.229.38.234 is the detected IP address in the example below: [wallarm-ingress-nginx-ingress-controller-775cf75564-6jlt9 nginx-ingress-controller] 25.229.38.234 - - [14/Mar/2020:23:55:11 +0000] \"GET /ping HTTP/1.1\" 200 893 \"-\" \"curl/7.64.1\" 172 0.020 [default-sise-80] [] 172.17.0.5:8080 893 0.020 200 d8402076753798d3b065269c16d4b34f Go to your Wallarm Console for the US cloud or for the EU cloud \u2192 the Events section and expand request details. An IP address is displayed in the Source field. For example: If the list of attacks is empty, you can send a test attack to the application protected by the Wallarm Ingress controller. How to check that the Ingress controller is receiving the X-FORWARDED-FOR request header? \u00b6 Please go to Wallarm Console for the US cloud or for the EU cloud \u2192 the Events section and expand the request details. In the displayed request details, pay attention to the X-FORWARDED-FOR header. For example: If the list of attacks is empty, you can send a test attack to the application protected by the Wallarm Ingress controller.","title":"Installing Wallarm WAF Ingress controller"},{"location":"faq/waf-ingress-installation/#installing-wallarm-waf-ingress-controller","text":"","title":"Installing Wallarm WAF Ingress controller"},{"location":"faq/waf-ingress-installation/#how-to-check-which-clients-ip-addresses-are-detectedused-by-the-ingress-controller","text":"Take a look at the controller container\u2019s log and find records about handled requests. In the default logging format the first reported field is the detected client IP address. 25.229.38.234 is the detected IP address in the example below: [wallarm-ingress-nginx-ingress-controller-775cf75564-6jlt9 nginx-ingress-controller] 25.229.38.234 - - [14/Mar/2020:23:55:11 +0000] \"GET /ping HTTP/1.1\" 200 893 \"-\" \"curl/7.64.1\" 172 0.020 [default-sise-80] [] 172.17.0.5:8080 893 0.020 200 d8402076753798d3b065269c16d4b34f Go to your Wallarm Console for the US cloud or for the EU cloud \u2192 the Events section and expand request details. An IP address is displayed in the Source field. For example: If the list of attacks is empty, you can send a test attack to the application protected by the Wallarm Ingress controller.","title":"How to check which clients' IP addresses are detected/used by the Ingress controller?"},{"location":"faq/waf-ingress-installation/#how-to-check-that-the-ingress-controller-is-receiving-the-x-forwarded-for-request-header","text":"Please go to Wallarm Console for the US cloud or for the EU cloud \u2192 the Events section and expand the request details. In the displayed request details, pay attention to the X-FORWARDED-FOR header. For example: If the list of attacks is empty, you can send a test attack to the application protected by the Wallarm Ingress controller.","title":"How to check that the Ingress controller is receiving the X-FORWARDED-FOR request header?"},{"location":"faq/waf-nginx-compatibility/","text":"Compatibility of Wallarm WAF with NGINX versions \u00b6 Is Wallarm WAF compatible with NGINX mainline? \u00b6 No, Wallarm WAF is incompatible with NGINX mainline . You can install Wallarm WAF in the following ways: connect to the official open source NGINX stable following these instructions connect to NGINX installed from Debian/CentOS repositories following these instructions connect to the official commercial NGINX Plus following these instructions Is Wallarm WAF compatible with the custom build of NGINX? \u00b6 Yes, Wallarm WAF module can be connected to the custom build of NGINX after rebuilding WAF packages. To rebuild the packages, please contact Wallarm technical support team and send the following data: Linux kernel version: uname -a Linux distributive: cat /etc/*release NGINX version: NGINX official build : /usr/sbin/nginx -V NGINX custom build: <path to nginx>/nginx -V Compatibility signature: NGINX official build : egrep -ao '.,.,.,[01]{33}' /usr/sbin/nginx NGINX custom build: egrep -ao '.,.,.,[01]{33}' <path to nginx>/nginx The user (and the user's group) who is running the NGINX worker processes: grep -w 'user' <path-to-the-NGINX-configuration-files/nginx.conf>","title":"Compatibility of Wallarm WAF with NGINX versions"},{"location":"faq/waf-nginx-compatibility/#compatibility-of-wallarm-waf-with-nginx-versions","text":"","title":"Compatibility of Wallarm WAF with NGINX versions"},{"location":"faq/waf-nginx-compatibility/#is-wallarm-waf-compatible-with-nginx-mainline","text":"No, Wallarm WAF is incompatible with NGINX mainline . You can install Wallarm WAF in the following ways: connect to the official open source NGINX stable following these instructions connect to NGINX installed from Debian/CentOS repositories following these instructions connect to the official commercial NGINX Plus following these instructions","title":"Is Wallarm WAF compatible with NGINX mainline?"},{"location":"faq/waf-nginx-compatibility/#is-wallarm-waf-compatible-with-the-custom-build-of-nginx","text":"Yes, Wallarm WAF module can be connected to the custom build of NGINX after rebuilding WAF packages. To rebuild the packages, please contact Wallarm technical support team and send the following data: Linux kernel version: uname -a Linux distributive: cat /etc/*release NGINX version: NGINX official build : /usr/sbin/nginx -V NGINX custom build: <path to nginx>/nginx -V Compatibility signature: NGINX official build : egrep -ao '.,.,.,[01]{33}' /usr/sbin/nginx NGINX custom build: egrep -ao '.,.,.,[01]{33}' <path to nginx>/nginx The user (and the user's group) who is running the NGINX worker processes: grep -w 'user' <path-to-the-NGINX-configuration-files/nginx.conf>","title":"Is Wallarm WAF compatible with the custom build of NGINX?"},{"location":"faq/wallarm-status-page/","text":"Wallarm service status page \u00b6 Does Wallarm have the page which displays the status of Wallarm service availability? \u00b6 Yes, the Wallarm status page is available at https://status.wallarm.com . The page displays live and historical data on the availability of the Wallarm Console and Wallarm API services for each Wallarm Cloud: Wallarm EU Cloud Wallarm US Cloud Will I receive a notification when a service status changes? \u00b6 Yes, if you are subscribed to updates. To subscribe, please click SUBSCRIBE TO UPDATES and select the subscription channel: Email to receive notifications when Wallarm creates, updates, or resolves an incident. SMS to receive notifications when Wallarm creates or resolves an incident. Slack to receive incident updates and maintenance status messages. Webhook to receive notifications when Wallarm creates an incident, updates an incident, resolves an incident, or changes a service status. What do the services statuses mean? \u00b6 Degraded performance means the service is working but is slow or otherwise impacted in a minor way. Partial outage means the services are completely broken for a subset of clients. Major outage means services are completely unavailable. When is an incident created? \u00b6 Incidents are created when services are having downtime. During a downtime-related event, we add a page describing the issue, what we are doing about it, and when we expect the issue will be fixed. As time goes on, the cause of the incident is identified, the identified incident is then repaired, and the incident status is updated to reflect the current status.","title":"Wallarm service status page"},{"location":"faq/wallarm-status-page/#wallarm-service-status-page","text":"","title":"Wallarm service status page"},{"location":"faq/wallarm-status-page/#does-wallarm-have-the-page-which-displays-the-status-of-wallarm-service-availability","text":"Yes, the Wallarm status page is available at https://status.wallarm.com . The page displays live and historical data on the availability of the Wallarm Console and Wallarm API services for each Wallarm Cloud: Wallarm EU Cloud Wallarm US Cloud","title":"Does Wallarm have the page which displays the status of Wallarm service availability?"},{"location":"faq/wallarm-status-page/#will-i-receive-a-notification-when-a-service-status-changes","text":"Yes, if you are subscribed to updates. To subscribe, please click SUBSCRIBE TO UPDATES and select the subscription channel: Email to receive notifications when Wallarm creates, updates, or resolves an incident. SMS to receive notifications when Wallarm creates or resolves an incident. Slack to receive incident updates and maintenance status messages. Webhook to receive notifications when Wallarm creates an incident, updates an incident, resolves an incident, or changes a service status.","title":"Will I receive a notification when a service status changes?"},{"location":"faq/wallarm-status-page/#what-do-the-services-statuses-mean","text":"Degraded performance means the service is working but is slow or otherwise impacted in a minor way. Partial outage means the services are completely broken for a subset of clients. Major outage means services are completely unavailable.","title":"What do the services statuses mean?"},{"location":"faq/wallarm-status-page/#when-is-an-incident-created","text":"Incidents are created when services are having downtime. During a downtime-related event, we add a page describing the issue, what we are doing about it, and when we expect the issue will be fixed. As time goes on, the cause of the incident is identified, the identified incident is then repaired, and the incident status is updated to reflect the current status.","title":"When is an incident created?"},{"location":"partner-waf-node/connecting-clients/","text":"Creating and linking clients \u00b6 Requirements \u00b6 Access to the technical client account with the Global administrator user role and disabled two\u2011factor authentication in the Wallarm Console Partner UUID Procedure for creating and linking clients \u00b6 Clients are created and linked to the partner account via the Wallarm API. Authenticated requests to Wallarm API can be sent from the own client or from the API Reference UI. The method of request authentication depends on the client sending the request: For requests sent from the API Reference UI , it is required to sign in to the Wallarm Console with the Global administrator user role and update the API Reference by the link: https://apiconsole.eu1.wallarm.com/ for the EU Cloud https://apiconsole.us1.wallarm.com/ for the US Cloud For requests sent from the own client , it is required to pass in the request the user UUID and secret key Step 1: Create the client via the Wallarm API \u00b6 At this step, a partner client account will be created and linked to the partner account. Send the POST request to the route /v1/objects/client/create with the following parameters: Parameter Description Request part Required name Client name. Body Yes vuln_prefix Vulnerability prefix that Wallarm will use for vulnerability tracking and association with the client. The prefix must contain four capital letters or numbers and be related to a client name. For example: CLNT for the client Client . Body Yes partner_uuid Partner UUID . Body Yes language Language for the client account in the Wallarm Console interface: en or ru . By default, the language specififed when switching the account to the partner status is used. Body No X-WallarmAPI-UUID User UUID . Header Yes, when sending a request from own client X-WallarmAPI-Secret Secret key . Header Yes, when sending a request from own client Show an example of the request sent from own client EU Cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/client/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"name\\\": \\\"Client\\\", \\\"vuln_prefix\\\": \\\"CLNT\\\", \\\"partner_uuid\\\": \\\"YOUR_PARTNER_UUID\\\"}\" US Cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/client/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"name\\\": \\\"Client\\\", \\\"vuln_prefix\\\": \\\"CLNT\\\", \\\"partner_uuid\\\": \\\"YOUR_PARTNER_UUID\\\"}\" Copy the values of the id and partnerid parameters from the response to the request. The parameters will be used when linking a client to a partner account. Created clients will be visually presented in the Wallarm Console for the global partner users . For example, Client 1 and Client 2 : Step 2: Link the client to a partner account via the Wallarm API \u00b6 At this step, ID will be set for a partner-client application link. One client might have several applications and several IDs, respectively. This ID will be used in NGINX configuration ( wallarm_instance ) for splitting the traffic by client applications. Send the POST request to the route /v2/partner/<partnerid>/partner_client with the following parameters: Parameter Description Request part Required partnerid The partnerid value obtained after the client was created. Path Yes clientid Client ID obtained after client creation ( id ). Body Yes id ID for the partner-client link. Can be an arbitrary positive integer. Body Yes X-WallarmAPI-UUID User UUID . Header Yes, when sending a request from own client X-WallarmAPI-Secret Secret key . Header Yes, when sending a request from own client Show an example of the request sent from own client EU Cloud curl -v -X POST \"https://api.wallarm.com/v2/partner/111/partner_client\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"clientid\\\": 888, \\\"id\\\": \\\"13\\\"}\" US Cloud curl -v -X POST \"https://us1.api.wallarm.com/v2/partner/111/partner_client\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"clientid\\\": 888, \\\"id\\\": \\\"14\\\"}\" Copy and save the id value you passed in the request. This ID will be used in NGINX configuration ( wallarm_instance ) for splitting several clients traffic. If you configure the WAF node for several applications of the client, send the API request for each application passing different id value. When the client resource gets the traffic, the configured id will be displayed in the Wallarm Console \u2192 Settings \u2192 Applications for an appropriate partner client account. Providing clients with access to the Wallarm Console \u00b6 You can provide your clients with access to their accounts in the Wallarm Console. Clients will be able to track blocked requests, analyze discovered vulnerabilities, and perform additional configuration of the WAF node via the Wallarm Console. To provide a client with access to an account, go to the appropriate partner client account via the client selector \u2192 section Settings \u2192 Users and add users with the required roles. Only regular roles can be set for users of the partner client account. Reqular roles give access to only one partner client account. Open the roles description \u2192","title":"Creating and linking clients"},{"location":"partner-waf-node/connecting-clients/#creating-and-linking-clients","text":"","title":"Creating and linking clients"},{"location":"partner-waf-node/connecting-clients/#requirements","text":"Access to the technical client account with the Global administrator user role and disabled two\u2011factor authentication in the Wallarm Console Partner UUID","title":"Requirements"},{"location":"partner-waf-node/connecting-clients/#procedure-for-creating-and-linking-clients","text":"Clients are created and linked to the partner account via the Wallarm API. Authenticated requests to Wallarm API can be sent from the own client or from the API Reference UI. The method of request authentication depends on the client sending the request: For requests sent from the API Reference UI , it is required to sign in to the Wallarm Console with the Global administrator user role and update the API Reference by the link: https://apiconsole.eu1.wallarm.com/ for the EU Cloud https://apiconsole.us1.wallarm.com/ for the US Cloud For requests sent from the own client , it is required to pass in the request the user UUID and secret key","title":"Procedure for creating and linking clients"},{"location":"partner-waf-node/connecting-clients/#step-1-create-the-client-via-the-wallarm-api","text":"At this step, a partner client account will be created and linked to the partner account. Send the POST request to the route /v1/objects/client/create with the following parameters: Parameter Description Request part Required name Client name. Body Yes vuln_prefix Vulnerability prefix that Wallarm will use for vulnerability tracking and association with the client. The prefix must contain four capital letters or numbers and be related to a client name. For example: CLNT for the client Client . Body Yes partner_uuid Partner UUID . Body Yes language Language for the client account in the Wallarm Console interface: en or ru . By default, the language specififed when switching the account to the partner status is used. Body No X-WallarmAPI-UUID User UUID . Header Yes, when sending a request from own client X-WallarmAPI-Secret Secret key . Header Yes, when sending a request from own client Show an example of the request sent from own client EU Cloud curl -v -X POST \"https://api.wallarm.com/v1/objects/client/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"name\\\": \\\"Client\\\", \\\"vuln_prefix\\\": \\\"CLNT\\\", \\\"partner_uuid\\\": \\\"YOUR_PARTNER_UUID\\\"}\" US Cloud curl -v -X POST \"https://us1.api.wallarm.com/v1/objects/client/create\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"name\\\": \\\"Client\\\", \\\"vuln_prefix\\\": \\\"CLNT\\\", \\\"partner_uuid\\\": \\\"YOUR_PARTNER_UUID\\\"}\" Copy the values of the id and partnerid parameters from the response to the request. The parameters will be used when linking a client to a partner account. Created clients will be visually presented in the Wallarm Console for the global partner users . For example, Client 1 and Client 2 :","title":"Step 1: Create the client via the Wallarm API"},{"location":"partner-waf-node/connecting-clients/#step-2-link-the-client-to-a-partner-account-via-the-wallarm-api","text":"At this step, ID will be set for a partner-client application link. One client might have several applications and several IDs, respectively. This ID will be used in NGINX configuration ( wallarm_instance ) for splitting the traffic by client applications. Send the POST request to the route /v2/partner/<partnerid>/partner_client with the following parameters: Parameter Description Request part Required partnerid The partnerid value obtained after the client was created. Path Yes clientid Client ID obtained after client creation ( id ). Body Yes id ID for the partner-client link. Can be an arbitrary positive integer. Body Yes X-WallarmAPI-UUID User UUID . Header Yes, when sending a request from own client X-WallarmAPI-Secret Secret key . Header Yes, when sending a request from own client Show an example of the request sent from own client EU Cloud curl -v -X POST \"https://api.wallarm.com/v2/partner/111/partner_client\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"clientid\\\": 888, \\\"id\\\": \\\"13\\\"}\" US Cloud curl -v -X POST \"https://us1.api.wallarm.com/v2/partner/111/partner_client\" -H \"X-WallarmAPI-UUID: YOUR_UUID\" -H \"X-WallarmAPI-Secret: YOUR_SECRET_KEY\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d \"{ \\\"clientid\\\": 888, \\\"id\\\": \\\"14\\\"}\" Copy and save the id value you passed in the request. This ID will be used in NGINX configuration ( wallarm_instance ) for splitting several clients traffic. If you configure the WAF node for several applications of the client, send the API request for each application passing different id value. When the client resource gets the traffic, the configured id will be displayed in the Wallarm Console \u2192 Settings \u2192 Applications for an appropriate partner client account.","title":"Step 2: Link the client to a partner account via the Wallarm API"},{"location":"partner-waf-node/connecting-clients/#providing-clients-with-access-to-the-wallarm-console","text":"You can provide your clients with access to their accounts in the Wallarm Console. Clients will be able to track blocked requests, analyze discovered vulnerabilities, and perform additional configuration of the WAF node via the Wallarm Console. To provide a client with access to an account, go to the appropriate partner client account via the client selector \u2192 section Settings \u2192 Users and add users with the required roles. Only regular roles can be set for users of the partner client account. Reqular roles give access to only one partner client account. Open the roles description \u2192","title":"Providing clients with access to the Wallarm Console"},{"location":"partner-waf-node/creating-partner-account/","text":"Creating and configuring a partner account \u00b6 Procedure for creating a partner account \u00b6 To create a partner account: Sign up for the Wallarm Console and send a request for switching your account to partner status to Wallarm technical support. Get access to the technical client's account and obtain the parameters required to install partner WAF nodes from the Wallarm technical support. Step 1: Sign up and send a request to enable partner status \u00b6 Fill in and confirm the registration form in the Wallarm Console in the EU Coud or US Cloud . Corporate email Please sign up using a corporate email address. Open your email inbox and activate the account using the link from received message. Send a request for switching your account to a partner status and for creating a technical client account to the Wallarm technical support . Send the following data with the request: Name of the used Wallarm Cloud (EU Cloud or US Cloud) Names for a partner account and technical client account Email addresses of employees who should have access to the technical client account (after switching your account to a partner status, you will be able to add employees yourself) Logo for branded Wallarm Console, emails and reports Language for the Wallarm Console interface (English or Russian) Custom domain for the Wallarm Console, certificate and encryption key for the domain Your technical support email address Step 2: Access the partner account and get parameters for the WAF node configuration \u00b6 After switching your account to partner status and creating a technical client account , Wallarm technical support staff will: Add you to the list of users of the technical client account with the role Global administrator . If you sent email adrresses of your employees, the Wallarm technical support will add employees to the list of users of the technical client account with the role Global read only . Unregistered employees will receive emails with the link for setting a new password to access the technical client account. Send you the partner UUID. The credential will be used for linking clients. Providing employees with access to a technical client account \u00b6 To manage partner WAF nodes with your team, you can provide your employees with access to the technical client account . Employees can be added to the account with the following user roles: Regular roles provide access only to the technical client account Global roles provide access to the technical client account and to the accounts of all partner clients linked to the partner account Open the roles description \u2192","title":"Creating and configuring a partner account"},{"location":"partner-waf-node/creating-partner-account/#creating-and-configuring-a-partner-account","text":"","title":"Creating and configuring a partner account"},{"location":"partner-waf-node/creating-partner-account/#procedure-for-creating-a-partner-account","text":"To create a partner account: Sign up for the Wallarm Console and send a request for switching your account to partner status to Wallarm technical support. Get access to the technical client's account and obtain the parameters required to install partner WAF nodes from the Wallarm technical support.","title":"Procedure for creating a partner account"},{"location":"partner-waf-node/creating-partner-account/#step-1-sign-up-and-send-a-request-to-enable-partner-status","text":"Fill in and confirm the registration form in the Wallarm Console in the EU Coud or US Cloud . Corporate email Please sign up using a corporate email address. Open your email inbox and activate the account using the link from received message. Send a request for switching your account to a partner status and for creating a technical client account to the Wallarm technical support . Send the following data with the request: Name of the used Wallarm Cloud (EU Cloud or US Cloud) Names for a partner account and technical client account Email addresses of employees who should have access to the technical client account (after switching your account to a partner status, you will be able to add employees yourself) Logo for branded Wallarm Console, emails and reports Language for the Wallarm Console interface (English or Russian) Custom domain for the Wallarm Console, certificate and encryption key for the domain Your technical support email address","title":"Step 1: Sign up and send a request to enable partner status"},{"location":"partner-waf-node/creating-partner-account/#step-2-access-the-partner-account-and-get-parameters-for-the-waf-node-configuration","text":"After switching your account to partner status and creating a technical client account , Wallarm technical support staff will: Add you to the list of users of the technical client account with the role Global administrator . If you sent email adrresses of your employees, the Wallarm technical support will add employees to the list of users of the technical client account with the role Global read only . Unregistered employees will receive emails with the link for setting a new password to access the technical client account. Send you the partner UUID. The credential will be used for linking clients.","title":"Step 2: Access the partner account and get parameters for the WAF node configuration"},{"location":"partner-waf-node/creating-partner-account/#providing-employees-with-access-to-a-technical-client-account","text":"To manage partner WAF nodes with your team, you can provide your employees with access to the technical client account . Employees can be added to the account with the following user roles: Regular roles provide access only to the technical client account Global roles provide access to the technical client account and to the accounts of all partner clients linked to the partner account Open the roles description \u2192","title":"Providing employees with access to a technical client account"},{"location":"partner-waf-node/installing-partner-waf-node/","text":"Installing and configuring a partner WAF node \u00b6 Requirements \u00b6 Partner account in the Wallarm system and a parther UUID Linked clients and IDs of the partner-client links Execution of commands by the user with the Global administrator or Deploy / Administrator role. The user with the Deploy / Administrator role must be added to the technical client or partner client account depending on which account the WAF node should be created Disabled two\u2011factor authentication for the user executing the commands Supported platform for the WAF node installation Partner WAF node characteristics \u00b6 Partner WAF node has the following characteristics: Can be installed on the same platforms and according to the same instructions as a regular WAF node. Can be installed on the technical client or partner client level. If you want to provide a client with access to the Wallarm Console, the WAF node must be installed at the corresponding partner client level. Can be configured according to the same instructions as a regular WAF node, except for: The directive wallarm_instance is used to split settings by the client applications. To enable blocking of requests by IP addresses, please send a request to Wallarm technical support . After blocking is enabled, to block IP addresses, you need to add them to the blacklist at an appropriate partner client account level. Recommendations for a partner WAF node installation \u00b6 If the client should access the Wallarm Console, the WAF node should be created within an appropriate partner client account Describe the WAF node configuration in the client's NGINX configuration file Procedure for a partner WAF node installation \u00b6 Select a WAF node installation form and follow the appropriate instructions: Module for NGINX stable from the NGINX repository \u041c\u043e\u0434\u0443\u043b\u044c \u0434\u043b\u044f NGINX stable from the Debian/CentOS repository Module for NGINX Plus Docker container with NGINX modules Docker container with Envoy modules NGINX Ingress controller Sidecar container AWS image Google Cloud Platform image Yandex.Cloud image Module for Kong Send a request for switching the WAF node to partner status to the Wallarm technical support . Send the following data with the request: Name of the used Wallarm Cloud (EU Cloud or US Cloud) Name of the partner account Partner UUID obtained when creating a partner account Installed WAF node UUID displayed in the Wallarm Console \u2192 section Nodes Open the client's NGINX configuration file and specify the partner-client link ID in the wallarm_instance directive. Example of the client's NGINX configuration file: server { listen 80 ; server_name client1.com ; wallarm_mode block ; wallarm_instance 13 ; location / { proxy_pass http://upstream1:8080 ; } } server { listen 80 ; server_name client2.com ; wallarm_mode monitoring ; wallarm_instance 14 ; location / { proxy_pass http://upstream2:8080 ; } } On the client side, the DNS A records with the partner IP address are configured On the partner side, proxying of requests to the addresses of clients ( http://upstream1:8080 for the client with the partner-client link ID 13 and http://upstream2:8080 for the client with the partner-client link ID 14) is configured All incoming requests are processed on the partner address, legitimate requests are sent to http://upstream1:8080 for the client with the partner-client link ID 13 and to http://upstream2:8080 for the client with the partner-client link ID 14 Configuring a partner WAF node \u00b6 To customize the WAF node settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Configuring dynamic DNS resolution in NGINX Double\u2011detection of attacks with libdetection","title":"Installing and configuring a partner node"},{"location":"partner-waf-node/installing-partner-waf-node/#installing-and-configuring-a-partner-waf-node","text":"","title":"Installing and configuring a partner WAF node"},{"location":"partner-waf-node/installing-partner-waf-node/#requirements","text":"Partner account in the Wallarm system and a parther UUID Linked clients and IDs of the partner-client links Execution of commands by the user with the Global administrator or Deploy / Administrator role. The user with the Deploy / Administrator role must be added to the technical client or partner client account depending on which account the WAF node should be created Disabled two\u2011factor authentication for the user executing the commands Supported platform for the WAF node installation","title":"Requirements"},{"location":"partner-waf-node/installing-partner-waf-node/#partner-waf-node-characteristics","text":"Partner WAF node has the following characteristics: Can be installed on the same platforms and according to the same instructions as a regular WAF node. Can be installed on the technical client or partner client level. If you want to provide a client with access to the Wallarm Console, the WAF node must be installed at the corresponding partner client level. Can be configured according to the same instructions as a regular WAF node, except for: The directive wallarm_instance is used to split settings by the client applications. To enable blocking of requests by IP addresses, please send a request to Wallarm technical support . After blocking is enabled, to block IP addresses, you need to add them to the blacklist at an appropriate partner client account level.","title":"Partner WAF node characteristics"},{"location":"partner-waf-node/installing-partner-waf-node/#recommendations-for-a-partner-waf-node-installation","text":"If the client should access the Wallarm Console, the WAF node should be created within an appropriate partner client account Describe the WAF node configuration in the client's NGINX configuration file","title":"Recommendations for a partner WAF node installation"},{"location":"partner-waf-node/installing-partner-waf-node/#procedure-for-a-partner-waf-node-installation","text":"Select a WAF node installation form and follow the appropriate instructions: Module for NGINX stable from the NGINX repository \u041c\u043e\u0434\u0443\u043b\u044c \u0434\u043b\u044f NGINX stable from the Debian/CentOS repository Module for NGINX Plus Docker container with NGINX modules Docker container with Envoy modules NGINX Ingress controller Sidecar container AWS image Google Cloud Platform image Yandex.Cloud image Module for Kong Send a request for switching the WAF node to partner status to the Wallarm technical support . Send the following data with the request: Name of the used Wallarm Cloud (EU Cloud or US Cloud) Name of the partner account Partner UUID obtained when creating a partner account Installed WAF node UUID displayed in the Wallarm Console \u2192 section Nodes Open the client's NGINX configuration file and specify the partner-client link ID in the wallarm_instance directive. Example of the client's NGINX configuration file: server { listen 80 ; server_name client1.com ; wallarm_mode block ; wallarm_instance 13 ; location / { proxy_pass http://upstream1:8080 ; } } server { listen 80 ; server_name client2.com ; wallarm_mode monitoring ; wallarm_instance 14 ; location / { proxy_pass http://upstream2:8080 ; } } On the client side, the DNS A records with the partner IP address are configured On the partner side, proxying of requests to the addresses of clients ( http://upstream1:8080 for the client with the partner-client link ID 13 and http://upstream2:8080 for the client with the partner-client link ID 14) is configured All incoming requests are processed on the partner address, legitimate requests are sent to http://upstream1:8080 for the client with the partner-client link ID 13 and to http://upstream2:8080 for the client with the partner-client link ID 14","title":"Procedure for a partner WAF node installation"},{"location":"partner-waf-node/installing-partner-waf-node/#configuring-a-partner-waf-node","text":"To customize the WAF node settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Configuring dynamic DNS resolution in NGINX Double\u2011detection of attacks with libdetection","title":"Configuring a partner WAF node"},{"location":"partner-waf-node/overview/","text":"Partner scheme overview \u00b6 Definition of a partner and partner WAF node \u00b6 Partner is an organization that installs a WAF node within its system infrastructure and distributes the system with a WAF node to its own clients. Partner WAF node is a WAF node installed by the partner. Scheme of traffic processing by a partner WAF node \u00b6 If one partner WAF node is installed within the partner infrastructure, client traffic is processed as follows: One WAF node processes traffic of several clients (Client 1, Client 2). The WAF node identifies the client that receives the traffic by the partner-client link ID ( wallarm_instance ). For the domains https://client1.com and https://client2.com , the DNS A records with the partner IP address 225.130.128.241 are configured. This setting is shown as an example, a different setting can be used on the partner and client side. On the partner's side, proxying of legitimate requests to the addresses of clients Client 1 ( http://upstream1:8080 ) and Client 2 ( http://upstream2:8080 ) is configured. This setting is shown as an example, a different setting can be used on the partner and client side. If several partner WAF nodes are installed within the partner infrastructure, client traffic is processed similarly but on several partner servers. Partner account \u00b6 Partner account features \u00b6 Wallarm supports particular partner accounts to work with partners. A partner account allows the partner to: Install one or several partner WAF nodes within its system infrastructure and define settings for client traffic processing Create separate accounts for clients in the Wallarm Console and provide clients with access to these accounts Brand the Wallarm Console and select the language of the Wallarm Console interface (English or Russian) Host the Wallarm Console on its own domain Brand client emails and reports Set the email address of own technical support to recieve messages from clients Depending on the Wallarm WAF subscription plan, some features may not be available. Partner account components \u00b6 Partner account includes several components: Technical client account for access of partner users to the Wallarm Console, for adding global partner users . Partner client accounts for access of global partner users and client users to the Wallarm Console, for setting ID for the partner-client link. All partner account components are visually presented in the Wallarm Console for the global partner users . For example: Demo partner is a partner account Technical client is a technical client account Client 1 and Client 2 are partner client accounts Partner WAF node characteristics \u00b6 Partner WAF node has the following characteristics: Can be installed on the same platforms and according to the same instructions as a regular WAF node: Platform Installation options NGINX stable installed on the 64-bit operating system from the list: Debian 9.x (stretch) Debian 10.x (buster) Ubuntu 18.04 LTS (bionic) Ubuntu 20.04 LTS (focal) CentOS 7.x CentOS 8.x Amazon Linux 2 Module for NGINX stable from the NGINX repository Module for NGINX stable from the Debian/CentOS repository NGINX Plus Module for NGINX Plus Docker Docker container with NGINX modules Docker container with Envoy modules Kubernetes platform version 1.20 and lower NGINX Ingress controller You can deploy the Ingress controller on the Konvoy by D2IQ (formerly Mesosphere). The instructions mentioned above are suitable if you are deploying the Ingress controller with integrated Wallarm WAF services on the Konvoy. However, you may want to look at the D2IQ's installation instructions . Sidecar container Cloud platforms AWS image Google Cloud Platform image Yandex.Cloud image Kong 1.4.3 and lower installed on the 64-bit operating system from the list: Debian 9.x (stretch) Ubuntu 18.04 LTS (bionic) CentOS 7.x Module for Kong Can be installed on the technical client or partner client level. If you want to provide a client with access to the Wallarm Console, the WAF node must be installed at the corresponding partner client level. Can be configured according to the same instructions as a regular WAF node, except for: The directive wallarm_instance is used to split settings by the client applications. To enable blocking of requests by IP addresses, please send a request to Wallarm technical support . After blocking is enabled, to block IP addresses, you need to add them to the blacklist at an appropriate partner client account level. How to become a partner and install a partner WAF node \u00b6 To become a partner and install a partner WAF node within your infrastructure: Create a partner account in the Wallarm system. Create and link clients. Install and configure a partner WAF node.","title":"Partner scheme overview"},{"location":"partner-waf-node/overview/#partner-scheme-overview","text":"","title":"Partner scheme overview"},{"location":"partner-waf-node/overview/#definition-of-a-partner-and-partner-waf-node","text":"Partner is an organization that installs a WAF node within its system infrastructure and distributes the system with a WAF node to its own clients. Partner WAF node is a WAF node installed by the partner.","title":"Definition of a partner and partner WAF node"},{"location":"partner-waf-node/overview/#scheme-of-traffic-processing-by-a-partner-waf-node","text":"If one partner WAF node is installed within the partner infrastructure, client traffic is processed as follows: One WAF node processes traffic of several clients (Client 1, Client 2). The WAF node identifies the client that receives the traffic by the partner-client link ID ( wallarm_instance ). For the domains https://client1.com and https://client2.com , the DNS A records with the partner IP address 225.130.128.241 are configured. This setting is shown as an example, a different setting can be used on the partner and client side. On the partner's side, proxying of legitimate requests to the addresses of clients Client 1 ( http://upstream1:8080 ) and Client 2 ( http://upstream2:8080 ) is configured. This setting is shown as an example, a different setting can be used on the partner and client side. If several partner WAF nodes are installed within the partner infrastructure, client traffic is processed similarly but on several partner servers.","title":"Scheme of traffic processing by a partner WAF node"},{"location":"partner-waf-node/overview/#partner-account","text":"","title":"Partner account"},{"location":"partner-waf-node/overview/#partner-account-features","text":"Wallarm supports particular partner accounts to work with partners. A partner account allows the partner to: Install one or several partner WAF nodes within its system infrastructure and define settings for client traffic processing Create separate accounts for clients in the Wallarm Console and provide clients with access to these accounts Brand the Wallarm Console and select the language of the Wallarm Console interface (English or Russian) Host the Wallarm Console on its own domain Brand client emails and reports Set the email address of own technical support to recieve messages from clients Depending on the Wallarm WAF subscription plan, some features may not be available.","title":"Partner account features"},{"location":"partner-waf-node/overview/#partner-account-components","text":"Partner account includes several components: Technical client account for access of partner users to the Wallarm Console, for adding global partner users . Partner client accounts for access of global partner users and client users to the Wallarm Console, for setting ID for the partner-client link. All partner account components are visually presented in the Wallarm Console for the global partner users . For example: Demo partner is a partner account Technical client is a technical client account Client 1 and Client 2 are partner client accounts","title":"Partner account components"},{"location":"partner-waf-node/overview/#partner-waf-node-characteristics","text":"Partner WAF node has the following characteristics: Can be installed on the same platforms and according to the same instructions as a regular WAF node: Platform Installation options NGINX stable installed on the 64-bit operating system from the list: Debian 9.x (stretch) Debian 10.x (buster) Ubuntu 18.04 LTS (bionic) Ubuntu 20.04 LTS (focal) CentOS 7.x CentOS 8.x Amazon Linux 2 Module for NGINX stable from the NGINX repository Module for NGINX stable from the Debian/CentOS repository NGINX Plus Module for NGINX Plus Docker Docker container with NGINX modules Docker container with Envoy modules Kubernetes platform version 1.20 and lower NGINX Ingress controller You can deploy the Ingress controller on the Konvoy by D2IQ (formerly Mesosphere). The instructions mentioned above are suitable if you are deploying the Ingress controller with integrated Wallarm WAF services on the Konvoy. However, you may want to look at the D2IQ's installation instructions . Sidecar container Cloud platforms AWS image Google Cloud Platform image Yandex.Cloud image Kong 1.4.3 and lower installed on the 64-bit operating system from the list: Debian 9.x (stretch) Ubuntu 18.04 LTS (bionic) CentOS 7.x Module for Kong Can be installed on the technical client or partner client level. If you want to provide a client with access to the Wallarm Console, the WAF node must be installed at the corresponding partner client level. Can be configured according to the same instructions as a regular WAF node, except for: The directive wallarm_instance is used to split settings by the client applications. To enable blocking of requests by IP addresses, please send a request to Wallarm technical support . After blocking is enabled, to block IP addresses, you need to add them to the blacklist at an appropriate partner client account level.","title":"Partner WAF node characteristics"},{"location":"partner-waf-node/overview/#how-to-become-a-partner-and-install-a-partner-waf-node","text":"To become a partner and install a partner WAF node within your infrastructure: Create a partner account in the Wallarm system. Create and link clients. Install and configure a partner WAF node.","title":"How to become a partner and install a partner WAF node"},{"location":"quickstart-en/qs-check-operation-en/","text":"Checking the WAF node operation \u00b6 If everything is configured correctly, Wallarm filters the requests and proxies the filtered requests in accordance with the configuration file settings. 1. Execute the wallarm-status Request \u00b6 You can get filter node operation statistics by requesting the /wallarm-status URL. Run the command: curl http://127.0.0.8/wallarm-status The output will be like: { \"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0, \"requests_lost\":0,\"segfaults\":0,\"memfaults\":0, \"softmemfaults\":0,\"time_detect\":0,\"db_id\":46, \"lom_id\":16767,\"proton_instances\": { \"total\":1,\"success\":1,\"fallback\":0,\"failed\":0 }, \"stalled_workers_count\":0,\"stalled_workers\":[] } This means that the filter node statistics service is running and working properly. The Statistics Service You can read more about the statistics service and how to configure it here . 2. Run a Test Attack \u00b6 To check if Wallarm correctly detects attacks, send an invalid request to the protected resource. For example: http://<resource_URL>/?id='or+1=1--a-<script>prompt(1)</script>' Wallarm must detect in the request the following: SQLI XSS Now the counter of the number of attacks will increase when a request for wallarm-status is executed, which means that the filter node is operating normally. If the operation check is successful, the initial installation and setup is complete. More available configuration options \u2192 User guides \u2192","title":"Qs check operation en"},{"location":"quickstart-en/qs-check-operation-en/#checking-the-waf-node-operation","text":"If everything is configured correctly, Wallarm filters the requests and proxies the filtered requests in accordance with the configuration file settings.","title":"Checking the WAF node operation"},{"location":"quickstart-en/qs-check-operation-en/#1-execute-the-wallarm-status-request","text":"You can get filter node operation statistics by requesting the /wallarm-status URL. Run the command: curl http://127.0.0.8/wallarm-status The output will be like: { \"requests\":0,\"attacks\":0,\"blocked\":0,\"abnormal\":0,\"tnt_errors\":0,\"api_errors\":0, \"requests_lost\":0,\"segfaults\":0,\"memfaults\":0, \"softmemfaults\":0,\"time_detect\":0,\"db_id\":46, \"lom_id\":16767,\"proton_instances\": { \"total\":1,\"success\":1,\"fallback\":0,\"failed\":0 }, \"stalled_workers_count\":0,\"stalled_workers\":[] } This means that the filter node statistics service is running and working properly. The Statistics Service You can read more about the statistics service and how to configure it here .","title":"1. Execute the wallarm-status Request"},{"location":"quickstart-en/qs-check-operation-en/#2-run-a-test-attack","text":"To check if Wallarm correctly detects attacks, send an invalid request to the protected resource. For example: http://<resource_URL>/?id='or+1=1--a-<script>prompt(1)</script>' Wallarm must detect in the request the following: SQLI XSS Now the counter of the number of attacks will increase when a request for wallarm-status is executed, which means that the filter node is operating normally. If the operation check is successful, the initial installation and setup is complete. More available configuration options \u2192 User guides \u2192","title":"2. Run a Test Attack"},{"location":"quickstart-en/qs-install-node-en/","text":"Quickstart with WAF \u00b6 Create account Run getwallarm.sh","title":"Quickstart"},{"location":"quickstart-en/qs-install-node-en/#quickstart-with-waf","text":"Create account Run getwallarm.sh","title":"Quickstart with WAF"},{"location":"quickstart-en/qs-setup-proxy-en/","text":"Configuring traffic proxying \u00b6 To process the HTTP requests, Wallarm uses the web and proxy server NGINX with additional modules to analyze the traffic. 1. Edit the NGINX Configuration Files \u00b6 The /etc/nginx/conf.d directory contains NGINX and Wallarm filter node configuration files. By default, this directory contains the following configuration files: The default.conf file defines the configuration of NGINX. The wallarm.conf file defines the global configuration of Wallarm filter node. The wallarm-status.conf file defines the Wallarm monitoring configuration. You can create your own configuration files to define the operation of NGINX and Wallarm. It is recommended to create a separate configuration file with the server block for each group of the domains that should be processed in the same way. To see detailed information about working with NGINX configuration files, proceed to the official NGINX documentation . Wallarm directives define the operation logic of the Wallarm filter node. To see the list of Wallarm directives available, proceed to the Wallarm configuration options page. A Configuration File Example \u00b6 Let us suppose that you need to configure the server to work in the following conditions: Only HTTP traffic is processed. There are no HTTPS requests processed. The following domains receive the requests: example.com and www.example.com . All requests must be passed to the server 10.80.0.5 . All incoming requests are considered less than 1MB in size (default setting). The processing of a request takes no more than 60 seconds (default setting). Wallarm must operate in the monitor mode. Clients access the filter node directly, without an intermediate HTTP load balancer. Creating a configuration file You can create a custom NGINX configuration file (e.g. example.com.conf ) or modify the default NGINX configuration file ( default.conf ). When creating a custom configuration file, make sure that NGINX listens to the incoming connections on the free port. To meet the listed conditions, the contents of the configuration file must be the following: server { listen 80 ; listen [ :: ] :80 ipv6only = on ; # the domains for which traffic is processed server_name example.com ; server_name www.example.com ; # turn on the monitoring mode of traffic processing wallarm_mode monitoring ; # wallarm_instance 1; location / { # setting the address for request forwarding proxy_pass http://10.80.0.5 ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; } } 2. Set up the Filter Node for Using a Proxy Server \u00b6 Info This setup step is intended for users who use their own proxy server for the operation of the protected web applications. If you do not use a proxy server, skip this step of the setup. You need to assign new values to the environment variables, which define the proxy server used, to configure Wallarm node for using your proxy server. Add new values of the environment variables to the /etc/environment file: Add https_proxy to define a proxy for the https protocol. Add http_proxy to define a proxy for the http protocol. Add no_proxy to define the list of the resources proxy should not be used for. Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables. <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for. <proxy_user> defines the username for proxy authorization. <proxy_pass> defines the password for proxy authorization. <host> defines a host of the proxy server. <port> defines a port of the proxy server. Assign a \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" array value, where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains, to the no_proxy variable to define a list of the resources which proxy should not be used for. This array should consist of IP addresses and/or domains. Resources that need to be addressed without a proxy Add the following IP addresses and domain to the list of the resources that have to be addressed without a proxy for the system to operate correctly: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . The 127.0.0.8 and 127.0.0.9 IP addresses are used for the operation of the Wallarm filter node. The example of the correct /etc/environment file contents below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy=http://admin:01234@1.2.3.4:1234 http_proxy=http://admin:01234@1.2.3.4:1234 no_proxy=\"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\" 3. Restart NGINX \u00b6 After saving the edited configuration file, restart NGINX: Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx Perform the checking to see that the WAF node is operational and filters traffic. See Check the WAF node operation \u2192","title":"Qs setup proxy en"},{"location":"quickstart-en/qs-setup-proxy-en/#configuring-traffic-proxying","text":"To process the HTTP requests, Wallarm uses the web and proxy server NGINX with additional modules to analyze the traffic.","title":"Configuring traffic proxying"},{"location":"quickstart-en/qs-setup-proxy-en/#1-edit-the-nginx-configuration-files","text":"The /etc/nginx/conf.d directory contains NGINX and Wallarm filter node configuration files. By default, this directory contains the following configuration files: The default.conf file defines the configuration of NGINX. The wallarm.conf file defines the global configuration of Wallarm filter node. The wallarm-status.conf file defines the Wallarm monitoring configuration. You can create your own configuration files to define the operation of NGINX and Wallarm. It is recommended to create a separate configuration file with the server block for each group of the domains that should be processed in the same way. To see detailed information about working with NGINX configuration files, proceed to the official NGINX documentation . Wallarm directives define the operation logic of the Wallarm filter node. To see the list of Wallarm directives available, proceed to the Wallarm configuration options page.","title":"1. Edit the NGINX Configuration Files"},{"location":"quickstart-en/qs-setup-proxy-en/#a-configuration-file-example","text":"Let us suppose that you need to configure the server to work in the following conditions: Only HTTP traffic is processed. There are no HTTPS requests processed. The following domains receive the requests: example.com and www.example.com . All requests must be passed to the server 10.80.0.5 . All incoming requests are considered less than 1MB in size (default setting). The processing of a request takes no more than 60 seconds (default setting). Wallarm must operate in the monitor mode. Clients access the filter node directly, without an intermediate HTTP load balancer. Creating a configuration file You can create a custom NGINX configuration file (e.g. example.com.conf ) or modify the default NGINX configuration file ( default.conf ). When creating a custom configuration file, make sure that NGINX listens to the incoming connections on the free port. To meet the listed conditions, the contents of the configuration file must be the following: server { listen 80 ; listen [ :: ] :80 ipv6only = on ; # the domains for which traffic is processed server_name example.com ; server_name www.example.com ; # turn on the monitoring mode of traffic processing wallarm_mode monitoring ; # wallarm_instance 1; location / { # setting the address for request forwarding proxy_pass http://10.80.0.5 ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; } }","title":"A Configuration File Example"},{"location":"quickstart-en/qs-setup-proxy-en/#2-set-up-the-filter-node-for-using-a-proxy-server","text":"Info This setup step is intended for users who use their own proxy server for the operation of the protected web applications. If you do not use a proxy server, skip this step of the setup. You need to assign new values to the environment variables, which define the proxy server used, to configure Wallarm node for using your proxy server. Add new values of the environment variables to the /etc/environment file: Add https_proxy to define a proxy for the https protocol. Add http_proxy to define a proxy for the http protocol. Add no_proxy to define the list of the resources proxy should not be used for. Assign the <scheme>://<proxy_user>:<proxy_pass>@<host>:<port> string values to the https_proxy and http_proxy variables. <scheme> defines the protocol used. It should match the protocol that the current environment variable sets up proxy for. <proxy_user> defines the username for proxy authorization. <proxy_pass> defines the password for proxy authorization. <host> defines a host of the proxy server. <port> defines a port of the proxy server. Assign a \"<res_1>, <res_2>, <res_3>, <res_4>, ...\" array value, where <res_1> , <res_2> , <res_3> , and <res_4> are the IP addresses and/or domains, to the no_proxy variable to define a list of the resources which proxy should not be used for. This array should consist of IP addresses and/or domains. Resources that need to be addressed without a proxy Add the following IP addresses and domain to the list of the resources that have to be addressed without a proxy for the system to operate correctly: 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . The 127.0.0.8 and 127.0.0.9 IP addresses are used for the operation of the Wallarm filter node. The example of the correct /etc/environment file contents below demonstrates the following configuration: HTTPS and HTTP requests are proxied to the 1.2.3.4 host with the 1234 port, using the admin username and the 01234 password for authorization on the proxy server. Proxying is disabled for the requests sent to 127.0.0.1 , 127.0.0.8 , 127.0.0.9 , and localhost . https_proxy=http://admin:01234@1.2.3.4:1234 http_proxy=http://admin:01234@1.2.3.4:1234 no_proxy=\"127.0.0.1, 127.0.0.8, 127.0.0.9, localhost\"","title":"2. Set up the Filter Node for Using a Proxy Server"},{"location":"quickstart-en/qs-setup-proxy-en/#3-restart-nginx","text":"After saving the edited configuration file, restart NGINX: Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx Perform the checking to see that the WAF node is operational and filters traffic. See Check the WAF node operation \u2192","title":"3. Restart NGINX"},{"location":"updating-migrating/cloud-image/","text":"Updating the cloud WAF node image \u00b6 These instructions describe the steps to update the cloud WAF node image deployed on AWS, GCP, or Yandex.Cloud up to 3.0. Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 . Update procedure \u00b6 To update the version of the WAF node deployed in the cloud: Inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. Launch a new virtual machine based on the WAF node 3.0 image. Copy the WAF node settings from the previous version to the new version. Delete the previous WAF node instance. A more detailed description of the upgrade steps is provided below. Step 1: Inform Wallarm technical support that you are updating WAF node modules \u00b6 Please inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available. Step 2: Launch a new instance with the WAF node 3.0 \u00b6 Open the Wallarm WAF node image on the cloud platform marketplace and proceed to the image launch: Amazon Marketplace GCP Marketplace Yandex.Cloud marketplace At the launch step, set the following settings: Select the image version 3.0.x For AWS, select the created security group in the field Security Group Settings For AWS, select the name of the created key pair in the field Key Pair Settings Confirm the instance launch. For GCP, configure the instance following these instructions . Step 3: Connect the WAF node to Wallarm Cloud \u00b6 Connect to the WAF node instance via SSH. More detailed instructions for connecting to the instances are available in the cloud platform documentation: AWS documentation GCP documentation Yandex.Cloud documentation Connect the WAF node to Wallarm Cloud using a new cloud node token or username and password to the Wallarm Console as described in the instructions for the cloud platform: AWS GCP Yandex.Cloud Step 4: Copy the WAF node settings from the previous version to the new version \u00b6 Copy the settings for processing and proxying requests from the following configuration files of the previous WAF node version to the files of the WAF node 3.0: /etc/nginx/nginx.conf and other files with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings /etc/nginx/conf.d/wallarm-status.conf with the WAF node monitoring service settings /etc/environment with environment variables /etc/default/wallarm-tarantool with Tarantool settings other files with custom settings for processing and proxying requests Migrate whitelists and blacklists configuration from previous WAF node version to 3.0 following the instructions . Restart NGINX to apply the settings: sudo systemctl restart nginx Detailed information about working with NGINX configuration files is available in the official NGINX documentation . The list of WAF node directives is available here . Step 5: Test WAF node operation \u00b6 Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Step 6: Creating the virtual machine image based on the WAF node 3.0 in AWS or GCP \u00b6 To create the virtual machine image based on the WAF node 3.0, please follow the instructions for AWS or GCP . Step 7: Delete the previous WAF node instance \u00b6 If the new version of the WAF node is successfully configured and tested, remove the instance and virtual machine image with the previous version of the WAF node using the AWS, GCP, or Yandex.Cloud management console.","title":"Upgrading the cloud node image"},{"location":"updating-migrating/cloud-image/#updating-the-cloud-waf-node-image","text":"These instructions describe the steps to update the cloud WAF node image deployed on AWS, GCP, or Yandex.Cloud up to 3.0. Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 .","title":"Updating the cloud WAF node image"},{"location":"updating-migrating/cloud-image/#update-procedure","text":"To update the version of the WAF node deployed in the cloud: Inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. Launch a new virtual machine based on the WAF node 3.0 image. Copy the WAF node settings from the previous version to the new version. Delete the previous WAF node instance. A more detailed description of the upgrade steps is provided below.","title":"Update procedure"},{"location":"updating-migrating/cloud-image/#step-1-inform-wallarm-technical-support-that-you-are-updating-waf-node-modules","text":"Please inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available.","title":"Step 1: Inform Wallarm technical support that you are updating WAF node modules"},{"location":"updating-migrating/cloud-image/#step-2-launch-a-new-instance-with-the-waf-node-30","text":"Open the Wallarm WAF node image on the cloud platform marketplace and proceed to the image launch: Amazon Marketplace GCP Marketplace Yandex.Cloud marketplace At the launch step, set the following settings: Select the image version 3.0.x For AWS, select the created security group in the field Security Group Settings For AWS, select the name of the created key pair in the field Key Pair Settings Confirm the instance launch. For GCP, configure the instance following these instructions .","title":"Step 2: Launch a new instance with the WAF node 3.0"},{"location":"updating-migrating/cloud-image/#step-3-connect-the-waf-node-to-wallarm-cloud","text":"Connect to the WAF node instance via SSH. More detailed instructions for connecting to the instances are available in the cloud platform documentation: AWS documentation GCP documentation Yandex.Cloud documentation Connect the WAF node to Wallarm Cloud using a new cloud node token or username and password to the Wallarm Console as described in the instructions for the cloud platform: AWS GCP Yandex.Cloud","title":"Step 3: Connect the WAF node to Wallarm Cloud"},{"location":"updating-migrating/cloud-image/#step-4-copy-the-waf-node-settings-from-the-previous-version-to-the-new-version","text":"Copy the settings for processing and proxying requests from the following configuration files of the previous WAF node version to the files of the WAF node 3.0: /etc/nginx/nginx.conf and other files with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings /etc/nginx/conf.d/wallarm-status.conf with the WAF node monitoring service settings /etc/environment with environment variables /etc/default/wallarm-tarantool with Tarantool settings other files with custom settings for processing and proxying requests Migrate whitelists and blacklists configuration from previous WAF node version to 3.0 following the instructions . Restart NGINX to apply the settings: sudo systemctl restart nginx Detailed information about working with NGINX configuration files is available in the official NGINX documentation . The list of WAF node directives is available here .","title":"Step 4: Copy the WAF node settings from the previous version to the new version"},{"location":"updating-migrating/cloud-image/#step-5-test-waf-node-operation","text":"Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"Step 5: Test WAF node operation"},{"location":"updating-migrating/cloud-image/#step-6-creating-the-virtual-machine-image-based-on-the-waf-node-30-in-aws-or-gcp","text":"To create the virtual machine image based on the WAF node 3.0, please follow the instructions for AWS or GCP .","title":"Step 6: Creating the virtual machine image based on the WAF node 3.0 in AWS or GCP"},{"location":"updating-migrating/cloud-image/#step-7-delete-the-previous-waf-node-instance","text":"If the new version of the WAF node is successfully configured and tested, remove the instance and virtual machine image with the previous version of the WAF node using the AWS, GCP, or Yandex.Cloud management console.","title":"Step 7: Delete the previous WAF node instance"},{"location":"updating-migrating/docker-container/","text":"Updating the running Docker NGINX- or Envoy-based image \u00b6 These instructions describe the steps to update the running Docker NGINX- or Envoy-based image to the version 3.0. Using credentials of already existing WAF node We do not recommend to use the already existing WAF node of the previous version. Please follow these instructions to create a new WAF node of the version 3.0 and deploy it as the Docker container. Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 . Requirements \u00b6 Access to the account with the Deploy or Administrator role and two\u2011factor authentication disabled in Wallarm Console in the EU Cloud or US Cloud Access to https://api.wallarm.com:444 if working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 if working with US Wallarm Cloud. Please ensure the access is not blocked by a firewall Step 1: Inform Wallarm technical support that you are updating WAF node modules \u00b6 Please inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available. Step 2: Download the updated WAF node image \u00b6 NGINX-based image docker pull wallarm/node:3.0.0-2 Envoy-based image docker pull wallarm/envoy:3.0.0-1 Step 3: Stop the running container \u00b6 docker stop <RUNNING_CONTAINER_NAME> Step 4: Run the container using the updated image \u00b6 Migrate whitelists and blacklists configuration from previous WAF node version to 3.0 following the instructions . Run the container using the updated image. You can pass the same configuration parameters that were passed when running a previous image version except for the WALLARM_ACL_ENABLE variable. There are two options for running the container using the updated image: With the environment variables specifying basic WAF node configuration Instructions for the NGINX-based Docker container \u2192 Instructions for the Envoy-based Docker container \u2192 In the mounted configuration file specifying advanced WAF node configuration Instructions for the NGINX-based Docker container \u2192 Instructions for the Envoy-based Docker container \u2192 Step 5: Test the WAF node operation \u00b6 Send the request with test SQLI and XSS attacks to the protected resource address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Step 6: Delete the WAF node of the previous version \u00b6 If the deployed image of the version 3.0 operates correctly, you can delete the WAF node of the previous version in the Wallarm Console \u2192 Nodes section.","title":"Upgrading the running Docker NGINX- or Envoy-based image"},{"location":"updating-migrating/docker-container/#updating-the-running-docker-nginx-or-envoy-based-image","text":"These instructions describe the steps to update the running Docker NGINX- or Envoy-based image to the version 3.0. Using credentials of already existing WAF node We do not recommend to use the already existing WAF node of the previous version. Please follow these instructions to create a new WAF node of the version 3.0 and deploy it as the Docker container. Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 .","title":"Updating the running Docker NGINX- or Envoy-based image"},{"location":"updating-migrating/docker-container/#requirements","text":"Access to the account with the Deploy or Administrator role and two\u2011factor authentication disabled in Wallarm Console in the EU Cloud or US Cloud Access to https://api.wallarm.com:444 if working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 if working with US Wallarm Cloud. Please ensure the access is not blocked by a firewall","title":"Requirements"},{"location":"updating-migrating/docker-container/#step-1-inform-wallarm-technical-support-that-you-are-updating-waf-node-modules","text":"Please inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available.","title":"Step 1: Inform Wallarm technical support that you are updating WAF node modules"},{"location":"updating-migrating/docker-container/#step-2-download-the-updated-waf-node-image","text":"NGINX-based image docker pull wallarm/node:3.0.0-2 Envoy-based image docker pull wallarm/envoy:3.0.0-1","title":"Step 2: Download the updated WAF node image"},{"location":"updating-migrating/docker-container/#step-3-stop-the-running-container","text":"docker stop <RUNNING_CONTAINER_NAME>","title":"Step 3: Stop the running container"},{"location":"updating-migrating/docker-container/#step-4-run-the-container-using-the-updated-image","text":"Migrate whitelists and blacklists configuration from previous WAF node version to 3.0 following the instructions . Run the container using the updated image. You can pass the same configuration parameters that were passed when running a previous image version except for the WALLARM_ACL_ENABLE variable. There are two options for running the container using the updated image: With the environment variables specifying basic WAF node configuration Instructions for the NGINX-based Docker container \u2192 Instructions for the Envoy-based Docker container \u2192 In the mounted configuration file specifying advanced WAF node configuration Instructions for the NGINX-based Docker container \u2192 Instructions for the Envoy-based Docker container \u2192","title":"Step 4: Run the container using the updated image"},{"location":"updating-migrating/docker-container/#step-5-test-the-waf-node-operation","text":"Send the request with test SQLI and XSS attacks to the protected resource address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"Step 5: Test the WAF node operation"},{"location":"updating-migrating/docker-container/#step-6-delete-the-waf-node-of-the-previous-version","text":"If the deployed image of the version 3.0 operates correctly, you can delete the WAF node of the previous version in the Wallarm Console \u2192 Nodes section.","title":"Step 6: Delete the WAF node of the previous version"},{"location":"updating-migrating/general-recommendations/","text":"Recommendations for a safe WAF node update process \u00b6 This document describes recommendations and associated risks for a safe update of Wallarm WAF node up to 3.0. Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 . Common recommendations \u00b6 Carefully plan and monitor the WAF node update process. Estimated release dates for new versions of WAF nodes are published in the WAF node versioning policy . If your infrastructure has multiple WAF nodes installed, update them gradually. After updating the first WAF node, monitor the WAF node modules operation within a day and gradually update other WAF nodes if the first WAF node operates correctly. For the model with separated development and production environments, update the WAF node gradually. First, apply and test new version in non-production environments, then in production environments. Detailed recommendations are described in the instructions for configuring WAF nodes for separated environments . Before updating the WAF node, set the WAF node filtration mode to monitoring . If all modules work correctly and there is no abnormal number of new false positives in the monitoring mode for a day, then put the WAF node in the block mode. Update NGINX to the latest version available before applying WAF node updates. If your infrastructure needs to use a specific version of NGINX, please contact the Wallarm technical support to build a WAF module for a custom version of NGINX. Possible risks \u00b6 Below are the risks that may occur when updating the WAF node. To reduce the impact of the risks, please follow the appropriate guidelines when updating. Changed functionality \u00b6 WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. Set of changes in WAF node 3.0 Changes in supported installation platforms Dropped support for the operating system Ubuntu 16.04 LTS (xenial) See the full list of supported platforms \u2192 Changes in supported WAF node configuration parameters Dropped support for all acl NGINX directives, Envoy parameters, and environment variables used to configure IP addresses blacklist. Manual configuration of IP blacklisting is no longer required. Details on migrating blacklist configuration \u2192 Added new NGINX directive and Envoy parameter disable_acl . This parameter allows to disabled request origin analysis. Details on the disable_acl NGINX directive \u2192 Details on the disable_acl Envoy parameter \u2192 Changes in system requirements for the WAF node installation Starting with version 3.0, the WAF node supports IP addresses whitelists, blacklists, and greylists . The Wallarm Console allows adding both single IPs and countries or data centers to any IP list type. The WAF node downloads an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers from GCP storage. By default, access to this storage can be restricted in your system. Allowing access to GCP storage is a new requirement for the virtual machine on which the WAF node is installed. Range of GCP IP addresses that should be allowed \u2192 New features Support for new filtration mode safe_blocking and IP addresses greylist . The WAF node operating in safe_blocking mode blocks only those malicious requests originated from greylisted IP addresses that allow a significant reduction of false positives numbers. New reaction of triggers Add to greyist allowing to automatically greylist IP addresses originated a specific number of malicious requests. Example of the trigger that greylists IP addresses \u2192 Management of IP addresses whitelist via the Wallarm Console. Automated whitelisting of Wallarm Vulnerability Scanner IP addresses. Manual whitelisting of Scanner IP addresses is no longer required. New parameters of the file node.yaml for configuring the synchronization of the Wallarm Cloud and WAF nodes: api.local_host and api.local_port . New parameters allow specifying a local IP address and port of the network interface through which requests to Wallarm API are sent. See the full list of node.yaml parameters for Wallarm Cloud and WAF node synchronization setup \u2192 Ability to whitelist, blacklist, or greylist a subnet, Tor network IPs, VPN IPs, a group of IP addresses registered in a specific country or data center. Details on adding IPs to the whitelist, blacklist, and greylist \u2192 New false positives \u00b6 We improve the traffic analysis with each new version of the WAF node. This means that the number of false positives decreases with each new version. However, each protected application has its own specificities, so we recommend analyzing the work of the new version of the WAF node in the monitoring mode before enabling the blocking mode ( block ). To analyze the number of new false positives after the update: Deploy the new version of the WAF node in the monitoring mode and send the traffic to the WAF node. After some time, open the Wallarm Console \u2192 Events section and analyze the number of requests that are mistakenly recognized as attacks. If you find abnormal growth in the number of false positives, please contact the Wallarm technical support . Increased amount of used resources \u00b6 Usage of some new WAF node features may cause changes in the amount of used resources. Information about changes in the amount of used resources is highlighted in the What is new section. Also, it is recommended to monitor the WAF node operation: if you find significant differences in the actual amount of used resources and in the amount specified in the documentation, please contact the Wallarm technical support . Update process \u00b6 The WAF node update process depends on the platform and installation forms. Please select the installation form and follow the appropriate instructions: Modules for NGINX, NGINX Plus, Kong Docker container with the modules for NGINX NGINX Ingress controller with integrated Wallarm WAF Cloud WAF node image Migrating whitelists and blacklists from previous WAF node versions to 3.0","title":"Recommendations for a safe upgrade process"},{"location":"updating-migrating/general-recommendations/#recommendations-for-a-safe-waf-node-update-process","text":"This document describes recommendations and associated risks for a safe update of Wallarm WAF node up to 3.0. Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 .","title":"Recommendations for a safe WAF node update process"},{"location":"updating-migrating/general-recommendations/#common-recommendations","text":"Carefully plan and monitor the WAF node update process. Estimated release dates for new versions of WAF nodes are published in the WAF node versioning policy . If your infrastructure has multiple WAF nodes installed, update them gradually. After updating the first WAF node, monitor the WAF node modules operation within a day and gradually update other WAF nodes if the first WAF node operates correctly. For the model with separated development and production environments, update the WAF node gradually. First, apply and test new version in non-production environments, then in production environments. Detailed recommendations are described in the instructions for configuring WAF nodes for separated environments . Before updating the WAF node, set the WAF node filtration mode to monitoring . If all modules work correctly and there is no abnormal number of new false positives in the monitoring mode for a day, then put the WAF node in the block mode. Update NGINX to the latest version available before applying WAF node updates. If your infrastructure needs to use a specific version of NGINX, please contact the Wallarm technical support to build a WAF module for a custom version of NGINX.","title":"Common recommendations"},{"location":"updating-migrating/general-recommendations/#possible-risks","text":"Below are the risks that may occur when updating the WAF node. To reduce the impact of the risks, please follow the appropriate guidelines when updating.","title":"Possible risks"},{"location":"updating-migrating/general-recommendations/#changed-functionality","text":"WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. Set of changes in WAF node 3.0 Changes in supported installation platforms Dropped support for the operating system Ubuntu 16.04 LTS (xenial) See the full list of supported platforms \u2192 Changes in supported WAF node configuration parameters Dropped support for all acl NGINX directives, Envoy parameters, and environment variables used to configure IP addresses blacklist. Manual configuration of IP blacklisting is no longer required. Details on migrating blacklist configuration \u2192 Added new NGINX directive and Envoy parameter disable_acl . This parameter allows to disabled request origin analysis. Details on the disable_acl NGINX directive \u2192 Details on the disable_acl Envoy parameter \u2192 Changes in system requirements for the WAF node installation Starting with version 3.0, the WAF node supports IP addresses whitelists, blacklists, and greylists . The Wallarm Console allows adding both single IPs and countries or data centers to any IP list type. The WAF node downloads an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers from GCP storage. By default, access to this storage can be restricted in your system. Allowing access to GCP storage is a new requirement for the virtual machine on which the WAF node is installed. Range of GCP IP addresses that should be allowed \u2192 New features Support for new filtration mode safe_blocking and IP addresses greylist . The WAF node operating in safe_blocking mode blocks only those malicious requests originated from greylisted IP addresses that allow a significant reduction of false positives numbers. New reaction of triggers Add to greyist allowing to automatically greylist IP addresses originated a specific number of malicious requests. Example of the trigger that greylists IP addresses \u2192 Management of IP addresses whitelist via the Wallarm Console. Automated whitelisting of Wallarm Vulnerability Scanner IP addresses. Manual whitelisting of Scanner IP addresses is no longer required. New parameters of the file node.yaml for configuring the synchronization of the Wallarm Cloud and WAF nodes: api.local_host and api.local_port . New parameters allow specifying a local IP address and port of the network interface through which requests to Wallarm API are sent. See the full list of node.yaml parameters for Wallarm Cloud and WAF node synchronization setup \u2192 Ability to whitelist, blacklist, or greylist a subnet, Tor network IPs, VPN IPs, a group of IP addresses registered in a specific country or data center. Details on adding IPs to the whitelist, blacklist, and greylist \u2192","title":"Changed functionality"},{"location":"updating-migrating/general-recommendations/#new-false-positives","text":"We improve the traffic analysis with each new version of the WAF node. This means that the number of false positives decreases with each new version. However, each protected application has its own specificities, so we recommend analyzing the work of the new version of the WAF node in the monitoring mode before enabling the blocking mode ( block ). To analyze the number of new false positives after the update: Deploy the new version of the WAF node in the monitoring mode and send the traffic to the WAF node. After some time, open the Wallarm Console \u2192 Events section and analyze the number of requests that are mistakenly recognized as attacks. If you find abnormal growth in the number of false positives, please contact the Wallarm technical support .","title":"New false positives"},{"location":"updating-migrating/general-recommendations/#increased-amount-of-used-resources","text":"Usage of some new WAF node features may cause changes in the amount of used resources. Information about changes in the amount of used resources is highlighted in the What is new section. Also, it is recommended to monitor the WAF node operation: if you find significant differences in the actual amount of used resources and in the amount specified in the documentation, please contact the Wallarm technical support .","title":"Increased amount of used resources"},{"location":"updating-migrating/general-recommendations/#update-process","text":"The WAF node update process depends on the platform and installation forms. Please select the installation form and follow the appropriate instructions: Modules for NGINX, NGINX Plus, Kong Docker container with the modules for NGINX NGINX Ingress controller with integrated Wallarm WAF Cloud WAF node image Migrating whitelists and blacklists from previous WAF node versions to 3.0","title":"Update process"},{"location":"updating-migrating/ingress-controller/","text":"Updating NGINX Ingress controller with integrated Wallarm WAF \u00b6 These instructions describe the steps to update deployed Wallarm Ingress Controller to the new version with WAF node 3.0. To update Wallarm Ingress controller, you need to clone new Helm chart version and apply updates to the installed version. Current Ingress controller settings and Ingress annotations will be saved and applied to a new version automatically. Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 . Updating \u00b6 Inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available. Clone new Helm chart version from the Wallarm repository: git clone https://github.com/wallarm/ingress-chart --branch 3 .0.0-3 --single-branch Update the previous Helm chart: EU Cloud helm upgrade --set controller.wallarm.enabled = true,controller.wallarm.token = <YOUR_CLOUD_NODE_TOKEN> <INGRESS_CONTROLLER_NAME> ingress-chart/wallarm-ingress -n <KUBERNETES_NAMESPACE> US Cloud helm upgrade --set controller.wallarm.enabled = true,controller.wallarm.token = <YOUR_CLOUD_NODE_TOKEN>,controller.wallarm.apiHost = us1.api.wallarm.com <INGRESS_CONTROLLER_NAME> ingress-chart/wallarm-ingress -n <KUBERNETES_NAMESPACE> <YOUR_CLOUD_NODE_TOKEN> is the token of the cloud WAF node received when installing Wallarm Ingress controller <INGRESS_CONTROLLER_NAME> is the name of the Wallarm Ingress controller to update <KUBERNETES_NAMESPACE> is the namespace of your Ingress Migrate whitelists and blacklists configuration from previous WAF node version to 3.0 following the instructions . Testing \u00b6 Check that the version of Helm chart was updated: helm ls The chart version should correspond to wallarm-ingress-1.8.x . Get the list of pods specifying the name of the Wallarm Ingress controller in <INGRESS_CONTROLLER_NAME> : kubectl get pods -l release = <INGRESS_CONTROLLER_NAME> Each pod status should be STATUS: Running or READY: N/N . For example: NAME READY STATUS RESTARTS AGE ingress-controller-nginx-ingress-controller-675c68d46d-cfck8 3/3 Running 0 5m ingress-controller-nginx-ingress-controller-wallarm-tarantljj8g 8/8 Running 0 5m ingress-controller-nginx-ingress-default-backend-584ffc6c7xj5xx 1/1 Running 0 5m Send the request with test SQLI and XSS attacks to the Wallarm Ingress controller address: curl http://<INGRESS_CONTROLLER_IP>/?id = 'or+1=1--a-<script>prompt(1)</script>' If the WAF node is working in the block mode, the code 403 Forbidden will be returned in the response to the request and attacks will be displayed in the Wallarm Console \u2192 Nodes . Configuring \u00b6 Ingress controller settings and Ingress annotations will be automatically moved from the previous version to the new version. The list of all settings and annotations is available here . Configuration use cases: Proper Reporting of End User Public IP Address Management of IP Addresses Blocking High Availability Considerations Ingress Controller Monitoring","title":"Upgrading NGINX Ingress controller with integrated Wallarm"},{"location":"updating-migrating/ingress-controller/#updating-nginx-ingress-controller-with-integrated-wallarm-waf","text":"These instructions describe the steps to update deployed Wallarm Ingress Controller to the new version with WAF node 3.0. To update Wallarm Ingress controller, you need to clone new Helm chart version and apply updates to the installed version. Current Ingress controller settings and Ingress annotations will be saved and applied to a new version automatically. Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 .","title":"Updating NGINX Ingress controller with integrated Wallarm WAF"},{"location":"updating-migrating/ingress-controller/#updating","text":"Inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available. Clone new Helm chart version from the Wallarm repository: git clone https://github.com/wallarm/ingress-chart --branch 3 .0.0-3 --single-branch Update the previous Helm chart: EU Cloud helm upgrade --set controller.wallarm.enabled = true,controller.wallarm.token = <YOUR_CLOUD_NODE_TOKEN> <INGRESS_CONTROLLER_NAME> ingress-chart/wallarm-ingress -n <KUBERNETES_NAMESPACE> US Cloud helm upgrade --set controller.wallarm.enabled = true,controller.wallarm.token = <YOUR_CLOUD_NODE_TOKEN>,controller.wallarm.apiHost = us1.api.wallarm.com <INGRESS_CONTROLLER_NAME> ingress-chart/wallarm-ingress -n <KUBERNETES_NAMESPACE> <YOUR_CLOUD_NODE_TOKEN> is the token of the cloud WAF node received when installing Wallarm Ingress controller <INGRESS_CONTROLLER_NAME> is the name of the Wallarm Ingress controller to update <KUBERNETES_NAMESPACE> is the namespace of your Ingress Migrate whitelists and blacklists configuration from previous WAF node version to 3.0 following the instructions .","title":"Updating"},{"location":"updating-migrating/ingress-controller/#testing","text":"Check that the version of Helm chart was updated: helm ls The chart version should correspond to wallarm-ingress-1.8.x . Get the list of pods specifying the name of the Wallarm Ingress controller in <INGRESS_CONTROLLER_NAME> : kubectl get pods -l release = <INGRESS_CONTROLLER_NAME> Each pod status should be STATUS: Running or READY: N/N . For example: NAME READY STATUS RESTARTS AGE ingress-controller-nginx-ingress-controller-675c68d46d-cfck8 3/3 Running 0 5m ingress-controller-nginx-ingress-controller-wallarm-tarantljj8g 8/8 Running 0 5m ingress-controller-nginx-ingress-default-backend-584ffc6c7xj5xx 1/1 Running 0 5m Send the request with test SQLI and XSS attacks to the Wallarm Ingress controller address: curl http://<INGRESS_CONTROLLER_IP>/?id = 'or+1=1--a-<script>prompt(1)</script>' If the WAF node is working in the block mode, the code 403 Forbidden will be returned in the response to the request and attacks will be displayed in the Wallarm Console \u2192 Nodes .","title":"Testing"},{"location":"updating-migrating/ingress-controller/#configuring","text":"Ingress controller settings and Ingress annotations will be automatically moved from the previous version to the new version. The list of all settings and annotations is available here . Configuration use cases: Proper Reporting of End User Public IP Address Management of IP Addresses Blocking High Availability Considerations Ingress Controller Monitoring","title":"Configuring"},{"location":"updating-migrating/migrate-ip-lists-to-node-3/","text":"Migrating whitelists and blacklists from previous WAF node versions to 3.0 \u00b6 Starting with WAF node 3.0, the method of IP addresses whitelist and blacklist configuration has been changed. This document instructs how to migrate whitelists ad blacklists configured in WAF node 2.18 or lower to WAF node 3.0. What has changed? \u00b6 Configuration of IP addresses whitelist and blacklist has been changed as follows: The wallarm_acl_* NGINX directives, acl Envoy parameters, and WALLARM_ACL_* environment variables have been deprecated. Now, IP lists are configured as follows: Additional steps to enable IP whitelisting or blacklisting functionality are not required. The WAF node downloads IP addresses lists from the Wallarm Cloud by default and applies downloaded data when processing incoming requests. Blocking page and error code returned in the response to the blocked request are configured using the wallarm_block_page directive instead of wallarm_acl_block_page . Whitelisted and blacklisted IP addresses are managed via the Wallarm Console. IP addresses of Wallarm Vulnerability Scanner are whitelisted by default. Manual whitelisting of Scanner IP addresses is no longer required. Procedure for whitelists and blacklists configuration migration \u00b6 Inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available. Update the WAF node modules up to version 3.0 following appropriate instructions . Remove the whitelist of Wallarm Scanner IP addresses from WAF node configuration files. Starting with the WAF node 3.0, Scanner IP addresses are whitelisted by default. In previous WAF node versions, the whitelist could be configured by the following methods: Disabled filtration mode for Scanner IP addresses (for example: NGINX configuration , K8s sidecar container , K8s Ingress controller ). NGINX directive allow . If listed methods are used to whitelist other IP addresses that should not be blocked by the WAF node, please move them to the whitelist in the Wallarm Console . If you have used the directive wallarm_acl_block_page to configure the blocking page and error code returned when the blacklisted IP originated the request, please replace the directive name by wallarm_block_page and update its value following the instructions . Remove the NGINX and Envoy environment variables WALLARM_ACL_* from the docker run commands. (Optional) Remove the NGINX directives wallarm_acl_* and acl Envoy parameters from WAF node configuration files.","title":"Migrating whitelists and blacklists from previous versions to 3.0"},{"location":"updating-migrating/migrate-ip-lists-to-node-3/#migrating-whitelists-and-blacklists-from-previous-waf-node-versions-to-30","text":"Starting with WAF node 3.0, the method of IP addresses whitelist and blacklist configuration has been changed. This document instructs how to migrate whitelists ad blacklists configured in WAF node 2.18 or lower to WAF node 3.0.","title":"Migrating whitelists and blacklists from previous WAF node versions to 3.0"},{"location":"updating-migrating/migrate-ip-lists-to-node-3/#what-has-changed","text":"Configuration of IP addresses whitelist and blacklist has been changed as follows: The wallarm_acl_* NGINX directives, acl Envoy parameters, and WALLARM_ACL_* environment variables have been deprecated. Now, IP lists are configured as follows: Additional steps to enable IP whitelisting or blacklisting functionality are not required. The WAF node downloads IP addresses lists from the Wallarm Cloud by default and applies downloaded data when processing incoming requests. Blocking page and error code returned in the response to the blocked request are configured using the wallarm_block_page directive instead of wallarm_acl_block_page . Whitelisted and blacklisted IP addresses are managed via the Wallarm Console. IP addresses of Wallarm Vulnerability Scanner are whitelisted by default. Manual whitelisting of Scanner IP addresses is no longer required.","title":"What has changed?"},{"location":"updating-migrating/migrate-ip-lists-to-node-3/#procedure-for-whitelists-and-blacklists-configuration-migration","text":"Inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available. Update the WAF node modules up to version 3.0 following appropriate instructions . Remove the whitelist of Wallarm Scanner IP addresses from WAF node configuration files. Starting with the WAF node 3.0, Scanner IP addresses are whitelisted by default. In previous WAF node versions, the whitelist could be configured by the following methods: Disabled filtration mode for Scanner IP addresses (for example: NGINX configuration , K8s sidecar container , K8s Ingress controller ). NGINX directive allow . If listed methods are used to whitelist other IP addresses that should not be blocked by the WAF node, please move them to the whitelist in the Wallarm Console . If you have used the directive wallarm_acl_block_page to configure the blocking page and error code returned when the blacklisted IP originated the request, please replace the directive name by wallarm_block_page and update its value following the instructions . Remove the NGINX and Envoy environment variables WALLARM_ACL_* from the docker run commands. (Optional) Remove the NGINX directives wallarm_acl_* and acl Envoy parameters from WAF node configuration files.","title":"Procedure for whitelists and blacklists configuration migration"},{"location":"updating-migrating/nginx-modules/","text":"Updating Linux WAF packages \u00b6 These instructions describe the steps to update Linux WAF packages to version 3.0. Linux WAF packages are packages installed in accordance with one of the following instructions: NGINX stable module Module for NGINX from CentOS/Debian repositories NGINX Plus module Kong module Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 . Update procedure \u00b6 If WAF node and postanalytics modules are installed on the same server, then follow the instrutions below to update all packages. If WAF node and postanalytics modules are installed on different servers, then first update the postanalytics module following these instructions and perform the steps below for WAF node modules. Step 1: Inform Wallarm technical support that you are updating WAF node modules \u00b6 Please inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available. Step 2: Add new Wallarm WAF repository \u00b6 Delete the previous Wallarm WAF repository address and add a repository with a new WAF node version package. Please use the commands for the appropriate platform. CentOS and Amazon Linux 2 CentOS 7 and Amazon Linux 2 sudo yum remove wallarm-node-repo sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8 sudo yum remove wallarm-node-repo sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm Debian and Ubuntu Open the file with the Wallarm WAF repository address in the installed text editor. In these instructions, vim is used. sudo vim /etc/apt/sources.list.d/wallarm.list Comment out or delete the previous repository address. Add a new repository address: Debian 9.x (stretch) deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/ Debian 9.x (stretch-backports) deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/ deb http://repo.wallarm.com/debian/wallarm-node stretch-backports/3.0/ Debian 10.x (buster) deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/ Ubuntu 18.04 LTS (bionic) deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/ Step 3: Migrate whitelists and blacklists from previous WAF node version to 3.0 \u00b6 Migrate whitelists and blacklists configuration from previous WAF node version to 3.0 following the instructions . Step 4: Update Wallarm WAF packages \u00b6 WAF node and postanalytics on the same server \u00b6 Debian sudo apt update sudo apt dist-upgrade Ubuntu sudo apt update sudo apt dist-upgrade CentOS \u0438\u043b\u0438 Amazon Linux 2 sudo yum update WAF node and postanalytics on different servers \u00b6 Sequence of steps to update the WAF node and postanalytics modules If the WAF node and postanalytics modules are installed on different servers, then it is required to update the postanalytics packages before updating the WAF node packages. Update postanalytics packages following these instructions . Update WAF node packages: Debian sudo apt update sudo apt dist-upgrade Ubuntu sudo apt update sudo apt dist-upgrade CentOS \u0438\u043b\u0438 Amazon Linux 2 sudo yum update Step 5: Restart NGINX \u00b6 Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx Step 6: Test Wallarm WAF operation \u00b6 Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Settings customization \u00b6 Wallarm WAF modules are updated to version 3.0. Previous WAF node settings will be applied to the new version automatically. To make additional settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Configuring dynamic DNS resolution in NGINX Double\u2011detection of attacks with libdetection","title":"Upgrading Linux packages"},{"location":"updating-migrating/nginx-modules/#updating-linux-waf-packages","text":"These instructions describe the steps to update Linux WAF packages to version 3.0. Linux WAF packages are packages installed in accordance with one of the following instructions: NGINX stable module Module for NGINX from CentOS/Debian repositories NGINX Plus module Kong module Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 .","title":"Updating Linux WAF packages"},{"location":"updating-migrating/nginx-modules/#update-procedure","text":"If WAF node and postanalytics modules are installed on the same server, then follow the instrutions below to update all packages. If WAF node and postanalytics modules are installed on different servers, then first update the postanalytics module following these instructions and perform the steps below for WAF node modules.","title":"Update procedure"},{"location":"updating-migrating/nginx-modules/#step-1-inform-wallarm-technical-support-that-you-are-updating-waf-node-modules","text":"Please inform Wallarm technical support that you are updating WAF node modules up to 3.0 and ask to enable new IP lists logic for your Wallarm account. When new IP lists logic is enabled, please open the Wallarm Console and ensure that the section IP lists is available.","title":"Step 1: Inform Wallarm technical support that you are updating WAF node modules"},{"location":"updating-migrating/nginx-modules/#step-2-add-new-wallarm-waf-repository","text":"Delete the previous Wallarm WAF repository address and add a repository with a new WAF node version package. Please use the commands for the appropriate platform. CentOS and Amazon Linux 2 CentOS 7 and Amazon Linux 2 sudo yum remove wallarm-node-repo sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8 sudo yum remove wallarm-node-repo sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm Debian and Ubuntu Open the file with the Wallarm WAF repository address in the installed text editor. In these instructions, vim is used. sudo vim /etc/apt/sources.list.d/wallarm.list Comment out or delete the previous repository address. Add a new repository address: Debian 9.x (stretch) deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/ Debian 9.x (stretch-backports) deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/ deb http://repo.wallarm.com/debian/wallarm-node stretch-backports/3.0/ Debian 10.x (buster) deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/ Ubuntu 18.04 LTS (bionic) deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/","title":"Step 2: Add new Wallarm WAF repository"},{"location":"updating-migrating/nginx-modules/#step-3-migrate-whitelists-and-blacklists-from-previous-waf-node-version-to-30","text":"Migrate whitelists and blacklists configuration from previous WAF node version to 3.0 following the instructions .","title":"Step 3: Migrate whitelists and blacklists from previous WAF node version to 3.0"},{"location":"updating-migrating/nginx-modules/#step-4-update-wallarm-waf-packages","text":"","title":"Step 4: Update Wallarm WAF packages"},{"location":"updating-migrating/nginx-modules/#waf-node-and-postanalytics-on-the-same-server","text":"Debian sudo apt update sudo apt dist-upgrade Ubuntu sudo apt update sudo apt dist-upgrade CentOS \u0438\u043b\u0438 Amazon Linux 2 sudo yum update","title":"WAF node and postanalytics on the same server"},{"location":"updating-migrating/nginx-modules/#waf-node-and-postanalytics-on-different-servers","text":"Sequence of steps to update the WAF node and postanalytics modules If the WAF node and postanalytics modules are installed on different servers, then it is required to update the postanalytics packages before updating the WAF node packages. Update postanalytics packages following these instructions . Update WAF node packages: Debian sudo apt update sudo apt dist-upgrade Ubuntu sudo apt update sudo apt dist-upgrade CentOS \u0438\u043b\u0438 Amazon Linux 2 sudo yum update","title":"WAF node and postanalytics on different servers"},{"location":"updating-migrating/nginx-modules/#step-5-restart-nginx","text":"Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx","title":"Step 5: Restart NGINX"},{"location":"updating-migrating/nginx-modules/#step-6-test-wallarm-waf-operation","text":"Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"Step 6: Test Wallarm WAF operation"},{"location":"updating-migrating/nginx-modules/#settings-customization","text":"Wallarm WAF modules are updated to version 3.0. Previous WAF node settings will be applied to the new version automatically. To make additional settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Configuring dynamic DNS resolution in NGINX Double\u2011detection of attacks with libdetection","title":"Settings customization"},{"location":"updating-migrating/separate-postanalytics/","text":"Updating the Separately Installed Postanalytics Module \u00b6 These instructions describe the steps to update the postanalytics module installed on a separate server. Postanalytics module must be updated before updating Linux WAF packages . Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 . Step 1: Add new Wallarm WAF repository \u00b6 Delete the previous Wallarm WAF repository address and add a repository with a new WAF node version packages. Please use the commands for the appropriate platform. CentOS and Amazon Linux 2 CentOS 7 and Amazon Linux 2 sudo yum remove wallarm-node-repo sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8 sudo yum remove wallarm-node-repo sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm Debian and Ubuntu Open the file with the Wallarm WAF repository address in the installed text editor. In this instruction, vim is used. sudo vim /etc/apt/sources.list.d/wallarm.list Comment out or delete the previous repository address. Add a new repository address: Debian 9.x (stretch) deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/ Debian 9.x (stretch-backports) deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/ deb http://repo.wallarm.com/debian/wallarm-node stretch-backports/3.0/ Debian 10.x (buster) deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/ Ubuntu 18.04 LTS (bionic) deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/ Step 2: Update the Tarantool packages \u00b6 Debian sudo apt update sudo apt dist-upgrade Ubuntu sudo apt update sudo apt dist-upgrade CentOS \u0438\u043b\u0438 Amazon Linux 2 sudo yum update Step 3: Restart the postanalytics module \u00b6 Debian sudo systemctl restart wallarm-tarantool Ubuntu sudo service wallarm-tarantool restart CentOS 7.x \u0438\u043b\u0438 Amazon Linux 2 sudo systemctl restart wallarm-tarantool","title":"Upgrading the separately installed postanalytics module"},{"location":"updating-migrating/separate-postanalytics/#updating-the-separately-installed-postanalytics-module","text":"These instructions describe the steps to update the postanalytics module installed on a separate server. Postanalytics module must be updated before updating Linux WAF packages . Breaking changes and skipping partner WAF node update The WAF node 3.0 is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes and consider a possible configuration change. We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported only in partner WAF node 3.2 .","title":"Updating the Separately Installed Postanalytics Module"},{"location":"updating-migrating/separate-postanalytics/#step-1-add-new-wallarm-waf-repository","text":"Delete the previous Wallarm WAF repository address and add a repository with a new WAF node version packages. Please use the commands for the appropriate platform. CentOS and Amazon Linux 2 CentOS 7 and Amazon Linux 2 sudo yum remove wallarm-node-repo sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8 sudo yum remove wallarm-node-repo sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm Debian and Ubuntu Open the file with the Wallarm WAF repository address in the installed text editor. In this instruction, vim is used. sudo vim /etc/apt/sources.list.d/wallarm.list Comment out or delete the previous repository address. Add a new repository address: Debian 9.x (stretch) deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/ Debian 9.x (stretch-backports) deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/ deb http://repo.wallarm.com/debian/wallarm-node stretch-backports/3.0/ Debian 10.x (buster) deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/ Ubuntu 18.04 LTS (bionic) deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/","title":"Step 1: Add new Wallarm WAF repository"},{"location":"updating-migrating/separate-postanalytics/#step-2-update-the-tarantool-packages","text":"Debian sudo apt update sudo apt dist-upgrade Ubuntu sudo apt update sudo apt dist-upgrade CentOS \u0438\u043b\u0438 Amazon Linux 2 sudo yum update","title":"Step 2: Update the Tarantool packages"},{"location":"updating-migrating/separate-postanalytics/#step-3-restart-the-postanalytics-module","text":"Debian sudo systemctl restart wallarm-tarantool Ubuntu sudo service wallarm-tarantool restart CentOS 7.x \u0438\u043b\u0438 Amazon Linux 2 sudo systemctl restart wallarm-tarantool","title":"Step 3: Restart the postanalytics module"},{"location":"updating-migrating/versioning-policy/","text":"WAF node versioning policy \u00b6 This policy describes the method of versioning of different WAF node artifacts: Linux packages, Docker containers, Helm charts, etc. You can use this document to select the WAF node version for installation and to schedule updates of installed packages. Artifact The artifact is the result of WAF node development that is used to install the WAF node on the platform. For example: Linux packages, Kong API modules, Docker containers, etc. Version list \u00b6 WAF node version Release date Support until 2.14 and lower June 2021 2.16 October 2020 July 2021 2.18 February 2021 November 2021 3.0 July 2021 3.2 August 2021 4.0 4 th quarter 2021 Version format \u00b6 WAF node artifact versions have the following format: <MAJOR_VERSION>.<MINOR_VERSION>.<PATCH_VERSION> [ -<BUILD_NUMBER> ] Parameter Description Average release rate <MAJOR_VERSION> Major WAF node version: Major rework of the component Incompatible changes Initial value is 2 . The value increases by 1, for example: 2.14.0 , 3.1.0 . No release expected <MINOR_VERSION> Minor WAF node version: New product features Major bug fixes Other compatible changes The value increases by 2, for example: 2.12 , 2.14 . Once a quarter <PATCH_VERSION> WAF node patch version: Minor bug fixes New features added after a special request Initial value is 0 . The value increases by 1, for example: 2.14.0 , 2.14.1 . Once a month <BUILD_NUMBER> (optional) WAF node build version. The value is assigned automatically by the employed package build platform. The value will not be assigned to artifacts built using a manual process. The value increases by 1, for example: 2.14.0-1 , 2.14.0-2 . If the first build fails, the build is run again and the value is incremented. As new <PATCH_VERSION> released We recommend using different WAF node version format when downloading the packages or images. The format depends on the WAF node installation form : <MAJOR_VERSION>.<MINOR_VERSION> for Linux packages <MAJOR_VERSION>.<MINOR_VERSION>.<PATCH_VERSION>[-<BUILD_NUMBER>] for Docker and cloud images, and Helm charts When pulling Wallarm Docker images, you can also specify the version of the WAF node in the format <MAJOR_VERSION>.<MINOR_VERSION> . Since pulled version of the WAF node contains changes of the latest available patch version, behavior of the same <MAJOR_VERSION>.<MINOR_VERSION> image version pulled in different time periods may differ. Versions of WAF node packages may differ within the same artifact. For example, if only one package needs to be updated, then the remaining packages retain the previous version. Version support \u00b6 Wallarm supports 3 latest versions of the WAF node in the following ways: In the latest version, releases bug fixes. May release new features after a special request. In the previous version, releases bug fixes. In the third available version, releases bug fixes for 3 months after the date of the latest version release. In 3 months, the version will be deprecated. When installing a WAF node for the first time, it is recommended to use the latest available version. When installing an additional WAF node in the environment with already installed WAF nodes, it is recommended to use the same minor version in all installations for full compatibility. Version update \u00b6 It is assumed that you are using the latest available version of the WAF node when installing, updating, or configuring the product. The WAF node instructions describe commands that automatically install the latest available patch and build. New version notification \u00b6 Wallarm publishes information about the new minor version in the following sources: Public Documentation News portal Wallarm Console Information about available updates for minor WAF node versions and for WAF node patch versions is also displayed in Wallarm Console \u2192 Nodes for regular nodes. Each package has the status Up to date or the list of available updates. For example, the card of the WAF node with the latest component versions installed looks like: Update procedure \u00b6 Along with the release of the new WAF node minor version, installation instructions are also published. To access instructions regarding how to update installed artifacts, please use the appropriate instructions from the Updating and Migrating section. After updating to a new minor version or patch version, all previous WAF node settings will be saved and applied to the new version.","title":"Versioning policy"},{"location":"updating-migrating/versioning-policy/#waf-node-versioning-policy","text":"This policy describes the method of versioning of different WAF node artifacts: Linux packages, Docker containers, Helm charts, etc. You can use this document to select the WAF node version for installation and to schedule updates of installed packages. Artifact The artifact is the result of WAF node development that is used to install the WAF node on the platform. For example: Linux packages, Kong API modules, Docker containers, etc.","title":"WAF node versioning policy"},{"location":"updating-migrating/versioning-policy/#version-list","text":"WAF node version Release date Support until 2.14 and lower June 2021 2.16 October 2020 July 2021 2.18 February 2021 November 2021 3.0 July 2021 3.2 August 2021 4.0 4 th quarter 2021","title":"Version list"},{"location":"updating-migrating/versioning-policy/#version-format","text":"WAF node artifact versions have the following format: <MAJOR_VERSION>.<MINOR_VERSION>.<PATCH_VERSION> [ -<BUILD_NUMBER> ] Parameter Description Average release rate <MAJOR_VERSION> Major WAF node version: Major rework of the component Incompatible changes Initial value is 2 . The value increases by 1, for example: 2.14.0 , 3.1.0 . No release expected <MINOR_VERSION> Minor WAF node version: New product features Major bug fixes Other compatible changes The value increases by 2, for example: 2.12 , 2.14 . Once a quarter <PATCH_VERSION> WAF node patch version: Minor bug fixes New features added after a special request Initial value is 0 . The value increases by 1, for example: 2.14.0 , 2.14.1 . Once a month <BUILD_NUMBER> (optional) WAF node build version. The value is assigned automatically by the employed package build platform. The value will not be assigned to artifacts built using a manual process. The value increases by 1, for example: 2.14.0-1 , 2.14.0-2 . If the first build fails, the build is run again and the value is incremented. As new <PATCH_VERSION> released We recommend using different WAF node version format when downloading the packages or images. The format depends on the WAF node installation form : <MAJOR_VERSION>.<MINOR_VERSION> for Linux packages <MAJOR_VERSION>.<MINOR_VERSION>.<PATCH_VERSION>[-<BUILD_NUMBER>] for Docker and cloud images, and Helm charts When pulling Wallarm Docker images, you can also specify the version of the WAF node in the format <MAJOR_VERSION>.<MINOR_VERSION> . Since pulled version of the WAF node contains changes of the latest available patch version, behavior of the same <MAJOR_VERSION>.<MINOR_VERSION> image version pulled in different time periods may differ. Versions of WAF node packages may differ within the same artifact. For example, if only one package needs to be updated, then the remaining packages retain the previous version.","title":"Version format"},{"location":"updating-migrating/versioning-policy/#version-support","text":"Wallarm supports 3 latest versions of the WAF node in the following ways: In the latest version, releases bug fixes. May release new features after a special request. In the previous version, releases bug fixes. In the third available version, releases bug fixes for 3 months after the date of the latest version release. In 3 months, the version will be deprecated. When installing a WAF node for the first time, it is recommended to use the latest available version. When installing an additional WAF node in the environment with already installed WAF nodes, it is recommended to use the same minor version in all installations for full compatibility.","title":"Version support"},{"location":"updating-migrating/versioning-policy/#version-update","text":"It is assumed that you are using the latest available version of the WAF node when installing, updating, or configuring the product. The WAF node instructions describe commands that automatically install the latest available patch and build.","title":"Version update"},{"location":"updating-migrating/versioning-policy/#new-version-notification","text":"Wallarm publishes information about the new minor version in the following sources: Public Documentation News portal Wallarm Console Information about available updates for minor WAF node versions and for WAF node patch versions is also displayed in Wallarm Console \u2192 Nodes for regular nodes. Each package has the status Up to date or the list of available updates. For example, the card of the WAF node with the latest component versions installed looks like:","title":"New version notification"},{"location":"updating-migrating/versioning-policy/#update-procedure","text":"Along with the release of the new WAF node minor version, installation instructions are also published. To access instructions regarding how to update installed artifacts, please use the appropriate instructions from the Updating and Migrating section. After updating to a new minor version or patch version, all previous WAF node settings will be saved and applied to the new version.","title":"Update procedure"},{"location":"updating-migrating/what-is-new/","text":"What is new in WAF node 3.0 \u00b6 We have released WAF node 3.0 that is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes listed below and general recommendations . Which WAF nodes are recommended to be updated? \u00b6 We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported in partner WAF node 3.2 . Regular (client) WAF node can be updated up to version 3.0. Before updating the modules, we recommend to carefully review the list of WAF node 3.0 changes listed below and other recommendations . Changes in supported installation platforms \u00b6 Dropped support for the operating system Ubuntu 16.04 LTS (xenial) See the full list of supported platforms \u2192 Changes in supported WAF node configuration parameters \u00b6 Dropped support for all acl NGINX directives, Envoy parameters, and environment variables used to configure IP addresses blacklist. Manual configuration of IP blacklisting is no longer required. Details on migrating blacklist configuration \u2192 Added new NGINX directive and Envoy parameter disable_acl . This parameter allows to disabled request origin analysis. Details on the disable_acl NGINX directive \u2192 Details on the disable_acl Envoy parameter \u2192 Changes in system requirements for the WAF node installation \u00b6 Starting with version 3.0, the WAF node supports IP addresses whitelists, blacklists, and greylists . The Wallarm Console allows adding both single IPs and countries or data centers to any IP list type. The WAF node downloads an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers from GCP storage. By default, access to this storage can be restricted in your system. Allowing access to GCP storage is a new requirement for the virtual machine on which the WAF node is installed. Range of GCP IP addresses that should be allowed \u2192 New features \u00b6 Support for new filtration mode safe_blocking and IP addresses greylist . The WAF node operating in safe_blocking mode blocks only those malicious requests originated from greylisted IP addresses that allow a significant reduction of false positives numbers. New reaction of triggers Add to greyist allowing to automatically greylist IP addresses originated a specific number of malicious requests. Example of the trigger that greylists IP addresses \u2192 Management of IP addresses whitelist via the Wallarm Console. Automated whitelisting of Wallarm Vulnerability Scanner IP addresses. Manual whitelisting of Scanner IP addresses is no longer required. New parameters of the file node.yaml for configuring the synchronization of the Wallarm Cloud and WAF nodes: api.local_host and api.local_port . New parameters allow specifying a local IP address and port of the network interface through which requests to Wallarm API are sent. See the full list of node.yaml parameters for Wallarm Cloud and WAF node synchronization setup \u2192 Ability to whitelist, blacklist, or greylist a subnet, Tor network IPs, VPN IPs, a group of IP addresses registered in a specific country or data center. Details on adding IPs to the whitelist, blacklist, and greylist \u2192 Update process \u00b6 Review recommendations for the modules update . Update installed modules following the instructions for your WAF node deployment option: General recommendations for a safe WAF node update process Updating modules for NGINX, NGINX Plus, Kong Updating the Docker container with the modules for NGINX or Envoy Updating NGINX Ingress controller with integrated Wallarm WAF Cloud WAF node image Migrate whitelists and blacklists configuration from previous WAF node versions to 3.0 Other updates in Wallarm products and components \u2192","title":"What is new in version\u00a03.0"},{"location":"updating-migrating/what-is-new/#what-is-new-in-waf-node-30","text":"We have released WAF node 3.0 that is totally incompatible with previous WAF node versions . Before updating the modules up to 3.0, please carefully review the list of WAF node 3.0 changes listed below and general recommendations .","title":"What is new in WAF node 3.0"},{"location":"updating-migrating/what-is-new/#which-waf-nodes-are-recommended-to-be-updated","text":"We do NOT recommend updating partner WAF node up to version 3.0, since most changes will be fully supported in partner WAF node 3.2 . Regular (client) WAF node can be updated up to version 3.0. Before updating the modules, we recommend to carefully review the list of WAF node 3.0 changes listed below and other recommendations .","title":"Which WAF nodes are recommended to be updated?"},{"location":"updating-migrating/what-is-new/#changes-in-supported-installation-platforms","text":"Dropped support for the operating system Ubuntu 16.04 LTS (xenial) See the full list of supported platforms \u2192","title":"Changes in supported installation platforms"},{"location":"updating-migrating/what-is-new/#changes-in-supported-waf-node-configuration-parameters","text":"Dropped support for all acl NGINX directives, Envoy parameters, and environment variables used to configure IP addresses blacklist. Manual configuration of IP blacklisting is no longer required. Details on migrating blacklist configuration \u2192 Added new NGINX directive and Envoy parameter disable_acl . This parameter allows to disabled request origin analysis. Details on the disable_acl NGINX directive \u2192 Details on the disable_acl Envoy parameter \u2192","title":"Changes in supported WAF node configuration parameters"},{"location":"updating-migrating/what-is-new/#changes-in-system-requirements-for-the-waf-node-installation","text":"Starting with version 3.0, the WAF node supports IP addresses whitelists, blacklists, and greylists . The Wallarm Console allows adding both single IPs and countries or data centers to any IP list type. The WAF node downloads an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers from GCP storage. By default, access to this storage can be restricted in your system. Allowing access to GCP storage is a new requirement for the virtual machine on which the WAF node is installed. Range of GCP IP addresses that should be allowed \u2192","title":"Changes in system requirements for the WAF node installation"},{"location":"updating-migrating/what-is-new/#new-features","text":"Support for new filtration mode safe_blocking and IP addresses greylist . The WAF node operating in safe_blocking mode blocks only those malicious requests originated from greylisted IP addresses that allow a significant reduction of false positives numbers. New reaction of triggers Add to greyist allowing to automatically greylist IP addresses originated a specific number of malicious requests. Example of the trigger that greylists IP addresses \u2192 Management of IP addresses whitelist via the Wallarm Console. Automated whitelisting of Wallarm Vulnerability Scanner IP addresses. Manual whitelisting of Scanner IP addresses is no longer required. New parameters of the file node.yaml for configuring the synchronization of the Wallarm Cloud and WAF nodes: api.local_host and api.local_port . New parameters allow specifying a local IP address and port of the network interface through which requests to Wallarm API are sent. See the full list of node.yaml parameters for Wallarm Cloud and WAF node synchronization setup \u2192 Ability to whitelist, blacklist, or greylist a subnet, Tor network IPs, VPN IPs, a group of IP addresses registered in a specific country or data center. Details on adding IPs to the whitelist, blacklist, and greylist \u2192","title":"New features"},{"location":"updating-migrating/what-is-new/#update-process","text":"Review recommendations for the modules update . Update installed modules following the instructions for your WAF node deployment option: General recommendations for a safe WAF node update process Updating modules for NGINX, NGINX Plus, Kong Updating the Docker container with the modules for NGINX or Envoy Updating NGINX Ingress controller with integrated Wallarm WAF Cloud WAF node image Migrate whitelists and blacklists configuration from previous WAF node versions to 3.0 Other updates in Wallarm products and components \u2192","title":"Update process"},{"location":"user-guides/use-sso/","text":"Single sign\u2011on to Wallarm Console \u00b6 Document structure (can be divided into several document) Steps to configure the SSO provider Steps to configure SSO on the Wallarm side How to use SSO","title":"Single sign\u2011on to Wallarm Console"},{"location":"user-guides/use-sso/#single-signon-to-wallarm-console","text":"Document structure (can be divided into several document) Steps to configure the SSO provider Steps to configure SSO on the Wallarm side How to use SSO","title":"Single sign\u2011on to Wallarm Console"},{"location":"user-guides/user-intro/","text":"Introduction to the User Guide \u00b6 This guide provides information on Wallarm operation options. All of these operations are performed using the Wallarm portal. Required Access Rights You must have analyst access to perform most of the operations described in this guide. You can set analyst access on the Settings \u2192 Users tab on the Wallarm portal in the EU or US cloud. Wallarm Portal Overview \u00b6 After logging in to the Wallarm portal, you will be presented with the following: A sidebar with tabs on the left side of the portal. Use these tabs to navigate from one component of the Wallarm solution to another. Some components' pages contain their own tabs, horizontally aligned. The Help & Docs button is located at the bottom of the sidebar. Upon clicking this button, the Quick Help sidebar will be opened on the right side of the portal, allowing you to explore various product information and submit a message to the support team. A note on Component Availability Your Wallarm portal may look different from the screenshots demonstrated in this User Guide. The availability of some components and UI elements depends on the subscriptions in use. See the \u201cSubscriptions\u201d document for more details. A search box at the top of the portal. This search box is available everywhere in the portal except for the Events tab. Type in a search query and you will be redirected to the search results on the Events tab. The \u201cUsing Search\u201d document introduces you to the search query syntax and explains how to use search. A Settings button at the upper right corner of the portal near the search box. This button looks like small gear. Use this button to open the Wallarm settings page. Depending on the actions you take, some additional sidebars might be displayed to you at the very right of the portal. Demo videos \u00b6","title":"User intro"},{"location":"user-guides/user-intro/#introduction-to-the-user-guide","text":"This guide provides information on Wallarm operation options. All of these operations are performed using the Wallarm portal. Required Access Rights You must have analyst access to perform most of the operations described in this guide. You can set analyst access on the Settings \u2192 Users tab on the Wallarm portal in the EU or US cloud.","title":"Introduction to the User Guide"},{"location":"user-guides/user-intro/#wallarm-portal-overview","text":"After logging in to the Wallarm portal, you will be presented with the following: A sidebar with tabs on the left side of the portal. Use these tabs to navigate from one component of the Wallarm solution to another. Some components' pages contain their own tabs, horizontally aligned. The Help & Docs button is located at the bottom of the sidebar. Upon clicking this button, the Quick Help sidebar will be opened on the right side of the portal, allowing you to explore various product information and submit a message to the support team. A note on Component Availability Your Wallarm portal may look different from the screenshots demonstrated in this User Guide. The availability of some components and UI elements depends on the subscriptions in use. See the \u201cSubscriptions\u201d document for more details. A search box at the top of the portal. This search box is available everywhere in the portal except for the Events tab. Type in a search query and you will be redirected to the search results on the Events tab. The \u201cUsing Search\u201d document introduces you to the search query syntax and explains how to use search. A Settings button at the upper right corner of the portal near the search box. This button looks like small gear. Use this button to open the Wallarm settings page. Depending on the actions you take, some additional sidebars might be displayed to you at the very right of the portal.","title":"Wallarm Portal Overview"},{"location":"user-guides/user-intro/#demo-videos","text":"","title":"Demo videos"},{"location":"user-guides/dashboard/intro/","text":"Dashboards Overview \u00b6 The Dashboards section on your Wallarm account shows the event history's graphs and metrics of the system, as well as the current status of resources. Data is displayed on two dashboards: WAF dashboard with statistics on the WAF node operation Scanner Dashboard with statistics on the detected vulnerabilities Getting started with WAF The dashboard section also includes the Get Started button for new users. When you click the button, you will get a list of basic steps for installing and configuring a WAF node.","title":"Statistics on detected events"},{"location":"user-guides/dashboard/intro/#dashboards-overview","text":"The Dashboards section on your Wallarm account shows the event history's graphs and metrics of the system, as well as the current status of resources. Data is displayed on two dashboards: WAF dashboard with statistics on the WAF node operation Scanner Dashboard with statistics on the detected vulnerabilities Getting started with WAF The dashboard section also includes the Get Started button for new users. When you click the button, you will get a list of basic steps for installing and configuring a WAF node.","title":"Dashboards Overview"},{"location":"user-guides/dashboard/scanner/","text":"Scanner Dashboard \u00b6 The scanner dashboard shows statistics on vulnerabilities detected in the scope : The number of vulnerabilities of all risk levels detected over the selected period. Clicking the counter redirects to the Events tab with a list of active vulnerabilities of the selected risk level The number of active vulnerabilities of all risk levels at the end of the selected period Changes in the number of vulnerabilities of all risk levels for the selected period You can filter data on the Scanner dashboard by time period. By default, the dashboard displays the data for one month prior to the current date.","title":"Scanner"},{"location":"user-guides/dashboard/scanner/#scanner-dashboard","text":"The scanner dashboard shows statistics on vulnerabilities detected in the scope : The number of vulnerabilities of all risk levels detected over the selected period. Clicking the counter redirects to the Events tab with a list of active vulnerabilities of the selected risk level The number of active vulnerabilities of all risk levels at the end of the selected period Changes in the number of vulnerabilities of all risk levels for the selected period You can filter data on the Scanner dashboard by time period. By default, the dashboard displays the data for one month prior to the current date.","title":"Scanner Dashboard"},{"location":"user-guides/dashboard/waf/","text":"WAF Dashboard \u00b6 The WAF dashboard consists of the following data blocks: Statistics for the current month and the speed of request encountering Normal and malicious traffic Top targets Attack types Attack sources Blacklisted IP addresses You can filter data from the last five blocks by the following parameters: Application . By default, all applications added in the settings. Time period . By default, one month prior to the current date. Statistics for the current month and the speed of request encountering \u00b6 The block contains the following parameters: The number of requests, hits , and blocked hits in the current month The real-time speed at which requests and hits are encountered Normal and malicious traffic \u00b6 The graph shows the following statistics for the selected period: The amount of traffic The number of requests, hits, and incidents The estimated cost of attacks for the attacker: the value considers the approximate cost of IP address renting and the attacks duration Points on the graph shows parameter values at a specific time: Hover the mouse pointer over a point on the graph to get summary information on traffic at a specific time. Click on a graph point to be redirected to the to Events section and view detailed information on each hit and incident at that point in time. Top targets \u00b6 The chart shows the ratio of hits in each application for the selected period. To hide the application from the chart, uncheck the box next to the application name in the table. The table shows data for each application for the selected period: The number of detected incidents The number of detected hits Trends: change in the hits' number for a selected period and for the same previous period. For example: if you check the statistics for the last month, the trend displays the difference in the hits number between the last and previous months as a percentage Attack types \u00b6 The block contains the top types of attacks detected in requests for a selected period. Types are divided into blocks: Top types among all attacks Top types among requests that triggered the incident Clicking the attack type redirects to the Events section with a list of hits or incidents of this type of attack for the selected period. Attack sources \u00b6 The block shows statistics on the attack sources for the selected period: World map with the attack number distributed by source country Top attack source countries Top attack source resources: statistics on data centers IP addresses from which requests were received Blacklisted IP addresses \u00b6 The block shows the following data: IP addresses that are currently blacklisted. In addition, a reason for blocking and the time until it will be unblocked are also shown for each entry in the list. To manage the list via the Blacklist section click the Full list button. A graph with statistics on IP address blocking events for a selected period.","title":"Waf"},{"location":"user-guides/dashboard/waf/#waf-dashboard","text":"The WAF dashboard consists of the following data blocks: Statistics for the current month and the speed of request encountering Normal and malicious traffic Top targets Attack types Attack sources Blacklisted IP addresses You can filter data from the last five blocks by the following parameters: Application . By default, all applications added in the settings. Time period . By default, one month prior to the current date.","title":"WAF Dashboard"},{"location":"user-guides/dashboard/waf/#statistics-for-the-current-month-and-the-speed-of-request-encountering","text":"The block contains the following parameters: The number of requests, hits , and blocked hits in the current month The real-time speed at which requests and hits are encountered","title":"Statistics for the current month and the speed of request encountering"},{"location":"user-guides/dashboard/waf/#normal-and-malicious-traffic","text":"The graph shows the following statistics for the selected period: The amount of traffic The number of requests, hits, and incidents The estimated cost of attacks for the attacker: the value considers the approximate cost of IP address renting and the attacks duration Points on the graph shows parameter values at a specific time: Hover the mouse pointer over a point on the graph to get summary information on traffic at a specific time. Click on a graph point to be redirected to the to Events section and view detailed information on each hit and incident at that point in time.","title":"Normal and malicious traffic"},{"location":"user-guides/dashboard/waf/#top-targets","text":"The chart shows the ratio of hits in each application for the selected period. To hide the application from the chart, uncheck the box next to the application name in the table. The table shows data for each application for the selected period: The number of detected incidents The number of detected hits Trends: change in the hits' number for a selected period and for the same previous period. For example: if you check the statistics for the last month, the trend displays the difference in the hits number between the last and previous months as a percentage","title":"Top targets"},{"location":"user-guides/dashboard/waf/#attack-types","text":"The block contains the top types of attacks detected in requests for a selected period. Types are divided into blocks: Top types among all attacks Top types among requests that triggered the incident Clicking the attack type redirects to the Events section with a list of hits or incidents of this type of attack for the selected period.","title":"Attack types"},{"location":"user-guides/dashboard/waf/#attack-sources","text":"The block shows statistics on the attack sources for the selected period: World map with the attack number distributed by source country Top attack source countries Top attack source resources: statistics on data centers IP addresses from which requests were received","title":"Attack sources"},{"location":"user-guides/dashboard/waf/#blacklisted-ip-addresses","text":"The block shows the following data: IP addresses that are currently blacklisted. In addition, a reason for blocking and the time until it will be unblocked are also shown for each entry in the list. To manage the list via the Blacklist section click the Full list button. A graph with statistics on IP address blocking events for a selected period.","title":"Blacklisted IP addresses"},{"location":"user-guides/events/analyze-attack/","text":"Analyzing Attacks \u00b6 You can check attacks in the Events tab of the Wallarm interface. Wallarm automatically groups associated malicious requests into one entity \u2014 an attack. Analyze an Attack \u00b6 You can get information about an attack by investigating all the table columns described in \u201cChecking Attacks and Incidents.\u201d Analyze Requests in an Attack \u00b6 Select an attack. Click the number in the Requests column. Clicking the number will unfold all requests in the selected attack. Each request displays the associated information in the following columns: Date : Date and time of the request. Payload : Attack vector . Clicking the value in the payload column displays reference information on the attack type. Source : The IP address from which the request originated. Clicking the IP address adds the IP address value into the search field. The following information is also displayed if it was found in the Wallarm databases: The country in which the IP address is registered Which data center the given IP addresses belong to: the AWS tag for Amazon, the GCP tag for Google, the Azure tag for Microsoft data centers, and DC for other data centers The Tor tag if the attack's source is the Tor network The VPN tag if IP address belongs to VPN The Public proxy or Web proxy tag if the request was sent from the public or web proxy server Status : The server's response status code from the request. Size : The server's response size. Time : The server's response time. If the attack is happening at the current moment, the \u201cnow\u201d label is shown under the request graph. Analyze a Request in Raw Format \u00b6 The raw format of a request is the maximum possible level of detail. Select an attack. Click the number in the Requests column. Click the arrow next to the date of the request. The Wallarm interface will display the request in its raw format. Sampling of hits \u00b6 The attack may consist of a large number of identical hits (more than 100). Storing all hits may increase the Wallarm Cloud load and require a considerable amount of time to analyze and search for attacks via the Wallarm Console. To optimize the data storage and analysis, we apply the sampling algorithm to hits: The first 5 identical hits for each hour are saved in the sample in the Wallarm Cloud. If several samples are the part of the same attack, these samples are grouped (for example, the hits in the samples may differ only in the IP addresses). The rest of the hits are not saved in the sample, but their number is recorded in a separate parameter for each attack. Examples If the attack consists of 20 hits (10 identical hits each originated from different IP addresses), data on the first 5 hits from each IP address will be saved in the sample in the Wallarm Cloud and the number of the rest hits (10) will be recorded in a separate variable. If the attack consists of 10 hits originated from different IP addresses, data on all hits will be saved in the Wallarm Cloud. Enabling the sampling algorithm For input validation attacks , the sampling algorithm is enabled if Wallarm detects a high percentage of attacks in your traffic. When the sampling algorithm is enabled, all users of the Administrator or Global Administrator role added to your company account will receive a corresponding email. Emails are sent once per 8 hours if the sampling algorithm is enabled / disabled due to the attack percentage change. For all behavioral attacks , the sampling algorithm is enabled by default. If hits sampling is enabled for your traffic, the appropriate status, the sample of saved hits, and the number of other hits are displayed in the Events section of the Wallarm Console. For example: Demo videos \u00b6","title":"Analyzing attacks"},{"location":"user-guides/events/analyze-attack/#analyzing-attacks","text":"You can check attacks in the Events tab of the Wallarm interface. Wallarm automatically groups associated malicious requests into one entity \u2014 an attack.","title":"Analyzing Attacks"},{"location":"user-guides/events/analyze-attack/#analyze-an-attack","text":"You can get information about an attack by investigating all the table columns described in \u201cChecking Attacks and Incidents.\u201d","title":"Analyze an Attack"},{"location":"user-guides/events/analyze-attack/#analyze-requests-in-an-attack","text":"Select an attack. Click the number in the Requests column. Clicking the number will unfold all requests in the selected attack. Each request displays the associated information in the following columns: Date : Date and time of the request. Payload : Attack vector . Clicking the value in the payload column displays reference information on the attack type. Source : The IP address from which the request originated. Clicking the IP address adds the IP address value into the search field. The following information is also displayed if it was found in the Wallarm databases: The country in which the IP address is registered Which data center the given IP addresses belong to: the AWS tag for Amazon, the GCP tag for Google, the Azure tag for Microsoft data centers, and DC for other data centers The Tor tag if the attack's source is the Tor network The VPN tag if IP address belongs to VPN The Public proxy or Web proxy tag if the request was sent from the public or web proxy server Status : The server's response status code from the request. Size : The server's response size. Time : The server's response time. If the attack is happening at the current moment, the \u201cnow\u201d label is shown under the request graph.","title":"Analyze Requests in an Attack"},{"location":"user-guides/events/analyze-attack/#analyze-a-request-in-raw-format","text":"The raw format of a request is the maximum possible level of detail. Select an attack. Click the number in the Requests column. Click the arrow next to the date of the request. The Wallarm interface will display the request in its raw format.","title":"Analyze a Request in Raw Format"},{"location":"user-guides/events/analyze-attack/#sampling-of-hits","text":"The attack may consist of a large number of identical hits (more than 100). Storing all hits may increase the Wallarm Cloud load and require a considerable amount of time to analyze and search for attacks via the Wallarm Console. To optimize the data storage and analysis, we apply the sampling algorithm to hits: The first 5 identical hits for each hour are saved in the sample in the Wallarm Cloud. If several samples are the part of the same attack, these samples are grouped (for example, the hits in the samples may differ only in the IP addresses). The rest of the hits are not saved in the sample, but their number is recorded in a separate parameter for each attack. Examples If the attack consists of 20 hits (10 identical hits each originated from different IP addresses), data on the first 5 hits from each IP address will be saved in the sample in the Wallarm Cloud and the number of the rest hits (10) will be recorded in a separate variable. If the attack consists of 10 hits originated from different IP addresses, data on all hits will be saved in the Wallarm Cloud. Enabling the sampling algorithm For input validation attacks , the sampling algorithm is enabled if Wallarm detects a high percentage of attacks in your traffic. When the sampling algorithm is enabled, all users of the Administrator or Global Administrator role added to your company account will receive a corresponding email. Emails are sent once per 8 hours if the sampling algorithm is enabled / disabled due to the attack percentage change. For all behavioral attacks , the sampling algorithm is enabled by default. If hits sampling is enabled for your traffic, the appropriate status, the sample of saved hits, and the number of other hits are displayed in the Events section of the Wallarm Console. For example:","title":"Sampling of hits"},{"location":"user-guides/events/analyze-attack/#demo-videos","text":"","title":"Demo videos"},{"location":"user-guides/events/check-attack/","text":"Checking events \u00b6 You can check detected attacks, incidents, and vulnerabilities in the Events section of Wallarm Console. To find required data, please use the search field as described here or manually set required search filters. Attacks \u00b6 Date : The date and time of the malicious request. If several requests of the same type were detected at short intervals, the attack duration appears under the date. Duration is the time period between the first request of a certain type and the last request of the same type in the specified timeframe. If the attack is happening at the current moment, an appropriate label is displayed. Requests (hits) : The number of requests (hits) in the attack in the specified time frame. Payloads : Attack type and the number of unique attack vectors . Top IP / Source : The IP address from which the malicious requests originated. When the malicious requests originate from several IP addresses, the interface shows the IP address responsible for the most requests. There is also the following data displayed for the IP address: The total number of IP addresses from which the requests in the same attack originated during the specified timeframe. The country in which the IP address is registered (if it was found in the Wallarm databases) Which data center the given IP addresses belong to: the AWS tag for Amazon, the GCP tag for Google, the Azure tag for Microsoft data centers, and DC for other data centers (if it was found in the Wallarm databases) The Tor tag if the attack's source is the Tor network (if it was found in the Wallarm databases) The VPN tag if IP address belongs to VPN (if it was found in the Wallarm databases) The Public proxy or Web proxy tag if the request was sent from the public or web proxy server (if it was found in the Wallarm databases) Domain / Path : The domain, path and the application ID that the request targeted. Color indicator displays the status of attack blocking: Orange indicator if all hits of the attack were recorded but not blocked by the WAF node because the WAF node operates in the monitoring mode . If your application blocked the attack (for example, the application returned 403 Forbidden ), the indicator would be still orange and the Code column would display the code returned by the application. Red indicator if all hits of the attack were blocked by the WAF node. Red with a white indicator if some hits of the attack were blocked and others were only recorded ( filtration mode for some hits is set to monitoring). Code : The server's response status code on the request. When there are several response status codes, the most frequent code and the total number of returned codes are displayed. If the WAF node blocked the request, the code would be 403 . Parameter : The malicious request's parameters and tags of parsers applied to the request Verification : The attack verification status. If the attack is ticked as false positive, the corresponding mark will be shown in this column ( FP ) and the attack will not be verified again. To find attacks by the false positive action, use the search filter below or specify the action in the search string as described here . To sort attacks by the time of the last request, you can use the Sort by latest hit switch. Incidents \u00b6 Incidents have the same parameters as attacks, except for one column: the Vulnerabilities column replaces the Verification column of the attacks. The Vulnerabilities column displays the vulnerability, that the corresponding incident exploited. Clicking the vulnerability brings you to its detailed description and instructions on how to fix it. To sort incidents by the time of the last request, you can use the Sort by latest hit switch. Vulnerabilities \u00b6 Date : The date and time of vulnerability discovery. Risk : The danger level of the vulnerability. Target : The side to be the victim in the case of vulnerability exploitation (client, server). Type : The type of the attack that exploits the vulnerability. Domain : The domain that the vulnerability was discovered at. ID : The unique identifier of the vulnerability in the Wallarm system. Title : The title of the vulnerability. Events that are currently happening \u00b6 You can check events in real time. If your company resources are receiving malicious requests, the following data is displayed in Wallarm Console: The number of events that have happened in the last 5 minutes, which will be displayed next to the Events section name and inside the section. Special label, which is shown under the event date in the attacks or the incidents table. You may also add the now keyword to the search field to only display those events happening at the moment: attacks now to display attacks happening right now. incidents now to display incidents happening right now. attacks incidents now to display attacks and incidents happening right now. Demo videos \u00b6","title":"Check attack"},{"location":"user-guides/events/check-attack/#checking-events","text":"You can check detected attacks, incidents, and vulnerabilities in the Events section of Wallarm Console. To find required data, please use the search field as described here or manually set required search filters.","title":"Checking events"},{"location":"user-guides/events/check-attack/#attacks","text":"Date : The date and time of the malicious request. If several requests of the same type were detected at short intervals, the attack duration appears under the date. Duration is the time period between the first request of a certain type and the last request of the same type in the specified timeframe. If the attack is happening at the current moment, an appropriate label is displayed. Requests (hits) : The number of requests (hits) in the attack in the specified time frame. Payloads : Attack type and the number of unique attack vectors . Top IP / Source : The IP address from which the malicious requests originated. When the malicious requests originate from several IP addresses, the interface shows the IP address responsible for the most requests. There is also the following data displayed for the IP address: The total number of IP addresses from which the requests in the same attack originated during the specified timeframe. The country in which the IP address is registered (if it was found in the Wallarm databases) Which data center the given IP addresses belong to: the AWS tag for Amazon, the GCP tag for Google, the Azure tag for Microsoft data centers, and DC for other data centers (if it was found in the Wallarm databases) The Tor tag if the attack's source is the Tor network (if it was found in the Wallarm databases) The VPN tag if IP address belongs to VPN (if it was found in the Wallarm databases) The Public proxy or Web proxy tag if the request was sent from the public or web proxy server (if it was found in the Wallarm databases) Domain / Path : The domain, path and the application ID that the request targeted. Color indicator displays the status of attack blocking: Orange indicator if all hits of the attack were recorded but not blocked by the WAF node because the WAF node operates in the monitoring mode . If your application blocked the attack (for example, the application returned 403 Forbidden ), the indicator would be still orange and the Code column would display the code returned by the application. Red indicator if all hits of the attack were blocked by the WAF node. Red with a white indicator if some hits of the attack were blocked and others were only recorded ( filtration mode for some hits is set to monitoring). Code : The server's response status code on the request. When there are several response status codes, the most frequent code and the total number of returned codes are displayed. If the WAF node blocked the request, the code would be 403 . Parameter : The malicious request's parameters and tags of parsers applied to the request Verification : The attack verification status. If the attack is ticked as false positive, the corresponding mark will be shown in this column ( FP ) and the attack will not be verified again. To find attacks by the false positive action, use the search filter below or specify the action in the search string as described here . To sort attacks by the time of the last request, you can use the Sort by latest hit switch.","title":"Attacks"},{"location":"user-guides/events/check-attack/#incidents","text":"Incidents have the same parameters as attacks, except for one column: the Vulnerabilities column replaces the Verification column of the attacks. The Vulnerabilities column displays the vulnerability, that the corresponding incident exploited. Clicking the vulnerability brings you to its detailed description and instructions on how to fix it. To sort incidents by the time of the last request, you can use the Sort by latest hit switch.","title":"Incidents"},{"location":"user-guides/events/check-attack/#vulnerabilities","text":"Date : The date and time of vulnerability discovery. Risk : The danger level of the vulnerability. Target : The side to be the victim in the case of vulnerability exploitation (client, server). Type : The type of the attack that exploits the vulnerability. Domain : The domain that the vulnerability was discovered at. ID : The unique identifier of the vulnerability in the Wallarm system. Title : The title of the vulnerability.","title":"Vulnerabilities"},{"location":"user-guides/events/check-attack/#events-that-are-currently-happening","text":"You can check events in real time. If your company resources are receiving malicious requests, the following data is displayed in Wallarm Console: The number of events that have happened in the last 5 minutes, which will be displayed next to the Events section name and inside the section. Special label, which is shown under the event date in the attacks or the incidents table. You may also add the now keyword to the search field to only display those events happening at the moment: attacks now to display attacks happening right now. incidents now to display incidents happening right now. attacks incidents now to display attacks and incidents happening right now.","title":"Events that are currently happening"},{"location":"user-guides/events/check-attack/#demo-videos","text":"","title":"Demo videos"},{"location":"user-guides/events/false-attack/","text":"Working with false attacks \u00b6 What is a false positive? \u00b6 False positive occurs when attack signs are detected in the legitimate request. After analyzing an attack, you may conclude that all or some requests in this attack are false positives. To prevent the WAF node from recognizing such requests as attacks in future traffic analysis, you can mark several requests or the entire attack as a false positive. How a false positive mark works? \u00b6 If a false positive mark is added for the attack of the type different from Information Exposure , the rule disabling analysis of the same requests for detected attack signs ( tokens ) is automatically created. If a false positive mark is added for the incident with the Information Exposure attack type, the rule disabling analysis of the same requests for detected vulnerability signs is automatically created. Created rule is applied when analyzing requests to the protected application. The rule is not displayed in the Wallarm Console and can be changed or removed only by the request sent to Wallarm technical support . Mark a hit as a false positive \u00b6 To mark one request (hit) as a false positive: Select an attack in the Events section. Collapse the list of requests in this attack. Define a valid request and click False in the Actions column. Mark an attack as a false positive \u00b6 To mark all requests (hits) in the attack as false positives: Select an attack with valid requests in the Events section. Click Report attack as false positive . If all the requests in the attack are marked as false positives, then the information about that attack will look like this: Remove a false positive mark \u00b6 To remove a false positive mark from the hit or attack, please send a request to Wallarm technical support . Also, you can undo a false positive mark in the dialog box in the Wallarm Console within a few seconds after the mark was applied.","title":"False attack"},{"location":"user-guides/events/false-attack/#working-with-false-attacks","text":"","title":"Working with false attacks"},{"location":"user-guides/events/false-attack/#what-is-a-false-positive","text":"False positive occurs when attack signs are detected in the legitimate request. After analyzing an attack, you may conclude that all or some requests in this attack are false positives. To prevent the WAF node from recognizing such requests as attacks in future traffic analysis, you can mark several requests or the entire attack as a false positive.","title":"What is a false positive?"},{"location":"user-guides/events/false-attack/#how-a-false-positive-mark-works","text":"If a false positive mark is added for the attack of the type different from Information Exposure , the rule disabling analysis of the same requests for detected attack signs ( tokens ) is automatically created. If a false positive mark is added for the incident with the Information Exposure attack type, the rule disabling analysis of the same requests for detected vulnerability signs is automatically created. Created rule is applied when analyzing requests to the protected application. The rule is not displayed in the Wallarm Console and can be changed or removed only by the request sent to Wallarm technical support .","title":"How a false positive mark works?"},{"location":"user-guides/events/false-attack/#mark-a-hit-as-a-false-positive","text":"To mark one request (hit) as a false positive: Select an attack in the Events section. Collapse the list of requests in this attack. Define a valid request and click False in the Actions column.","title":"Mark a hit as a false positive"},{"location":"user-guides/events/false-attack/#mark-an-attack-as-a-false-positive","text":"To mark all requests (hits) in the attack as false positives: Select an attack with valid requests in the Events section. Click Report attack as false positive . If all the requests in the attack are marked as false positives, then the information about that attack will look like this:","title":"Mark an attack as a false positive"},{"location":"user-guides/events/false-attack/#remove-a-false-positive-mark","text":"To remove a false positive mark from the hit or attack, please send a request to Wallarm technical support . Also, you can undo a false positive mark in the dialog box in the Wallarm Console within a few seconds after the mark was applied.","title":"Remove a false positive mark"},{"location":"user-guides/events/verify-attack/","text":"Verifying Attacks \u00b6 Wallarm automatically rechecks attacks. You can check the attack verification status and force an attack recheck on the Events tab. Check the Attack Verification Status \u00b6 Click the Events tab. Check the status in the \"Verification\" column. Attack Verification Status Legend \u00b6 Verified : The attack has been verified. Error : An attempt to verify an attack type that does not support verification. Forced : The attack has a raised priority in the verification queue. Scheduled : The attack is queued for verification. Could not connect to the server : It is not possible to access the server at this time. Forcing an Attack Verification \u00b6 Select an attack. Click the status sign in the \"Verification\" column. Click Force verification . Wallarm will raise the priority of the attack verification in the queue. Attack Types that Do Not Support Verification \u00b6 Attacks of the following types do not support verification: Brute-force . Forced browsing . Attacks with a request processing limit. Attacks for which the vulnerabilities have already been closed. Attacks that do not contain enough data for verification.","title":"Verify attack"},{"location":"user-guides/events/verify-attack/#verifying-attacks","text":"Wallarm automatically rechecks attacks. You can check the attack verification status and force an attack recheck on the Events tab.","title":"Verifying Attacks"},{"location":"user-guides/events/verify-attack/#check-the-attack-verification-status","text":"Click the Events tab. Check the status in the \"Verification\" column.","title":"Check the Attack Verification Status"},{"location":"user-guides/events/verify-attack/#attack-verification-status-legend","text":"Verified : The attack has been verified. Error : An attempt to verify an attack type that does not support verification. Forced : The attack has a raised priority in the verification queue. Scheduled : The attack is queued for verification. Could not connect to the server : It is not possible to access the server at this time.","title":"Attack Verification Status Legend"},{"location":"user-guides/events/verify-attack/#forcing-an-attack-verification","text":"Select an attack. Click the status sign in the \"Verification\" column. Click Force verification . Wallarm will raise the priority of the attack verification in the queue.","title":"Forcing an Attack Verification"},{"location":"user-guides/events/verify-attack/#attack-types-that-do-not-support-verification","text":"Attacks of the following types do not support verification: Brute-force . Forced browsing . Attacks with a request processing limit. Attacks for which the vulnerabilities have already been closed. Attacks that do not contain enough data for verification.","title":"Attack Types that Do Not Support Verification"},{"location":"user-guides/ip-lists/blacklist/","text":"IP addresses blacklist \u00b6 Blacklist is a list of IP addresses that are not allowed to access your applications. In any filtration mode , the WAF node blocks all requests originated from blacklisted IP addresses (if IPs are not duplicated in the whitelist ). In the Wallarm Console \u2192 IP lists \u2192 Blacklist , you can manage blocked IP addresses as follows: Add a single IP address or a subnet Add a group of IP addresses registered in a specific country, data center, network, etc. Customize the time and reason for storing the IP address in the list Delete IP address from the list Review the history of list changes Using the blacklist with the partner WAF node This document describes the IP blacklist configuration for the regular (client) WAF node 3.0. As for the partner WAF node, we recommend to skip updating modules up to 3.0 and keep using the IP blacklist page available in version 2.18 . Examples of IP blacklist usage \u00b6 Block IP addresses from which several consecutive attacks were originated. An attack may include several requests originated from one IP address and containing malicious payloads of different types. One of the methods to block such attacks is to block requests origin. You can configure automatic source IP blocking by configuring the threshold for source IP blocking and appropriate reaction in the trigger . Block behavioral-based attacks. The WAF node can block most harmful traffic request-by-request if a malicious payload is detected. However, for behavioral\u2011based attacks when every single request is legitimate (e.g. login attempts with username/password pairs) blocking by origin might be necessary. By default, automatic blocking of behavioral attacks source is disabled. Instructions on configuring brute force protection \u2192 Adding an object to the list \u00b6 To add an IP address, subnet, or group of IP addresses to the list: Click the Add object button. Specify an IP address or group of IP addresses in one of the following ways: Input a single IP address or a subnet Select a country (geolocation) to add all IP addresses registered in this country Select a source to add all IP addresses that belong to this source: Tor for IP addresses of the Tor network Proxy for IP addresses of public or web proxy servers VPN for IP addresses of virtual private networks AWS for IP addresses registered in Amazon AWS Azure for IP addresses registered in Microsoft Azure GCP for IP addresses registered in Google Cloud Platform Select the period for which an IP address or a group of IP addresses should be added to the list. The minimum value is 5 minutes, the maximum value is forever. Specify the reason for adding an IP address or a group of IP addresses to the list. Confirm adding an IP address or a group of IP addresses to the list. Analyzing objects added to the list \u00b6 The Wallarm Console displays the following data on each object added to the list: Object - IP address, subnet, country or IP source added to the list. Application - application to which access configuration of the object is applied. Since applying the object access configuration to specific applications is limited , this column always displays the value All . Source - source of a single IP address or subnet: Country (geolocation) where a single IP address or subnet is registered Data center where a single IP address or subnet is registered: AWS for Amazon, GCP for Google Cloud Platform, Azure for Microsoft Azure Tor for IP address of the Tor network Proxy for IP address of public or web proxy servers VPN for IP addresses of virtual private networks Reason - reason for adding an IP address or a group of IP addresses to the list. The reason is manually specified when adding objects to the list or automatically generated when IPs are added to the list by triggers . Adding date - date and time when an object was added to the list. Remove - time period after which an object will be deleted from the list. Filtering the list \u00b6 You can filter the objects in the list by: IP address or subnet specified in the search string Period for which you want to get a status of the list Country in which an IP address or a subnet is registered Source to which an IP address or a subnet belongs Changing the time that an object is on the list \u00b6 To change the time that an IP address is on the list: Select an object from the list. Click Change time period . Select a new date for removing an object from the list and confirm the action. Deleting an object from the list \u00b6 To delete an object from the list: Select one or several objects from the list. Click Delete . Re-adding deleted IP address After manually deleting the IP address added to the list by the trigger , the trigger will run again only after half of the previous time the IP address was in the list. For example: IP address was automatically added to the greylist for 1 hour because 4 different attack vectors were received from this IP address in 3 hours (as it is configured in the trigger ). User deleted this IP address from the greylist via the Wallarm Console. If 4 different attack vectors are sent from this IP address within 30 minutes, then this IP address will not be added to the greylist. Statistics on the blacklisted IP addresses \u00b6 Using the data of the WAF dashboard Blacklist section , you can analyze the statistics on blacklist changes and currently blocked objects.","title":"IP addresses blacklist"},{"location":"user-guides/ip-lists/blacklist/#ip-addresses-blacklist","text":"Blacklist is a list of IP addresses that are not allowed to access your applications. In any filtration mode , the WAF node blocks all requests originated from blacklisted IP addresses (if IPs are not duplicated in the whitelist ). In the Wallarm Console \u2192 IP lists \u2192 Blacklist , you can manage blocked IP addresses as follows: Add a single IP address or a subnet Add a group of IP addresses registered in a specific country, data center, network, etc. Customize the time and reason for storing the IP address in the list Delete IP address from the list Review the history of list changes Using the blacklist with the partner WAF node This document describes the IP blacklist configuration for the regular (client) WAF node 3.0. As for the partner WAF node, we recommend to skip updating modules up to 3.0 and keep using the IP blacklist page available in version 2.18 .","title":"IP addresses blacklist"},{"location":"user-guides/ip-lists/blacklist/#examples-of-ip-blacklist-usage","text":"Block IP addresses from which several consecutive attacks were originated. An attack may include several requests originated from one IP address and containing malicious payloads of different types. One of the methods to block such attacks is to block requests origin. You can configure automatic source IP blocking by configuring the threshold for source IP blocking and appropriate reaction in the trigger . Block behavioral-based attacks. The WAF node can block most harmful traffic request-by-request if a malicious payload is detected. However, for behavioral\u2011based attacks when every single request is legitimate (e.g. login attempts with username/password pairs) blocking by origin might be necessary. By default, automatic blocking of behavioral attacks source is disabled. Instructions on configuring brute force protection \u2192","title":"Examples of IP blacklist usage"},{"location":"user-guides/ip-lists/blacklist/#adding-an-object-to-the-list","text":"To add an IP address, subnet, or group of IP addresses to the list: Click the Add object button. Specify an IP address or group of IP addresses in one of the following ways: Input a single IP address or a subnet Select a country (geolocation) to add all IP addresses registered in this country Select a source to add all IP addresses that belong to this source: Tor for IP addresses of the Tor network Proxy for IP addresses of public or web proxy servers VPN for IP addresses of virtual private networks AWS for IP addresses registered in Amazon AWS Azure for IP addresses registered in Microsoft Azure GCP for IP addresses registered in Google Cloud Platform Select the period for which an IP address or a group of IP addresses should be added to the list. The minimum value is 5 minutes, the maximum value is forever. Specify the reason for adding an IP address or a group of IP addresses to the list. Confirm adding an IP address or a group of IP addresses to the list.","title":"Adding an object to the list"},{"location":"user-guides/ip-lists/blacklist/#analyzing-objects-added-to-the-list","text":"The Wallarm Console displays the following data on each object added to the list: Object - IP address, subnet, country or IP source added to the list. Application - application to which access configuration of the object is applied. Since applying the object access configuration to specific applications is limited , this column always displays the value All . Source - source of a single IP address or subnet: Country (geolocation) where a single IP address or subnet is registered Data center where a single IP address or subnet is registered: AWS for Amazon, GCP for Google Cloud Platform, Azure for Microsoft Azure Tor for IP address of the Tor network Proxy for IP address of public or web proxy servers VPN for IP addresses of virtual private networks Reason - reason for adding an IP address or a group of IP addresses to the list. The reason is manually specified when adding objects to the list or automatically generated when IPs are added to the list by triggers . Adding date - date and time when an object was added to the list. Remove - time period after which an object will be deleted from the list.","title":"Analyzing objects added to the list"},{"location":"user-guides/ip-lists/blacklist/#filtering-the-list","text":"You can filter the objects in the list by: IP address or subnet specified in the search string Period for which you want to get a status of the list Country in which an IP address or a subnet is registered Source to which an IP address or a subnet belongs","title":"Filtering the list"},{"location":"user-guides/ip-lists/blacklist/#changing-the-time-that-an-object-is-on-the-list","text":"To change the time that an IP address is on the list: Select an object from the list. Click Change time period . Select a new date for removing an object from the list and confirm the action.","title":"Changing the time that an object is on the list"},{"location":"user-guides/ip-lists/blacklist/#deleting-an-object-from-the-list","text":"To delete an object from the list: Select one or several objects from the list. Click Delete . Re-adding deleted IP address After manually deleting the IP address added to the list by the trigger , the trigger will run again only after half of the previous time the IP address was in the list. For example: IP address was automatically added to the greylist for 1 hour because 4 different attack vectors were received from this IP address in 3 hours (as it is configured in the trigger ). User deleted this IP address from the greylist via the Wallarm Console. If 4 different attack vectors are sent from this IP address within 30 minutes, then this IP address will not be added to the greylist.","title":"Deleting an object from the list"},{"location":"user-guides/ip-lists/blacklist/#statistics-on-the-blacklisted-ip-addresses","text":"Using the data of the WAF dashboard Blacklist section , you can analyze the statistics on blacklist changes and currently blocked objects.","title":"Statistics on the blacklisted IP addresses"},{"location":"user-guides/ip-lists/greylist/","text":"IP addresses greylist \u00b6 Greylist is a list of IP addresses that are allowed to access your applications only if requests originated from them do not contain signs of the following attacks: Input validation attacks Attacks of the vpatch type Attacks detected based on regular expressions The WAF node blocks requests with malicious payloads that originated from greylisted IP addresses only in the safe blocking mode . If there are no malicious payloads in requests, the WAF node forwards them to your applications. Behavior of the WAF node may differ if greylisted IP addresses are also whitelisted, more about list priorities . In the Wallarm Console \u2192 IP lists \u2192 Greylist , you can manage greylisted IP addresses as follows: Add a single IP address or a subnet Add a group of IP addresses registered in a specific country, data center, network, etc. Customize the time and reason for storing the IP address in the list Delete IP address from the list Review the history of list changes IP greylisting support IP greylisting is supported starting with the regular (client) WAF node of version 3.0. If you have already deployed the partner WAF node of version 2.18 or lower, we recommend to skip updating modules till WAF node 3.2 is released. In WAF node 3.2, IP lists will be fully supported by the partner WAF node. At present, the partner WAF node still supports only blacklist of IP addresses . If you have already deployed the regular (client) WAF node of version 2.18 or lower, before setting up IP lists, please update deployed modules and migrate current IP blacklists and whitelists to a new IP lists scheme . Examples of IP greylist usage \u00b6 Greylist IP addresses from which several consecutive attacks were originated. An attack may include several requests originated from one IP address and containing malicious payloads of different types. One of the methods to block most of the malicious requests and allow legitimate requests originated from this IP address is to greylist this IP. You can configure automatic source IP greylisting by configuring the threshold for source IP greylisting and appropriate reaction in the trigger . Source IP greylisting can significantly reduce the number of false positives . Greylist IP addresses, countries, data centers, networks (for example, Tor) that usually produce harmful traffic. The WAF node will allow legitimate requests produced by greylisted objects and block malicious requests. Adding an object to the list \u00b6 To add an IP address, subnet, or group of IP addresses to the list: Click the Add object button. Specify an IP address or group of IP addresses in one of the following ways: Input a single IP address or a subnet Select a country (geolocation) to add all IP addresses registered in this country Select a source to add all IP addresses that belong to this source: Tor for IP addresses of the Tor network Proxy for IP addresses of public or web proxy servers VPN for IP addresses of virtual private networks AWS for IP addresses registered in Amazon AWS Azure for IP addresses registered in Microsoft Azure GCP for IP addresses registered in Google Cloud Platform Select the period for which an IP address or a group of IP addresses should be added to the list. The minimum value is 5 minutes, the maximum value is forever. Specify the reason for adding an IP address or a group of IP addresses to the list. Confirm adding an IP address or a group of IP addresses to the list. Analyzing objects added to the list \u00b6 The Wallarm Console displays the following data on each object added to the list: Object - IP address, subnet, country or IP source added to the list. Application - application to which access configuration of the object is applied. Since applying the object access configuration to specific applications is limited , this column always displays the value All . Source - source of a single IP address or subnet: Country (geolocation) where a single IP address or subnet is registered Data center where a single IP address or subnet is registered: AWS for Amazon, GCP for Google Cloud Platform, Azure for Microsoft Azure Tor for IP address of the Tor network Proxy for IP address of public or web proxy servers VPN for IP addresses of virtual private networks Reason - reason for adding an IP address or a group of IP addresses to the list. The reason is manually specified when adding objects to the list or automatically generated when IPs are added to the list by triggers . Adding date - date and time when an object was added to the list. Remove - time period after which an object will be deleted from the list. Filtering the list \u00b6 You can filter the objects in the list by: IP address or subnet specified in the search string Period for which you want to get a status of the list Country in which an IP address or a subnet is registered Source to which an IP address or a subnet belongs Changing the time that an object is on the list \u00b6 To change the time that an IP address is on the list: Select an object from the list. Click Change time period . Select a new date for removing an object from the list and confirm the action. Deleting an object from the list \u00b6 To delete an object from the list: Select one or several objects from the list. Click Delete . Re-adding deleted IP address After manually deleting the IP address added to the list by the trigger , the trigger will run again only after half of the previous time the IP address was in the list. For example: IP address was automatically added to the greylist for 1 hour because 4 different attack vectors were received from this IP address in 3 hours (as it is configured in the trigger ). User deleted this IP address from the greylist via the Wallarm Console. If 4 different attack vectors are sent from this IP address within 30 minutes, then this IP address will not be added to the greylist.","title":"IP addresses greylist"},{"location":"user-guides/ip-lists/greylist/#ip-addresses-greylist","text":"Greylist is a list of IP addresses that are allowed to access your applications only if requests originated from them do not contain signs of the following attacks: Input validation attacks Attacks of the vpatch type Attacks detected based on regular expressions The WAF node blocks requests with malicious payloads that originated from greylisted IP addresses only in the safe blocking mode . If there are no malicious payloads in requests, the WAF node forwards them to your applications. Behavior of the WAF node may differ if greylisted IP addresses are also whitelisted, more about list priorities . In the Wallarm Console \u2192 IP lists \u2192 Greylist , you can manage greylisted IP addresses as follows: Add a single IP address or a subnet Add a group of IP addresses registered in a specific country, data center, network, etc. Customize the time and reason for storing the IP address in the list Delete IP address from the list Review the history of list changes IP greylisting support IP greylisting is supported starting with the regular (client) WAF node of version 3.0. If you have already deployed the partner WAF node of version 2.18 or lower, we recommend to skip updating modules till WAF node 3.2 is released. In WAF node 3.2, IP lists will be fully supported by the partner WAF node. At present, the partner WAF node still supports only blacklist of IP addresses . If you have already deployed the regular (client) WAF node of version 2.18 or lower, before setting up IP lists, please update deployed modules and migrate current IP blacklists and whitelists to a new IP lists scheme .","title":"IP addresses greylist"},{"location":"user-guides/ip-lists/greylist/#examples-of-ip-greylist-usage","text":"Greylist IP addresses from which several consecutive attacks were originated. An attack may include several requests originated from one IP address and containing malicious payloads of different types. One of the methods to block most of the malicious requests and allow legitimate requests originated from this IP address is to greylist this IP. You can configure automatic source IP greylisting by configuring the threshold for source IP greylisting and appropriate reaction in the trigger . Source IP greylisting can significantly reduce the number of false positives . Greylist IP addresses, countries, data centers, networks (for example, Tor) that usually produce harmful traffic. The WAF node will allow legitimate requests produced by greylisted objects and block malicious requests.","title":"Examples of IP greylist usage"},{"location":"user-guides/ip-lists/greylist/#adding-an-object-to-the-list","text":"To add an IP address, subnet, or group of IP addresses to the list: Click the Add object button. Specify an IP address or group of IP addresses in one of the following ways: Input a single IP address or a subnet Select a country (geolocation) to add all IP addresses registered in this country Select a source to add all IP addresses that belong to this source: Tor for IP addresses of the Tor network Proxy for IP addresses of public or web proxy servers VPN for IP addresses of virtual private networks AWS for IP addresses registered in Amazon AWS Azure for IP addresses registered in Microsoft Azure GCP for IP addresses registered in Google Cloud Platform Select the period for which an IP address or a group of IP addresses should be added to the list. The minimum value is 5 minutes, the maximum value is forever. Specify the reason for adding an IP address or a group of IP addresses to the list. Confirm adding an IP address or a group of IP addresses to the list.","title":"Adding an object to the list"},{"location":"user-guides/ip-lists/greylist/#analyzing-objects-added-to-the-list","text":"The Wallarm Console displays the following data on each object added to the list: Object - IP address, subnet, country or IP source added to the list. Application - application to which access configuration of the object is applied. Since applying the object access configuration to specific applications is limited , this column always displays the value All . Source - source of a single IP address or subnet: Country (geolocation) where a single IP address or subnet is registered Data center where a single IP address or subnet is registered: AWS for Amazon, GCP for Google Cloud Platform, Azure for Microsoft Azure Tor for IP address of the Tor network Proxy for IP address of public or web proxy servers VPN for IP addresses of virtual private networks Reason - reason for adding an IP address or a group of IP addresses to the list. The reason is manually specified when adding objects to the list or automatically generated when IPs are added to the list by triggers . Adding date - date and time when an object was added to the list. Remove - time period after which an object will be deleted from the list.","title":"Analyzing objects added to the list"},{"location":"user-guides/ip-lists/greylist/#filtering-the-list","text":"You can filter the objects in the list by: IP address or subnet specified in the search string Period for which you want to get a status of the list Country in which an IP address or a subnet is registered Source to which an IP address or a subnet belongs","title":"Filtering the list"},{"location":"user-guides/ip-lists/greylist/#changing-the-time-that-an-object-is-on-the-list","text":"To change the time that an IP address is on the list: Select an object from the list. Click Change time period . Select a new date for removing an object from the list and confirm the action.","title":"Changing the time that an object is on the list"},{"location":"user-guides/ip-lists/greylist/#deleting-an-object-from-the-list","text":"To delete an object from the list: Select one or several objects from the list. Click Delete . Re-adding deleted IP address After manually deleting the IP address added to the list by the trigger , the trigger will run again only after half of the previous time the IP address was in the list. For example: IP address was automatically added to the greylist for 1 hour because 4 different attack vectors were received from this IP address in 3 hours (as it is configured in the trigger ). User deleted this IP address from the greylist via the Wallarm Console. If 4 different attack vectors are sent from this IP address within 30 minutes, then this IP address will not be added to the greylist.","title":"Deleting an object from the list"},{"location":"user-guides/ip-lists/overview/","text":"Types and core logic of IP lists \u00b6 In the IP lists section of the Wallarm Console, you can control access to your applications by whitelisting, blacklisting, and greylisting IP addresses. Whitelist is a list of trusted IP addresses that are allowed to access your applications even if requests originated from them contain attack signs. Blacklist is a list of IP addresses that are not allowed to access your applications. WAF node blocks all requests originated from blacklisted IP addresses. Greylist is a list of IP addresses that are allowed to access your applications only if requests originated from them do not contain attack signs. IP lists support Controlling access to your applications by whitelisted, blacklisted and greylisted IP addresses is supported starting with the regular (client) WAF node of version 3.0. If you have already deployed the partner WAF node of version 2.18 or lower, we recommend to skip updating modules till WAF node 3.2 is released. In WAF node 3.2, IP lists will be fully supported by the partner WAF node. At present, the partner WAF node still supports only blacklist of IP addresses . If you have already deployed the regular (client) WAF node of version 2.18 or lower, before setting up IP lists, please update deployed modules and migrate current IP blacklists and whitelists to a new IP lists scheme . Algorithm of IP lists processing \u00b6 In any filtration mode , the WAF node inspects whether source IPs of incoming requests matches entries of IP lists as follows: Request filtering is disabled or performed in monitoring mode : If a source IP of an incoming request is added to the whitelist, the WAF node forwards an incoming request to your application. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is added to the blacklist, the WAF node blocks an incoming request. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is neither in the blacklist nor in the whitelist, the WAF node forwards an incoming request to your application event if it contains attack signs. Request filtering is performed in safe blocking mode : If a source IP of an incoming request is added to the whitelist, the WAF node forwards an incoming request to your application. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is added to the blacklist, the WAF node blocks an incoming request. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is added to the greylist and an incoming request contains attack signs, the WAF node blocks an incoming request. If an incoming request does not contain attack signs, the WAF node forwards it to your application. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is not in any of the lists, the WAF node forwards an incoming request to your application event if it contains attack signs. Request filtering is performed in blocking mode : If a source IP of an incoming request is added to the whitelist, the WAF node forwards an incoming request to your application. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is added to the blacklist, the WAF node blocks an incoming request. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is neither in the blacklist nor in the whitelist and an incoming request contains attack signs, the WAF node blocks it. If an incoming request does not contain attack signs, the WAF node forwards it to your application. WAF node analyzes IP lists starting with whitelists, continuing with blacklists, and ending with greylists. For example, if an IP address is added to both whitelist and blacklist, the WAF node considers this IP address as a trusted source and forwards all requests originated from it to your applications regardless of whether an incoming request contains attack signs. IP lists configuration \u00b6 To configure IP lists: IF WAF node is located behind a load balancer or CDN, please make sure to configure your WAF node to properly report end-user IP addresses: Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images, Docker node container, and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller Add request sources to IP lists: Whitelist Blacklist Greylist Using additional traffic filtering facilities Note that if you use additional facilities (software or hardware) to automatically filter and block traffic, it is recommended that you configure a whitelist with the IP addresses for the Wallarm Scanner . This will allow Wallarm components to seamlessly scan your resources for vulnerabilities. Scanner IP address registered in Wallarm EU Cloud Scanner IP address registered in Wallarm US Cloud Known caveats of IP lists configuration \u00b6 Applying access configuration of certain IP to a specific application is not supported. You can apply all IP lists either to all your applications (by default) or to a specific application. To apply IP lists to a specific application, you can move the parameter disable_acl off to an appropriate block of NGINX or Envoy configuration file. If the WAF has the trigger configured to automatically block an IP address (for example, trigger to add IP addresses to the blacklist ), the system will block the IP for all application instances in a Wallarm account. Similarly for other methods of changing any of the IP lists. If you have deployed the partner WAF node , IP lists will not be supported till WAF node 3.2 is released. At present, the partner WAF node still supports only the blacklist of IP addresses .","title":"Types and core logic of IP lists"},{"location":"user-guides/ip-lists/overview/#types-and-core-logic-of-ip-lists","text":"In the IP lists section of the Wallarm Console, you can control access to your applications by whitelisting, blacklisting, and greylisting IP addresses. Whitelist is a list of trusted IP addresses that are allowed to access your applications even if requests originated from them contain attack signs. Blacklist is a list of IP addresses that are not allowed to access your applications. WAF node blocks all requests originated from blacklisted IP addresses. Greylist is a list of IP addresses that are allowed to access your applications only if requests originated from them do not contain attack signs. IP lists support Controlling access to your applications by whitelisted, blacklisted and greylisted IP addresses is supported starting with the regular (client) WAF node of version 3.0. If you have already deployed the partner WAF node of version 2.18 or lower, we recommend to skip updating modules till WAF node 3.2 is released. In WAF node 3.2, IP lists will be fully supported by the partner WAF node. At present, the partner WAF node still supports only blacklist of IP addresses . If you have already deployed the regular (client) WAF node of version 2.18 or lower, before setting up IP lists, please update deployed modules and migrate current IP blacklists and whitelists to a new IP lists scheme .","title":"Types and core logic of IP lists"},{"location":"user-guides/ip-lists/overview/#algorithm-of-ip-lists-processing","text":"In any filtration mode , the WAF node inspects whether source IPs of incoming requests matches entries of IP lists as follows: Request filtering is disabled or performed in monitoring mode : If a source IP of an incoming request is added to the whitelist, the WAF node forwards an incoming request to your application. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is added to the blacklist, the WAF node blocks an incoming request. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is neither in the blacklist nor in the whitelist, the WAF node forwards an incoming request to your application event if it contains attack signs. Request filtering is performed in safe blocking mode : If a source IP of an incoming request is added to the whitelist, the WAF node forwards an incoming request to your application. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is added to the blacklist, the WAF node blocks an incoming request. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is added to the greylist and an incoming request contains attack signs, the WAF node blocks an incoming request. If an incoming request does not contain attack signs, the WAF node forwards it to your application. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is not in any of the lists, the WAF node forwards an incoming request to your application event if it contains attack signs. Request filtering is performed in blocking mode : If a source IP of an incoming request is added to the whitelist, the WAF node forwards an incoming request to your application. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is added to the blacklist, the WAF node blocks an incoming request. If an IP address is not in the list, the next step is performed. If a source IP of an incoming request is neither in the blacklist nor in the whitelist and an incoming request contains attack signs, the WAF node blocks it. If an incoming request does not contain attack signs, the WAF node forwards it to your application. WAF node analyzes IP lists starting with whitelists, continuing with blacklists, and ending with greylists. For example, if an IP address is added to both whitelist and blacklist, the WAF node considers this IP address as a trusted source and forwards all requests originated from it to your applications regardless of whether an incoming request contains attack signs.","title":"Algorithm of IP lists processing"},{"location":"user-guides/ip-lists/overview/#ip-lists-configuration","text":"To configure IP lists: IF WAF node is located behind a load balancer or CDN, please make sure to configure your WAF node to properly report end-user IP addresses: Instructions for NGINX-based WAF nodes (including AWS / GCP / Yandex.Cloud images, Docker node container, and Kubernetes sidecars) Instructions for the WAF nodes deployed as the Wallarm Kubernetes Ingress controller Add request sources to IP lists: Whitelist Blacklist Greylist Using additional traffic filtering facilities Note that if you use additional facilities (software or hardware) to automatically filter and block traffic, it is recommended that you configure a whitelist with the IP addresses for the Wallarm Scanner . This will allow Wallarm components to seamlessly scan your resources for vulnerabilities. Scanner IP address registered in Wallarm EU Cloud Scanner IP address registered in Wallarm US Cloud","title":"IP lists configuration"},{"location":"user-guides/ip-lists/overview/#known-caveats-of-ip-lists-configuration","text":"Applying access configuration of certain IP to a specific application is not supported. You can apply all IP lists either to all your applications (by default) or to a specific application. To apply IP lists to a specific application, you can move the parameter disable_acl off to an appropriate block of NGINX or Envoy configuration file. If the WAF has the trigger configured to automatically block an IP address (for example, trigger to add IP addresses to the blacklist ), the system will block the IP for all application instances in a Wallarm account. Similarly for other methods of changing any of the IP lists. If you have deployed the partner WAF node , IP lists will not be supported till WAF node 3.2 is released. At present, the partner WAF node still supports only the blacklist of IP addresses .","title":"Known caveats of IP lists configuration"},{"location":"user-guides/ip-lists/whitelist/","text":"IP addresses whitelist \u00b6 Whitelist is a list of trusted IP addresses that are allowed to access your applications even if requests originated from them contain attack signs. Since the whitelist has the highest priority among other lists, the WAF node in any filtration mode will not block requests originated from whitelisted IP addresses. In the Wallarm Console \u2192 IP lists \u2192 Whitelist , you can manage whitelisted IP addresses as follows: Add a single IP address or a subnet Add a group of IP addresses registered in a specific country, data center, network, etc. Customize the time and reason for storing the IP address in the list Delete IP address from the list Review the history of list changes IP whitelisting support IP whitelisting is supported starting with the regular (client) WAF node of version 3.0. If you have already deployed the partner WAF node of version 2.18 or lower, we recommend to skip updating modules till WAF node 3.2 is released. In WAF node 3.2, IP lists will be fully supported by the partner WAF node. At present, the partner WAF node still supports only blacklist of IP addresses . If you have already deployed the regular (client) WAF node of version 2.18 or lower, before setting up IP lists, please update deployed modules and migrate current IP blacklists and whitelists to a new IP lists scheme . Examples of IP whitelist usage \u00b6 To search for vulnerabilities in the system, you can use Wallarm Vulnerability Scanner . The Scanner sends malicious requests to your application addresses and analyzes application responses. If Scanner IP addresses are not whitelisted, the WAF node can block requests sent by Scanner. To allow Wallarm components to seamlessly scan your resources for vulnerabilities, it is necessary to whitelist Scanner IP addresses. Starting with WAF node 3.0, Wallarm automatically whitelists Scanner IP addresses. If you use other trusted tools that originate potentially malicious requests, it is necessary to manually add source IPs of these tools to the whitelist. Adding an object to the list \u00b6 To add an IP address, subnet, or group of IP addresses to the list: Click the Add object button. Specify an IP address or group of IP addresses in one of the following ways: Input a single IP address or a subnet Select a country (geolocation) to add all IP addresses registered in this country Select a source to add all IP addresses that belong to this source: Tor for IP addresses of the Tor network Proxy for IP addresses of public or web proxy servers VPN for IP addresses of virtual private networks AWS for IP addresses registered in Amazon AWS Azure for IP addresses registered in Microsoft Azure GCP for IP addresses registered in Google Cloud Platform Select the period for which an IP address or a group of IP addresses should be added to the list. The minimum value is 5 minutes, the maximum value is forever. Specify the reason for adding an IP address or a group of IP addresses to the list. Confirm adding an IP address or a group of IP addresses to the list. Analyzing objects added to the list \u00b6 The Wallarm Console displays the following data on each object added to the list: Object - IP address, subnet, country or IP source added to the list. Application - application to which access configuration of the object is applied. Since applying the object access configuration to specific applications is limited , this column always displays the value All . Source - source of a single IP address or subnet: Country (geolocation) where a single IP address or subnet is registered Data center where a single IP address or subnet is registered: AWS for Amazon, GCP for Google Cloud Platform, Azure for Microsoft Azure Tor for IP address of the Tor network Proxy for IP address of public or web proxy servers VPN for IP addresses of virtual private networks Reason - reason for adding an IP address or a group of IP addresses to the list. The reason is manually specified when adding objects to the list or automatically generated when IPs are added to the list by triggers . Adding date - date and time when an object was added to the list. Remove - time period after which an object will be deleted from the list. Filtering the list \u00b6 You can filter the objects in the list by: IP address or subnet specified in the search string Period for which you want to get a status of the list Country in which an IP address or a subnet is registered Source to which an IP address or a subnet belongs Changing the time that an object is on the list \u00b6 To change the time that an IP address is on the list: Select an object from the list. Click Change time period . Select a new date for removing an object from the list and confirm the action. Deleting an object from the list \u00b6 To delete an object from the list: Select one or several objects from the list. Click Delete .","title":"IP addresses whitelist"},{"location":"user-guides/ip-lists/whitelist/#ip-addresses-whitelist","text":"Whitelist is a list of trusted IP addresses that are allowed to access your applications even if requests originated from them contain attack signs. Since the whitelist has the highest priority among other lists, the WAF node in any filtration mode will not block requests originated from whitelisted IP addresses. In the Wallarm Console \u2192 IP lists \u2192 Whitelist , you can manage whitelisted IP addresses as follows: Add a single IP address or a subnet Add a group of IP addresses registered in a specific country, data center, network, etc. Customize the time and reason for storing the IP address in the list Delete IP address from the list Review the history of list changes IP whitelisting support IP whitelisting is supported starting with the regular (client) WAF node of version 3.0. If you have already deployed the partner WAF node of version 2.18 or lower, we recommend to skip updating modules till WAF node 3.2 is released. In WAF node 3.2, IP lists will be fully supported by the partner WAF node. At present, the partner WAF node still supports only blacklist of IP addresses . If you have already deployed the regular (client) WAF node of version 2.18 or lower, before setting up IP lists, please update deployed modules and migrate current IP blacklists and whitelists to a new IP lists scheme .","title":"IP addresses whitelist"},{"location":"user-guides/ip-lists/whitelist/#examples-of-ip-whitelist-usage","text":"To search for vulnerabilities in the system, you can use Wallarm Vulnerability Scanner . The Scanner sends malicious requests to your application addresses and analyzes application responses. If Scanner IP addresses are not whitelisted, the WAF node can block requests sent by Scanner. To allow Wallarm components to seamlessly scan your resources for vulnerabilities, it is necessary to whitelist Scanner IP addresses. Starting with WAF node 3.0, Wallarm automatically whitelists Scanner IP addresses. If you use other trusted tools that originate potentially malicious requests, it is necessary to manually add source IPs of these tools to the whitelist.","title":"Examples of IP whitelist usage"},{"location":"user-guides/ip-lists/whitelist/#adding-an-object-to-the-list","text":"To add an IP address, subnet, or group of IP addresses to the list: Click the Add object button. Specify an IP address or group of IP addresses in one of the following ways: Input a single IP address or a subnet Select a country (geolocation) to add all IP addresses registered in this country Select a source to add all IP addresses that belong to this source: Tor for IP addresses of the Tor network Proxy for IP addresses of public or web proxy servers VPN for IP addresses of virtual private networks AWS for IP addresses registered in Amazon AWS Azure for IP addresses registered in Microsoft Azure GCP for IP addresses registered in Google Cloud Platform Select the period for which an IP address or a group of IP addresses should be added to the list. The minimum value is 5 minutes, the maximum value is forever. Specify the reason for adding an IP address or a group of IP addresses to the list. Confirm adding an IP address or a group of IP addresses to the list.","title":"Adding an object to the list"},{"location":"user-guides/ip-lists/whitelist/#analyzing-objects-added-to-the-list","text":"The Wallarm Console displays the following data on each object added to the list: Object - IP address, subnet, country or IP source added to the list. Application - application to which access configuration of the object is applied. Since applying the object access configuration to specific applications is limited , this column always displays the value All . Source - source of a single IP address or subnet: Country (geolocation) where a single IP address or subnet is registered Data center where a single IP address or subnet is registered: AWS for Amazon, GCP for Google Cloud Platform, Azure for Microsoft Azure Tor for IP address of the Tor network Proxy for IP address of public or web proxy servers VPN for IP addresses of virtual private networks Reason - reason for adding an IP address or a group of IP addresses to the list. The reason is manually specified when adding objects to the list or automatically generated when IPs are added to the list by triggers . Adding date - date and time when an object was added to the list. Remove - time period after which an object will be deleted from the list.","title":"Analyzing objects added to the list"},{"location":"user-guides/ip-lists/whitelist/#filtering-the-list","text":"You can filter the objects in the list by: IP address or subnet specified in the search string Period for which you want to get a status of the list Country in which an IP address or a subnet is registered Source to which an IP address or a subnet belongs","title":"Filtering the list"},{"location":"user-guides/ip-lists/whitelist/#changing-the-time-that-an-object-is-on-the-list","text":"To change the time that an IP address is on the list: Select an object from the list. Click Change time period . Select a new date for removing an object from the list and confirm the action.","title":"Changing the time that an object is on the list"},{"location":"user-guides/ip-lists/whitelist/#deleting-an-object-from-the-list","text":"To delete an object from the list: Select one or several objects from the list. Click Delete .","title":"Deleting an object from the list"},{"location":"user-guides/nodes/cloud-node/","text":"Cloud WAF nodes \u00b6 The cloud WAF node is used in cloud\u2011based deployments on Amazon AWS, Google Cloud Platform, and in Kubernetes Ingress controller deployments. Creating WAF node \u00b6 You can create a cloud node while setting up integration with the platform or using the following instructions: Open Wallarm UI \u2192 Nodes . Click Create node and select Cloud node . Enter node name and click Create WAF node . Copy the token of the created cloud node. You can also copy the token from the node card. To complete the node installation, follow the instructions for your respective platfrom: Amazon AWS Google Cloud Platfrom Yandex.Cloud NGINX Ingress controller Viewing details of WAF node \u00b6 Details of installed the WAF node are displayed in the table and card of each WAF node. To open the card, click the appropriate table record. The following node properties and metrics are available: Node name that was given to the node upon creation The average number of requests per second (RPS) Node IP address Unique node identifier (UUID) Token of the cloud WAF node Time of the last synchronization of the WAF node and Wallarm cloud Date of the WAF node creation Number of requests processed by the node in the current month If one cloud WAF node is installed for multiple instances, then the corresponding number of WAF nodes is grouped into one record in the table. Properties and metrics will be available for each instance. Regenerating the token for cloud WAF node \u00b6 Token regeneration creates a new token for the node. Open Wallarm UI \u2192 Nodes . Click Regenerate token in the cloud node menu or card. If the cloud node is already installed in your infrastructure, copy the new token value and specify it within the installed node settings. Deleting WAF node \u00b6 When the WAF node is deleted, filtration of requests to your application will be stopped. Deleting the WAF node cannot be undone. The WAF node will be deleted from the list of nodes permanently. Open Wallarm UI \u2192 Nodes . Select one or more WAF nodes and click Delete . You can also delete the WAF node by selecting a button off the node menu or node card. Confirm the action.","title":"Cloud node"},{"location":"user-guides/nodes/cloud-node/#cloud-waf-nodes","text":"The cloud WAF node is used in cloud\u2011based deployments on Amazon AWS, Google Cloud Platform, and in Kubernetes Ingress controller deployments.","title":"Cloud WAF nodes"},{"location":"user-guides/nodes/cloud-node/#creating-waf-node","text":"You can create a cloud node while setting up integration with the platform or using the following instructions: Open Wallarm UI \u2192 Nodes . Click Create node and select Cloud node . Enter node name and click Create WAF node . Copy the token of the created cloud node. You can also copy the token from the node card. To complete the node installation, follow the instructions for your respective platfrom: Amazon AWS Google Cloud Platfrom Yandex.Cloud NGINX Ingress controller","title":"Creating WAF node"},{"location":"user-guides/nodes/cloud-node/#viewing-details-of-waf-node","text":"Details of installed the WAF node are displayed in the table and card of each WAF node. To open the card, click the appropriate table record. The following node properties and metrics are available: Node name that was given to the node upon creation The average number of requests per second (RPS) Node IP address Unique node identifier (UUID) Token of the cloud WAF node Time of the last synchronization of the WAF node and Wallarm cloud Date of the WAF node creation Number of requests processed by the node in the current month If one cloud WAF node is installed for multiple instances, then the corresponding number of WAF nodes is grouped into one record in the table. Properties and metrics will be available for each instance.","title":"Viewing details of WAF node"},{"location":"user-guides/nodes/cloud-node/#regenerating-the-token-for-cloud-waf-node","text":"Token regeneration creates a new token for the node. Open Wallarm UI \u2192 Nodes . Click Regenerate token in the cloud node menu or card. If the cloud node is already installed in your infrastructure, copy the new token value and specify it within the installed node settings.","title":"Regenerating the token for cloud WAF node"},{"location":"user-guides/nodes/cloud-node/#deleting-waf-node","text":"When the WAF node is deleted, filtration of requests to your application will be stopped. Deleting the WAF node cannot be undone. The WAF node will be deleted from the list of nodes permanently. Open Wallarm UI \u2192 Nodes . Select one or more WAF nodes and click Delete . You can also delete the WAF node by selecting a button off the node menu or node card. Confirm the action.","title":"Deleting WAF node"},{"location":"user-guides/nodes/nodes/","text":"WAF nodes overview \u00b6 The Nodes section of Wallarm UI allows you to manage WAF nodes: View properties and metrics of installed WAF nodes Regenerate the token for cloud WAF nodes Delete WAF nodes Create new WAF nodes Administrator access The creating, deleting, and regenerating of WAF nodes/tokens is only available to users with the Administrator role. Viewing the details of installed WAF nodes is available to all users. WAF node types \u00b6 The WAF node type depends on the platform: Regular node is used in Linux\u2011based, Kubernetes sidecar, and Docker\u2011based deployments. Cloud node is used in cloud\u2011based deployments on Amazon AWS, Google Cloud Platform, and in Kubernetes Ingress controller deployments. Detailed information regarding working with different WAF node types can be found in the instructions linked above. Filtering WAF nodes \u00b6 To filter displayed WAF nodes, you can enter the name, UUID, token, or IP address of the node in the search field or use the tabs: All with active and inactive regular and cloud nodes. Regular with active and inactive regular nodes . Cloud with active and inactive cloud nodes . Inactive with inactive regular and cloud nodes. WAF node is inactive if it is installed for the inactive instance.","title":"Nodes"},{"location":"user-guides/nodes/nodes/#waf-nodes-overview","text":"The Nodes section of Wallarm UI allows you to manage WAF nodes: View properties and metrics of installed WAF nodes Regenerate the token for cloud WAF nodes Delete WAF nodes Create new WAF nodes Administrator access The creating, deleting, and regenerating of WAF nodes/tokens is only available to users with the Administrator role. Viewing the details of installed WAF nodes is available to all users.","title":"WAF nodes overview"},{"location":"user-guides/nodes/nodes/#waf-node-types","text":"The WAF node type depends on the platform: Regular node is used in Linux\u2011based, Kubernetes sidecar, and Docker\u2011based deployments. Cloud node is used in cloud\u2011based deployments on Amazon AWS, Google Cloud Platform, and in Kubernetes Ingress controller deployments. Detailed information regarding working with different WAF node types can be found in the instructions linked above.","title":"WAF node types"},{"location":"user-guides/nodes/nodes/#filtering-waf-nodes","text":"To filter displayed WAF nodes, you can enter the name, UUID, token, or IP address of the node in the search field or use the tabs: All with active and inactive regular and cloud nodes. Regular with active and inactive regular nodes . Cloud with active and inactive cloud nodes . Inactive with inactive regular and cloud nodes. WAF node is inactive if it is installed for the inactive instance.","title":"Filtering WAF nodes"},{"location":"user-guides/nodes/regular-node/","text":"Regular WAF nodes \u00b6 The regular WAF node is used in Linux\u2011based, Kubernetes sidecar and Docker\u2011based deployments. Creating WAF node \u00b6 The regular WAF node is created while setting up integration with the platform: NGINX NGINX Plus Docker Kubernetes sidecar container Kong If the integration is successfully finished, then the created WAF node will be displayed in the list of nodes in Wallarm UI. Viewing details of WAF node \u00b6 The details of the installed WAF node are displayed in the table and card of each WAF node. To open the card, click the appropriate table record. The following node properties and metrics are available: Node name that was given to the node upon creation The average number of requests per second (RPS) Node IP address Unique node identifier (UUID) Time of the last synchronization of the WAF node and Wallarm cloud Date of the WAF node creation Number of requests processed by the node in the current month Versions of used LOM and proton.db Versions of installed Wallarm WAF packages, NGINX, and Envoy (if any) Indicator of available component updates Deleting WAF node \u00b6 When the WAF node is deleted, filtration of requests to your application will be stopped. The deleting of the WAF node cannot be undone. The WAF node will be deleted from the list of nodes permanently. Open Wallarm UI \u2192 Nodes . Select one or more WAF nodes and click Delete . You can also delete the WAF node by selecting a button off the node menu or node card. Confirm the action.","title":"Regular node"},{"location":"user-guides/nodes/regular-node/#regular-waf-nodes","text":"The regular WAF node is used in Linux\u2011based, Kubernetes sidecar and Docker\u2011based deployments.","title":"Regular WAF nodes"},{"location":"user-guides/nodes/regular-node/#creating-waf-node","text":"The regular WAF node is created while setting up integration with the platform: NGINX NGINX Plus Docker Kubernetes sidecar container Kong If the integration is successfully finished, then the created WAF node will be displayed in the list of nodes in Wallarm UI.","title":"Creating WAF node"},{"location":"user-guides/nodes/regular-node/#viewing-details-of-waf-node","text":"The details of the installed WAF node are displayed in the table and card of each WAF node. To open the card, click the appropriate table record. The following node properties and metrics are available: Node name that was given to the node upon creation The average number of requests per second (RPS) Node IP address Unique node identifier (UUID) Time of the last synchronization of the WAF node and Wallarm cloud Date of the WAF node creation Number of requests processed by the node in the current month Versions of used LOM and proton.db Versions of installed Wallarm WAF packages, NGINX, and Envoy (if any) Indicator of available component updates","title":"Viewing details of WAF node"},{"location":"user-guides/nodes/regular-node/#deleting-waf-node","text":"When the WAF node is deleted, filtration of requests to your application will be stopped. The deleting of the WAF node cannot be undone. The WAF node will be deleted from the list of nodes permanently. Open Wallarm UI \u2192 Nodes . Select one or more WAF nodes and click Delete . You can also delete the WAF node by selecting a button off the node menu or node card. Confirm the action.","title":"Deleting WAF node"},{"location":"user-guides/rules/add-rule/","text":"Adding Rules in the Application Profile \u00b6 To add a new rule, go to the Profile & Rules tab. Rules can be added to both existing and new branches. They can be created from scratch or based on one of the existing branches. To add a rule to an existing branch, click Add rule (after hovering the mouse cursor over the branch description line, the button will appear in the pop-up menu on the right). You can also perform this operation on the rule page of this branch. If necessary, it is possible to modify the branch to which a rule will be added. For this, click on the If request is clause in the rule-adding form and make changes to the branch description conditions. If a new branch is created, it will appear on the screen, and the application structure view will be updated. Branch Description \u00b6 A branch description consists of a set of conditions for various parameters that an HTTP request must fulfill; otherwise, the rules associated with this branch will not be applied. Each line in the If request is section of the rule-adding form refers to a separate condition comprised of three fields: point, type, and comparison argument. The rules described in the branch are only applied to the request if all the conditions are fulfilled. Points \u00b6 The point field indicates which parameter value should be extracted from the request for comparison. At present, not all of the points that can be analyzed by the filter node, are supported. The following points are currently supported: application : application ID proto : HTTP protocol version (1.0, 1.1, 2.0, ...) scheme : http or https uri : part of the request URL without the domain (for example, /blogs/123/index.php?q=aaa for the request sent to http://example.com/blogs/123/index.php?q=aaa ) path , action_name , action_ext : URL elements (recommended format for URL describing). More details are provided in the request analysis description get : GET parameters in the request header : request headers method : request methods Condition types \u00b6 EQUAL \u00b6 The point value must match precisely with the comparison argument. For example, only example matches with the point value example . EQUAL condition type for the HOST header value To cover more requests with the rules, we have restricted the EQUAL condition type for the HOST header. Instead of the EQUAL type, we recommend using the type IEQUAL that allows parameter values in any register. If you have previously used the EQUAL type, it will be automatically replaced with the IEQUAL type. IEQUAL \u00b6 The point value must match with the comparison argument in any case. For example: example , ExAmple , exampLe match with the point value example . REGEX \u00b6 The point value must match the regular expression. Regular expression syntax To match requests with regular expressions, the Pire library is used. Mostly, the syntax of expressions is standard but has some specifics as described below and in the README file of Pire repository . Show regular expression syntax Characters that can be used as\u2011is: Lowercase Latin letters: a b c d e f g h i j k l m n o p q r s t u v w x y z Capital Latin letters: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z Digits: 0 1 3 4 5 6 7 8 9 Special characters: ! \" # % ' , - / : ; < = > @ ] _ ` } Whitespaces Characters that must be placed into square brackets [] instead of escaping with \\ : . $ ^ { [ ( | ) * + ? \\ & ~ Characters that must be converted to ASCII according to ISO\u20118859: UTF\u20118 characters (for example, the character \u0283 converted to ASCII is \u00ca\u0083 ) Character groups: . for any character except a newline () for grouping regular expressions, searching symbols present inside () or establishing a precedence order [] for a single character present inside [] (case sensitive); the group can be used for the specific cases: to ignore case (for example, [cC] ) [a-z] to match one of lowercase Latin letters [A-Z] to match one of capital Latin letters [0-9] to match one of digits [a-zA-Z0-9[.]] to match one of lowercase, or capital Latin letters, or digits, or dot Logic characters: ~ is equal to NOT. The inverted expression and the character must be placed into () , for example: (~(a)) | is equal to OR & is equal to AND Characters to specify string boundaries: ^ for the start of the string $ for the end of the string Quantifiers: * for 0 or more repetitions of the preceding regular expression + for 1 or more repetitions of the preceding regular expression ? for 0 or 1 repetitions of the preceding regular expression {m} for m repetitions of the preceding regular expression {m,n} for m to n repetitions of the preceding regular expression; omitting n specifies an infinite upper bound Character combinations that work with specifics: ^.*$ is equal to ^.+$ (empty values does not match with ^.*$ ) ^.?$ , ^.{0,}$ , ^.{0,n}$ are equal to ^.+$ Temporarily not supported: Character classes like \\W for non-alphabetics, \\w for alphabetics, \\D for any non-digits, \\d for any decimals, \\S for non-whitespaces, \\s for whitespaces Not supported syntax: Three-digit octal codes \\NNN , \\oNNN , \\ONNN \\cN passing control characters via \\c (for example, \\cC for CTRL+C) \\A for the start of the string \\z for the end of the string \\b before or after the whitespace character in the end of the string ?? , *? , +? lazy quantifiers Conditionals Testing regular expressions To test the regular expression, you can use the cpire utility on supported Debian or Ubuntu: Add Wallarm repository: Debian 9.x (stretch) sudo apt update sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 10.x (buster) sudo apt update sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) sudo apt update curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 20.04 LTS (focal) sudo apt update curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node focal/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Install the cpire utility: sudo apt install libcpire-utils Run the cpire utility: cpire-runner -r '<YOUR_REGULAR_EXPRESSION>' Enter the value to check whether it matches with the regular expression. The utility will return the result: 0 if the value matches with the regular expression FAIL if the value does not match with the regular expression Error message if the regular expression is invalid Specifics of handling the \\ character If the expression includes \\ , please escape it with [] and \\ (for example, [\\\\] ). Examples of regular expressions added via the Wallarm Console To match any string that includes /.git /[.]git To match any string that includes .example.com [.]example[.]com To match any string ending with /.example.*.com /[.]example[.].*[.]com$ To match all IP addresses excluding 1.2.3.4 and 5.6.7.8 ^(~((1[.]2[.]3[.]4)|(5[.]6[.]7[.]8)))$ To match any string ending with /.example.com.php /[.]example[.]com[.]php$ To match any string that includes sqlmap with letters in lower and upper case: sqLmAp , SqLMap , etc [sS][qQ][lL][mM][aA][pP] To match any string that includes one or several values: admin\\.exe , admin\\.bat , admin\\.sh , cmd\\.exe , cmd\\.bat , cmd\\.sh (admin|cmd)[\\].(exe|bat|sh) To match any string that includes one or several values: onmouse with letters in lower and upper case, onload with letters in lower and upper case, win\\.ini , prompt [oO][nN][mM][oO][uU][sS][eE]|[oO][nN][lL][oO][aA][dD]|win[\\].ini|prompt To match any string that starts with Mozilla but does not contain the string 1aa875F49III ^(Mozilla(~(.*1aa875F49III.*)))$ To match any string with one of the values: python-requests/ , PostmanRuntime/ , okhttp/3.14.0 , node-fetch/1.0 ^(python-requests/|PostmanRuntime/|okhttp/3.14.0|node-fetch/1.0) ABSENT \u00b6 The request should not contain the designated point. In this case, the comparison argument is not used. Rule \u00b6 The added request processing rule is described in the Then section. The following rules are supported: Set the filtration mode Mask sensitive data Tag requests as a brute-force / forced browsing attack Rewrite attack before active verification Apply a virtual patch User-defined detection rules","title":"Adding Rules in the Application Profile"},{"location":"user-guides/rules/add-rule/#adding-rules-in-the-application-profile","text":"To add a new rule, go to the Profile & Rules tab. Rules can be added to both existing and new branches. They can be created from scratch or based on one of the existing branches. To add a rule to an existing branch, click Add rule (after hovering the mouse cursor over the branch description line, the button will appear in the pop-up menu on the right). You can also perform this operation on the rule page of this branch. If necessary, it is possible to modify the branch to which a rule will be added. For this, click on the If request is clause in the rule-adding form and make changes to the branch description conditions. If a new branch is created, it will appear on the screen, and the application structure view will be updated.","title":"Adding Rules in the Application Profile"},{"location":"user-guides/rules/add-rule/#branch-description","text":"A branch description consists of a set of conditions for various parameters that an HTTP request must fulfill; otherwise, the rules associated with this branch will not be applied. Each line in the If request is section of the rule-adding form refers to a separate condition comprised of three fields: point, type, and comparison argument. The rules described in the branch are only applied to the request if all the conditions are fulfilled.","title":"Branch Description"},{"location":"user-guides/rules/add-rule/#points","text":"The point field indicates which parameter value should be extracted from the request for comparison. At present, not all of the points that can be analyzed by the filter node, are supported. The following points are currently supported: application : application ID proto : HTTP protocol version (1.0, 1.1, 2.0, ...) scheme : http or https uri : part of the request URL without the domain (for example, /blogs/123/index.php?q=aaa for the request sent to http://example.com/blogs/123/index.php?q=aaa ) path , action_name , action_ext : URL elements (recommended format for URL describing). More details are provided in the request analysis description get : GET parameters in the request header : request headers method : request methods","title":"Points"},{"location":"user-guides/rules/add-rule/#condition-types","text":"","title":"Condition types"},{"location":"user-guides/rules/add-rule/#equal","text":"The point value must match precisely with the comparison argument. For example, only example matches with the point value example . EQUAL condition type for the HOST header value To cover more requests with the rules, we have restricted the EQUAL condition type for the HOST header. Instead of the EQUAL type, we recommend using the type IEQUAL that allows parameter values in any register. If you have previously used the EQUAL type, it will be automatically replaced with the IEQUAL type.","title":"EQUAL"},{"location":"user-guides/rules/add-rule/#iequal","text":"The point value must match with the comparison argument in any case. For example: example , ExAmple , exampLe match with the point value example .","title":"IEQUAL"},{"location":"user-guides/rules/add-rule/#regex","text":"The point value must match the regular expression. Regular expression syntax To match requests with regular expressions, the Pire library is used. Mostly, the syntax of expressions is standard but has some specifics as described below and in the README file of Pire repository . Show regular expression syntax Characters that can be used as\u2011is: Lowercase Latin letters: a b c d e f g h i j k l m n o p q r s t u v w x y z Capital Latin letters: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z Digits: 0 1 3 4 5 6 7 8 9 Special characters: ! \" # % ' , - / : ; < = > @ ] _ ` } Whitespaces Characters that must be placed into square brackets [] instead of escaping with \\ : . $ ^ { [ ( | ) * + ? \\ & ~ Characters that must be converted to ASCII according to ISO\u20118859: UTF\u20118 characters (for example, the character \u0283 converted to ASCII is \u00ca\u0083 ) Character groups: . for any character except a newline () for grouping regular expressions, searching symbols present inside () or establishing a precedence order [] for a single character present inside [] (case sensitive); the group can be used for the specific cases: to ignore case (for example, [cC] ) [a-z] to match one of lowercase Latin letters [A-Z] to match one of capital Latin letters [0-9] to match one of digits [a-zA-Z0-9[.]] to match one of lowercase, or capital Latin letters, or digits, or dot Logic characters: ~ is equal to NOT. The inverted expression and the character must be placed into () , for example: (~(a)) | is equal to OR & is equal to AND Characters to specify string boundaries: ^ for the start of the string $ for the end of the string Quantifiers: * for 0 or more repetitions of the preceding regular expression + for 1 or more repetitions of the preceding regular expression ? for 0 or 1 repetitions of the preceding regular expression {m} for m repetitions of the preceding regular expression {m,n} for m to n repetitions of the preceding regular expression; omitting n specifies an infinite upper bound Character combinations that work with specifics: ^.*$ is equal to ^.+$ (empty values does not match with ^.*$ ) ^.?$ , ^.{0,}$ , ^.{0,n}$ are equal to ^.+$ Temporarily not supported: Character classes like \\W for non-alphabetics, \\w for alphabetics, \\D for any non-digits, \\d for any decimals, \\S for non-whitespaces, \\s for whitespaces Not supported syntax: Three-digit octal codes \\NNN , \\oNNN , \\ONNN \\cN passing control characters via \\c (for example, \\cC for CTRL+C) \\A for the start of the string \\z for the end of the string \\b before or after the whitespace character in the end of the string ?? , *? , +? lazy quantifiers Conditionals Testing regular expressions To test the regular expression, you can use the cpire utility on supported Debian or Ubuntu: Add Wallarm repository: Debian 9.x (stretch) sudo apt update sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 10.x (buster) sudo apt update sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) sudo apt update curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 20.04 LTS (focal) sudo apt update curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node focal/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Install the cpire utility: sudo apt install libcpire-utils Run the cpire utility: cpire-runner -r '<YOUR_REGULAR_EXPRESSION>' Enter the value to check whether it matches with the regular expression. The utility will return the result: 0 if the value matches with the regular expression FAIL if the value does not match with the regular expression Error message if the regular expression is invalid Specifics of handling the \\ character If the expression includes \\ , please escape it with [] and \\ (for example, [\\\\] ). Examples of regular expressions added via the Wallarm Console To match any string that includes /.git /[.]git To match any string that includes .example.com [.]example[.]com To match any string ending with /.example.*.com /[.]example[.].*[.]com$ To match all IP addresses excluding 1.2.3.4 and 5.6.7.8 ^(~((1[.]2[.]3[.]4)|(5[.]6[.]7[.]8)))$ To match any string ending with /.example.com.php /[.]example[.]com[.]php$ To match any string that includes sqlmap with letters in lower and upper case: sqLmAp , SqLMap , etc [sS][qQ][lL][mM][aA][pP] To match any string that includes one or several values: admin\\.exe , admin\\.bat , admin\\.sh , cmd\\.exe , cmd\\.bat , cmd\\.sh (admin|cmd)[\\].(exe|bat|sh) To match any string that includes one or several values: onmouse with letters in lower and upper case, onload with letters in lower and upper case, win\\.ini , prompt [oO][nN][mM][oO][uU][sS][eE]|[oO][nN][lL][oO][aA][dD]|win[\\].ini|prompt To match any string that starts with Mozilla but does not contain the string 1aa875F49III ^(Mozilla(~(.*1aa875F49III.*)))$ To match any string with one of the values: python-requests/ , PostmanRuntime/ , okhttp/3.14.0 , node-fetch/1.0 ^(python-requests/|PostmanRuntime/|okhttp/3.14.0|node-fetch/1.0)","title":"REGEX"},{"location":"user-guides/rules/add-rule/#absent","text":"The request should not contain the designated point. In this case, the comparison argument is not used.","title":"ABSENT"},{"location":"user-guides/rules/add-rule/#rule","text":"The added request processing rule is described in the Then section. The following rules are supported: Set the filtration mode Mask sensitive data Tag requests as a brute-force / forced browsing attack Rewrite attack before active verification Apply a virtual patch User-defined detection rules","title":"Rule"},{"location":"user-guides/rules/change-request-for-active-verification/","text":"Rewriting the request before attack replaying \u00b6 Rule overview \u00b6 The rule Rewrite attack before active verification is used to modify the original request elements before the attack replaying . The following elements can be modified: Header with the request authentication data to replace original authentication data with test data . Header HOST if your infrastructure has the load balancer forwarding the requests to different application instances depending on the HOST header value. For example, the header HOST could be modified to replay the attack on staging or test environment . Path to rewrite the application address used for the attack replaying . Modification of any original request element The rule Rewrite attack before active verification allows modifying of only headers ( header ) and paths ( uri ) of the original requests. Other request elements cannot be modified or added. Since the rule allows modifying of only those request elements that were originally passed, the application IP address cannot be modified. Replacing original authentication data with test data \u00b6 If authentication parameters were passed in the original request, the module Attack rechecker deletes these parameters and replays the attack without them. If authentication parameters are required to access protected application API, the code 401 or other code will be returned in the response to the replayed attack. Since returned code shows no vulnerability signs, the module Attack rechecker will not detect the vulnerability that could be actually detected with authentication parameters passed in the request. To replay the original requests with required authentication parameters, you may add test values \u200b\u200bfor these parameters using the Rewrite attack before active verification rule. For example: API key, token, password or other parameters. Reusing test authentication data It is recommended to generate test authentication credentials that will only be used by the Wallarm module Attack rechecker . Modifying the application address for attack replaying \u00b6 By default, replayed attacks are sent to the application address and path passed in the original request. You may replace the original address and path with other values that will be used when replaying the attack. Values are replaced using the Rewrite attack before active verification rule in the following way: If your infrastructure has the load balancer forwarding the requests to different application instances depending on the HOST header value, you may replace the original value of the header HOST with a different application instance address. For example, a separate application instance could be a staging or test environment. Specifics of replacing the HOST value After the HOST header value is replaced with a new value, requests will still be sent to the original application IP address. The request will be forwarded to the address specified in the HOST header if the appropriate configuration is implemented on the load balancer for the original IP address. Replace the path of the original request with the path to the test environment or staging, or to the path to the target server to bypass the proxy server when replaying the attack. To replace both the value of the HOST header and the path of the original request, you'll need to create two separate rules with the action type Rewrite attack before active verification . Creating and applying the rule \u00b6 To create and apply the rule: Create the rule Rewrite attack before active verification in the Profile & Rules section of the Wallarm Console. The rule consists of the following components: Condition describes the request that should be modified before attack replaying. Rules sets the new value for the parameter selected in the Part of request field. A set value will be used when replaying the attack. The value must be decoded and set using the template language Liquid as follows: placed in double curly braces {{}} and single quotes '' . For example: {{'example.com'}} . To replace the path of the original request ( uri ), you should pass the full value of the new path. Part of request points to the original request element that should be modified before replaying the attack. Possible values of the field Part of request Possible values of the Part of request field are header (request header) and uri (request path). Wait for the rule compilation to complete . To set several conditions for the original request modification or to replace the values of several request elements, you may create several rules. Rule examples \u00b6 When replaying the attacks sent to example.com , pass the value PHPSESSID=mntdtbgt87j3auaq60iori2i63; security=low in the COOKIE header. The format of the header value is {{'PHPSESSID=mntdtbgt87j3auaq60iori2i63; security=low'}} . Replay attacks originally sent to example.com on the test environment example-test.env.srv.loc . The load balancer on example.com must be configured to forward requests to the address passed in HOST . The format of the address is {{'example-test.env.srv.loc'}} .","title":"Rewriting the request before attack replaying"},{"location":"user-guides/rules/change-request-for-active-verification/#rewriting-the-request-before-attack-replaying","text":"","title":"Rewriting the request before attack replaying"},{"location":"user-guides/rules/change-request-for-active-verification/#rule-overview","text":"The rule Rewrite attack before active verification is used to modify the original request elements before the attack replaying . The following elements can be modified: Header with the request authentication data to replace original authentication data with test data . Header HOST if your infrastructure has the load balancer forwarding the requests to different application instances depending on the HOST header value. For example, the header HOST could be modified to replay the attack on staging or test environment . Path to rewrite the application address used for the attack replaying . Modification of any original request element The rule Rewrite attack before active verification allows modifying of only headers ( header ) and paths ( uri ) of the original requests. Other request elements cannot be modified or added. Since the rule allows modifying of only those request elements that were originally passed, the application IP address cannot be modified.","title":"Rule overview"},{"location":"user-guides/rules/change-request-for-active-verification/#replacing-original-authentication-data-with-test-data","text":"If authentication parameters were passed in the original request, the module Attack rechecker deletes these parameters and replays the attack without them. If authentication parameters are required to access protected application API, the code 401 or other code will be returned in the response to the replayed attack. Since returned code shows no vulnerability signs, the module Attack rechecker will not detect the vulnerability that could be actually detected with authentication parameters passed in the request. To replay the original requests with required authentication parameters, you may add test values \u200b\u200bfor these parameters using the Rewrite attack before active verification rule. For example: API key, token, password or other parameters. Reusing test authentication data It is recommended to generate test authentication credentials that will only be used by the Wallarm module Attack rechecker .","title":"Replacing original authentication data with test data"},{"location":"user-guides/rules/change-request-for-active-verification/#modifying-the-application-address-for-attack-replaying","text":"By default, replayed attacks are sent to the application address and path passed in the original request. You may replace the original address and path with other values that will be used when replaying the attack. Values are replaced using the Rewrite attack before active verification rule in the following way: If your infrastructure has the load balancer forwarding the requests to different application instances depending on the HOST header value, you may replace the original value of the header HOST with a different application instance address. For example, a separate application instance could be a staging or test environment. Specifics of replacing the HOST value After the HOST header value is replaced with a new value, requests will still be sent to the original application IP address. The request will be forwarded to the address specified in the HOST header if the appropriate configuration is implemented on the load balancer for the original IP address. Replace the path of the original request with the path to the test environment or staging, or to the path to the target server to bypass the proxy server when replaying the attack. To replace both the value of the HOST header and the path of the original request, you'll need to create two separate rules with the action type Rewrite attack before active verification .","title":"Modifying the application address for attack replaying"},{"location":"user-guides/rules/change-request-for-active-verification/#creating-and-applying-the-rule","text":"To create and apply the rule: Create the rule Rewrite attack before active verification in the Profile & Rules section of the Wallarm Console. The rule consists of the following components: Condition describes the request that should be modified before attack replaying. Rules sets the new value for the parameter selected in the Part of request field. A set value will be used when replaying the attack. The value must be decoded and set using the template language Liquid as follows: placed in double curly braces {{}} and single quotes '' . For example: {{'example.com'}} . To replace the path of the original request ( uri ), you should pass the full value of the new path. Part of request points to the original request element that should be modified before replaying the attack. Possible values of the field Part of request Possible values of the Part of request field are header (request header) and uri (request path). Wait for the rule compilation to complete . To set several conditions for the original request modification or to replace the values of several request elements, you may create several rules.","title":"Creating and applying the rule"},{"location":"user-guides/rules/change-request-for-active-verification/#rule-examples","text":"When replaying the attacks sent to example.com , pass the value PHPSESSID=mntdtbgt87j3auaq60iori2i63; security=low in the COOKIE header. The format of the header value is {{'PHPSESSID=mntdtbgt87j3auaq60iori2i63; security=low'}} . Replay attacks originally sent to example.com on the test environment example-test.env.srv.loc . The load balancer on example.com must be configured to forward requests to the address passed in HOST . The format of the address is {{'example-test.env.srv.loc'}} .","title":"Rule examples"},{"location":"user-guides/rules/compiling/","text":"Building and unloading of a custom ruleset \u00b6 A custom ruleset defines specifics of processing particular client traffic (for example, allows setting up custom attack detection rules or masking sensitive data). The WAF node relies on the custom ruleset during incoming requests analysis. Changes of custom rules do NOT take effect instantly. Changes are applied to the request analysis process only after the custom ruleset building and unloading to the WAF node are finished. Custom ruleset building \u00b6 Adding a new rule, deleting or changing existing rules in the Wallarm Console \u2192 Proile&Rules launch a custom ruleset build. During the building process, rules are optimized and compiled into a format adopted for the WAF node. The process of building a custom ruleset typically takes from a few seconds for a small number of rules to up to an hour for complex rule trees. Custom ruleset build status and expected completion time are displayed in the Wallarm Console. If there is no build in progress, the interface displays the date of the last completed build. Unloading a custom ruleset to the WAF node \u00b6 Custom ruleset build is unloaded to the WAF node during the WAF node and Wallarm Cloud synchronization. By default, synchronization of the WAF node and Wallarm Cloud is launched every 2\u20114 minutes. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 The status of unloading a custom ruleset to the WAF node is logged to the file /var/log/wallarm/syncnode.log .","title":"Building and unloading of a custom ruleset"},{"location":"user-guides/rules/compiling/#building-and-unloading-of-a-custom-ruleset","text":"A custom ruleset defines specifics of processing particular client traffic (for example, allows setting up custom attack detection rules or masking sensitive data). The WAF node relies on the custom ruleset during incoming requests analysis. Changes of custom rules do NOT take effect instantly. Changes are applied to the request analysis process only after the custom ruleset building and unloading to the WAF node are finished.","title":"Building and unloading of a custom ruleset"},{"location":"user-guides/rules/compiling/#custom-ruleset-building","text":"Adding a new rule, deleting or changing existing rules in the Wallarm Console \u2192 Proile&Rules launch a custom ruleset build. During the building process, rules are optimized and compiled into a format adopted for the WAF node. The process of building a custom ruleset typically takes from a few seconds for a small number of rules to up to an hour for complex rule trees. Custom ruleset build status and expected completion time are displayed in the Wallarm Console. If there is no build in progress, the interface displays the date of the last completed build.","title":"Custom ruleset building"},{"location":"user-guides/rules/compiling/#unloading-a-custom-ruleset-to-the-waf-node","text":"Custom ruleset build is unloaded to the WAF node during the WAF node and Wallarm Cloud synchronization. By default, synchronization of the WAF node and Wallarm Cloud is launched every 2\u20114 minutes. More details on the WAF node and Wallarm Cloud synchronization configuration \u2192 The status of unloading a custom ruleset to the WAF node is logged to the file /var/log/wallarm/syncnode.log .","title":"Unloading a custom ruleset to the WAF node"},{"location":"user-guides/rules/define-counters/","text":"Rules defining attack counters \u00b6 Rules overview \u00b6 Rules Tag requests as a forced browsing attack and Tag requests as a brute-force attack are used to add tags to specific requests. The postanalytics module uses these tags to detect dirbust (forced browsing) and brute\u2011force attacks respectively. Applying the rule to real traffic To apply the rule to real traffic, you need to set a threshold to trigger the rule: Number of 404 responses for the rule Tag requests as a forced browsing attack Number of requests for the rule Tag requests as a brute-force attack Thresholds are configured via triggers. Examples of triggers are available at this link . Creating and applying the rule \u00b6 To create and apply the rule: Create the rule Tag requests as a forced browsing attack or Tag requests as a brute-force attack in the Profile & Rules section of the Wallarm Console. The rule consists of the following components: Condition describes the request to add the brute\u2011force or forced browsing tags to. Counter name defines the name of the tag which will be added to the request. The name should correspond to the following format: d:<name> for the rule Tag requests as a forced browsing attack b:<name> for the rule Tag requests as a brute-force attack Message about inherited counter If you have a Default rule defining attack counter, you can get a message Inherited counter: <name> when creating a rule with defined conditions that will trigger this rule. Default rule is applied to all incoming requests as it does not have defined conditions that trigger the rule. The message is for informational purposes only. Operation of the rule with defined trigger conditions will not be affected as it has a higher priority than the Default rule . Number of tags created for one condition Only one tag of any type ( b: or d: ) can be created for one condition. One tag cannot be reused with several conditions. Create a trigger with the filter by the tag and a threshold for request blocking. Examples of triggers are available at this link . Rule examples \u00b6 Add a forced browsing attack tag d:api_fr_user_passwords to requests sent to the path api/frontend/user/passwords of the protected resource Add a brute-force attack tag b:api_fr_user_login to requests sent to the path api/frontend/user/login of the protected resource","title":"Rules defining attack counters"},{"location":"user-guides/rules/define-counters/#rules-defining-attack-counters","text":"","title":"Rules defining attack counters"},{"location":"user-guides/rules/define-counters/#rules-overview","text":"Rules Tag requests as a forced browsing attack and Tag requests as a brute-force attack are used to add tags to specific requests. The postanalytics module uses these tags to detect dirbust (forced browsing) and brute\u2011force attacks respectively. Applying the rule to real traffic To apply the rule to real traffic, you need to set a threshold to trigger the rule: Number of 404 responses for the rule Tag requests as a forced browsing attack Number of requests for the rule Tag requests as a brute-force attack Thresholds are configured via triggers. Examples of triggers are available at this link .","title":"Rules overview"},{"location":"user-guides/rules/define-counters/#creating-and-applying-the-rule","text":"To create and apply the rule: Create the rule Tag requests as a forced browsing attack or Tag requests as a brute-force attack in the Profile & Rules section of the Wallarm Console. The rule consists of the following components: Condition describes the request to add the brute\u2011force or forced browsing tags to. Counter name defines the name of the tag which will be added to the request. The name should correspond to the following format: d:<name> for the rule Tag requests as a forced browsing attack b:<name> for the rule Tag requests as a brute-force attack Message about inherited counter If you have a Default rule defining attack counter, you can get a message Inherited counter: <name> when creating a rule with defined conditions that will trigger this rule. Default rule is applied to all incoming requests as it does not have defined conditions that trigger the rule. The message is for informational purposes only. Operation of the rule with defined trigger conditions will not be affected as it has a higher priority than the Default rule . Number of tags created for one condition Only one tag of any type ( b: or d: ) can be created for one condition. One tag cannot be reused with several conditions. Create a trigger with the filter by the tag and a threshold for request blocking. Examples of triggers are available at this link .","title":"Creating and applying the rule"},{"location":"user-guides/rules/define-counters/#rule-examples","text":"Add a forced browsing attack tag d:api_fr_user_passwords to requests sent to the path api/frontend/user/passwords of the protected resource Add a brute-force attack tag b:api_fr_user_login to requests sent to the path api/frontend/user/login of the protected resource","title":"Rule examples"},{"location":"user-guides/rules/intro/","text":"Application Profile Rules \u00b6 On the Profile & Rules tab you may review and change the rules for handling requests enabled for the current application profile. The application profile is a collection of known information about protected applications. It is used to fine-tune the behavior of the system during the analysis of requests and their further processing in the post-analysis module as well as in the cloud. For a better understanding of how the traffic processing rules are applied, it is advisable to learn how the filter node analyzes the requests . One important thing about making changes to the rules is that these changes don't take effect immediately. It may take some time to compile the rules and download them into filter nodes. Terminology \u00b6 Point \u00b6 Each parameter of the HTTP request in the Wallarm system is described with a sequence of filters applied for request processing, e.g., headers, body, URL, Base64, etc. This sequence is called the point . Request processing filters are also called parsers. Rule Branch \u00b6 The set of HTTP request parameters and their conditions is called the branch . If the conditions are fulfilled, the rules related to this branch will be applied. For example, the rule branch example.com/**/*.* describes the conditions matching all requests to any URL of the domain example.com . Endpoint (Endpoint Branch) \u00b6 A branch without nested rule branches is called an endpoint branch . Ideally, an application endpoint corresponds to one business function of the protected application. For instance, such business function as authorization can be an endpoint rule branch of example.com/login.php . Rule \u00b6 A request processing setting for the filter node, the post-analysis module, or the cloud is called a rule . Processing rules are linked to the branches or endpoints. A rule is applied to a request only if the request matches all the conditions described in the branch.","title":"Application Profile Rules"},{"location":"user-guides/rules/intro/#application-profile-rules","text":"On the Profile & Rules tab you may review and change the rules for handling requests enabled for the current application profile. The application profile is a collection of known information about protected applications. It is used to fine-tune the behavior of the system during the analysis of requests and their further processing in the post-analysis module as well as in the cloud. For a better understanding of how the traffic processing rules are applied, it is advisable to learn how the filter node analyzes the requests . One important thing about making changes to the rules is that these changes don't take effect immediately. It may take some time to compile the rules and download them into filter nodes.","title":"Application Profile Rules"},{"location":"user-guides/rules/intro/#terminology","text":"","title":"Terminology"},{"location":"user-guides/rules/intro/#point","text":"Each parameter of the HTTP request in the Wallarm system is described with a sequence of filters applied for request processing, e.g., headers, body, URL, Base64, etc. This sequence is called the point . Request processing filters are also called parsers.","title":"Point"},{"location":"user-guides/rules/intro/#rule-branch","text":"The set of HTTP request parameters and their conditions is called the branch . If the conditions are fulfilled, the rules related to this branch will be applied. For example, the rule branch example.com/**/*.* describes the conditions matching all requests to any URL of the domain example.com .","title":"Rule Branch"},{"location":"user-guides/rules/intro/#endpoint-endpoint-branch","text":"A branch without nested rule branches is called an endpoint branch . Ideally, an application endpoint corresponds to one business function of the protected application. For instance, such business function as authorization can be an endpoint rule branch of example.com/login.php .","title":"Endpoint (Endpoint Branch)"},{"location":"user-guides/rules/intro/#rule","text":"A request processing setting for the filter node, the post-analysis module, or the cloud is called a rule . Processing rules are linked to the branches or endpoints. A rule is applied to a request only if the request matches all the conditions described in the branch.","title":"Rule"},{"location":"user-guides/rules/regex-rule/","text":"User-Defined Detection Rules \u00b6 In some cases, it may prove useful to add a signature for attack detection manually or to create a so-called virtual patch . As such, Wallarm does not use regular expressions to detect attacks, but it does allow users to add additional signatures based on regular expressions. Adding a New Detection Rule \u00b6 To do this, you need to create the rule Define a request as an attack based on a regular expression and fill in the fields: Regex : regular expression (signature). If the value of the following parameter matches the expression, that request is detected as an attack. Syntax and specifics of regular expressions are described in the instructions on adding rules . Attack : the type of attack that will be detected when the parameter value in the request matches the regular expression. Experimental : this flag allows you to safely check the triggering of a regular expression without blocking requests. The requests won't be blocked even when the filter node is set to the blocking mode. These requests will be considered as attacks detected by the experimental method. They can be accessed using search query experimental attacks . in this part of request : determines a point in the request, where the system should detect the corresponding attacks. Example: Blocking All Headers with an Incorrect X-Authentication Header \u00b6 If the following conditions take place: the application is accessible at the domain example.com the application uses the X-Authentication header for user authentication the header format is 32 hex symbols Then , to create a rule for rejecting incorrect format tokens: Go to the Rules tab Find the branch for example.com/**/*.* and click Add rule Select Define as an attack on the basis of a regular expression Set Regex value as [^0-9a-f]|^.{33,}$|^.{0,31}$ Choose Virtual patch as the type of Attack Set the point Header X-AUTHENTICATION Click Create Partial Disabling of a New Detection Rule \u00b6 If the created rule should be partially disabled for a particular branch, this can easily be done by creating the rule Disable attack detection by the regular expressions with the following fields: Regex ID : identifiers of the previously created regular expressions that must be ignored. in this part of request : indicates the parameter that requires setting up an exception. Getting an ID of a Regular Expression \u00b6 Identifier is generated automatically when you add a new regular expression rule. To get an ID of a regular expression, proceed to the following steps: In the Rules tab, welect the branch which the desired regular expression was set for. Select the group of rules which contains the desired regular expression. Click the desired regular expression entry and copy a regular expression ID. Example: Permit an Incorrect X-Authentication Header for a Designated URL. \u00b6 Let's say you have a script at example.com/test.php , and you want to change the format of the tokens for it. To create the relevant rule: Go to the Rules tab Find or create the branch for example.com/test.php and click Add rule Choose Disable attack detection by the regular expressions Enter the ID of the rule that you want to disable into the Regex ID field Set the point Header X-AUTHENTICATION Click Create","title":"User\u2011Defined Detection Rules"},{"location":"user-guides/rules/regex-rule/#user-defined-detection-rules","text":"In some cases, it may prove useful to add a signature for attack detection manually or to create a so-called virtual patch . As such, Wallarm does not use regular expressions to detect attacks, but it does allow users to add additional signatures based on regular expressions.","title":"User-Defined Detection Rules"},{"location":"user-guides/rules/regex-rule/#adding-a-new-detection-rule","text":"To do this, you need to create the rule Define a request as an attack based on a regular expression and fill in the fields: Regex : regular expression (signature). If the value of the following parameter matches the expression, that request is detected as an attack. Syntax and specifics of regular expressions are described in the instructions on adding rules . Attack : the type of attack that will be detected when the parameter value in the request matches the regular expression. Experimental : this flag allows you to safely check the triggering of a regular expression without blocking requests. The requests won't be blocked even when the filter node is set to the blocking mode. These requests will be considered as attacks detected by the experimental method. They can be accessed using search query experimental attacks . in this part of request : determines a point in the request, where the system should detect the corresponding attacks.","title":"Adding a New Detection Rule"},{"location":"user-guides/rules/regex-rule/#example-blocking-all-headers-with-an-incorrect-x-authentication-header","text":"If the following conditions take place: the application is accessible at the domain example.com the application uses the X-Authentication header for user authentication the header format is 32 hex symbols Then , to create a rule for rejecting incorrect format tokens: Go to the Rules tab Find the branch for example.com/**/*.* and click Add rule Select Define as an attack on the basis of a regular expression Set Regex value as [^0-9a-f]|^.{33,}$|^.{0,31}$ Choose Virtual patch as the type of Attack Set the point Header X-AUTHENTICATION Click Create","title":"Example: Blocking All Headers with an Incorrect X-Authentication Header"},{"location":"user-guides/rules/regex-rule/#partial-disabling-of-a-new-detection-rule","text":"If the created rule should be partially disabled for a particular branch, this can easily be done by creating the rule Disable attack detection by the regular expressions with the following fields: Regex ID : identifiers of the previously created regular expressions that must be ignored. in this part of request : indicates the parameter that requires setting up an exception.","title":"Partial Disabling of a New Detection Rule"},{"location":"user-guides/rules/regex-rule/#getting-an-id-of-a-regular-expression","text":"Identifier is generated automatically when you add a new regular expression rule. To get an ID of a regular expression, proceed to the following steps: In the Rules tab, welect the branch which the desired regular expression was set for. Select the group of rules which contains the desired regular expression. Click the desired regular expression entry and copy a regular expression ID.","title":"Getting an ID of a Regular Expression"},{"location":"user-guides/rules/regex-rule/#example-permit-an-incorrect-x-authentication-header-for-a-designated-url","text":"Let's say you have a script at example.com/test.php , and you want to change the format of the tokens for it. To create the relevant rule: Go to the Rules tab Find or create the branch for example.com/test.php and click Add rule Choose Disable attack detection by the regular expressions Enter the ID of the rule that you want to disable into the Regex ID field Set the point Header X-AUTHENTICATION Click Create","title":"Example: Permit an Incorrect X-Authentication Header for a Designated URL."},{"location":"user-guides/rules/request-processing/","text":"Analyzing and parsing requests \u00b6 Principles of analyzing and parsing requests \u00b6 For an effective request analysis, Wallarm WAF uses the principles: Work with the same data as the protected application. For example: If an application provides a JSON API, then the processed parameters will be also encoded in JSON format. To get parameter values, Wallarm WAF uses JSON parser. There are also more complex cases where the data is encoded several times \u2014 for example, JSON to Base64 to JSON. Such cases require decoding with several parsers. Consider the context of data processing. For example: The parameter name can be passed in creation requests both as the product name and as a username. However, the processing code for such requests can be different. To define the method of analyzing such parameters, Wallarm WAF may use the URL from which the requests were sent to or other parameters. Identifying and parsing the request parts \u00b6 Starting from the top level of the HTTP request, the WAF node attempts to sequentially apply each of the suitable parsers to each part. The list of applied parsers depends on the nature of the data and the results of the previous training of the system. The output from the parsers becomes an additional set of parameters that has to be analyzed in a similar way. Parser output sometimes becomes a complex structure like JSON, array, or associative array. Parser tags Each parser has an identifier (tag). For example, header for the parser of request headers. The set of tags used in the request analysis is displayed in Wallarm Console within the event details. This data demonstrates the request part with the detected attack and parsers that were used. For example, if an attack was detected in the SOAPACTION header: URL \u00b6 Each HTTP request contains an URL. To find attacks, the WAF node analyzes both the original value and its individual components: path , action_name , action_ext , get . The following tags correspond to the URL parser: uri for the original URL value without the domain (for example, /blogs/123/index.php?q=aaa for the request sent to http://example.com/blogs/123/index.php?q=aaa ). path for an array with URL parts separated by the / symbol (the last URL part is not included in the array). If there is only one part in the URL, the array will be empty. action_name for the last part of the URL after the / symbol and before the first period ( . ). This part of the URL is always present in the request, even if its value is an empty string. action_ext for the part of the URL after the last period ( . ). It may be missing in the request. get for GET parameters after the ? symbol. Example: /blogs/123/index.php?q=aaa [uri] \u2014 /blogs/123/index.php?q=aaa [path, 0] \u2014 blogs [path, 1] \u2014 123 [action_name] \u2014 index [action_ext] \u2014 php [get, 'q'] \u2014 aaa GET parameters \u00b6 GET parameters are passed to the application in the request URL after the character ? in the key=value format. The get tag corresponds to the parser. Request example GET parameters and values /?q=some+text&check=yes [get, 'q'] \u2014 some text [get, 'check'] \u2014 yes /?p1[x]=1&p1[y]=2&p2[]=aaa&p2[]=bbb [get, 'p1', hash, 'x'] \u2014 1 [get, 'p1', hash, 'y'] \u2014 2 [get, 'p2', array, 0] \u2014 aaa [get, 'p2', array, 1] \u2014 bbb /?p3=1&p3=2 [get, 'p3', array, 0] \u2014 1 [get, 'p3', array, 1] \u2014 2 [get, 'p3', pollution] \u2014 1,2 Headers \u00b6 Headers are presented in the HTTP request and some other formats (e.g., multipart ). The header tag corresponds to the parser. Header names are always converted to uppercase. Example: GET / HTTP/1.1 Host: example.com X-Test: aaa X-Test: bbb [header, 'HOST'] \u2014 example.com [header, 'X-TEST', array, 0] \u2014 aaa [header, 'X-TEST', array, 1] \u2014 aaa [header, 'X-TEST', pollution] \u2014 aaa,bbb Metadata \u00b6 The following tags correspond to the parser for HTTP request metadata: post for the HTTP request body method for the HTTP request method: GET , POST , PUT , DELETE proto for the HTTP protocol version scheme : http/https application for the application ID Additional parsers \u00b6 Complex request parts may require additional parsing (for example, if the data is Base64 encoded or presented in the array format). In such cases, the parsers listed below are applied to request parts additionally. base64 \u00b6 Decodes Base64 encoded data, and can be applied to any part of the request. gzip \u00b6 Decodes GZIP encoded data, and can be applied to any part of the request. htmljs \u00b6 Converts HTML and JS symbols to the text format, and can be applied to any part of the request. Example: &#x22;&#97;&#97;&#97;&#x22; will be converted to \"aaa\" . json_doc \u00b6 Parses the data in JSON format, and can be applied to any part of the request. Filters: json_array or array for the value of the array element json_obj or hash for the value of the associative array key ( key:value ) Example: {\"p1\":\"value\",\"p2\":[\"v1\",\"v2\"],\"p3\":{\"somekey\":\"somevalue\"}} [..., json_doc, hash, 'p1'] \u2014 value [..., json_doc, hash, 'p2', array, 0] \u2014 v1 [..., json_doc, hash, 'p2', array, 1] \u2014 v2 [..., json_doc, hash, 'p3', hash, 'somekey'] \u2014 somevalue xml \u00b6 Parses the data in XML format, and can be applied to any part of the request. Filters: xml_comment for an array with comments in the body of an XML document xml_dtd for the address of the external DTD schema being used xml_dtd_entity for an array defined in the Entity DTD document xml_pi for an array of instructions to process xml_tag or hash for an associative array of tags xml_tag_array or array for an array of tag values xml_attr for an associative array of attributes; can only be used after the xml_tag filter The XML parser does not differentiate between the contents of the tag and the first element in the array of values for the tag. That is, the parameters [..., xml, xml_tag, 't1'] and [..., xml, xml_tag, 't1', array, 0] are identical and interchangeable. Example: <?xml version=\"1.0\"?> <!DOCTYPE foo [<!ENTITY xxe SYSTEM \"aaaa\">]> <?xml-stylesheet type=\"text/xsl\" href=\"style.xsl\"?> <!-- test --> <methodCall> <methodName>&xxe;</methodName> <methodArgs check=\"true\">123</methodArgs> <methodArgs>234</methodArgs> </methodCall> [..., xml, xml_dtd_entity, 0] \u2014 name = xxe , value = aaaa [..., xml, xml_pi, 0] \u2014 name = xml-stylesheet , value = type=\"text/xsl\" href=\"style.xsl\" [..., xml, xml_comment, 0] \u2014 test [..., xml, xml_tag, 'methodCall', xml_tag, 'methodName'] \u2014 aaaa [..., xml, xml_tag, 'methodCall', xml_tag, 'methodArgs'] \u2014 123 [..., xml, xml_tag, 'methodCall', xml_tag, 'methodArgs', xml_attr, 'check'] \u2014 true [..., xml, xml_tag, 'methodCall', xml_tag, 'methodArgs', array, 1] \u2014 234 array \u00b6 Parses data array. Can be applied to any part of the request. Example: /?p1[x]=1&p1[y]=2&p2[]=aaa&p2[]=bbb [get, 'p2', array, 0] \u2014 aaa [get, 'p2', array, 1] \u2014 bbb hash \u00b6 Parses the associative data array ( key:value ), and can be applied to any part of the request. Example: /?p1[x]=1&p1[y]=2&p2[]=aaa&p2[]=bbb [get, 'p1', hash, 'x'] \u2014 1 [get, 'p1', hash, 'y'] \u2014 2 pollution \u00b6 Combines the values of the parameters with the same name, and can be applied to any part of the request in the initial or decoded format. Example: /?p3=1&p3=2 [get, 'p3', pollution] \u2014 1,2 percent \u00b6 Decodes the URL symbols, and can be applied only to the uri component of URL. cookie \u00b6 Parses the Cookie request parameters, and can be applied only to the request headers. Example: GET / HTTP/1.1 Cookie: a=1; b=2 [header, 'COOKIE', cookie, 'a'] = 1 ; [header, 'COOKIE', cookie, 'b'] = 2 . form_urlencoded \u00b6 Parses the request body passed in the application/x-www-form-urlencoded format, and can be applied only to the request body. Example: p1=1&p2[a]=2&p2[b]=3&p3[]=4&p3[]=5&p4=6&p4=7 [post, form_urlencoded, 'p1'] \u2014 1 [post, form_urlencoded, 'p2', hash, 'a'] \u2014 2 [post, form_urlencoded, 'p2', hash, 'b'] \u2014 3 [post, form_urlencoded, 'p3', array, 0] \u2014 4 [post, form_urlencoded, 'p3', array, 1] \u2014 5 [post, form_urlencoded, 'p4', array, 0] \u2014 6 [post, form_urlencoded, 'p4', array, 1] \u2014 7 [post, form_urlencoded, 'p4', pollution] \u2014 6,7 grpc \u00b6 Parses gRPC API requests, and can be applied only to the request body. Supports the protobuf filter for the Protocol Buffers data. multipart \u00b6 Parses the request body passed in the multipart format, and can be applied only to the request body. Supports the header filter for the headers in the request body. Example: p1=1&p2[a]=2&p2[b]=3&p3[]=4&p3[]=5&p4=6&p4=7 [post, multipart, 'p1'] \u2014 1 [post, multipart, 'p2', hash, 'a'] \u2014 2 [post, multipart, 'p2', hash, 'b'] \u2014 3 [post, multipart, 'p3', array, 0] \u2014 4 [post, multipart, 'p3', array, 1] \u2014 5 [post, multipart, 'p4', array, 0] \u2014 6 [post, multipart, 'p4', array, 1] \u2014 7 [post, multipart, 'p4', pollution] \u2014 6,7 If a file name is specified in the Content-Disposition header, then the file is considered to be loaded in this parameter. The parameter will look like this: [post, multipart, 'someparam', file] \u2014 file contents viewstate \u00b6 Designed to analyze the session state. The technology is used by Microsoft ASP.NET, and can be applied only to the request body. Filters: viewstate_array for an array viewstate_pair for an array viewstate_triplet for an array viewstate_dict for an associative array viewstate_dict_key for a string viewstate_dict_value for a string viewstate_sparse_array for an associative array Norms \u00b6 The norms are applied to parsers for array and key data types. Norms are used to define the boundaries of data analysis. The value of the norm is indicated in the parser tag. For example: hash_all , hash_default , hash_name . If the norm is not specified, then the identifier of the entity that requires processing is passed to the parser. For example: the name of the JSON object or other identifier is passed after hash . all \u00b6 Used to get values of all elements, parameters, or objects. For example: get_all for all GET parameter values header_all for all header values array_all for all array element values hash_all for all JSON object or XML attribute values default \u00b6 Used to get default values of elements, parameters, or objects. For example: get_default for default values of GET parameters header_default for default values of headers array_default for default values of array elements hash_default for default values of JSON objects or XML attributes name \u00b6 Used to get names of all elements, parameters, or objects. For example: get_name for all GET parameter names header_name for all header names hash_name for all JSON object or XML attribute names","title":"Analyzing and Parsing Requests"},{"location":"user-guides/rules/request-processing/#analyzing-and-parsing-requests","text":"","title":"Analyzing and parsing requests"},{"location":"user-guides/rules/request-processing/#principles-of-analyzing-and-parsing-requests","text":"For an effective request analysis, Wallarm WAF uses the principles: Work with the same data as the protected application. For example: If an application provides a JSON API, then the processed parameters will be also encoded in JSON format. To get parameter values, Wallarm WAF uses JSON parser. There are also more complex cases where the data is encoded several times \u2014 for example, JSON to Base64 to JSON. Such cases require decoding with several parsers. Consider the context of data processing. For example: The parameter name can be passed in creation requests both as the product name and as a username. However, the processing code for such requests can be different. To define the method of analyzing such parameters, Wallarm WAF may use the URL from which the requests were sent to or other parameters.","title":"Principles of analyzing and parsing requests"},{"location":"user-guides/rules/request-processing/#identifying-and-parsing-the-request-parts","text":"Starting from the top level of the HTTP request, the WAF node attempts to sequentially apply each of the suitable parsers to each part. The list of applied parsers depends on the nature of the data and the results of the previous training of the system. The output from the parsers becomes an additional set of parameters that has to be analyzed in a similar way. Parser output sometimes becomes a complex structure like JSON, array, or associative array. Parser tags Each parser has an identifier (tag). For example, header for the parser of request headers. The set of tags used in the request analysis is displayed in Wallarm Console within the event details. This data demonstrates the request part with the detected attack and parsers that were used. For example, if an attack was detected in the SOAPACTION header:","title":"Identifying and parsing the request parts"},{"location":"user-guides/rules/request-processing/#url","text":"Each HTTP request contains an URL. To find attacks, the WAF node analyzes both the original value and its individual components: path , action_name , action_ext , get . The following tags correspond to the URL parser: uri for the original URL value without the domain (for example, /blogs/123/index.php?q=aaa for the request sent to http://example.com/blogs/123/index.php?q=aaa ). path for an array with URL parts separated by the / symbol (the last URL part is not included in the array). If there is only one part in the URL, the array will be empty. action_name for the last part of the URL after the / symbol and before the first period ( . ). This part of the URL is always present in the request, even if its value is an empty string. action_ext for the part of the URL after the last period ( . ). It may be missing in the request. get for GET parameters after the ? symbol. Example: /blogs/123/index.php?q=aaa [uri] \u2014 /blogs/123/index.php?q=aaa [path, 0] \u2014 blogs [path, 1] \u2014 123 [action_name] \u2014 index [action_ext] \u2014 php [get, 'q'] \u2014 aaa","title":"URL"},{"location":"user-guides/rules/request-processing/#get-parameters","text":"GET parameters are passed to the application in the request URL after the character ? in the key=value format. The get tag corresponds to the parser. Request example GET parameters and values /?q=some+text&check=yes [get, 'q'] \u2014 some text [get, 'check'] \u2014 yes /?p1[x]=1&p1[y]=2&p2[]=aaa&p2[]=bbb [get, 'p1', hash, 'x'] \u2014 1 [get, 'p1', hash, 'y'] \u2014 2 [get, 'p2', array, 0] \u2014 aaa [get, 'p2', array, 1] \u2014 bbb /?p3=1&p3=2 [get, 'p3', array, 0] \u2014 1 [get, 'p3', array, 1] \u2014 2 [get, 'p3', pollution] \u2014 1,2","title":"GET parameters"},{"location":"user-guides/rules/request-processing/#headers","text":"Headers are presented in the HTTP request and some other formats (e.g., multipart ). The header tag corresponds to the parser. Header names are always converted to uppercase. Example: GET / HTTP/1.1 Host: example.com X-Test: aaa X-Test: bbb [header, 'HOST'] \u2014 example.com [header, 'X-TEST', array, 0] \u2014 aaa [header, 'X-TEST', array, 1] \u2014 aaa [header, 'X-TEST', pollution] \u2014 aaa,bbb","title":"Headers"},{"location":"user-guides/rules/request-processing/#metadata","text":"The following tags correspond to the parser for HTTP request metadata: post for the HTTP request body method for the HTTP request method: GET , POST , PUT , DELETE proto for the HTTP protocol version scheme : http/https application for the application ID","title":"Metadata"},{"location":"user-guides/rules/request-processing/#additional-parsers","text":"Complex request parts may require additional parsing (for example, if the data is Base64 encoded or presented in the array format). In such cases, the parsers listed below are applied to request parts additionally.","title":"Additional parsers"},{"location":"user-guides/rules/request-processing/#base64","text":"Decodes Base64 encoded data, and can be applied to any part of the request.","title":"base64"},{"location":"user-guides/rules/request-processing/#gzip","text":"Decodes GZIP encoded data, and can be applied to any part of the request.","title":"gzip"},{"location":"user-guides/rules/request-processing/#htmljs","text":"Converts HTML and JS symbols to the text format, and can be applied to any part of the request. Example: &#x22;&#97;&#97;&#97;&#x22; will be converted to \"aaa\" .","title":"htmljs"},{"location":"user-guides/rules/request-processing/#json_doc","text":"Parses the data in JSON format, and can be applied to any part of the request. Filters: json_array or array for the value of the array element json_obj or hash for the value of the associative array key ( key:value ) Example: {\"p1\":\"value\",\"p2\":[\"v1\",\"v2\"],\"p3\":{\"somekey\":\"somevalue\"}} [..., json_doc, hash, 'p1'] \u2014 value [..., json_doc, hash, 'p2', array, 0] \u2014 v1 [..., json_doc, hash, 'p2', array, 1] \u2014 v2 [..., json_doc, hash, 'p3', hash, 'somekey'] \u2014 somevalue","title":"json_doc"},{"location":"user-guides/rules/request-processing/#xml","text":"Parses the data in XML format, and can be applied to any part of the request. Filters: xml_comment for an array with comments in the body of an XML document xml_dtd for the address of the external DTD schema being used xml_dtd_entity for an array defined in the Entity DTD document xml_pi for an array of instructions to process xml_tag or hash for an associative array of tags xml_tag_array or array for an array of tag values xml_attr for an associative array of attributes; can only be used after the xml_tag filter The XML parser does not differentiate between the contents of the tag and the first element in the array of values for the tag. That is, the parameters [..., xml, xml_tag, 't1'] and [..., xml, xml_tag, 't1', array, 0] are identical and interchangeable. Example: <?xml version=\"1.0\"?> <!DOCTYPE foo [<!ENTITY xxe SYSTEM \"aaaa\">]> <?xml-stylesheet type=\"text/xsl\" href=\"style.xsl\"?> <!-- test --> <methodCall> <methodName>&xxe;</methodName> <methodArgs check=\"true\">123</methodArgs> <methodArgs>234</methodArgs> </methodCall> [..., xml, xml_dtd_entity, 0] \u2014 name = xxe , value = aaaa [..., xml, xml_pi, 0] \u2014 name = xml-stylesheet , value = type=\"text/xsl\" href=\"style.xsl\" [..., xml, xml_comment, 0] \u2014 test [..., xml, xml_tag, 'methodCall', xml_tag, 'methodName'] \u2014 aaaa [..., xml, xml_tag, 'methodCall', xml_tag, 'methodArgs'] \u2014 123 [..., xml, xml_tag, 'methodCall', xml_tag, 'methodArgs', xml_attr, 'check'] \u2014 true [..., xml, xml_tag, 'methodCall', xml_tag, 'methodArgs', array, 1] \u2014 234","title":"xml"},{"location":"user-guides/rules/request-processing/#array","text":"Parses data array. Can be applied to any part of the request. Example: /?p1[x]=1&p1[y]=2&p2[]=aaa&p2[]=bbb [get, 'p2', array, 0] \u2014 aaa [get, 'p2', array, 1] \u2014 bbb","title":"array"},{"location":"user-guides/rules/request-processing/#hash","text":"Parses the associative data array ( key:value ), and can be applied to any part of the request. Example: /?p1[x]=1&p1[y]=2&p2[]=aaa&p2[]=bbb [get, 'p1', hash, 'x'] \u2014 1 [get, 'p1', hash, 'y'] \u2014 2","title":"hash"},{"location":"user-guides/rules/request-processing/#pollution","text":"Combines the values of the parameters with the same name, and can be applied to any part of the request in the initial or decoded format. Example: /?p3=1&p3=2 [get, 'p3', pollution] \u2014 1,2","title":"pollution"},{"location":"user-guides/rules/request-processing/#percent","text":"Decodes the URL symbols, and can be applied only to the uri component of URL.","title":"percent"},{"location":"user-guides/rules/request-processing/#cookie","text":"Parses the Cookie request parameters, and can be applied only to the request headers. Example: GET / HTTP/1.1 Cookie: a=1; b=2 [header, 'COOKIE', cookie, 'a'] = 1 ; [header, 'COOKIE', cookie, 'b'] = 2 .","title":"cookie"},{"location":"user-guides/rules/request-processing/#form_urlencoded","text":"Parses the request body passed in the application/x-www-form-urlencoded format, and can be applied only to the request body. Example: p1=1&p2[a]=2&p2[b]=3&p3[]=4&p3[]=5&p4=6&p4=7 [post, form_urlencoded, 'p1'] \u2014 1 [post, form_urlencoded, 'p2', hash, 'a'] \u2014 2 [post, form_urlencoded, 'p2', hash, 'b'] \u2014 3 [post, form_urlencoded, 'p3', array, 0] \u2014 4 [post, form_urlencoded, 'p3', array, 1] \u2014 5 [post, form_urlencoded, 'p4', array, 0] \u2014 6 [post, form_urlencoded, 'p4', array, 1] \u2014 7 [post, form_urlencoded, 'p4', pollution] \u2014 6,7","title":"form_urlencoded"},{"location":"user-guides/rules/request-processing/#grpc","text":"Parses gRPC API requests, and can be applied only to the request body. Supports the protobuf filter for the Protocol Buffers data.","title":"grpc"},{"location":"user-guides/rules/request-processing/#multipart","text":"Parses the request body passed in the multipart format, and can be applied only to the request body. Supports the header filter for the headers in the request body. Example: p1=1&p2[a]=2&p2[b]=3&p3[]=4&p3[]=5&p4=6&p4=7 [post, multipart, 'p1'] \u2014 1 [post, multipart, 'p2', hash, 'a'] \u2014 2 [post, multipart, 'p2', hash, 'b'] \u2014 3 [post, multipart, 'p3', array, 0] \u2014 4 [post, multipart, 'p3', array, 1] \u2014 5 [post, multipart, 'p4', array, 0] \u2014 6 [post, multipart, 'p4', array, 1] \u2014 7 [post, multipart, 'p4', pollution] \u2014 6,7 If a file name is specified in the Content-Disposition header, then the file is considered to be loaded in this parameter. The parameter will look like this: [post, multipart, 'someparam', file] \u2014 file contents","title":"multipart"},{"location":"user-guides/rules/request-processing/#viewstate","text":"Designed to analyze the session state. The technology is used by Microsoft ASP.NET, and can be applied only to the request body. Filters: viewstate_array for an array viewstate_pair for an array viewstate_triplet for an array viewstate_dict for an associative array viewstate_dict_key for a string viewstate_dict_value for a string viewstate_sparse_array for an associative array","title":"viewstate"},{"location":"user-guides/rules/request-processing/#norms","text":"The norms are applied to parsers for array and key data types. Norms are used to define the boundaries of data analysis. The value of the norm is indicated in the parser tag. For example: hash_all , hash_default , hash_name . If the norm is not specified, then the identifier of the entity that requires processing is passed to the parser. For example: the name of the JSON object or other identifier is passed after hash .","title":"Norms"},{"location":"user-guides/rules/request-processing/#all","text":"Used to get values of all elements, parameters, or objects. For example: get_all for all GET parameter values header_all for all header values array_all for all array element values hash_all for all JSON object or XML attribute values","title":"all"},{"location":"user-guides/rules/request-processing/#default","text":"Used to get default values of elements, parameters, or objects. For example: get_default for default values of GET parameters header_default for default values of headers array_default for default values of array elements hash_default for default values of JSON objects or XML attributes","title":"default"},{"location":"user-guides/rules/request-processing/#name","text":"Used to get names of all elements, parameters, or objects. For example: get_name for all GET parameter names header_name for all header names hash_name for all JSON object or XML attribute names","title":"name"},{"location":"user-guides/rules/sensitive-data-rule/","text":"Rules for Data Masking \u00b6 The WAF node sends the following data to the Wallarm Cloud: Serialized requests with attacks Wallarm system counters System statistics: CPU load, RAM usage, etc. Wallarm system statistics: number of processed NGINX requests, Tarantool statistics, etc. Information on the nature of the traffic that Wallarm needs to correctly detect application structure Some data should not be transferred outside of the server on which it is processed. Typically, this category includes authorization (cookies, tokens, passwords), personal data and payment credentials. WAF Node supports data masking in requests. The real values will be replaced by * and will not be accessible either in the Wallarm Cloud or in the local post-analysis module. This method ensures that the protected data cannot leak outside the trusted environment. It can affect the display of attacks, active attack (threat) verification, and the detection of brute force attacks. Example: Masking of a Cookie Value \u00b6 If the following conditions take place: the application is accessible at the domain example.com the application uses a PHPSESSID cookie for user authentication security policies deny access to this information for employees using Wallarm Then , to create a data masking rule for this cookie, the following actions should be performed: Go to the Rules tab Find the branch for example.com/**/*.* and click Add rule Choose Mask sensitive data Select the Header parameter and enter its value COOKIE ; select the cookie parameter and enter its value PHPSESSID after in this part of request Click Create","title":"Rules for Data Masking"},{"location":"user-guides/rules/sensitive-data-rule/#rules-for-data-masking","text":"The WAF node sends the following data to the Wallarm Cloud: Serialized requests with attacks Wallarm system counters System statistics: CPU load, RAM usage, etc. Wallarm system statistics: number of processed NGINX requests, Tarantool statistics, etc. Information on the nature of the traffic that Wallarm needs to correctly detect application structure Some data should not be transferred outside of the server on which it is processed. Typically, this category includes authorization (cookies, tokens, passwords), personal data and payment credentials. WAF Node supports data masking in requests. The real values will be replaced by * and will not be accessible either in the Wallarm Cloud or in the local post-analysis module. This method ensures that the protected data cannot leak outside the trusted environment. It can affect the display of attacks, active attack (threat) verification, and the detection of brute force attacks.","title":"Rules for Data Masking"},{"location":"user-guides/rules/sensitive-data-rule/#example-masking-of-a-cookie-value","text":"If the following conditions take place: the application is accessible at the domain example.com the application uses a PHPSESSID cookie for user authentication security policies deny access to this information for employees using Wallarm Then , to create a data masking rule for this cookie, the following actions should be performed: Go to the Rules tab Find the branch for example.com/**/*.* and click Add rule Choose Mask sensitive data Select the Header parameter and enter its value COOKIE ; select the cookie parameter and enter its value PHPSESSID after in this part of request Click Create","title":"Example: Masking of a Cookie Value"},{"location":"user-guides/rules/view/","text":"Inspecting Application Profile Rules \u00b6 Application Structure Display \u00b6 To view the application structure, go to the Profile & Rules tab. This section represents branches and endpoints that are already known. The system automatically groups the rules by branches, highlighting common conditions and building a tree-like structure. As a result, a branch may have child branches. To show or hide nested branches, click on the blue circle to the left of the branch description. Two asterisks ** in a branch description refer to any number of nested paths. For instance, the branch /**/*.php will contain both /index.php and /app/admin/install.php . The size of the blue circle indicates the relative quantity of the nested branches. Its color indicates the relative quantity of the rules within the branch and its sub-branches. On each nesting level, the size and color of the circles are independent from each other. To the right of the branch description, the system may display an orange number, which indicates the number of rules in that branch (only the direct descendants, not the nested rules). If no number is displayed, then that branch is \"virtual\" \u2014 it is used only for grouping similar sub-branches. Branches with no rules available for the user (according to the privilege model) are automatically hidden . Rule Display \u00b6 In each branch, the user can look through the list of rules attached to it. To switch over to the page with the rule list, click on the description of the corresponding branch. The rules within a branch are grouped by the point field. The rules that affect the entire request, rather than individual parameters, are grouped together into one line. To see the entire list, click on the line. For each rule, the system displays the following parameters: last modified time, quantity, types, and point. By default, only the rules linked to the selected branch are shown. To see the rules inherited from more common branches, click on the Hidden button.","title":"Inspecting Application Profile Rules"},{"location":"user-guides/rules/view/#inspecting-application-profile-rules","text":"","title":"Inspecting Application Profile Rules"},{"location":"user-guides/rules/view/#application-structure-display","text":"To view the application structure, go to the Profile & Rules tab. This section represents branches and endpoints that are already known. The system automatically groups the rules by branches, highlighting common conditions and building a tree-like structure. As a result, a branch may have child branches. To show or hide nested branches, click on the blue circle to the left of the branch description. Two asterisks ** in a branch description refer to any number of nested paths. For instance, the branch /**/*.php will contain both /index.php and /app/admin/install.php . The size of the blue circle indicates the relative quantity of the nested branches. Its color indicates the relative quantity of the rules within the branch and its sub-branches. On each nesting level, the size and color of the circles are independent from each other. To the right of the branch description, the system may display an orange number, which indicates the number of rules in that branch (only the direct descendants, not the nested rules). If no number is displayed, then that branch is \"virtual\" \u2014 it is used only for grouping similar sub-branches. Branches with no rules available for the user (according to the privilege model) are automatically hidden .","title":"Application Structure Display"},{"location":"user-guides/rules/view/#rule-display","text":"In each branch, the user can look through the list of rules attached to it. To switch over to the page with the rule list, click on the description of the corresponding branch. The rules within a branch are grouped by the point field. The rules that affect the entire request, rather than individual parameters, are grouped together into one line. To see the entire list, click on the line. For each rule, the system displays the following parameters: last modified time, quantity, types, and point. By default, only the rules linked to the selected branch are shown. To see the rules inherited from more common branches, click on the Hidden button.","title":"Rule Display"},{"location":"user-guides/rules/vpatch-rule/","text":"Virtual Patching \u00b6 A virtual patch allows blocking malicious requests even in monitoring mode or when a request does not seem to contain any known attack vectors. Virtual patches are especially useful in cases when it is impossible to fix a critical vulnerability in the code or install the necessary security updates quickly. If attack types are selected, the request will be blocked only if the filter node detects an attack of one of the listed types in the corresponding parameter. If the setting Any request is selected, the system will block the requests with the defined parameter, even if it does not contain an attack vector. Example: Blocking SQLi Attack in the GET Parameter id \u00b6 If the following conditions take place: the application is accessible at the domain example.com the application's parameter id is vulnerable to SQL injection attacks the filter node is set to monitoring mode attempts at vulnerability exploitation must be blocked Then , to create a virtual patch Go to the Rules tab Find the branch example.com/**/*.* and click Add rule Choose Create a virtual patch Choose SQLi as the type of attack Select the GET parameter and enter its value id after in this part of request Click Create Example: Block All Requests With the GET Parameter refresh \u00b6 If the following conditions take place: the application is accessible at the domain example.com the application crashes upon processing the GET parameter refresh attempts at vulnerability exploitation must be blocked Then , to create a virtual patch Go to the Rules tab Find the branch example.com/**/*.* and click Add rule Choose Create a virtual patch Choose Any request Select the GET parameter and enter its value refresh after in this part of request Click Create","title":"Virtual Patching"},{"location":"user-guides/rules/vpatch-rule/#virtual-patching","text":"A virtual patch allows blocking malicious requests even in monitoring mode or when a request does not seem to contain any known attack vectors. Virtual patches are especially useful in cases when it is impossible to fix a critical vulnerability in the code or install the necessary security updates quickly. If attack types are selected, the request will be blocked only if the filter node detects an attack of one of the listed types in the corresponding parameter. If the setting Any request is selected, the system will block the requests with the defined parameter, even if it does not contain an attack vector.","title":"Virtual Patching"},{"location":"user-guides/rules/vpatch-rule/#example-blocking-sqli-attack-in-the-get-parameter-id","text":"If the following conditions take place: the application is accessible at the domain example.com the application's parameter id is vulnerable to SQL injection attacks the filter node is set to monitoring mode attempts at vulnerability exploitation must be blocked Then , to create a virtual patch Go to the Rules tab Find the branch example.com/**/*.* and click Add rule Choose Create a virtual patch Choose SQLi as the type of attack Select the GET parameter and enter its value id after in this part of request Click Create","title":"Example: Blocking SQLi Attack in the GET Parameter id"},{"location":"user-guides/rules/vpatch-rule/#example-block-all-requests-with-the-get-parameter-refresh","text":"If the following conditions take place: the application is accessible at the domain example.com the application crashes upon processing the GET parameter refresh attempts at vulnerability exploitation must be blocked Then , to create a virtual patch Go to the Rules tab Find the branch example.com/**/*.* and click Add rule Choose Create a virtual patch Choose Any request Select the GET parameter and enter its value refresh after in this part of request Click Create","title":"Example: Block All Requests With the GET Parameter refresh"},{"location":"user-guides/rules/wallarm-mode-rule/","text":"Filtration mode rule \u00b6 The filtration mode allows you to enable and disable the blocking of requests to various parts of a web application. To set a filtration mode, create a Set traffic filtration mode rule and select the appropriate mode. The filtration mode can take one of the following values: Default : the system will work in accordance with the parameters specified in the NGINX configuration files. Disable : the analysis and filtration of requests are disabled completely. Monitorig : the requests are analyzed and displayed in the interface but they are not blocked. Safe blocking : malicious requests are blocked only if they are originated from greylisted IPs . Blocking : malicious requests are blocked and displayed in the interface. To implement this rule, the NGINX configuration files must permit centralized management of the operation mode . Example: Disabling Request Blocking During User Registration \u00b6 If the following conditions take place: new user registration is available at example.com/signup it is better to overlook an attack than to lose a customer Then , to create a rule disabling blocking during user registration Go to the Rules tab Find the branch for example.com/signup , and click Add rule Choose Set traffic filtration mode Choose operation mode monitoring Click Create","title":"Filtration mode rule"},{"location":"user-guides/rules/wallarm-mode-rule/#filtration-mode-rule","text":"The filtration mode allows you to enable and disable the blocking of requests to various parts of a web application. To set a filtration mode, create a Set traffic filtration mode rule and select the appropriate mode. The filtration mode can take one of the following values: Default : the system will work in accordance with the parameters specified in the NGINX configuration files. Disable : the analysis and filtration of requests are disabled completely. Monitorig : the requests are analyzed and displayed in the interface but they are not blocked. Safe blocking : malicious requests are blocked only if they are originated from greylisted IPs . Blocking : malicious requests are blocked and displayed in the interface. To implement this rule, the NGINX configuration files must permit centralized management of the operation mode .","title":"Filtration mode rule"},{"location":"user-guides/rules/wallarm-mode-rule/#example-disabling-request-blocking-during-user-registration","text":"If the following conditions take place: new user registration is available at example.com/signup it is better to overlook an attack than to lose a customer Then , to create a rule disabling blocking during user registration Go to the Rules tab Find the branch for example.com/signup , and click Add rule Choose Set traffic filtration mode Choose operation mode monitoring Click Create","title":"Example: Disabling Request Blocking During User Registration"},{"location":"user-guides/scanner/check-scope/","text":"Working with the Scope \u00b6 You can see information on the company's public resources on the Scanner tab of the Wallarm interface. The Wallarm scanner discovers the scope. Scanner Settings Overview \u00b6 In the Scanner block, the following data is shown: current scanner settings time of the last scan To learn more about configuring the scanner, see the \u201cScanner settings\u201d page. Scope Overview \u00b6 In the Scope block of the Scanner tab, the elements are shown in three columns: Domains , IPs , and Services . The total number of elements of a certain type is shown near the column name in a small grey font. If Wallarm can determine which data center the given IP address belongs to, then the corresponding tag will be displayed to the right of the element or group of elements: the AWS tag for Amazon, the GCP tag for Google, the Azure tag for Microsoft data centers, and DC for other data centers. Info By default, the scope scanning starts with the domain of the email address that was specified upon Wallarm account creation. The elements' names displayed in bold font correspond with the group of observed resources. The number of resources the certain group contains is shown in small grey font near its name. Click the group name to expand the list of the resources it contains. Use the search field to find elements by their names. You can also search for substrings. For example, the query domain.com displays all domains that have \u201cdomain.com\u201d as a substring: \u201ca.domain.com\u201d, \u201cb.domain.com\u201d and their associations. Click one of the buttons on the bar to filter elements by their status: All elements : display all of the resources within the scope. New : display the newly discovered resources that have not been viewed yet. Disabled : display the resources for which scanning is disabled. Check the Resource Associations \u00b6 Click one of the scope elements. The Wallarm interface will display the selected element's associations. The resources' domain, IP address, and port are interdependent. A domain always has a higher priority than an IP address, and an IP address always has priority over a port. When you disable the scanning of or delete a resource with a lower priority, the resource with a higher priority remains active. For example, when you disable the scanning of a domain, the system will also disable the scanning of the IP address and ports that depend on that domain. When you delete an IP address, the system will also delete the associated ports, but keep the domain active, because a domain may have more than one IP address. You can disable the resources' connections to manage each resource independently. Disable and Enable the Resource Connection \u00b6 You can disable the resources' interconnection to manage each resource's scanning settings independently. To disable the resources' interconnection: Select one resource from the resource pair you need to disconnect from each other; Click the switch next to the resource paired with the current one. Determining the current resource The name of the current resource is shown in bold. The web-interface also displays its discovery date. To enable resource interconnection, follow the same steps as when you were disabling the interconnection. Disable Resource Scanning \u00b6 You can disable scanning for any of the resources within the scope. In so doing, the resource you selected will remain in the system as detectable, but will not be scanned for vulnerabilities. Click one of the scope elements. Click the switch next to the selected element. Delete a Resource from the Scope \u00b6 You can delete any resource from the scope. The purpose of this operation is to delete an accidentally added resource. Select the desired resources by clicking the checkboxes next to their names. Click Delete element(s) . Recovering the deleted resources The deleted resources will not be discovered in future scannings. If you have deleted the resource by mistake, contact the Wallarm support team . Add a Domain or an IP Address \u00b6 You can manually add a domain or an IP address. Click Add domain or IP . In the window that appears, enter the new domain or IP and click Add . After the new domain or IP address is added, the scanner will launch the scanning procedure to search for elements connected with the resource and will add them to the scope. Reserved domains Reserved domains and subdomains can only be added to the scope by a certain client. Wallarm reserves domain for a client on request. You cannot add a domain that is reserved by another client to your scope. To see detailed information about reserved domains, proceed to this link . Limit Scanning Speed \u00b6 You can limit the speed of domain or IP address scanning. The total speed of sending requests by the scanner will not exceed the specified value. Select one of the scope elements of the following types: Domain , IP . Click the Set RPS limits button or the current limit value. Fill in the Domain RPS field for the domain or the IP RPS field for the IP address. You can also limit the RPS for each of the domain's dependent IP addresses. To set this limit, enter the desired value in the RPS per IP field. Click Save . To return to the default settings, use an empty value or enter 0 . See also Scanner overview Scanner settings","title":"Check scope"},{"location":"user-guides/scanner/check-scope/#working-with-the-scope","text":"You can see information on the company's public resources on the Scanner tab of the Wallarm interface. The Wallarm scanner discovers the scope.","title":"Working with the Scope"},{"location":"user-guides/scanner/check-scope/#scanner-settings-overview","text":"In the Scanner block, the following data is shown: current scanner settings time of the last scan To learn more about configuring the scanner, see the \u201cScanner settings\u201d page.","title":"Scanner Settings Overview"},{"location":"user-guides/scanner/check-scope/#scope-overview","text":"In the Scope block of the Scanner tab, the elements are shown in three columns: Domains , IPs , and Services . The total number of elements of a certain type is shown near the column name in a small grey font. If Wallarm can determine which data center the given IP address belongs to, then the corresponding tag will be displayed to the right of the element or group of elements: the AWS tag for Amazon, the GCP tag for Google, the Azure tag for Microsoft data centers, and DC for other data centers. Info By default, the scope scanning starts with the domain of the email address that was specified upon Wallarm account creation. The elements' names displayed in bold font correspond with the group of observed resources. The number of resources the certain group contains is shown in small grey font near its name. Click the group name to expand the list of the resources it contains. Use the search field to find elements by their names. You can also search for substrings. For example, the query domain.com displays all domains that have \u201cdomain.com\u201d as a substring: \u201ca.domain.com\u201d, \u201cb.domain.com\u201d and their associations. Click one of the buttons on the bar to filter elements by their status: All elements : display all of the resources within the scope. New : display the newly discovered resources that have not been viewed yet. Disabled : display the resources for which scanning is disabled.","title":"Scope Overview"},{"location":"user-guides/scanner/check-scope/#check-the-resource-associations","text":"Click one of the scope elements. The Wallarm interface will display the selected element's associations. The resources' domain, IP address, and port are interdependent. A domain always has a higher priority than an IP address, and an IP address always has priority over a port. When you disable the scanning of or delete a resource with a lower priority, the resource with a higher priority remains active. For example, when you disable the scanning of a domain, the system will also disable the scanning of the IP address and ports that depend on that domain. When you delete an IP address, the system will also delete the associated ports, but keep the domain active, because a domain may have more than one IP address. You can disable the resources' connections to manage each resource independently.","title":"Check the Resource Associations"},{"location":"user-guides/scanner/check-scope/#disable-and-enable-the-resource-connection","text":"You can disable the resources' interconnection to manage each resource's scanning settings independently. To disable the resources' interconnection: Select one resource from the resource pair you need to disconnect from each other; Click the switch next to the resource paired with the current one. Determining the current resource The name of the current resource is shown in bold. The web-interface also displays its discovery date. To enable resource interconnection, follow the same steps as when you were disabling the interconnection.","title":"Disable and Enable the Resource Connection"},{"location":"user-guides/scanner/check-scope/#disable-resource-scanning","text":"You can disable scanning for any of the resources within the scope. In so doing, the resource you selected will remain in the system as detectable, but will not be scanned for vulnerabilities. Click one of the scope elements. Click the switch next to the selected element.","title":"Disable Resource Scanning"},{"location":"user-guides/scanner/check-scope/#delete-a-resource-from-the-scope","text":"You can delete any resource from the scope. The purpose of this operation is to delete an accidentally added resource. Select the desired resources by clicking the checkboxes next to their names. Click Delete element(s) . Recovering the deleted resources The deleted resources will not be discovered in future scannings. If you have deleted the resource by mistake, contact the Wallarm support team .","title":"Delete a Resource from the Scope"},{"location":"user-guides/scanner/check-scope/#add-a-domain-or-an-ip-address","text":"You can manually add a domain or an IP address. Click Add domain or IP . In the window that appears, enter the new domain or IP and click Add . After the new domain or IP address is added, the scanner will launch the scanning procedure to search for elements connected with the resource and will add them to the scope. Reserved domains Reserved domains and subdomains can only be added to the scope by a certain client. Wallarm reserves domain for a client on request. You cannot add a domain that is reserved by another client to your scope. To see detailed information about reserved domains, proceed to this link .","title":"Add a Domain or an IP Address"},{"location":"user-guides/scanner/check-scope/#limit-scanning-speed","text":"You can limit the speed of domain or IP address scanning. The total speed of sending requests by the scanner will not exceed the specified value. Select one of the scope elements of the following types: Domain , IP . Click the Set RPS limits button or the current limit value. Fill in the Domain RPS field for the domain or the IP RPS field for the IP address. You can also limit the RPS for each of the domain's dependent IP addresses. To set this limit, enter the desired value in the RPS per IP field. Click Save . To return to the default settings, use an empty value or enter 0 . See also Scanner overview Scanner settings","title":"Limit Scanning Speed"},{"location":"user-guides/scanner/configure-scanner-modules/","text":"Configuring Scanner Modules \u00b6 Click the Configure link under the scanner toggle to configure the scanner. Configuring the Vulnerabilities Detection List \u00b6 The scanner consists of multiple modules, each of which is responsible for detecting a certain type of vulnerability. The full modules list is specified in the \u201cConfigure Scanner\u201d menu. Filtering Modules by Tag \u00b6 You can filter modules by their tags, which are grouped by type: The vulnerability type\u2014tags for different vulnerability types, such as Remote Code Execution, Path Traversal, or Cross\u2011Site Scripting. The vulnerable technology\u2014tags for different technologies and software, that if used may cause a vulnerability detection. The presence of the vulnerability in the Common Vulnerabilities and Exposures (CVE) database\u2014such vulnerabilities contain the CVE tag. To filter the scanner modules by tag, perform the following actions: Click the Filter by tag field. In the drop-down list that appears the tags are grouped by their type. Select the desired tags by clicking them. You can remove a tag from the filtering field by clicking the tick next to the tag name. Filtering by multiple tags If the filtering field contains multiple tags, the result will consist of only those modules that are marked with all of the specified tags. After you filter the modules by tag, the Wallarm interface displays the total number of the modules that correspond with the specified tags and the number of the filtered modules that correspond with each of the vulnerability classes in the Common Weakness Enumeration (CWE) . Now you can disable all of the filtered modules at once by clicking the toggle next to the Modules found label. You can also disable all of the filtered modules that correspond with a certain vulnerability class by clicking the necessary toggle. Disabling and Enabling All Modules \u00b6 You can disable or enable all of the modules at once by clicking the All modules toggle that is available when no tags are selected in the filtering field. Disabling and Enabling All Modules Detecting Certain Classes of Vulnerabilities \u00b6 In the left column, all of the modules are grouped in accordance with the CWE . You can disable and enable all modules that detect vulnerabilities of a certain class by clicking the corresponding toggle. Disabling and Enabling Individual Modules \u00b6 In the right column, all of the modules are filtered according to the filtering field. Here you can individually enable and disable modules.## Disabling and Enabling Vulnerability Rechecking During the active vulnerability check, the scanner restarts tests to check whether the previously detected vulnerabilities are still present. If a previously detected vulnerability is not found after the recheck, the scanner marks it as resolved. You can disable or enable vulnerability rechecking using the Recheck vulnerabilities toggle. Demo videos \u00b6","title":"Configure scanner modules"},{"location":"user-guides/scanner/configure-scanner-modules/#configuring-scanner-modules","text":"Click the Configure link under the scanner toggle to configure the scanner.","title":"Configuring Scanner Modules"},{"location":"user-guides/scanner/configure-scanner-modules/#configuring-the-vulnerabilities-detection-list","text":"The scanner consists of multiple modules, each of which is responsible for detecting a certain type of vulnerability. The full modules list is specified in the \u201cConfigure Scanner\u201d menu.","title":"Configuring the Vulnerabilities Detection List"},{"location":"user-guides/scanner/configure-scanner-modules/#filtering-modules-by-tag","text":"You can filter modules by their tags, which are grouped by type: The vulnerability type\u2014tags for different vulnerability types, such as Remote Code Execution, Path Traversal, or Cross\u2011Site Scripting. The vulnerable technology\u2014tags for different technologies and software, that if used may cause a vulnerability detection. The presence of the vulnerability in the Common Vulnerabilities and Exposures (CVE) database\u2014such vulnerabilities contain the CVE tag. To filter the scanner modules by tag, perform the following actions: Click the Filter by tag field. In the drop-down list that appears the tags are grouped by their type. Select the desired tags by clicking them. You can remove a tag from the filtering field by clicking the tick next to the tag name. Filtering by multiple tags If the filtering field contains multiple tags, the result will consist of only those modules that are marked with all of the specified tags. After you filter the modules by tag, the Wallarm interface displays the total number of the modules that correspond with the specified tags and the number of the filtered modules that correspond with each of the vulnerability classes in the Common Weakness Enumeration (CWE) . Now you can disable all of the filtered modules at once by clicking the toggle next to the Modules found label. You can also disable all of the filtered modules that correspond with a certain vulnerability class by clicking the necessary toggle.","title":"Filtering Modules by Tag"},{"location":"user-guides/scanner/configure-scanner-modules/#disabling-and-enabling-all-modules","text":"You can disable or enable all of the modules at once by clicking the All modules toggle that is available when no tags are selected in the filtering field.","title":"Disabling and Enabling All Modules"},{"location":"user-guides/scanner/configure-scanner-modules/#disabling-and-enabling-all-modules-detecting-certain-classes-of-vulnerabilities","text":"In the left column, all of the modules are grouped in accordance with the CWE . You can disable and enable all modules that detect vulnerabilities of a certain class by clicking the corresponding toggle.","title":"Disabling and Enabling All Modules Detecting Certain Classes of Vulnerabilities"},{"location":"user-guides/scanner/configure-scanner-modules/#disabling-and-enabling-individual-modules","text":"In the right column, all of the modules are filtered according to the filtering field. Here you can individually enable and disable modules.## Disabling and Enabling Vulnerability Rechecking During the active vulnerability check, the scanner restarts tests to check whether the previously detected vulnerabilities are still present. If a previously detected vulnerability is not found after the recheck, the scanner marks it as resolved. You can disable or enable vulnerability rechecking using the Recheck vulnerabilities toggle.","title":"Disabling and Enabling Individual Modules"},{"location":"user-guides/scanner/configure-scanner-modules/#demo-videos","text":"","title":"Demo videos"},{"location":"user-guides/scanner/configure-scanner/","text":"General Scanner Settings \u00b6 To access scanner settings, go to the Scanner tab and click the Settings button on the left. Scanner \u00b6 This setting enables or disables the discovery of resources in the scope and searches for typical vulnerabilities. To manually restart the scanning, if the setting is on, switch it off, and then switch it back on again. Viewing detected vulnerabilities The vulnerabilities found by the scanner can be viewed on the Vulnerabilities tab of the web interface. In the scanner settings, you can perform the following actions: Configuring the list of detected vulnerabilities Disabling and enabling vulnerability re-check Configuring the scanner To see detailed information about configuring the scanner, proceed to this link . Active Threat Verification \u00b6 This setting enables or completely disables automatic attack reproduction by the scanner. You can later manually run the attack verification by clicking the Check button or using the Rules tab. Scanner's RPS Limits \u00b6 This setting limits the maximum load on the web application that will be generated by the scanner requests. The maximum number of requests per second (RPS) can be configured to a domain to an IP address If multiple domains are associated with the same IP address, then the speed of requests to this IP address will not exceed the limits for the IP address. If multiple IP addresses are associated with one domain, then the total speed of requests to these IP addresses within this domain will not exceed the limits for the domain. Limits can be overridden for individual IP addresses or domains. This is done within the network scope section. To restore default scanner RPS settings, click the Restore defaults button. Click Save to apply any changes made to the scanner settings.","title":"Configure scanner"},{"location":"user-guides/scanner/configure-scanner/#general-scanner-settings","text":"To access scanner settings, go to the Scanner tab and click the Settings button on the left.","title":"General Scanner Settings"},{"location":"user-guides/scanner/configure-scanner/#scanner","text":"This setting enables or disables the discovery of resources in the scope and searches for typical vulnerabilities. To manually restart the scanning, if the setting is on, switch it off, and then switch it back on again. Viewing detected vulnerabilities The vulnerabilities found by the scanner can be viewed on the Vulnerabilities tab of the web interface. In the scanner settings, you can perform the following actions: Configuring the list of detected vulnerabilities Disabling and enabling vulnerability re-check Configuring the scanner To see detailed information about configuring the scanner, proceed to this link .","title":"Scanner"},{"location":"user-guides/scanner/configure-scanner/#active-threat-verification","text":"This setting enables or completely disables automatic attack reproduction by the scanner. You can later manually run the attack verification by clicking the Check button or using the Rules tab.","title":"Active Threat Verification"},{"location":"user-guides/scanner/configure-scanner/#scanners-rps-limits","text":"This setting limits the maximum load on the web application that will be generated by the scanner requests. The maximum number of requests per second (RPS) can be configured to a domain to an IP address If multiple domains are associated with the same IP address, then the speed of requests to this IP address will not exceed the limits for the IP address. If multiple IP addresses are associated with one domain, then the total speed of requests to these IP addresses within this domain will not exceed the limits for the domain. Limits can be overridden for individual IP addresses or domains. This is done within the network scope section. To restore default scanner RPS settings, click the Restore defaults button. Click Save to apply any changes made to the scanner settings.","title":"Scanner's RPS Limits"},{"location":"user-guides/scanner/intro/","text":"Scanner Overview \u00b6 Scanner performs the following tasks: Network scope scanning Searching for typical vulnerabilities and security issues Active threat verification Updating the status of previously detected vulnerabilities Network Scope Scanning \u00b6 A network scope is a company's public resources (domains and IP addresses) connected to public networks. It defines an area to be scanned for typical vulnerabilities and is the cornerstone of the security process. As the project develops, the number of resources in the scope steadily increases and control over them inevitably decreases. The resources may be located not only in the company's data centers but also on shared hostings \u2014 for example, your marketers will create new landing pages and start new campaigns. These resources are placed on subdomains of the main project and can jeopardize the project's security. Hackers always choose the least protected resources on the company's scope and attempt to compromise these resources first. Wallarm integrates all the scope discovery mechanisms used by white hat hackers when assessing a company's security and running penetration tests. The scope discovery does not end at the domain and IP address mapping but also discovers the network resources that can be accessed from the Internet. To do this, Wallarm first scans ports and then detects the network resources on these ports. Various methods are used in the continuous process of collecting and updating scope data: Automatic modes DNS zone transfer ( AXFR ) NS and MX records receiving SPF records data receiving Subdomain dictionary search SSL certificate parsing Manual data entry via web interface or Wallarm API . This results in a map of the company's resources that is of the same quality as the one done by white hat hackers when doing penetration testing. Searching for Typical Vulnerabilities and Security Issues \u00b6 After collecting the network scope, the scanner checks all IP addresses and domains within it for any typical vulnerabilities. Active Threat Verification \u00b6 The scanner will automatically reproduce each attack from the traffic. This mechanism allows the detection of vulnerabilities that could have been exploited during the attack. For safety reasons, when reproducing attacks from requests, the authentication data (cookies, basic-auth, viewstate) is deleted. Correct operation of this functionality may require additional configuration from the application side. Updating the Status of Previously Detected Vulnerabilities \u00b6 The scanner regularly checks the status of vulnerabilities and automatically marks them as fixed or, on the contrary, reopens newly reproduced ones. Current vulnerabilities and vulnerabilities fixed less than a month ago are checked once a day. Vulnerabilities that were fixed more than a month ago are checked once a week. Vulnerabilities marked as false are not checked. Demo videos \u00b6","title":"Using Wallarm Scanner"},{"location":"user-guides/scanner/intro/#scanner-overview","text":"Scanner performs the following tasks: Network scope scanning Searching for typical vulnerabilities and security issues Active threat verification Updating the status of previously detected vulnerabilities","title":"Scanner Overview"},{"location":"user-guides/scanner/intro/#network-scope-scanning","text":"A network scope is a company's public resources (domains and IP addresses) connected to public networks. It defines an area to be scanned for typical vulnerabilities and is the cornerstone of the security process. As the project develops, the number of resources in the scope steadily increases and control over them inevitably decreases. The resources may be located not only in the company's data centers but also on shared hostings \u2014 for example, your marketers will create new landing pages and start new campaigns. These resources are placed on subdomains of the main project and can jeopardize the project's security. Hackers always choose the least protected resources on the company's scope and attempt to compromise these resources first. Wallarm integrates all the scope discovery mechanisms used by white hat hackers when assessing a company's security and running penetration tests. The scope discovery does not end at the domain and IP address mapping but also discovers the network resources that can be accessed from the Internet. To do this, Wallarm first scans ports and then detects the network resources on these ports. Various methods are used in the continuous process of collecting and updating scope data: Automatic modes DNS zone transfer ( AXFR ) NS and MX records receiving SPF records data receiving Subdomain dictionary search SSL certificate parsing Manual data entry via web interface or Wallarm API . This results in a map of the company's resources that is of the same quality as the one done by white hat hackers when doing penetration testing.","title":"Network Scope Scanning"},{"location":"user-guides/scanner/intro/#searching-for-typical-vulnerabilities-and-security-issues","text":"After collecting the network scope, the scanner checks all IP addresses and domains within it for any typical vulnerabilities.","title":"Searching for Typical Vulnerabilities and Security Issues"},{"location":"user-guides/scanner/intro/#active-threat-verification","text":"The scanner will automatically reproduce each attack from the traffic. This mechanism allows the detection of vulnerabilities that could have been exploited during the attack. For safety reasons, when reproducing attacks from requests, the authentication data (cookies, basic-auth, viewstate) is deleted. Correct operation of this functionality may require additional configuration from the application side.","title":"Active Threat Verification"},{"location":"user-guides/scanner/intro/#updating-the-status-of-previously-detected-vulnerabilities","text":"The scanner regularly checks the status of vulnerabilities and automatically marks them as fixed or, on the contrary, reopens newly reproduced ones. Current vulnerabilities and vulnerabilities fixed less than a month ago are checked once a day. Vulnerabilities that were fixed more than a month ago are checked once a week. Vulnerabilities marked as false are not checked.","title":"Updating the Status of Previously Detected Vulnerabilities"},{"location":"user-guides/scanner/intro/#demo-videos","text":"","title":"Demo videos"},{"location":"user-guides/scanner/reserved-domains/","text":"Domain Reservation \u00b6 If you reserve your domain in Wallarm, other customers cannot add them to their network scopes. Upon trying adding the reserved domain into another network scope, the following error message appears: This domain is reserved for a different Wallarm customer and cannot be added . How To Reserve Your Domain in Wallarm \u00b6 To reserve your domain, contact the Wallarm support team .","title":"Reserved domains"},{"location":"user-guides/scanner/reserved-domains/#domain-reservation","text":"If you reserve your domain in Wallarm, other customers cannot add them to their network scopes. Upon trying adding the reserved domain into another network scope, the following error message appears: This domain is reserved for a different Wallarm customer and cannot be added .","title":"Domain Reservation"},{"location":"user-guides/scanner/reserved-domains/#how-to-reserve-your-domain-in-wallarm","text":"To reserve your domain, contact the Wallarm support team .","title":"How To Reserve Your Domain in Wallarm"},{"location":"user-guides/search-and-filters/custom-report/","text":"Creating a Custom Report \u00b6 You can create a PDF report from your customized search results. Wallarm will email the custom report to you. Create a Report \u00b6 Go to the Events tab. Use the Search field to customize your search results. Click the report button on the right. Put in your email address and click the report button next to the email field. Wallarm will generate the report and email it to you.","title":"Creating a Custom Report"},{"location":"user-guides/search-and-filters/custom-report/#creating-a-custom-report","text":"You can create a PDF report from your customized search results. Wallarm will email the custom report to you.","title":"Creating a Custom Report"},{"location":"user-guides/search-and-filters/custom-report/#create-a-report","text":"Go to the Events tab. Use the Search field to customize your search results. Click the report button on the right. Put in your email address and click the report button next to the email field. Wallarm will generate the report and email it to you.","title":"Create a Report"},{"location":"user-guides/search-and-filters/use-search/","text":"Using search and filters \u00b6 Wallarm provides convenient methods for searching detected attacks, incidents and vulnerabilities. In the Events section of the Wallarm Console, there are the following search methods available: Filters to select filtering criteria Search field to input search queries with attributes and modifiers similar to human language The values set in the filters are automatically duplicated in the search field, and vice versa. Any search query or a filter combination can be saved by clicking Save a query . Filters \u00b6 Available filters are presented in the Wallarm Console in multiple forms: Filters panel that is expanded and collapsed using the Filter button Quick filters for excluding or showing only events with the specific parameter values When values of different filters are selected, the results will meet all those conditions. When different values for the same filter are specified, the results will meet any of those conditions. Search field \u00b6 The search field accepts queries with attributes and modifiers similar to human language which makes submitting queries intuitive. For example: attacks xss : to search for all XSS-attacks attacks today : to search for all attacks that happened today vulns sqli : to search for SQL-injection vulnerabilities vulns 11/01/2020-11/10/2020 : to search for vulnerabilities within a certain period of time xss 12/14/2020 : to search for all vulnerabilities, suspicions, attacks, and incidents of cross\u2011site scripting on 14 December 2020 p:xss 12/14/2020 : to search for all vulnerabilities, suspicions, attacks, and incidents of all types within the xss HTTP request parameter (i.e. http://localhost/?xss=attack-here ) as of 14 December 2020 attacks 9-12/2020 : to search for all attacks from September to December 2020 rce /catalog/import.php : to search for all RCE attacks, incidents, and vulnerabilities on /catalog/import.php path since yesterday When values of different parameters are specified, the results will meet all those conditions. When different values for the same parameter are specified, the results will meet any of those conditions. Setting the attribute value to NOT To negate the attribute value, please use ! before the attribute or modifier name. For example: attacks !ip:111.111.111.111 to show all attacks originated from any IP address excluding 111.111.111.111 . Below you will find the list of attributes and modifiers available for use in search queries. Search by object type \u00b6 Specify in the search string: attack , attacks : to search only for the attacks that are not aimed at known vulnerabilities. incident , incidents : to search only for incidents (attacks exploiting a known vulnerability). vuln , vulns , vulnerability , vulnerabilities : to search only for vulnerabilities. Search by attack type or vulnerability type \u00b6 Specify in the search string: sqli : to search for SQL injection attacks/vulnerabilities. xss : to search for Cross Site Scripting attacks/vulnerabilities. rce : to search for OS Commanding attacks/vulnerabilities. brute : to search for brute-force attacks. ptrav : to search for path traversal attacks. crlf : to search for CRLF injection attacks/vulnerabilities. redir : to search for open redirect vulnerabilities. nosqli : to search for NoSQL injection attacks/vulnerabilities. logic_bomb : to search for logic bomb attacks. overlimit_res : to search for attacks aimed at overlimiting of computational resources . xxe : to search for XML External Entity attacks. vpatch : to search for virtual patches . dirbust : to search for forced browsing attacks. ldapi : to search for LDAP injection attacks/vulnerabilities. scanner : to search for port scanner attacks/vulnerabilities. infoleak : to search for attacks/vulnerabilities of information disclosure . An attack or vulnerability name can be specified in both uppercase and lowercase letters: SQLI , sqli , and SQLi are equally correct. Search by the attack target or the vulnerability target \u00b6 Specify in the search string: client : to search for client data attacks/vulnerabilities. database : to search for database attacks/vulnerabilities. server : to search for app server attacks/vulnerabilities. Search by risk level \u00b6 Specify the risk level in the search string: low : low risk level. medium : medium risk level. high : high risk level. Search by vulnerability identifier \u00b6 To search for a certain vulnerability, specify its identifier. It can be specified in two ways: either fully: WLRM-ABCD-X0123 or in abbreviated form: X0123 Search by vulnerability status \u00b6 Specify vulnerability status in the search string. Vulnerability can have one of the three statuses: open : currently relevant vulnerability closed : fixed vulnerability Search by event time \u00b6 Specify time period in the search string. If the period is not specified, the search is conducted within the events that occurred during the last 24 hours. There are the following methods to specify the period: By date: 11/10/2020-11/14/2020 By date and time (seconds are disregarded): 11/10/2020 11:11 , 11:30-12:22 , 11/10/2020 11:12-01/14/2020 12:14 With relation to a certain moment of time: >11/10/20 Using string aliases: yesterday equal to yesterday's date today equal to today's date last <unit> equal to the period from the entire past unit start to current date and time week , month , year or the number of these units can be used as <unit> . For example: last week , last 3 month or last 3 months . this <unit> equal to current unit week , month , year can be used as <unit> . For example: this week will return events detected on Monday, Tuesday and Wednesday this week if today is Wednesday. Date and time format depends on the settings specified in your profile : MM/DD/YYYY if MDY is selected DD/MM/YYYY if DMY is selected 13:00 if 24\u2011hour is ticked 1pm if 24\u2011hour is unticked The month can be specified as both number and name: 01 , 1 , January , Jan for January. The year can be specified in both full form ( 2020 ) and shortened form ( 20 ). If the year is not specified in the date, then the current year is used. Search by IP address \u00b6 To search by IP address, use the ip: prefix, after which you can specify A specific IP address, for example 192.168.0.1 \u2014in this case, all attacks and incidents will be found for which the source address of the attack corresponds to this IP address. An expression describing a range of IP addresses. A total number of IP addresses related to an attack or incident. Search by IP address range \u00b6 To set a required range of IP addresses, you can use An explicit IP address range: 192.168.0.0-192.168.63.255 10.0.0.0-10.255.255.255 A part of an IP address: 192.168. \u2014equivalent to 192.168.0.0-192.168.255.255 . Redundant format with the * modifier is allowed\u2014 192.168.* 192.168.0. \u2014equivalent to 192.168.0.0-192.168.0.255 An IP address or part of it with a range of values inside the last octet in the expression: 192.168.1.0-255 \u2014equivalent to 192.168.1.0-192.168.1.255 192.168.0-255 \u2014equivalent to 192.168.0.0-192.168.255.255 Important When using a range of values within an octet, a dot is not set at the end. Subnet prefixes ( CIDR notation ): 192.168.1.0/24 \u2014equivalent to 192.168.1.0-192.168.1.255 192.168.0.0/17 \u2014equivalent to 192.168.0.1-192.168.127.255 Note You can combine the above methods for defining IP address ranges. To do this, list all the necessary ranges with the ip: prefix separately. Example : ip:192.168.0.0/24 ip:10.10. ip:10.0.10.0-128 Search by number of IP addresses \u00b6 It is possible to search by the total number of IP addresses that are related to an attack or an incident (only for attacks and incidents): ip:1000+ last month \u2014search for attacks and incidents over the past month for which the number of unique IP addresses is more than 1000 (equivalent to attacks incidents ip:1000+ last month ). xss ip:100+ \u2014search for all cross\u2011site scripting attacks and incidents. The search result will be empty if the number of attacking IP addresses (with the XSS attack type) is less than 100. xss p:id ip:100+ \u2014search for all XSS attacks and incidents related to the id parameter ( ?id=aaa ). This will return results only if the number of different IP addresses exceeds 100. Search by the data center the IP address belongs to \u00b6 To search by the data center, to which the IP address originated the attacks belongs, use the source: prefix. This attribute value can be: tor for the Tor network proxy for the public or web proxy server vpn for VPN aws for Amazon azure for Microsoft Azure gce for Google Cloud Platform Search by the country in which the IP address is registered \u00b6 To search by the country, in which the IP address originated the attacks is registered, use the country: prefix. The country name should be passed to the attribute in the format corresponding to the standard ISO 3166-1 in uppercase or lowercase letters. For example: country:CN or country:cn for attacks originated from China. Search by server response status \u00b6 To search by server response status, specify statuscode: prefix. Response status can be specified as: a number from 100 to 999. \u00abN\u2013M\u00bb range, where N and M are figures from 100 to 999. \u00abN+\u00bb and \u00abN-\u00bb ranges, where N is a number from 100 to 999. Search by server response size \u00b6 To search by the server response size, use the s: or size: prefix. You can search for any integer value. Figures above 999 can be specified without a prefix. The \u00abN\u2013M\u00bb, \u00abN+\u00bb and \u00abN-\u00bb ranges can be specified, where figures above 999 can also be specified without a prefix. Search by HTTP request method \u00b6 To search by HTTP request method, specify the method: prefix. To search for GET , POST , PUT , DELETE , OPTIONS : if upper-case is used, then the search string can be specified without a prefix. For all other values, a prefix should be specified. Search by a number of hits within attack/incident \u00b6 To search attacks and incidents by a number of hits, specify the N: prefix. For example, you can search for attacks that have more than 100 hits: attacks N:>100 . Or search for attacks with less than 10 hits with attacks N:<10 . Search by domain \u00b6 To search by domain, use the d: or domain: prefix. Any string, that may be a domain of the second or a higher level can be specified without a prefix. Any string can be specified with a prefix. You may use masks within a domain. The symbol * replaces any number of characters; the symbol ? replaces any single character. Search by path \u00b6 To search by path, use the u: or url: prefix. Strings that start with / are processed without a prefix. Any string can be specified with a prefix. Search by application \u00b6 To search by the application to which the attack was sent or in which a vulnerability was found, use the pool: prefix. The attribute value is the application name set on the Applications tab in the Settings section. For example: pool:'Example application' . Search by parameter \u00b6 To search by parameter, use the p: , param: , or parameter: prefix and also the = suffix. For example, if you need to find attacks aimed at the xss parameter but not at XSS-attacks (for instance, SQL-injection attack having xss in the GET-parameter), then specify attacks p:xss in the search string. A string that does not start with / and ends with = is considered to be a parameter (wherein the ending = character is not included in the value). Any string can be specified with a prefix. Search for anomalies in attacks \u00b6 To search for anomalies in attacks, use the a: or anomaly: prefix. To refine an anomaly search, use the following parameters: size statuscode time stamps impression vector Example: attacks sqli a:size will search for all SQL-injection attacks, that have response size anomalies in their requests. Search by request identifier \u00b6 To search for attacks and incidents by request identifier, specify the request_id prefix. The request_id parameter has the following value form: a79199bcea606040cc79f913325401fb . In order to make it easier to read, this parameter has been replaced by the placeholder abbreviation <requestId> in the examples below. Examples: attacks incidents request_id:<requestId> : to search for an attack or an incident with the request_id equal to <requestId> . attacks incidents !request_id:<requestId> : to search for attacks and incidents with the request_id not equal to <requestId> . attacks incidents request_id : to search for attacks and incidents with any request_id . attacks incidents !request_id : to search for attacks and incidents without any request_id .","title":"Using events search"},{"location":"user-guides/search-and-filters/use-search/#using-search-and-filters","text":"Wallarm provides convenient methods for searching detected attacks, incidents and vulnerabilities. In the Events section of the Wallarm Console, there are the following search methods available: Filters to select filtering criteria Search field to input search queries with attributes and modifiers similar to human language The values set in the filters are automatically duplicated in the search field, and vice versa. Any search query or a filter combination can be saved by clicking Save a query .","title":"Using search and filters"},{"location":"user-guides/search-and-filters/use-search/#filters","text":"Available filters are presented in the Wallarm Console in multiple forms: Filters panel that is expanded and collapsed using the Filter button Quick filters for excluding or showing only events with the specific parameter values When values of different filters are selected, the results will meet all those conditions. When different values for the same filter are specified, the results will meet any of those conditions.","title":"Filters"},{"location":"user-guides/search-and-filters/use-search/#search-field","text":"The search field accepts queries with attributes and modifiers similar to human language which makes submitting queries intuitive. For example: attacks xss : to search for all XSS-attacks attacks today : to search for all attacks that happened today vulns sqli : to search for SQL-injection vulnerabilities vulns 11/01/2020-11/10/2020 : to search for vulnerabilities within a certain period of time xss 12/14/2020 : to search for all vulnerabilities, suspicions, attacks, and incidents of cross\u2011site scripting on 14 December 2020 p:xss 12/14/2020 : to search for all vulnerabilities, suspicions, attacks, and incidents of all types within the xss HTTP request parameter (i.e. http://localhost/?xss=attack-here ) as of 14 December 2020 attacks 9-12/2020 : to search for all attacks from September to December 2020 rce /catalog/import.php : to search for all RCE attacks, incidents, and vulnerabilities on /catalog/import.php path since yesterday When values of different parameters are specified, the results will meet all those conditions. When different values for the same parameter are specified, the results will meet any of those conditions. Setting the attribute value to NOT To negate the attribute value, please use ! before the attribute or modifier name. For example: attacks !ip:111.111.111.111 to show all attacks originated from any IP address excluding 111.111.111.111 . Below you will find the list of attributes and modifiers available for use in search queries.","title":"Search field"},{"location":"user-guides/search-and-filters/use-search/#search-by-object-type","text":"Specify in the search string: attack , attacks : to search only for the attacks that are not aimed at known vulnerabilities. incident , incidents : to search only for incidents (attacks exploiting a known vulnerability). vuln , vulns , vulnerability , vulnerabilities : to search only for vulnerabilities.","title":"Search by object type"},{"location":"user-guides/search-and-filters/use-search/#search-by-attack-type-or-vulnerability-type","text":"Specify in the search string: sqli : to search for SQL injection attacks/vulnerabilities. xss : to search for Cross Site Scripting attacks/vulnerabilities. rce : to search for OS Commanding attacks/vulnerabilities. brute : to search for brute-force attacks. ptrav : to search for path traversal attacks. crlf : to search for CRLF injection attacks/vulnerabilities. redir : to search for open redirect vulnerabilities. nosqli : to search for NoSQL injection attacks/vulnerabilities. logic_bomb : to search for logic bomb attacks. overlimit_res : to search for attacks aimed at overlimiting of computational resources . xxe : to search for XML External Entity attacks. vpatch : to search for virtual patches . dirbust : to search for forced browsing attacks. ldapi : to search for LDAP injection attacks/vulnerabilities. scanner : to search for port scanner attacks/vulnerabilities. infoleak : to search for attacks/vulnerabilities of information disclosure . An attack or vulnerability name can be specified in both uppercase and lowercase letters: SQLI , sqli , and SQLi are equally correct.","title":"Search by attack type or vulnerability type"},{"location":"user-guides/search-and-filters/use-search/#search-by-the-attack-target-or-the-vulnerability-target","text":"Specify in the search string: client : to search for client data attacks/vulnerabilities. database : to search for database attacks/vulnerabilities. server : to search for app server attacks/vulnerabilities.","title":"Search by the attack target or the vulnerability target"},{"location":"user-guides/search-and-filters/use-search/#search-by-risk-level","text":"Specify the risk level in the search string: low : low risk level. medium : medium risk level. high : high risk level.","title":"Search by risk level"},{"location":"user-guides/search-and-filters/use-search/#search-by-vulnerability-identifier","text":"To search for a certain vulnerability, specify its identifier. It can be specified in two ways: either fully: WLRM-ABCD-X0123 or in abbreviated form: X0123","title":"Search by vulnerability identifier"},{"location":"user-guides/search-and-filters/use-search/#search-by-vulnerability-status","text":"Specify vulnerability status in the search string. Vulnerability can have one of the three statuses: open : currently relevant vulnerability closed : fixed vulnerability","title":"Search by vulnerability status"},{"location":"user-guides/search-and-filters/use-search/#search-by-event-time","text":"Specify time period in the search string. If the period is not specified, the search is conducted within the events that occurred during the last 24 hours. There are the following methods to specify the period: By date: 11/10/2020-11/14/2020 By date and time (seconds are disregarded): 11/10/2020 11:11 , 11:30-12:22 , 11/10/2020 11:12-01/14/2020 12:14 With relation to a certain moment of time: >11/10/20 Using string aliases: yesterday equal to yesterday's date today equal to today's date last <unit> equal to the period from the entire past unit start to current date and time week , month , year or the number of these units can be used as <unit> . For example: last week , last 3 month or last 3 months . this <unit> equal to current unit week , month , year can be used as <unit> . For example: this week will return events detected on Monday, Tuesday and Wednesday this week if today is Wednesday. Date and time format depends on the settings specified in your profile : MM/DD/YYYY if MDY is selected DD/MM/YYYY if DMY is selected 13:00 if 24\u2011hour is ticked 1pm if 24\u2011hour is unticked The month can be specified as both number and name: 01 , 1 , January , Jan for January. The year can be specified in both full form ( 2020 ) and shortened form ( 20 ). If the year is not specified in the date, then the current year is used.","title":"Search by event time"},{"location":"user-guides/search-and-filters/use-search/#search-by-ip-address","text":"To search by IP address, use the ip: prefix, after which you can specify A specific IP address, for example 192.168.0.1 \u2014in this case, all attacks and incidents will be found for which the source address of the attack corresponds to this IP address. An expression describing a range of IP addresses. A total number of IP addresses related to an attack or incident.","title":"Search by IP address"},{"location":"user-guides/search-and-filters/use-search/#search-by-ip-address-range","text":"To set a required range of IP addresses, you can use An explicit IP address range: 192.168.0.0-192.168.63.255 10.0.0.0-10.255.255.255 A part of an IP address: 192.168. \u2014equivalent to 192.168.0.0-192.168.255.255 . Redundant format with the * modifier is allowed\u2014 192.168.* 192.168.0. \u2014equivalent to 192.168.0.0-192.168.0.255 An IP address or part of it with a range of values inside the last octet in the expression: 192.168.1.0-255 \u2014equivalent to 192.168.1.0-192.168.1.255 192.168.0-255 \u2014equivalent to 192.168.0.0-192.168.255.255 Important When using a range of values within an octet, a dot is not set at the end. Subnet prefixes ( CIDR notation ): 192.168.1.0/24 \u2014equivalent to 192.168.1.0-192.168.1.255 192.168.0.0/17 \u2014equivalent to 192.168.0.1-192.168.127.255 Note You can combine the above methods for defining IP address ranges. To do this, list all the necessary ranges with the ip: prefix separately. Example : ip:192.168.0.0/24 ip:10.10. ip:10.0.10.0-128","title":"Search by IP address range"},{"location":"user-guides/search-and-filters/use-search/#search-by-number-of-ip-addresses","text":"It is possible to search by the total number of IP addresses that are related to an attack or an incident (only for attacks and incidents): ip:1000+ last month \u2014search for attacks and incidents over the past month for which the number of unique IP addresses is more than 1000 (equivalent to attacks incidents ip:1000+ last month ). xss ip:100+ \u2014search for all cross\u2011site scripting attacks and incidents. The search result will be empty if the number of attacking IP addresses (with the XSS attack type) is less than 100. xss p:id ip:100+ \u2014search for all XSS attacks and incidents related to the id parameter ( ?id=aaa ). This will return results only if the number of different IP addresses exceeds 100.","title":"Search by number of IP addresses"},{"location":"user-guides/search-and-filters/use-search/#search-by-the-data-center-the-ip-address-belongs-to","text":"To search by the data center, to which the IP address originated the attacks belongs, use the source: prefix. This attribute value can be: tor for the Tor network proxy for the public or web proxy server vpn for VPN aws for Amazon azure for Microsoft Azure gce for Google Cloud Platform","title":"Search by the data center the IP address belongs to"},{"location":"user-guides/search-and-filters/use-search/#search-by-the-country-in-which-the-ip-address-is-registered","text":"To search by the country, in which the IP address originated the attacks is registered, use the country: prefix. The country name should be passed to the attribute in the format corresponding to the standard ISO 3166-1 in uppercase or lowercase letters. For example: country:CN or country:cn for attacks originated from China.","title":"Search by the country in which the IP address is registered"},{"location":"user-guides/search-and-filters/use-search/#search-by-server-response-status","text":"To search by server response status, specify statuscode: prefix. Response status can be specified as: a number from 100 to 999. \u00abN\u2013M\u00bb range, where N and M are figures from 100 to 999. \u00abN+\u00bb and \u00abN-\u00bb ranges, where N is a number from 100 to 999.","title":"Search by server response status"},{"location":"user-guides/search-and-filters/use-search/#search-by-server-response-size","text":"To search by the server response size, use the s: or size: prefix. You can search for any integer value. Figures above 999 can be specified without a prefix. The \u00abN\u2013M\u00bb, \u00abN+\u00bb and \u00abN-\u00bb ranges can be specified, where figures above 999 can also be specified without a prefix.","title":"Search by server response size"},{"location":"user-guides/search-and-filters/use-search/#search-by-http-request-method","text":"To search by HTTP request method, specify the method: prefix. To search for GET , POST , PUT , DELETE , OPTIONS : if upper-case is used, then the search string can be specified without a prefix. For all other values, a prefix should be specified.","title":"Search by HTTP request method"},{"location":"user-guides/search-and-filters/use-search/#search-by-a-number-of-hits-within-attackincident","text":"To search attacks and incidents by a number of hits, specify the N: prefix. For example, you can search for attacks that have more than 100 hits: attacks N:>100 . Or search for attacks with less than 10 hits with attacks N:<10 .","title":"Search by a number of hits within attack/incident"},{"location":"user-guides/search-and-filters/use-search/#search-by-domain","text":"To search by domain, use the d: or domain: prefix. Any string, that may be a domain of the second or a higher level can be specified without a prefix. Any string can be specified with a prefix. You may use masks within a domain. The symbol * replaces any number of characters; the symbol ? replaces any single character.","title":"Search by domain"},{"location":"user-guides/search-and-filters/use-search/#search-by-path","text":"To search by path, use the u: or url: prefix. Strings that start with / are processed without a prefix. Any string can be specified with a prefix.","title":"Search by path"},{"location":"user-guides/search-and-filters/use-search/#search-by-application","text":"To search by the application to which the attack was sent or in which a vulnerability was found, use the pool: prefix. The attribute value is the application name set on the Applications tab in the Settings section. For example: pool:'Example application' .","title":"Search by application"},{"location":"user-guides/search-and-filters/use-search/#search-by-parameter","text":"To search by parameter, use the p: , param: , or parameter: prefix and also the = suffix. For example, if you need to find attacks aimed at the xss parameter but not at XSS-attacks (for instance, SQL-injection attack having xss in the GET-parameter), then specify attacks p:xss in the search string. A string that does not start with / and ends with = is considered to be a parameter (wherein the ending = character is not included in the value). Any string can be specified with a prefix.","title":"Search by parameter"},{"location":"user-guides/search-and-filters/use-search/#search-for-anomalies-in-attacks","text":"To search for anomalies in attacks, use the a: or anomaly: prefix. To refine an anomaly search, use the following parameters: size statuscode time stamps impression vector Example: attacks sqli a:size will search for all SQL-injection attacks, that have response size anomalies in their requests.","title":"Search for anomalies in attacks"},{"location":"user-guides/search-and-filters/use-search/#search-by-request-identifier","text":"To search for attacks and incidents by request identifier, specify the request_id prefix. The request_id parameter has the following value form: a79199bcea606040cc79f913325401fb . In order to make it easier to read, this parameter has been replaced by the placeholder abbreviation <requestId> in the examples below. Examples: attacks incidents request_id:<requestId> : to search for an attack or an incident with the request_id equal to <requestId> . attacks incidents !request_id:<requestId> : to search for attacks and incidents with the request_id not equal to <requestId> . attacks incidents request_id : to search for attacks and incidents with any request_id . attacks incidents !request_id : to search for attacks and incidents without any request_id .","title":"Search by request identifier"},{"location":"user-guides/settings/account/","text":"Checking Your Profile \u00b6 To see your profile data and settings, proceed to Settings \u2192 Profile tab. In your profile, you can check your account information. Your Email is shown in large font at the top of the page. Your Role in the Wallarm system \u2014 Admin , Analyst , or Deploy is shown right below your email. Name & phone . Date & time format : your preferred date and time format to be used in the Wallarm system. Email reports : your preferred reports frequency and reported events. Security : your last password data change and Two-Factor authentication status. Recent sign in : your sign-in history in the Wallarm system. You can click the Sign out button to log out of your Wallarm account. Configuring Your Profile \u00b6 Changing Your Name and Phone Number \u00b6 Click your current name. In the form that appears, enter your new first name, last name, and phone number. Click the Save button to apply changes. Changing Your Date & Time Format \u00b6 Click your current preferred date&time format. Pick the desired date&time format from the drop-down list. Select the checkbox 24-hour time to display time in the 24-hour format. Changing Your Personal Reports Settings \u00b6 Click your current personal reports settings. In the form that appears, select your desired report frequency and reported events. Click the Save button to apply changes. Changing Your Password \u00b6 Click the Change button. In the form that appears, enter your current password, your new password, and a new password confirmation. Click the Change password button Enabling Two-Factor Authentication \u00b6 You can use Google Authenticator (or similar apps supporting TOTP) to enable two-factor authentication. Install the Google Authenticator app ( Android , iOS ) or any compatible one. Click Enable in Two-Factor Authentication setting. Scan the QR code that appears (or click the manual entry link and use the manual entry option). Enter the 6-digit verification code generated by your app. Click Confirm . Whenever you sign in you will be prompted for your second factor code after passing the password prompt. Get this code from your Google Authenticator app. The password is required if you want to turn two-factor authentication off. Compatibility You can use any application or device that supports Time\u2011Based One\u2011Time Password Algorithm (RFC6238) to generate one\u2011time codes.","title":"Account"},{"location":"user-guides/settings/account/#checking-your-profile","text":"To see your profile data and settings, proceed to Settings \u2192 Profile tab. In your profile, you can check your account information. Your Email is shown in large font at the top of the page. Your Role in the Wallarm system \u2014 Admin , Analyst , or Deploy is shown right below your email. Name & phone . Date & time format : your preferred date and time format to be used in the Wallarm system. Email reports : your preferred reports frequency and reported events. Security : your last password data change and Two-Factor authentication status. Recent sign in : your sign-in history in the Wallarm system. You can click the Sign out button to log out of your Wallarm account.","title":"Checking Your Profile"},{"location":"user-guides/settings/account/#configuring-your-profile","text":"","title":"Configuring Your Profile"},{"location":"user-guides/settings/account/#changing-your-name-and-phone-number","text":"Click your current name. In the form that appears, enter your new first name, last name, and phone number. Click the Save button to apply changes.","title":"Changing Your Name and Phone Number"},{"location":"user-guides/settings/account/#changing-your-date-time-format","text":"Click your current preferred date&time format. Pick the desired date&time format from the drop-down list. Select the checkbox 24-hour time to display time in the 24-hour format.","title":"Changing Your Date &amp; Time Format"},{"location":"user-guides/settings/account/#changing-your-personal-reports-settings","text":"Click your current personal reports settings. In the form that appears, select your desired report frequency and reported events. Click the Save button to apply changes.","title":"Changing Your Personal Reports Settings"},{"location":"user-guides/settings/account/#changing-your-password","text":"Click the Change button. In the form that appears, enter your current password, your new password, and a new password confirmation. Click the Change password button","title":"Changing Your Password"},{"location":"user-guides/settings/account/#enabling-two-factor-authentication","text":"You can use Google Authenticator (or similar apps supporting TOTP) to enable two-factor authentication. Install the Google Authenticator app ( Android , iOS ) or any compatible one. Click Enable in Two-Factor Authentication setting. Scan the QR code that appears (or click the manual entry link and use the manual entry option). Enter the 6-digit verification code generated by your app. Click Confirm . Whenever you sign in you will be prompted for your second factor code after passing the password prompt. Get this code from your Google Authenticator app. The password is required if you want to turn two-factor authentication off. Compatibility You can use any application or device that supports Time\u2011Based One\u2011Time Password Algorithm (RFC6238) to generate one\u2011time codes.","title":"Enabling Two-Factor Authentication"},{"location":"user-guides/settings/applications/","text":"Application Settings \u00b6 Administrator access Only users with the Administrator role can access this setting. You can add applications on the Settings \u2192 Applications tab of the Wallarm interface. If your company has several web applications, you may find it convenient not only to view the statistics of the entire company's traffic and vulnerabilities but also to view the statistics separately for each application. You can set any arbitrary numeric value as an application ID. Adding an Application \u00b6 Click Add application . Set an application ID and an application name. In the filter node configuration file, set the created ID in the wallarm_instance directive. If the ID is unique, the Dashboard tab will let you select the new application. Managing Applications \u00b6 The Edit and Delete buttons appear upon hovering the cursor over the application entry. Edit : change the name of the corresponding application. Delete : remove the corresponding application entry.","title":"Applications"},{"location":"user-guides/settings/applications/#application-settings","text":"Administrator access Only users with the Administrator role can access this setting. You can add applications on the Settings \u2192 Applications tab of the Wallarm interface. If your company has several web applications, you may find it convenient not only to view the statistics of the entire company's traffic and vulnerabilities but also to view the statistics separately for each application. You can set any arbitrary numeric value as an application ID.","title":"Application Settings"},{"location":"user-guides/settings/applications/#adding-an-application","text":"Click Add application . Set an application ID and an application name. In the filter node configuration file, set the created ID in the wallarm_instance directive. If the ID is unique, the Dashboard tab will let you select the new application.","title":"Adding an Application"},{"location":"user-guides/settings/applications/#managing-applications","text":"The Edit and Delete buttons appear upon hovering the cursor over the application entry. Edit : change the name of the corresponding application. Delete : remove the corresponding application entry.","title":"Managing Applications"},{"location":"user-guides/settings/audit-log/","text":"User activity log \u00b6 Activity log overview \u00b6 On the Settings \u2192 Activity log tab of the Wallarm Console, you can check the history of user actions in the Wallarm system. The logs include information about creating, updating and deleting the following objects: IP address or subnet from the network perimeter Domains from the network perimeter Services (ports) from the network perimeter Domains and associated IP addresses from the network perimeter Two\u2011factor authentication Users Traffic processing rules Cloud WAF nodes Triggers Integrations Blocked IP address Hit sampling The logs also include information on the following actions and objects: Vulnerability marked as the false positive Rechecked attack Filter the activity log records \u00b6 The following parameters can be used to filter the activity log records: Case sensitive data on the user performed the action If the action was performed by the Wallarm technical support team, the username is Technical support . This value cannot be used to sort the activity log records. Action type Name of the object on which the action was performed Date when the action was performed","title":"Audit log"},{"location":"user-guides/settings/audit-log/#user-activity-log","text":"","title":"User activity log"},{"location":"user-guides/settings/audit-log/#activity-log-overview","text":"On the Settings \u2192 Activity log tab of the Wallarm Console, you can check the history of user actions in the Wallarm system. The logs include information about creating, updating and deleting the following objects: IP address or subnet from the network perimeter Domains from the network perimeter Services (ports) from the network perimeter Domains and associated IP addresses from the network perimeter Two\u2011factor authentication Users Traffic processing rules Cloud WAF nodes Triggers Integrations Blocked IP address Hit sampling The logs also include information on the following actions and objects: Vulnerability marked as the false positive Rechecked attack","title":"Activity log overview"},{"location":"user-guides/settings/audit-log/#filter-the-activity-log-records","text":"The following parameters can be used to filter the activity log records: Case sensitive data on the user performed the action If the action was performed by the Wallarm technical support team, the username is Technical support . This value cannot be used to sort the activity log records. Action type Name of the object on which the action was performed Date when the action was performed","title":"Filter the activity log records"},{"location":"user-guides/settings/general/","text":"General Settings \u00b6 The General tab of the Settings section allows users to switch between different Wallarm operation modes: Local settings (default) : this mode exploits settings from a filter node configuration file. Safe blocking : all malicious requests originated from greylisted IPs are blocked. Monitoring : all requests are processed, but none of them are blocked even if an attack is detected. Blocking : all requests where an attack was detected are blocked. To learn more about available configuration options, proceed to the link . Qrator Those Wallarm customers plugged in with Qrator traffic filters have the Blocking with Qrator setting. This setting enables automatic malicious requests blocking. The blocking is done with the Qrator IP blacklists. Wallarm transfers to Qrator the data on those IP addresses from which the attacks originated.","title":"General"},{"location":"user-guides/settings/general/#general-settings","text":"The General tab of the Settings section allows users to switch between different Wallarm operation modes: Local settings (default) : this mode exploits settings from a filter node configuration file. Safe blocking : all malicious requests originated from greylisted IPs are blocked. Monitoring : all requests are processed, but none of them are blocked even if an attack is detected. Blocking : all requests where an attack was detected are blocked. To learn more about available configuration options, proceed to the link . Qrator Those Wallarm customers plugged in with Qrator traffic filters have the Blocking with Qrator setting. This setting enables automatic malicious requests blocking. The blocking is done with the Qrator IP blacklists. Wallarm transfers to Qrator the data on those IP addresses from which the attacks originated.","title":"General Settings"},{"location":"user-guides/settings/subscriptions/","text":"Subscriptions \u00b6 The section Settings \u2192 Subscriptions of the Wallarm Console displays your Wallarm WAF subscription plan details. Paid subscription plan \u00b6 If you have the paid Wallarm WAF subscription plan, the subscription card displays the following information: Subscription name and status Subscription expiration date Statistics on the requests processed by the WAF node The set of modules and features included to the subscription plan To activate, cancel, or change a subscription, please send a request to sales@wallarm.com . Trial period \u00b6 If you have recently registered in the Wallarm Console and are using the trial version of Wallarm WAF, the subscription card displays the following information: Subscription name and status Subscription expiration date The trial period lasts 14 days and can be extended for 14 days more once. The button to extend the trial period The trial period lasts 14 days and can be extended for 14 days more once. If the trial period was already extended, the button is unavailable. Statistics on the requests processed by the WAF node The set of enabled modules and features Wallarm WAF trial provides the maximum set of modules and features that can be included in a paid subscription to Wallarm WAF. If you have the approved PoC plan with the Wallarm team, link to the PoC results document To get more inormation on the subscription to Wallarm WAF or to activate a paid subscription plan, please send a request to sales@wallarm.com .","title":"Subscriptions"},{"location":"user-guides/settings/subscriptions/#subscriptions","text":"The section Settings \u2192 Subscriptions of the Wallarm Console displays your Wallarm WAF subscription plan details.","title":"Subscriptions"},{"location":"user-guides/settings/subscriptions/#paid-subscription-plan","text":"If you have the paid Wallarm WAF subscription plan, the subscription card displays the following information: Subscription name and status Subscription expiration date Statistics on the requests processed by the WAF node The set of modules and features included to the subscription plan To activate, cancel, or change a subscription, please send a request to sales@wallarm.com .","title":"Paid subscription plan"},{"location":"user-guides/settings/subscriptions/#trial-period","text":"If you have recently registered in the Wallarm Console and are using the trial version of Wallarm WAF, the subscription card displays the following information: Subscription name and status Subscription expiration date The trial period lasts 14 days and can be extended for 14 days more once. The button to extend the trial period The trial period lasts 14 days and can be extended for 14 days more once. If the trial period was already extended, the button is unavailable. Statistics on the requests processed by the WAF node The set of enabled modules and features Wallarm WAF trial provides the maximum set of modules and features that can be included in a paid subscription to Wallarm WAF. If you have the approved PoC plan with the Wallarm team, link to the PoC results document To get more inormation on the subscription to Wallarm WAF or to activate a paid subscription plan, please send a request to sales@wallarm.com .","title":"Trial period"},{"location":"user-guides/settings/users/","text":"Configuring Users \u00b6 You can manage user accounts in the Users tab located in Settings . Administrator access Only users with the Administrator role can access this setting. User Roles \u00b6 Users of Wallarm clients can have the following roles: Administrator with access to all Wallarm WAF settings Analyst with access to view main Wallarm WAF settings, information about attacks, incidents and vulnerabilities Read Only with access to view main Wallarm WAF settings Deploy with access to create WAF nodes using the addnode script and with no access to Wallarm Console For Wallarm partners, global roles Global Administrator , Global Analyst , Global Read Only are also available. Global roles provide users with access to the partner account and linked client accounts, regular roles provide users with access only to the partner account. More detailed information about access of different user roles to Wallarm WAF entitites is provided in the table below. Entity management covers entity creating, editing, and deleting. Entity Administrator / Global Administrator Analyst / Global Analyst Read Only / Global Read Only Deploy WAF nodes View and manage View View Create using the addnode and addcloudnode scripts Dashboard View View View - Events View and manage View and manage View - Vulnerabilities View and manage View and manage View and manage - Scanner View and manage View and manage View - Triggers View and manage - - - Blacklist View, manage, and export View, manage, and export View and export - Rules View and manage View and manage View - WAF mode View and manage View View - Applications View and manage View View - Integrations View and manage - - - Users View and manage - View - Activity log View - View - Viewing Users \u00b6 You can view user lists in the following tabs: The main Users tab contains all users of your company registered in the Wallarm cloud. In this tab, any disabled users are highlighted in gray. The Disabled tab contains only disabled users. You can click the cells in the table header to sort users by name, role, email, and last login date. You also can choose one or several users by checking the checkboxes on the left of the usernames; therefore, you will be able to do operations on a group of users. Searching Users \u00b6 You can use the search field above the table to search users by name, email, or system role. Create a User \u00b6 In the Users tab of the Settings section, click the Add user button. Select the user role from the dropdown list. Enter a first and last name, email, and temporary password for the user. Click the Add user button. The new user will receive an automatic email with a link to login and set a new password. Change the User Info \u00b6 To change the data on the user, perform the following actions: In the Users tab of the Settings section, select the user to edit. Open the user actions menu by clicking the button to the right of the corresponding user. Click Edit user settings . In the form that appears, enter the new user info and click the Save button. The old user info will be replaced with the new. Two-Factor Authentication Settings Reset \u00b6 To reset the two-factor authentication settings, perform the following actions: In the Users tab of the Settings section, select the desired user. Open the user actions menu by clicking the button to the right of the corresponding user. Click Disable 2FA . In the form that appears, enter your Wallarm administrator account password and click the Disable 2FA button. The 2-factor authentication function will be disabled for the selected user. Disable Access for a User \u00b6 Disabling access for a user disables their Wallarm account. To disable a particular user\u2019s Wallarm account, perform the following actions: In the Users tab of the Settings section, select the desired user. Open the user actions menu by clicking the button to the right of the corresponding user. Click Disable Access . Now the selected user from your company will not be able to use their Wallarm account. If it is necessary to disable access for several user accounts, select the users whose access you need to revoke. The action panel will appear. Click the Disable Access button on this panel. Enable Access for a User \u00b6 Enabling access for a user enables their Wallarm account. To enable a particular user\u2019s Wallarm account, perform the following actions: In the Users tab of the Settings section, select the desired user with disabled access. Open the user actions menu by clicking the button to the right of the corresponding user. Click Enable Access . Now the selected user from your company will be able to use their Wallarm account. If it is necessary to enable access for several user accounts, select the users you need to grant access to. The action panel will appear. Click the Enable Access button on this panel. Delete a User \u00b6 To delete a particular user account, perform the following actions: In the Users tab of the Settings section, select the user to delete. Open the user actions menu by clicking the button to the right of the corresponding user. Click Delete . If it is necessary to delete several user accounts, select the users whose accounts you need to delete. The action panel will appear. Click the Delete button on this panel.","title":"Managing access to Wallarm Console"},{"location":"user-guides/settings/users/#configuring-users","text":"You can manage user accounts in the Users tab located in Settings . Administrator access Only users with the Administrator role can access this setting.","title":"Configuring Users"},{"location":"user-guides/settings/users/#user-roles","text":"Users of Wallarm clients can have the following roles: Administrator with access to all Wallarm WAF settings Analyst with access to view main Wallarm WAF settings, information about attacks, incidents and vulnerabilities Read Only with access to view main Wallarm WAF settings Deploy with access to create WAF nodes using the addnode script and with no access to Wallarm Console For Wallarm partners, global roles Global Administrator , Global Analyst , Global Read Only are also available. Global roles provide users with access to the partner account and linked client accounts, regular roles provide users with access only to the partner account. More detailed information about access of different user roles to Wallarm WAF entitites is provided in the table below. Entity management covers entity creating, editing, and deleting. Entity Administrator / Global Administrator Analyst / Global Analyst Read Only / Global Read Only Deploy WAF nodes View and manage View View Create using the addnode and addcloudnode scripts Dashboard View View View - Events View and manage View and manage View - Vulnerabilities View and manage View and manage View and manage - Scanner View and manage View and manage View - Triggers View and manage - - - Blacklist View, manage, and export View, manage, and export View and export - Rules View and manage View and manage View - WAF mode View and manage View View - Applications View and manage View View - Integrations View and manage - - - Users View and manage - View - Activity log View - View -","title":"User Roles"},{"location":"user-guides/settings/users/#viewing-users","text":"You can view user lists in the following tabs: The main Users tab contains all users of your company registered in the Wallarm cloud. In this tab, any disabled users are highlighted in gray. The Disabled tab contains only disabled users. You can click the cells in the table header to sort users by name, role, email, and last login date. You also can choose one or several users by checking the checkboxes on the left of the usernames; therefore, you will be able to do operations on a group of users.","title":"Viewing Users"},{"location":"user-guides/settings/users/#searching-users","text":"You can use the search field above the table to search users by name, email, or system role.","title":"Searching Users"},{"location":"user-guides/settings/users/#create-a-user","text":"In the Users tab of the Settings section, click the Add user button. Select the user role from the dropdown list. Enter a first and last name, email, and temporary password for the user. Click the Add user button. The new user will receive an automatic email with a link to login and set a new password.","title":"Create a User"},{"location":"user-guides/settings/users/#change-the-user-info","text":"To change the data on the user, perform the following actions: In the Users tab of the Settings section, select the user to edit. Open the user actions menu by clicking the button to the right of the corresponding user. Click Edit user settings . In the form that appears, enter the new user info and click the Save button. The old user info will be replaced with the new.","title":"Change the User Info"},{"location":"user-guides/settings/users/#two-factor-authentication-settings-reset","text":"To reset the two-factor authentication settings, perform the following actions: In the Users tab of the Settings section, select the desired user. Open the user actions menu by clicking the button to the right of the corresponding user. Click Disable 2FA . In the form that appears, enter your Wallarm administrator account password and click the Disable 2FA button. The 2-factor authentication function will be disabled for the selected user.","title":"Two-Factor Authentication Settings Reset"},{"location":"user-guides/settings/users/#disable-access-for-a-user","text":"Disabling access for a user disables their Wallarm account. To disable a particular user\u2019s Wallarm account, perform the following actions: In the Users tab of the Settings section, select the desired user. Open the user actions menu by clicking the button to the right of the corresponding user. Click Disable Access . Now the selected user from your company will not be able to use their Wallarm account. If it is necessary to disable access for several user accounts, select the users whose access you need to revoke. The action panel will appear. Click the Disable Access button on this panel.","title":"Disable Access for a User"},{"location":"user-guides/settings/users/#enable-access-for-a-user","text":"Enabling access for a user enables their Wallarm account. To enable a particular user\u2019s Wallarm account, perform the following actions: In the Users tab of the Settings section, select the desired user with disabled access. Open the user actions menu by clicking the button to the right of the corresponding user. Click Enable Access . Now the selected user from your company will be able to use their Wallarm account. If it is necessary to enable access for several user accounts, select the users you need to grant access to. The action panel will appear. Click the Enable Access button on this panel.","title":"Enable Access for a User"},{"location":"user-guides/settings/users/#delete-a-user","text":"To delete a particular user account, perform the following actions: In the Users tab of the Settings section, select the user to delete. Open the user actions menu by clicking the button to the right of the corresponding user. Click Delete . If it is necessary to delete several user accounts, select the users whose accounts you need to delete. The action panel will appear. Click the Delete button on this panel.","title":"Delete a User"},{"location":"user-guides/settings/integrations/email/","text":"Email Report \u00b6 You can set additional email addresses that will be used to deliver scheduled reports and instant notifications. Sending messages to your primary email is configured by default. Scheduled reports can be sent on a daily, weekly, or monthly basis. Reports include detailed information about vulnerabilities, attacks, and incidents detected in your system over the selected period. Notifications include brief details of triggered events: System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains Setting up integration \u00b6 Open Settings \u2192 Integrations tab. Click the Email report block or click the Add integration button and choose Email report . Enter an integration name. Enter email addresses using a comma as a separator. Choose the frequency of sending security reports. If the frequency is not chosen, then reports will not be sent. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration . Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test notification example: Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Email Report"},{"location":"user-guides/settings/integrations/email/#email-report","text":"You can set additional email addresses that will be used to deliver scheduled reports and instant notifications. Sending messages to your primary email is configured by default. Scheduled reports can be sent on a daily, weekly, or monthly basis. Reports include detailed information about vulnerabilities, attacks, and incidents detected in your system over the selected period. Notifications include brief details of triggered events: System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains","title":"Email Report"},{"location":"user-guides/settings/integrations/email/#setting-up-integration","text":"Open Settings \u2192 Integrations tab. Click the Email report block or click the Add integration button and choose Email report . Enter an integration name. Enter email addresses using a comma as a separator. Choose the frequency of sending security reports. If the frequency is not chosen, then reports will not be sent. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration .","title":"Setting up integration"},{"location":"user-guides/settings/integrations/email/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test notification example:","title":"Testing integration"},{"location":"user-guides/settings/integrations/email/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/email/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/email/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting integration"},{"location":"user-guides/settings/integrations/insightconnect/","text":"InsightConnect \u00b6 You can set up Wallarm to send notifications to InsightConnect when the following events are triggered: Hits detected System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains Setting up integration \u00b6 First, generate and copy an API key as follows: Open the InsightConnect's UI \u2192 Settings \u2192 API Keys page and click New User Key . Enter an API key name (e.g. Wallarm API ) and click Generate . Copy the generated API key. Go to Wallarm UI \u2192 Settings \u2192 Integrations in the EU or US cloud and click InsightConnect . Paste the API key that you copied before into the API key field. Secondly, generate and copy an API URL as follows: Go back to the InsightConnect's UI, open the Automation \u2192 Workflows page and create a new workflow for the Wallarm notification. When asked to choose a trigger, choose the API Trigger . Copy the generated URL. Go back to Wallarm UI \u2192 InsightConnect configuration and paste the API URL that you copied before into the API URL field. Thirdly, finish the setup in Wallarm UI: Enter an integration name. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration . Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test InsightConnect message: Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"InsightConnect"},{"location":"user-guides/settings/integrations/insightconnect/#insightconnect","text":"You can set up Wallarm to send notifications to InsightConnect when the following events are triggered: Hits detected System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains","title":"InsightConnect"},{"location":"user-guides/settings/integrations/insightconnect/#setting-up-integration","text":"First, generate and copy an API key as follows: Open the InsightConnect's UI \u2192 Settings \u2192 API Keys page and click New User Key . Enter an API key name (e.g. Wallarm API ) and click Generate . Copy the generated API key. Go to Wallarm UI \u2192 Settings \u2192 Integrations in the EU or US cloud and click InsightConnect . Paste the API key that you copied before into the API key field. Secondly, generate and copy an API URL as follows: Go back to the InsightConnect's UI, open the Automation \u2192 Workflows page and create a new workflow for the Wallarm notification. When asked to choose a trigger, choose the API Trigger . Copy the generated URL. Go back to Wallarm UI \u2192 InsightConnect configuration and paste the API URL that you copied before into the API URL field. Thirdly, finish the setup in Wallarm UI: Enter an integration name. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration .","title":"Setting up integration"},{"location":"user-guides/settings/integrations/insightconnect/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test InsightConnect message:","title":"Testing integration"},{"location":"user-guides/settings/integrations/insightconnect/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/insightconnect/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/insightconnect/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting integration"},{"location":"user-guides/settings/integrations/integrations-intro/","text":"Integrations Overview \u00b6 The Settings \u2192 Integrations tab allows you to integrate with different systems to get scheduled reports and instant notifications through them: Scheduled reports can be sent on a daily, weekly, or monthly basis. Reports include detailed information about vulnerabilities, attacks, and incidents detected in your system over the selected period. Notifications are sent when vulnerabilities, hits, scope changes, or system related events are detected in your system. Notifications include brief details of detected activity. Administrator access The integration setup is available only for users with the Administrator role. Integration types \u00b6 The systems available for integration are grouped in the following blocks: Email and messengers , Incident management and SIEM systems and Other systems . Email and messengers \u00b6 Personal email \u2014 the reports and notifications that are sent to the email indicated upon registration. You can also configure these notifications on the Profile tab. Email report Slack Telegram Incident management and SIEM systems \u00b6 Opsgenie InsightConnect PagerDuty Splunk Sumo Logic Other systems \u00b6 Webhook to integrate with any system that accepts incoming webhooks via HTTPS protocol. For example: With Fluentd configured to forward logs to IBM QRadar , Splunk Enterprise , ArcSight Logger With Logstash configured to forward logs to IBM QRadar , Splunk Enterprise , ArcSight Logger Adding an integration \u00b6 To add a new integration, click the icon of the unconfigured system on the All tab or click the Add integration button and select the required system. Further steps are described in the selected system instructions. The number of integrations with one system is not limited. For example: to send security reports to 3 Slack channels, you can create 3 different integrations with Slack. Filtering integrations \u00b6 To filter displayed integrations, you can use the tabs: All with enabled, disabled, and not yet configured integrations Enabled with active configured integrations Disabled with disabled configured integrations Advanced notifications setup For advanced notification setup, you can use triggers . Retrying failed requests \u00b6 Notifications to the system are sent via requests. If the system responds to Wallarm request with any code other than 2xx , Wallarm resends the request with the interval until the 2xx code is received: The first cycle intervals: 1, 3, 5, 10, 10 seconds The second cycle intervals: 0, 1, 3, 5, 30 seconds The third cycle intervals: 1, 1, 3, 5, 10, 30 minutes If the percentage of unsuccessful requests reaches 60% in 12 hours, the integration is automatically disabled. A notification about disabled integration is sent to the configured systems and to the email of the account administrators. Demo videos \u00b6","title":"Integrations Overview"},{"location":"user-guides/settings/integrations/integrations-intro/#integrations-overview","text":"The Settings \u2192 Integrations tab allows you to integrate with different systems to get scheduled reports and instant notifications through them: Scheduled reports can be sent on a daily, weekly, or monthly basis. Reports include detailed information about vulnerabilities, attacks, and incidents detected in your system over the selected period. Notifications are sent when vulnerabilities, hits, scope changes, or system related events are detected in your system. Notifications include brief details of detected activity. Administrator access The integration setup is available only for users with the Administrator role.","title":"Integrations Overview"},{"location":"user-guides/settings/integrations/integrations-intro/#integration-types","text":"The systems available for integration are grouped in the following blocks: Email and messengers , Incident management and SIEM systems and Other systems .","title":"Integration types"},{"location":"user-guides/settings/integrations/integrations-intro/#email-and-messengers","text":"Personal email \u2014 the reports and notifications that are sent to the email indicated upon registration. You can also configure these notifications on the Profile tab. Email report Slack Telegram","title":"Email and messengers"},{"location":"user-guides/settings/integrations/integrations-intro/#incident-management-and-siem-systems","text":"Opsgenie InsightConnect PagerDuty Splunk Sumo Logic","title":"Incident management and SIEM systems"},{"location":"user-guides/settings/integrations/integrations-intro/#other-systems","text":"Webhook to integrate with any system that accepts incoming webhooks via HTTPS protocol. For example: With Fluentd configured to forward logs to IBM QRadar , Splunk Enterprise , ArcSight Logger With Logstash configured to forward logs to IBM QRadar , Splunk Enterprise , ArcSight Logger","title":"Other systems"},{"location":"user-guides/settings/integrations/integrations-intro/#adding-an-integration","text":"To add a new integration, click the icon of the unconfigured system on the All tab or click the Add integration button and select the required system. Further steps are described in the selected system instructions. The number of integrations with one system is not limited. For example: to send security reports to 3 Slack channels, you can create 3 different integrations with Slack.","title":"Adding an integration"},{"location":"user-guides/settings/integrations/integrations-intro/#filtering-integrations","text":"To filter displayed integrations, you can use the tabs: All with enabled, disabled, and not yet configured integrations Enabled with active configured integrations Disabled with disabled configured integrations Advanced notifications setup For advanced notification setup, you can use triggers .","title":"Filtering integrations"},{"location":"user-guides/settings/integrations/integrations-intro/#retrying-failed-requests","text":"Notifications to the system are sent via requests. If the system responds to Wallarm request with any code other than 2xx , Wallarm resends the request with the interval until the 2xx code is received: The first cycle intervals: 1, 3, 5, 10, 10 seconds The second cycle intervals: 0, 1, 3, 5, 30 seconds The third cycle intervals: 1, 1, 3, 5, 10, 30 minutes If the percentage of unsuccessful requests reaches 60% in 12 hours, the integration is automatically disabled. A notification about disabled integration is sent to the configured systems and to the email of the account administrators.","title":"Retrying failed requests"},{"location":"user-guides/settings/integrations/integrations-intro/#demo-videos","text":"","title":"Demo videos"},{"location":"user-guides/settings/integrations/opsgenie/","text":"Opsgenie \u00b6 You can set up Wallarm to send alerts to Opsgenie when the following events are triggered: Vulnerabilities detected Setting up integration \u00b6 In Opsgenie UI : Go to your team \u279d Integrations . Click the Add integration button and choose API . Enter the name for a new integration and click Save Integration . Copy the provided API key. In Wallarm UI: Open Settings \u2192 Integrations tab. Click the Opsgenie block or click the Add integration button and choose Opsgenie . Enter an integration name. Paste the copied API key to the API key field. If using the EU instance of Opsgenie, select the appropriate Opsgenie API endpoint from the list. By default, the US instance endpoint is set. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration . Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test Opsgenie notification: Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Opsgenie"},{"location":"user-guides/settings/integrations/opsgenie/#opsgenie","text":"You can set up Wallarm to send alerts to Opsgenie when the following events are triggered: Vulnerabilities detected","title":"Opsgenie"},{"location":"user-guides/settings/integrations/opsgenie/#setting-up-integration","text":"In Opsgenie UI : Go to your team \u279d Integrations . Click the Add integration button and choose API . Enter the name for a new integration and click Save Integration . Copy the provided API key. In Wallarm UI: Open Settings \u2192 Integrations tab. Click the Opsgenie block or click the Add integration button and choose Opsgenie . Enter an integration name. Paste the copied API key to the API key field. If using the EU instance of Opsgenie, select the appropriate Opsgenie API endpoint from the list. By default, the US instance endpoint is set. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration .","title":"Setting up integration"},{"location":"user-guides/settings/integrations/opsgenie/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test Opsgenie notification:","title":"Testing integration"},{"location":"user-guides/settings/integrations/opsgenie/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/opsgenie/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/opsgenie/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting integration"},{"location":"user-guides/settings/integrations/pagerduty/","text":"PagerDuty \u00b6 You can set up Wallarm to send incidents to PagerDuty when the following events are triggered: System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains Setting up integration \u00b6 In PagerDuty UI, set up an integration for any existing service or create a new service specifically for Wallarm: Go to Configuration \u2192 Services . Open the settings of the existing service or click the New Service button. Create a new integration: If you are configuring integrations of the existing service, go to the Integrations tab and click the New Integration button. If you are creating a new service, enter the service name and proceed to the Integration Settings section. Enter the integration name and select the Use our API directly option as an integration type. Save the settings: If you are configuring integrations of the existing service, click the Add Integration button. If you are creating a new service, configure the rest of the settings sections and click the Add Service button. Copy the provided Integration Key . In Wallarm UI: Open Settings \u2192 Integrations tab. Click the PagerDuty block or click the Add integration button and choose PagerDuty . Enter an integration name. Paste the Integration Key value into the appropriate field. Choose event types to trigger notifications. If the events are not chosen, PagerDuty incidents will not be added. Test the integration and ensure the settings are correct. Click Add integration . Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test PagerDuty notification: Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"PagerDuty"},{"location":"user-guides/settings/integrations/pagerduty/#pagerduty","text":"You can set up Wallarm to send incidents to PagerDuty when the following events are triggered: System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains","title":"PagerDuty"},{"location":"user-guides/settings/integrations/pagerduty/#setting-up-integration","text":"In PagerDuty UI, set up an integration for any existing service or create a new service specifically for Wallarm: Go to Configuration \u2192 Services . Open the settings of the existing service or click the New Service button. Create a new integration: If you are configuring integrations of the existing service, go to the Integrations tab and click the New Integration button. If you are creating a new service, enter the service name and proceed to the Integration Settings section. Enter the integration name and select the Use our API directly option as an integration type. Save the settings: If you are configuring integrations of the existing service, click the Add Integration button. If you are creating a new service, configure the rest of the settings sections and click the Add Service button. Copy the provided Integration Key . In Wallarm UI: Open Settings \u2192 Integrations tab. Click the PagerDuty block or click the Add integration button and choose PagerDuty . Enter an integration name. Paste the Integration Key value into the appropriate field. Choose event types to trigger notifications. If the events are not chosen, PagerDuty incidents will not be added. Test the integration and ensure the settings are correct. Click Add integration .","title":"Setting up integration"},{"location":"user-guides/settings/integrations/pagerduty/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test PagerDuty notification:","title":"Testing integration"},{"location":"user-guides/settings/integrations/pagerduty/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/pagerduty/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/pagerduty/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting integration"},{"location":"user-guides/settings/integrations/slack/","text":"Slack \u00b6 You can set up Wallarm to send notifications to your Slack channel when the following events are triggered: System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains Setting up integration \u00b6 Open the Settings \u2192 Integrations tab. Click the Slack block or click the Add integration button and choose Slack . Enter an integration name. Open Webhook settings in Slack and add a new Webhook choosing the channel to post messages to. Copy the provided Webhook URL and paste the value to the Webhook URL field in Wallarm UI. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration . Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test Slack message from the user wallarm : [Test message] [Test partner] Network perimeter has changed Notification type: new_scope_object_ips New IP addresses were discovered in the network perimeter: 8.8.8.8 Client: TestCompany Cloud: EU Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Slack"},{"location":"user-guides/settings/integrations/slack/#slack","text":"You can set up Wallarm to send notifications to your Slack channel when the following events are triggered: System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains","title":"Slack"},{"location":"user-guides/settings/integrations/slack/#setting-up-integration","text":"Open the Settings \u2192 Integrations tab. Click the Slack block or click the Add integration button and choose Slack . Enter an integration name. Open Webhook settings in Slack and add a new Webhook choosing the channel to post messages to. Copy the provided Webhook URL and paste the value to the Webhook URL field in Wallarm UI. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration .","title":"Setting up integration"},{"location":"user-guides/settings/integrations/slack/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test Slack message from the user wallarm : [Test message] [Test partner] Network perimeter has changed Notification type: new_scope_object_ips New IP addresses were discovered in the network perimeter: 8.8.8.8 Client: TestCompany Cloud: EU","title":"Testing integration"},{"location":"user-guides/settings/integrations/slack/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/slack/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/slack/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting integration"},{"location":"user-guides/settings/integrations/splunk/","text":"Splunk \u00b6 You can set up Wallarm to send alerts to Splunk when the following events are triggered: Hits detected System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains Setting up integration \u00b6 In Splunk UI: Open the Settings \u279d Add Data page and select Monitor . Select the HTTP Event Collector option, enter an integration name and click Next . Skip choosing the data type at the Input Settings page and continue to Review Settings . Review and Submit the settings. Copy the provided token. In Wallarm UI: Open Settings \u2192 Integrations tab. Click the Splunk block or click the Add integration button and choose Splunk . Enter an integration name. Paste the copied token into the HEC token field. Paste HEC URI and the port number of your Splunk instance into the HEC URI:PORT field. For example: https://hec.splunk.com:8088 . Choose event types to trigger notifications. If the events are not chosen, then Splunk alerts will not be sent. Test the integration and ensure the settings are correct. Click Add integration . Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test Splunk notification in the JSON format: { summary : \"[Test message] [Test partner(US)] New vulnerability detected\" , descrip t io n : \"Notification type: vuln New vulnerability was detected in your system. ID: Title: Test Domain: example.com Path: Method: Discovered by: Parameter: Type: Info Threat: Medium More details: https://us1.my.wallarm.com/object/555 Client: TestCompany Cloud: US \" , de ta ils :{ clie nt _ na me : \"TestCompany\" , cloud : \"US\" , n o t i f ica t io n _ t ype : \"vuln\" , vul n _li n k : \"https://us1.my.wallarm.com/object/555\" , vul n :{ domai n : \"example.com\" , id : null , me t hod : null , parame ter : null , pa t h : null , t i tle : \"Test\" , discovered_by : null , t hrea t : \"Medium\" , t ype : \"Info\" } } } Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Splunk"},{"location":"user-guides/settings/integrations/splunk/#splunk","text":"You can set up Wallarm to send alerts to Splunk when the following events are triggered: Hits detected System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains","title":"Splunk"},{"location":"user-guides/settings/integrations/splunk/#setting-up-integration","text":"In Splunk UI: Open the Settings \u279d Add Data page and select Monitor . Select the HTTP Event Collector option, enter an integration name and click Next . Skip choosing the data type at the Input Settings page and continue to Review Settings . Review and Submit the settings. Copy the provided token. In Wallarm UI: Open Settings \u2192 Integrations tab. Click the Splunk block or click the Add integration button and choose Splunk . Enter an integration name. Paste the copied token into the HEC token field. Paste HEC URI and the port number of your Splunk instance into the HEC URI:PORT field. For example: https://hec.splunk.com:8088 . Choose event types to trigger notifications. If the events are not chosen, then Splunk alerts will not be sent. Test the integration and ensure the settings are correct. Click Add integration .","title":"Setting up integration"},{"location":"user-guides/settings/integrations/splunk/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test Splunk notification in the JSON format: { summary : \"[Test message] [Test partner(US)] New vulnerability detected\" , descrip t io n : \"Notification type: vuln New vulnerability was detected in your system. ID: Title: Test Domain: example.com Path: Method: Discovered by: Parameter: Type: Info Threat: Medium More details: https://us1.my.wallarm.com/object/555 Client: TestCompany Cloud: US \" , de ta ils :{ clie nt _ na me : \"TestCompany\" , cloud : \"US\" , n o t i f ica t io n _ t ype : \"vuln\" , vul n _li n k : \"https://us1.my.wallarm.com/object/555\" , vul n :{ domai n : \"example.com\" , id : null , me t hod : null , parame ter : null , pa t h : null , t i tle : \"Test\" , discovered_by : null , t hrea t : \"Medium\" , t ype : \"Info\" } } }","title":"Testing integration"},{"location":"user-guides/settings/integrations/splunk/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/splunk/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/splunk/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting integration"},{"location":"user-guides/settings/integrations/sumologic/","text":"Sumo Logic \u00b6 You can set up Wallarm to send messages to Sumo Logic when the following events are triggered: Hits detected System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains Setting up integration \u00b6 In Sumo Logic UI: Configure a Hosted Collector following the instructions . Configure an HTTP Logs & Metrics Source following the instructions . Copy the provided HTTP Source Address (URL) . In Wallarm UI: Open the Settings \u2192 Integrations tab. Click the Sumo Logic block or click the Add integration button and choose Sumo Logic . Enter an integration name. Paste the copied value of HTTP Source Address (URL) to the HTTP Source Address (URL) field. Choose event types to trigger sending messages to Sumo Logic. If the events are not chosen, then messages will not be sent. Test the integration and ensure the settings are correct. Click Add integration . Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test Sumo Logic notification: { summary : \"[Test message] [Test partner(US)] New vulnerability detected\" , descrip t io n : \"Notification type: vuln New vulnerability was detected in your system. ID: Title: Test Domain: example.com Path: Method: Discovered by: Parameter: Type: Info Threat: Medium More details: https://us1.my.wallarm.com/object/555 Client: TestCompany Cloud: US \" , de ta ils :{ clie nt _ na me : \"TestCompany\" , cloud : \"US\" , n o t i f ica t io n _ t ype : \"vuln\" , vul n _li n k : \"https://us1.my.wallarm.com/object/555\" , vul n :{ domai n : \"example.com\" , id : null , me t hod : null , parame ter : null , pa t h : null , t i tle : \"Test\" , discovered_by : null , t hrea t : \"Medium\" , t ype : \"Info\" } } } Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Sumo Logic"},{"location":"user-guides/settings/integrations/sumologic/#sumo-logic","text":"You can set up Wallarm to send messages to Sumo Logic when the following events are triggered: Hits detected System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains","title":"Sumo Logic"},{"location":"user-guides/settings/integrations/sumologic/#setting-up-integration","text":"In Sumo Logic UI: Configure a Hosted Collector following the instructions . Configure an HTTP Logs & Metrics Source following the instructions . Copy the provided HTTP Source Address (URL) . In Wallarm UI: Open the Settings \u2192 Integrations tab. Click the Sumo Logic block or click the Add integration button and choose Sumo Logic . Enter an integration name. Paste the copied value of HTTP Source Address (URL) to the HTTP Source Address (URL) field. Choose event types to trigger sending messages to Sumo Logic. If the events are not chosen, then messages will not be sent. Test the integration and ensure the settings are correct. Click Add integration .","title":"Setting up integration"},{"location":"user-guides/settings/integrations/sumologic/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test Sumo Logic notification: { summary : \"[Test message] [Test partner(US)] New vulnerability detected\" , descrip t io n : \"Notification type: vuln New vulnerability was detected in your system. ID: Title: Test Domain: example.com Path: Method: Discovered by: Parameter: Type: Info Threat: Medium More details: https://us1.my.wallarm.com/object/555 Client: TestCompany Cloud: US \" , de ta ils :{ clie nt _ na me : \"TestCompany\" , cloud : \"US\" , n o t i f ica t io n _ t ype : \"vuln\" , vul n _li n k : \"https://us1.my.wallarm.com/object/555\" , vul n :{ domai n : \"example.com\" , id : null , me t hod : null , parame ter : null , pa t h : null , t i tle : \"Test\" , discovered_by : null , t hrea t : \"Medium\" , t ype : \"Info\" } } }","title":"Testing integration"},{"location":"user-guides/settings/integrations/sumologic/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/sumologic/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/sumologic/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting integration"},{"location":"user-guides/settings/integrations/telegram/","text":"Telegram \u00b6 You can set up Wallarm to send scheduled reports and instant notifications to Telegram. Scheduled reports can be sent on a daily, weekly, or monthly basis. Reports include detailed information about vulnerabilities, attacks, and incidents detected in your system over the selected period. Notifications include brief details of triggered events: System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains Setting up integration \u00b6 Open the Settings \u2192 Integrations tab. Click the Telegram block or click the Add integration button and choose Telegram . Add @WallarmBot (if you are using the Wallarm EU Cloud) or @WallarmUSBot (if you are using the Wallarm US Cloud) to the Telegram group receiving Wallarm notifications and follow the authentication link. After redirection to Wallarm UI, authenticate the bot. Enter an integration name. Choose the frequency of sending security reports. If the frequency is not chosen, then reports will not be sent. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration . You can also start the chat with @WallarmBot or @WallarmUSBot directly. The bot will send reports and notifications as well. Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. The integration with Telegram can be tested only if this integration is already created. Test Telegram message: [Test message] [Test partner] Network perimeter has changed Notification type: new_scope_object_ips New IP addresses were discovered in the network perimeter: 8.8.8.8 Client: TestCompany Cloud: EU Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Telegram"},{"location":"user-guides/settings/integrations/telegram/#telegram","text":"You can set up Wallarm to send scheduled reports and instant notifications to Telegram. Scheduled reports can be sent on a daily, weekly, or monthly basis. Reports include detailed information about vulnerabilities, attacks, and incidents detected in your system over the selected period. Notifications include brief details of triggered events: System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains","title":"Telegram"},{"location":"user-guides/settings/integrations/telegram/#setting-up-integration","text":"Open the Settings \u2192 Integrations tab. Click the Telegram block or click the Add integration button and choose Telegram . Add @WallarmBot (if you are using the Wallarm EU Cloud) or @WallarmUSBot (if you are using the Wallarm US Cloud) to the Telegram group receiving Wallarm notifications and follow the authentication link. After redirection to Wallarm UI, authenticate the bot. Enter an integration name. Choose the frequency of sending security reports. If the frequency is not chosen, then reports will not be sent. Choose event types to trigger notifications. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration . You can also start the chat with @WallarmBot or @WallarmUSBot directly. The bot will send reports and notifications as well.","title":"Setting up integration"},{"location":"user-guides/settings/integrations/telegram/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. The integration with Telegram can be tested only if this integration is already created. Test Telegram message: [Test message] [Test partner] Network perimeter has changed Notification type: new_scope_object_ips New IP addresses were discovered in the network perimeter: 8.8.8.8 Client: TestCompany Cloud: EU","title":"Testing integration"},{"location":"user-guides/settings/integrations/telegram/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/telegram/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/telegram/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting integration"},{"location":"user-guides/settings/integrations/webhook/","text":"Webhook \u00b6 You can set up Wallarm to send instant notifications to any system that accepts incoming webhooks via HTTPS protocol. For this, specify Webhook URL to receive the notifications for the following event types: Hits detected System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains Notification format \u00b6 Notifications are sent in JSON format. The set of JSON objects depend on the event for which the notification is sent. For example: Hit detected [ { \"summary\" : \"[Wallarm] New hit detected\" , \"details\" : { \"client_name\" : \"TestCompany\" , \"cloud\" : \"EU\" , \"notification_type\" : \"new_hits\" , \"hit\" : { \"domain\" : \"www.example.com\" , \"heur_distance\" : 0.01111 , \"method\" : \"POST\" , \"parameter\" : \"SOME_value\" , \"path\" : \"/news/some_path\" , \"payloads\" : [ \"say ni\" ], \"point\" : [ \"post\" ], \"probability\" : 0.01 , \"remote_country\" : \"PL\" , \"remote_port\" : 0 , \"remote_addr4\" : \"8.8.8.8\" , \"remote_addr6\" : \"\" , \"tor\" : \"none\" , \"request_time\" : 1603834606 , \"create_time\" : 1603834608 , \"response_len\" : 14 , \"response_status\" : 200 , \"response_time\" : 5 , \"stamps\" : [ 1111 ], \"regex\" : [], \"stamps_hash\" : -22222 , \"regex_hash\" : -33333 , \"type\" : \"sqli\" , \"block_status\" : \"monitored\" , \"id\" : [ \"hits_production_999_202010_v_1\" , \"c2dd33831a13be0d_AC9\" ], \"object_type\" : \"hit\" , \"anomaly\" : 0 } } } ] Vulnerability detected [ { summary : \"[Wallarm] New vulnerability detected\" , descrip t io n : \"Notification type: vuln New vulnerability was detected in your system. ID: Title: Test Domain: example.com Path: Method: Discovered by: Parameter: Type: Info Threat: Medium More details: https://us1.my.wallarm.com/object/555 Client: TestCompany Cloud: US \" , de ta ils :{ clie nt _ na me : \"TestCompany\" , cloud : \"US\" , n o t i f ica t io n _ t ype : \"vuln\" , vul n _li n k : \"https://us1.my.wallarm.com/object/555\" , vul n :{ domai n : \"example.com\" , id : null , me t hod : null , parame ter : null , pa t h : null , t i tle : \"Test\" , discovered_by : null , t hrea t : \"Medium\" , t ype : \"Info\" } } } ] Setting up integration \u00b6 Open Wallarm UI \u2192 Settings \u2192 Integrations . Click the Webhook block or click the Add integration button and choose Webhook . Enter an integration name. Enter target Webhook URL. If required, configure advanced settings: Request method : POST or PUT . By default, POST requests are sent. Request header and its value if the server requires a non-standard header to execute the request. The number of headers is not limited. CA certificate : add a self-signed certificate of your own CA that signed a server certificate (only if the webhook server uses a self-signed TLS certificate). If the webhook server uses a TLS certificate signed by a trusted CA, you can optionally add a trusted CA certificate. Verify the certificate : this setting allows to disable verification of the webhook server certificate. By default, Wallarm verifies whether a webhook server certificate is signed by a publicly trusted CA. We do not recommend disabling the verification of production server certificates. If your webhook server uses a self-signed TLS certificate, you can specify a self-signed CA certificate in the appropriate field to allow sending requests to this server. Request timeout, in seconds : if the server does not respond to the request within the specified time, the request fails. By default: 15 seconds. Connection timeout, in seconds : if the connection to the server cannot be established during the specified time, the request fails. By default: 20 seconds. Choose event types to trigger sending notifications to Webhook URL. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration . Examples of integrations \u00b6 Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status We described some examples of how to configure the integration with the popular log collectors forwarding logs to the SIEM systems: With Fluentd configured to forward logs to IBM QRadar , Splunk Enterprise , ArcSight Logger With Logstash configured to forward logs to IBM QRadar , Splunk Enterprise , ArcSight Logger Testing integration \u00b6 Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test webhook example: [ { summary : \"[Test message] [Test partner(US)] New vulnerability detected\" , descrip t io n : \"Notification type: vuln New vulnerability was detected in your system. ID: Title: Test Domain: example.com Path: Method: Discovered by: Parameter: Type: Info Threat: Medium More details: https://us1.my.wallarm.com/object/555 Client: TestCompany Cloud: US \" , de ta ils :{ clie nt _ na me : \"TestCompany\" , cloud : \"US\" , n o t i f ica t io n _ t ype : \"vuln\" , vul n _li n k : \"https://us1.my.wallarm.com/object/555\" , vul n :{ domai n : \"example.com\" , id : null , me t hod : null , parame ter : null , pa t h : null , t i tle : \"Test\" , discovered_by : null , t hrea t : \"Medium\" , t ype : \"Info\" } } } ] Updating integration \u00b6 To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save . Disabling integration \u00b6 To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable . Deleting Integration \u00b6 To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Webhook Integration Overview"},{"location":"user-guides/settings/integrations/webhook/#webhook","text":"You can set up Wallarm to send instant notifications to any system that accepts incoming webhooks via HTTPS protocol. For this, specify Webhook URL to receive the notifications for the following event types: Hits detected System related: newly added users, deleted or disabled integration Vulnerabilities detected Scope changed: updates in hosts, services, and domains","title":"Webhook"},{"location":"user-guides/settings/integrations/webhook/#notification-format","text":"Notifications are sent in JSON format. The set of JSON objects depend on the event for which the notification is sent. For example: Hit detected [ { \"summary\" : \"[Wallarm] New hit detected\" , \"details\" : { \"client_name\" : \"TestCompany\" , \"cloud\" : \"EU\" , \"notification_type\" : \"new_hits\" , \"hit\" : { \"domain\" : \"www.example.com\" , \"heur_distance\" : 0.01111 , \"method\" : \"POST\" , \"parameter\" : \"SOME_value\" , \"path\" : \"/news/some_path\" , \"payloads\" : [ \"say ni\" ], \"point\" : [ \"post\" ], \"probability\" : 0.01 , \"remote_country\" : \"PL\" , \"remote_port\" : 0 , \"remote_addr4\" : \"8.8.8.8\" , \"remote_addr6\" : \"\" , \"tor\" : \"none\" , \"request_time\" : 1603834606 , \"create_time\" : 1603834608 , \"response_len\" : 14 , \"response_status\" : 200 , \"response_time\" : 5 , \"stamps\" : [ 1111 ], \"regex\" : [], \"stamps_hash\" : -22222 , \"regex_hash\" : -33333 , \"type\" : \"sqli\" , \"block_status\" : \"monitored\" , \"id\" : [ \"hits_production_999_202010_v_1\" , \"c2dd33831a13be0d_AC9\" ], \"object_type\" : \"hit\" , \"anomaly\" : 0 } } } ] Vulnerability detected [ { summary : \"[Wallarm] New vulnerability detected\" , descrip t io n : \"Notification type: vuln New vulnerability was detected in your system. ID: Title: Test Domain: example.com Path: Method: Discovered by: Parameter: Type: Info Threat: Medium More details: https://us1.my.wallarm.com/object/555 Client: TestCompany Cloud: US \" , de ta ils :{ clie nt _ na me : \"TestCompany\" , cloud : \"US\" , n o t i f ica t io n _ t ype : \"vuln\" , vul n _li n k : \"https://us1.my.wallarm.com/object/555\" , vul n :{ domai n : \"example.com\" , id : null , me t hod : null , parame ter : null , pa t h : null , t i tle : \"Test\" , discovered_by : null , t hrea t : \"Medium\" , t ype : \"Info\" } } } ]","title":"Notification format"},{"location":"user-guides/settings/integrations/webhook/#setting-up-integration","text":"Open Wallarm UI \u2192 Settings \u2192 Integrations . Click the Webhook block or click the Add integration button and choose Webhook . Enter an integration name. Enter target Webhook URL. If required, configure advanced settings: Request method : POST or PUT . By default, POST requests are sent. Request header and its value if the server requires a non-standard header to execute the request. The number of headers is not limited. CA certificate : add a self-signed certificate of your own CA that signed a server certificate (only if the webhook server uses a self-signed TLS certificate). If the webhook server uses a TLS certificate signed by a trusted CA, you can optionally add a trusted CA certificate. Verify the certificate : this setting allows to disable verification of the webhook server certificate. By default, Wallarm verifies whether a webhook server certificate is signed by a publicly trusted CA. We do not recommend disabling the verification of production server certificates. If your webhook server uses a self-signed TLS certificate, you can specify a self-signed CA certificate in the appropriate field to allow sending requests to this server. Request timeout, in seconds : if the server does not respond to the request within the specified time, the request fails. By default: 15 seconds. Connection timeout, in seconds : if the connection to the server cannot be established during the specified time, the request fails. By default: 20 seconds. Choose event types to trigger sending notifications to Webhook URL. If the events are not chosen, then notifications will not be sent. Test the integration and ensure the settings are correct. Click Add integration .","title":"Setting up integration"},{"location":"user-guides/settings/integrations/webhook/#examples-of-integrations","text":"Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status We described some examples of how to configure the integration with the popular log collectors forwarding logs to the SIEM systems: With Fluentd configured to forward logs to IBM QRadar , Splunk Enterprise , ArcSight Logger With Logstash configured to forward logs to IBM QRadar , Splunk Enterprise , ArcSight Logger","title":"Examples of integrations"},{"location":"user-guides/settings/integrations/webhook/#testing-integration","text":"Integration testing allows checking configuration correctness, availability of the Wallarm Cloud, and the notification format. To test the integration, you can use the button Test integration when creating or editing the integration. The integration is tested as follows: Test notifications with the prefix [Test message] are sent to the selected system. Test integrations are sent for all events available for the selected system. If the integration card includes 3 event types, the system will receive 3 test notifications. If the integration card includes the event type System related , an appropriate test notification includes details on the newly added user. Test notifications include test data. Test webhook example: [ { summary : \"[Test message] [Test partner(US)] New vulnerability detected\" , descrip t io n : \"Notification type: vuln New vulnerability was detected in your system. ID: Title: Test Domain: example.com Path: Method: Discovered by: Parameter: Type: Info Threat: Medium More details: https://us1.my.wallarm.com/object/555 Client: TestCompany Cloud: US \" , de ta ils :{ clie nt _ na me : \"TestCompany\" , cloud : \"US\" , n o t i f ica t io n _ t ype : \"vuln\" , vul n _li n k : \"https://us1.my.wallarm.com/object/555\" , vul n :{ domai n : \"example.com\" , id : null , me t hod : null , parame ter : null , pa t h : null , t i tle : \"Test\" , discovered_by : null , t hrea t : \"Medium\" , t ype : \"Info\" } } } ]","title":"Testing integration"},{"location":"user-guides/settings/integrations/webhook/#updating-integration","text":"To update the settings of active integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration. Make required changes and click Save .","title":"Updating integration"},{"location":"user-guides/settings/integrations/webhook/#disabling-integration","text":"To stop sending reports and notifications temporarily, you can disable the integration: Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open an active integration and click Disable . To re-enable sending reports and notifications, open the disabled integration and click Enable .","title":"Disabling integration"},{"location":"user-guides/settings/integrations/webhook/#deleting-integration","text":"To stop sending reports and notifications permanently, you can delete the integration. Deleting an integration cannot be undone. The integration will be removed from the list permanently. Go to your Wallarm account \u2192 Settings \u2192 Integrations in the EU or US cloud. Open integration and click Delete . Confirm the action.","title":"Deleting Integration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-arcsight-logger/","text":"Micro Focus ArcSight Logger via Fluentd \u00b6 Example overview \u00b6 Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Fluentd log collector and forwarded to the ArcSight Logger system. Integration with the Enterprise version of ArcSight ESM To configure forwarding logs from Fluentd to the Enterprise version of ArcSight ESM, it is recommended to configure the Syslog Connector on the ArcSight side and then forward logs from Fluentd to the connector port. To get a more detailed description of the connectors, please download the SmartConnector User Guide from the official ArcSight SmartConnector documentation . Used resources \u00b6 ArcSight Logger 7.1 with the WEB URL https://192.168.1.73:443 installed on CentOS 7.8 Fluentd installed on Debian 10.4 (Buster) and available on https://192.168.1.65:9880 Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the ArcSight Logger and Fluentd services are cited as examples, they do not respond. ArcSight Logger configuration \u00b6 ArcSight Logger has logs receiver Wallarm Fluentd logs configured as follows: Logs are received via UDP ( Type = UDP Receiver ) Listening port is 514 Events are parsed with the syslog parser Other default settings To get a more detailed description of the receiver configuration, please download the Logger Installation Guide of an appropriate version from the official ArcSight Logger documentation . Fluentd configuration \u00b6 Fluentd is configured in the td-agent.conf file: Incoming webhook processing is configured in the source directive: Traffic is sent to port 9880 Fluentd is configured to accept only HTTPS connections Fluentd TLS certificate signed by a publicly trusted CA is located within the file /etc/ssl/certs/fluentd.crt Private key for TLS certificate is located within the file /etc/ssl/private/fluentd.key Forwarding logs to ArcSight Logger and log output are configured in the match directive: All event logs are copied from Fluentd and forwarded to ArcSight Logger at the IP address https://192.168.1.73:514 Logs are forwarded from Fluentd to ArcSight Logger in the JSON format according to the Syslog standard Connection with ArcSight Logger is established via UDP Fluentd logs are additionally printed on the command line in JSON format (19-22 code lines). The setting is used to verify that events are logged via Fluentd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <source> @type http # input plugin for HTTP and HTTPS traffic port 9880 # port for incoming requests <transport tls> # configuration for connections handling cert_path /etc/ssl/certs/fluentd.crt private_key_path /etc/ssl/private/fluentd.key </transport> </source> <match **> @type copy <store> @type remote_syslog # output plugin to forward logs from Fluentd via Syslog host 192 .168.1.73 # IP address to forward logs to port 514 # port to forward logs to protocol udp # connection protocol <format> @type json # format of forwarded logs </format> </store> <store> @type stdout # output plugin to print Fluentd logs on the command line output_type json # format of logs printed on the command line </store> </match> A more detailed description of configuration files is available in the official Fluentd documentation . Testing Fluentd configuration To check that Fluentd logs are created and forwarded to ArcSight Logger, the PUT or POST request can be sent to Fluentd. Request example: curl -X POST 'https://192.168.1.65:9880' -H \"Content-Type: application/json\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Fluentd logs: Event in ArcSight Logger: Configuration of webhook integration \u00b6 Webhooks are sent to https://192.168.1.65:9880 Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes Example testing \u00b6 To test the configuration, a new user is added in Wallarm Console: Fluentd will log the event as follows: The following entry will be displayed in ArcSight Logger events:","title":"Micro Focus ArcSight Logger via Fluentd"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-arcsight-logger/#micro-focus-arcsight-logger-via-fluentd","text":"","title":"Micro Focus ArcSight Logger via Fluentd"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-arcsight-logger/#example-overview","text":"Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Fluentd log collector and forwarded to the ArcSight Logger system. Integration with the Enterprise version of ArcSight ESM To configure forwarding logs from Fluentd to the Enterprise version of ArcSight ESM, it is recommended to configure the Syslog Connector on the ArcSight side and then forward logs from Fluentd to the connector port. To get a more detailed description of the connectors, please download the SmartConnector User Guide from the official ArcSight SmartConnector documentation .","title":"Example overview"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-arcsight-logger/#used-resources","text":"ArcSight Logger 7.1 with the WEB URL https://192.168.1.73:443 installed on CentOS 7.8 Fluentd installed on Debian 10.4 (Buster) and available on https://192.168.1.65:9880 Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the ArcSight Logger and Fluentd services are cited as examples, they do not respond.","title":"Used resources"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-arcsight-logger/#arcsight-logger-configuration","text":"ArcSight Logger has logs receiver Wallarm Fluentd logs configured as follows: Logs are received via UDP ( Type = UDP Receiver ) Listening port is 514 Events are parsed with the syslog parser Other default settings To get a more detailed description of the receiver configuration, please download the Logger Installation Guide of an appropriate version from the official ArcSight Logger documentation .","title":"ArcSight Logger configuration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-arcsight-logger/#fluentd-configuration","text":"Fluentd is configured in the td-agent.conf file: Incoming webhook processing is configured in the source directive: Traffic is sent to port 9880 Fluentd is configured to accept only HTTPS connections Fluentd TLS certificate signed by a publicly trusted CA is located within the file /etc/ssl/certs/fluentd.crt Private key for TLS certificate is located within the file /etc/ssl/private/fluentd.key Forwarding logs to ArcSight Logger and log output are configured in the match directive: All event logs are copied from Fluentd and forwarded to ArcSight Logger at the IP address https://192.168.1.73:514 Logs are forwarded from Fluentd to ArcSight Logger in the JSON format according to the Syslog standard Connection with ArcSight Logger is established via UDP Fluentd logs are additionally printed on the command line in JSON format (19-22 code lines). The setting is used to verify that events are logged via Fluentd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <source> @type http # input plugin for HTTP and HTTPS traffic port 9880 # port for incoming requests <transport tls> # configuration for connections handling cert_path /etc/ssl/certs/fluentd.crt private_key_path /etc/ssl/private/fluentd.key </transport> </source> <match **> @type copy <store> @type remote_syslog # output plugin to forward logs from Fluentd via Syslog host 192 .168.1.73 # IP address to forward logs to port 514 # port to forward logs to protocol udp # connection protocol <format> @type json # format of forwarded logs </format> </store> <store> @type stdout # output plugin to print Fluentd logs on the command line output_type json # format of logs printed on the command line </store> </match> A more detailed description of configuration files is available in the official Fluentd documentation . Testing Fluentd configuration To check that Fluentd logs are created and forwarded to ArcSight Logger, the PUT or POST request can be sent to Fluentd. Request example: curl -X POST 'https://192.168.1.65:9880' -H \"Content-Type: application/json\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Fluentd logs: Event in ArcSight Logger:","title":"Fluentd configuration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-arcsight-logger/#configuration-of-webhook-integration","text":"Webhooks are sent to https://192.168.1.65:9880 Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes","title":"Configuration of webhook integration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-arcsight-logger/#example-testing","text":"To test the configuration, a new user is added in Wallarm Console: Fluentd will log the event as follows: The following entry will be displayed in ArcSight Logger events:","title":"Example testing"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-qradar/","text":"IBM QRadar via Fluentd \u00b6 Example overview \u00b6 Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Fluentd log collector and forwarded to the QRadar SIEM system. Used resources \u00b6 Fluentd installed on Debian 10.4 (Buster) and available on https://fluentd-example-domain.com QRadar V7.3.3 installed on Linux Red Hat and available with the IP address https://109.111.35.11:514 Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the Fluentd and QRadar services are cited as examples, they do not respond. Fluentd configuration \u00b6 Fluentd is configured in the td-agent.conf file: Incoming webhook processing is configured in the source directive: Traffic is sent to port 9880 Fluentd is configured to accept only HTTPS connections Fluentd TLS certificate signed by a publicly trusted CA is located within the file /etc/ssl/certs/fluentd.crt Private key for TLS certificate is located within the file /etc/ssl/private/fluentd.key Forwarding logs to QRadar and log output are configured in the match directive: All event logs are copied from Fluentd and forwarded to QRadar at the IP address https://109.111.35.11:514 Logs are forwarded from Fluentd to QRadar in the JSON format according to the Syslog standard Connection with QRadar is established via TCP Fluentd logs are additionally printed on the command line in JSON format (19-22 code lines). The setting is used to verify that events are logged via Fluentd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <source> @type http # input plugin for HTTP and HTTPS traffic port 9880 # port for incoming requests <transport tls> # configuration for connections handling cert_path /etc/ssl/certs/fluentd.crt private_key_path /etc/ssl/private/fluentd.key </transport> </source> <match **> @type copy <store> @type remote_syslog # output plugin to forward logs from Fluentd via Syslog host 109 .111.35.11 # IP address to forward logs to port 514 # port to forward logs to protocol tcp # connection protocol <format> @type json # format of forwarded logs </format> </store> <store> @type stdout # output plugin to print Fluentd logs on the command line output_type json # format of logs printed on the command line </store> </match> A more detailed description of configuration files is available in the official Fluentd documentation . Testing Fluentd configuration To check that Fluentd logs are created and forwarded to QRadar, the PUT or POST request can be sent to Fluentd. Request example: curl -X POST 'https://fluentd-example-domain.com' -H \"Content-Type: application/json\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Fluentd logs: QRadar logs: QRadar log payload: QRadar configuration (optional) \u00b6 In QRadar, the log source is configured. It helps to easily find Fluentd logs in the list of all logs in QRadar, and can also be used for further log filtering. The log source is configured as follows: Log Source Name : Fluentd Log Source Description : Logs from Fluentd Log Source Type : type of incoming logs parser used with Syslog standard Universal LEEF Protocol Configuration : standard of logs forwarding Syslog Log Source Identifier : Fluentd IP address Other default settings A more detailed description of QRadar log source setup is available in the official IBM documentation . Configuration of webhook integration \u00b6 Webhooks are sent to https://fluentd-example-domain.com Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes Example testing \u00b6 To test the configuration, a new user is added in Wallarm Console: Fluentd will log the event as follows: The following data in JSON format will be displayed in the QRadar log payload:","title":"IBM QRadar via Fluentd"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-qradar/#ibm-qradar-via-fluentd","text":"","title":"IBM QRadar via Fluentd"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-qradar/#example-overview","text":"Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Fluentd log collector and forwarded to the QRadar SIEM system.","title":"Example overview"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-qradar/#used-resources","text":"Fluentd installed on Debian 10.4 (Buster) and available on https://fluentd-example-domain.com QRadar V7.3.3 installed on Linux Red Hat and available with the IP address https://109.111.35.11:514 Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the Fluentd and QRadar services are cited as examples, they do not respond.","title":"Used resources"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-qradar/#fluentd-configuration","text":"Fluentd is configured in the td-agent.conf file: Incoming webhook processing is configured in the source directive: Traffic is sent to port 9880 Fluentd is configured to accept only HTTPS connections Fluentd TLS certificate signed by a publicly trusted CA is located within the file /etc/ssl/certs/fluentd.crt Private key for TLS certificate is located within the file /etc/ssl/private/fluentd.key Forwarding logs to QRadar and log output are configured in the match directive: All event logs are copied from Fluentd and forwarded to QRadar at the IP address https://109.111.35.11:514 Logs are forwarded from Fluentd to QRadar in the JSON format according to the Syslog standard Connection with QRadar is established via TCP Fluentd logs are additionally printed on the command line in JSON format (19-22 code lines). The setting is used to verify that events are logged via Fluentd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <source> @type http # input plugin for HTTP and HTTPS traffic port 9880 # port for incoming requests <transport tls> # configuration for connections handling cert_path /etc/ssl/certs/fluentd.crt private_key_path /etc/ssl/private/fluentd.key </transport> </source> <match **> @type copy <store> @type remote_syslog # output plugin to forward logs from Fluentd via Syslog host 109 .111.35.11 # IP address to forward logs to port 514 # port to forward logs to protocol tcp # connection protocol <format> @type json # format of forwarded logs </format> </store> <store> @type stdout # output plugin to print Fluentd logs on the command line output_type json # format of logs printed on the command line </store> </match> A more detailed description of configuration files is available in the official Fluentd documentation . Testing Fluentd configuration To check that Fluentd logs are created and forwarded to QRadar, the PUT or POST request can be sent to Fluentd. Request example: curl -X POST 'https://fluentd-example-domain.com' -H \"Content-Type: application/json\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Fluentd logs: QRadar logs: QRadar log payload:","title":"Fluentd configuration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-qradar/#qradar-configuration-optional","text":"In QRadar, the log source is configured. It helps to easily find Fluentd logs in the list of all logs in QRadar, and can also be used for further log filtering. The log source is configured as follows: Log Source Name : Fluentd Log Source Description : Logs from Fluentd Log Source Type : type of incoming logs parser used with Syslog standard Universal LEEF Protocol Configuration : standard of logs forwarding Syslog Log Source Identifier : Fluentd IP address Other default settings A more detailed description of QRadar log source setup is available in the official IBM documentation .","title":"QRadar configuration (optional)"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-qradar/#configuration-of-webhook-integration","text":"Webhooks are sent to https://fluentd-example-domain.com Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes","title":"Configuration of webhook integration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-qradar/#example-testing","text":"To test the configuration, a new user is added in Wallarm Console: Fluentd will log the event as follows: The following data in JSON format will be displayed in the QRadar log payload:","title":"Example testing"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-splunk/","text":"Splunk Enterprise via Fluentd \u00b6 Example overview \u00b6 Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Fluentd log collector and forwarded to the Splunk SIEM system. Used resources \u00b6 Splunk Enterprise with WEB URL https://109.111.35.11:8000 and API URL https://109.111.35.11:8088 Fluentd installed on Debian 10.4 (Buster) and available on https://fluentd-example-domain.com Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the Splunk Enterprise and Fluentd services are cited as examples, they do not respond. Splunk Enterprise configuration \u00b6 Fluentd logs are sent to Splunk HTTP Event Controller with the name Wallarm Fluentd logs and other default settings: To access the HTTP Event Controller, the generated token f44b3179-91aa-44f5-a6f7-202265e10475 will be used. A more detailed description of Splunk HTTP Event Controller setup is available in the official Splunk documentation . Fluentd configuration \u00b6 Fluentd is configured in the td-agent.conf file: Incoming webhook processing is configured in the source directive: Traffic is sent to port 9880 Fluentd is configured to accept only HTTPS connections Fluentd TLS certificate signed by a publicly trusted CA is located within the file /etc/ssl/certs/fluentd.crt Private key for TLS certificate is located within the file /etc/ssl/private/fluentd.key Forwarding logs to Splunk and log output are configured in the match directive: All event logs are copied from Fluentd and forwarded to Splunk HTTP Event Controller via the output plugin fluent-plugin-splunk-hec Fluentd logs are additionally printed on the command line in JSON format (19-22 code lines). The setting is used to verify that events are logged via Fluentd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <source> @type http # input plugin for HTTP and HTTPS traffic port 9880 # port for incoming requests <transport tls> # configuration for connections handling cert_path /etc/ssl/certs/fluentd.crt private_key_path /etc/ssl/private/fluentd.key </transport> </source> <match **> @type copy <store> @type splunk_hec # output plugin fluent-plugin-splunk-hec to forward logs to Splunk API via HTTP Event Controller hec_host 109 .111.35.11 # Splunk host hec_port 8088 # Splunk API port hec_token f44b3179-91aa-44f5-a6f7-202265e10475 # HTTP Event Controller token <format> @type json # format of forwarded logs </format> </store> <store> @type stdout # output plugin to print Fluentd logs on the command line output_type json # format of logs printed on the command line </store> </match> A more detailed description of configuration files is available in the official Fluentd documentation . Testing Fluentd configuration To check that Fluentd logs are created and forwarded to Splunk, the PUT or POST request can be sent to Fluentd. Request example: curl -X POST 'https://fluentd-example-domain.com' -H \"Content-Type: application/json\" -H \"Authorization: Splunk f44b3179-91aa-44f5-a6f7-202265e10475\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Fluentd logs: Splunk logs: Configuration of webhook integration \u00b6 Webhooks are sent to https://fluentd-example-domain.com Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes Example testing \u00b6 To test the configuration, a new user is added in Wallarm Console: Fluentd will log the event as follows: The following entry will be displayed in Splunk events:","title":"Splunk Enterprise via Fluentd"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-splunk/#splunk-enterprise-via-fluentd","text":"","title":"Splunk Enterprise via Fluentd"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-splunk/#example-overview","text":"Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Fluentd log collector and forwarded to the Splunk SIEM system.","title":"Example overview"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-splunk/#used-resources","text":"Splunk Enterprise with WEB URL https://109.111.35.11:8000 and API URL https://109.111.35.11:8088 Fluentd installed on Debian 10.4 (Buster) and available on https://fluentd-example-domain.com Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the Splunk Enterprise and Fluentd services are cited as examples, they do not respond.","title":"Used resources"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-splunk/#splunk-enterprise-configuration","text":"Fluentd logs are sent to Splunk HTTP Event Controller with the name Wallarm Fluentd logs and other default settings: To access the HTTP Event Controller, the generated token f44b3179-91aa-44f5-a6f7-202265e10475 will be used. A more detailed description of Splunk HTTP Event Controller setup is available in the official Splunk documentation .","title":"Splunk Enterprise configuration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-splunk/#fluentd-configuration","text":"Fluentd is configured in the td-agent.conf file: Incoming webhook processing is configured in the source directive: Traffic is sent to port 9880 Fluentd is configured to accept only HTTPS connections Fluentd TLS certificate signed by a publicly trusted CA is located within the file /etc/ssl/certs/fluentd.crt Private key for TLS certificate is located within the file /etc/ssl/private/fluentd.key Forwarding logs to Splunk and log output are configured in the match directive: All event logs are copied from Fluentd and forwarded to Splunk HTTP Event Controller via the output plugin fluent-plugin-splunk-hec Fluentd logs are additionally printed on the command line in JSON format (19-22 code lines). The setting is used to verify that events are logged via Fluentd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 <source> @type http # input plugin for HTTP and HTTPS traffic port 9880 # port for incoming requests <transport tls> # configuration for connections handling cert_path /etc/ssl/certs/fluentd.crt private_key_path /etc/ssl/private/fluentd.key </transport> </source> <match **> @type copy <store> @type splunk_hec # output plugin fluent-plugin-splunk-hec to forward logs to Splunk API via HTTP Event Controller hec_host 109 .111.35.11 # Splunk host hec_port 8088 # Splunk API port hec_token f44b3179-91aa-44f5-a6f7-202265e10475 # HTTP Event Controller token <format> @type json # format of forwarded logs </format> </store> <store> @type stdout # output plugin to print Fluentd logs on the command line output_type json # format of logs printed on the command line </store> </match> A more detailed description of configuration files is available in the official Fluentd documentation . Testing Fluentd configuration To check that Fluentd logs are created and forwarded to Splunk, the PUT or POST request can be sent to Fluentd. Request example: curl -X POST 'https://fluentd-example-domain.com' -H \"Content-Type: application/json\" -H \"Authorization: Splunk f44b3179-91aa-44f5-a6f7-202265e10475\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Fluentd logs: Splunk logs:","title":"Fluentd configuration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-splunk/#configuration-of-webhook-integration","text":"Webhooks are sent to https://fluentd-example-domain.com Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes","title":"Configuration of webhook integration"},{"location":"user-guides/settings/integrations/webhook-examples/fluentd-splunk/#example-testing","text":"To test the configuration, a new user is added in Wallarm Console: Fluentd will log the event as follows: The following entry will be displayed in Splunk events:","title":"Example testing"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-arcsight-logger/","text":"Micro Focus ArcSight Logger via Logstash \u00b6 Example overview \u00b6 Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Logstash log collector and forwarded to the ArcSight Logger system. Integration with the Enterprise version of ArcSight ESM To configure forwarding logs from Logstash to the Enterprise version of ArcSight ESM, it is recommended to configure the Syslog Connector on the ArcSight side and then forward logs from Logstash to the connector port. To get a more detailed description of the connectors, please download the SmartConnector User Guide from the official ArcSight SmartConnector documentation . Used resources \u00b6 ArcSight Logger 7.1 with the WEB URL https://192.168.1.73:443 installed on CentOS 7.8 Logstash 7.7.0 installed on Debian 10.4 (Buster) and available on https://192.168.1.65:5044 Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the ArcSight Logger and Logstash services are cited as examples, they do not respond. ArcSight Logger configuration \u00b6 ArcSight Logger has logs receiver Wallarm Logstash logs configured as follows: Logs are received via UDP ( Type = UDP Receiver ) Listening port is 514 Events are parsed with the syslog parser Other default settings To get a more detailed description of the receiver configuration, please download the Logger Installation Guide of an appropriate version from the official ArcSight Logger documentation . Logstash configuration \u00b6 Logstash is configured in the logstash-sample.conf file: Incoming webhook processing is configured in the input section: Traffic is sent to port 5044 Logstash is configured to accept only HTTPS connections Logstash TLS certificate signed by a publicly trusted CA is located within the file /etc/server.crt Private key for TLS certificate is located within the file /etc/server.key Forwarding logs to ArcSight Logger and log output are configured in the output section: All event logs are forwarded from Logstash to ArcSight Logger at the IP address https://192.168.1.73:514 Logs are forwarded from Logstash to ArcSight Logger in the JSON format according to the Syslog standard Connection with ArcSight Logger is established via UDP Logstash logs are additionally printed on the command line (15 th code line). The setting is used to verify that events are logged via Logstash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 input { http { # input plugin for HTTP and HTTPS traffic port = > 5044 # port for incoming requests ssl = > true # HTTPS traffic processing ssl_certificate = > \"/etc/server.crt\" # Logstash TLS certificate ssl_key = > \"/etc/server.key\" # private key for TLS certificate } } output { syslog { # output plugin to forward logs from Logstash via Syslog host = > \"192.168.1.73\" # IP address to forward logs to port = > \"514\" # port to forward logs to protocol = > \"udp\" # connection protocol codec = > json # format of forwarded logs } stdout {} # output plugin to print Logstash logs on the command line } A more detailed description of the configuration files is available in the official Logstash documentation . Testing Logstash configuration To check that Logstash logs are created and forwarded to ArcSight Logger, the POST request can be sent to Logstash. Request example: curl -X POST 'https://192.168.1.65:5044' -H \"Content-Type: application/json\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Logstash logs: Event in ArcSight Logger: Configuration of webhook integration \u00b6 Webhooks are sent to https://192.168.1.65:5044 Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes Example testing \u00b6 To test the configuration, a new user is added in Wallarm Console: Logstash will log the event as follows: The following entry will be displayed in ArcSight Logger events:","title":"Micro Focus ArcSight Logger via Logstash"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-arcsight-logger/#micro-focus-arcsight-logger-via-logstash","text":"","title":"Micro Focus ArcSight Logger via Logstash"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-arcsight-logger/#example-overview","text":"Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Logstash log collector and forwarded to the ArcSight Logger system. Integration with the Enterprise version of ArcSight ESM To configure forwarding logs from Logstash to the Enterprise version of ArcSight ESM, it is recommended to configure the Syslog Connector on the ArcSight side and then forward logs from Logstash to the connector port. To get a more detailed description of the connectors, please download the SmartConnector User Guide from the official ArcSight SmartConnector documentation .","title":"Example overview"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-arcsight-logger/#used-resources","text":"ArcSight Logger 7.1 with the WEB URL https://192.168.1.73:443 installed on CentOS 7.8 Logstash 7.7.0 installed on Debian 10.4 (Buster) and available on https://192.168.1.65:5044 Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the ArcSight Logger and Logstash services are cited as examples, they do not respond.","title":"Used resources"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-arcsight-logger/#arcsight-logger-configuration","text":"ArcSight Logger has logs receiver Wallarm Logstash logs configured as follows: Logs are received via UDP ( Type = UDP Receiver ) Listening port is 514 Events are parsed with the syslog parser Other default settings To get a more detailed description of the receiver configuration, please download the Logger Installation Guide of an appropriate version from the official ArcSight Logger documentation .","title":"ArcSight Logger configuration"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-arcsight-logger/#logstash-configuration","text":"Logstash is configured in the logstash-sample.conf file: Incoming webhook processing is configured in the input section: Traffic is sent to port 5044 Logstash is configured to accept only HTTPS connections Logstash TLS certificate signed by a publicly trusted CA is located within the file /etc/server.crt Private key for TLS certificate is located within the file /etc/server.key Forwarding logs to ArcSight Logger and log output are configured in the output section: All event logs are forwarded from Logstash to ArcSight Logger at the IP address https://192.168.1.73:514 Logs are forwarded from Logstash to ArcSight Logger in the JSON format according to the Syslog standard Connection with ArcSight Logger is established via UDP Logstash logs are additionally printed on the command line (15 th code line). The setting is used to verify that events are logged via Logstash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 input { http { # input plugin for HTTP and HTTPS traffic port = > 5044 # port for incoming requests ssl = > true # HTTPS traffic processing ssl_certificate = > \"/etc/server.crt\" # Logstash TLS certificate ssl_key = > \"/etc/server.key\" # private key for TLS certificate } } output { syslog { # output plugin to forward logs from Logstash via Syslog host = > \"192.168.1.73\" # IP address to forward logs to port = > \"514\" # port to forward logs to protocol = > \"udp\" # connection protocol codec = > json # format of forwarded logs } stdout {} # output plugin to print Logstash logs on the command line } A more detailed description of the configuration files is available in the official Logstash documentation . Testing Logstash configuration To check that Logstash logs are created and forwarded to ArcSight Logger, the POST request can be sent to Logstash. Request example: curl -X POST 'https://192.168.1.65:5044' -H \"Content-Type: application/json\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Logstash logs: Event in ArcSight Logger:","title":"Logstash configuration"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-arcsight-logger/#configuration-of-webhook-integration","text":"Webhooks are sent to https://192.168.1.65:5044 Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes","title":"Configuration of webhook integration"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-arcsight-logger/#example-testing","text":"To test the configuration, a new user is added in Wallarm Console: Logstash will log the event as follows: The following entry will be displayed in ArcSight Logger events:","title":"Example testing"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-qradar/","text":"IBM QRadar via Logstash \u00b6 Example overview \u00b6 Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Logstash log collector and forwarded to the QRadar SIEM system. Used resources \u00b6 Logstash 7.7.0 installed on Debian 10.4 (Buster) and available on https://logstash.example.domain.com QRadar V7.3.3 installed on Linux Red Hat and available with the IP address https://109.111.35.11:514 Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the Logstash and QRadar services are cited as examples, they do not respond. Logstash configuration \u00b6 Logstash is configured in the logstash-sample.conf file: Incoming webhook processing is configured in the input section: Traffic is sent to port 5044 Logstash is configured to accept only HTTPS connections Logstash TLS certificate signed by a publicly trusted CA is located within the file /etc/server.crt Private key for TLS certificate is located within the file /etc/server.key Forwarding logs to QRadar and log output are configured in the output section: All event logs are forwarded from Logstash to QRadar at the IP address https://109.111.35.11:514 Logs are forwarded from Logstash to QRadar in the JSON format according to the Syslog standard Connection with QRadar is established via TCP Logstash logs are additionally printed on the command line (15 th code line). The setting is used to verify that events are logged via Logstash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 input { http { # input plugin for HTTP and HTTPS traffic port = > 5044 # port for incoming requests ssl = > true # HTTPS traffic processing ssl_certificate = > \"/etc/server.crt\" # Logstash TLS certificate ssl_key = > \"/etc/server.key\" # private key for TLS certificate } } output { syslog { # output plugin to forward logs from Logstash via Syslog host = > \"109.111.35.11\" # IP address to forward logs to port = > \"514\" # port to forward logs to protocol = > \"tcp\" # connection protocol codec = > json # format of forwarded logs } stdout {} # output plugin to print Logstash logs on the command line } A more detailed description of the configuration files is available in the official Logstash documentation . Testing Logstash configuration To check that Logstash logs are created and forwarded to QRadar, the POST request can be sent to Logstash. Request example: curl -X POST 'https://logstash.example.domain.com' -H \"Content-Type: application/json\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Logstash logs: QRadar logs: QRadar log payload: QRadar configuration (optional) \u00b6 In QRadar, the log source is configured. It helps to easily find Logstash logs in the list of all logs in QRadar, and can also be used for further log filtering. The log source is configured as follows: Log Source Name : Logstash Log Source Description : Logs from Logstash Log Source Type : type of incoming logs parser used with Syslog standard Universal LEEF Protocol Configuration : standard of logs forwarding Syslog Log Source Identifier : Logstash IP address Other default settings A more detailed description of the QRadar log source setup is available in the official IBM documentation . Configuration of webhook integration \u00b6 Webhooks are sent to https://logstash.example.domain.com Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes Example testing \u00b6 To test the configuration, a new user is added in Wallarm Console: Logstash will log the event as follows: The following data in JSON format will be displayed in the QRadar log payload:","title":"IBM QRadar via Logstash"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-qradar/#ibm-qradar-via-logstash","text":"","title":"IBM QRadar via Logstash"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-qradar/#example-overview","text":"Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Logstash log collector and forwarded to the QRadar SIEM system.","title":"Example overview"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-qradar/#used-resources","text":"Logstash 7.7.0 installed on Debian 10.4 (Buster) and available on https://logstash.example.domain.com QRadar V7.3.3 installed on Linux Red Hat and available with the IP address https://109.111.35.11:514 Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the Logstash and QRadar services are cited as examples, they do not respond.","title":"Used resources"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-qradar/#logstash-configuration","text":"Logstash is configured in the logstash-sample.conf file: Incoming webhook processing is configured in the input section: Traffic is sent to port 5044 Logstash is configured to accept only HTTPS connections Logstash TLS certificate signed by a publicly trusted CA is located within the file /etc/server.crt Private key for TLS certificate is located within the file /etc/server.key Forwarding logs to QRadar and log output are configured in the output section: All event logs are forwarded from Logstash to QRadar at the IP address https://109.111.35.11:514 Logs are forwarded from Logstash to QRadar in the JSON format according to the Syslog standard Connection with QRadar is established via TCP Logstash logs are additionally printed on the command line (15 th code line). The setting is used to verify that events are logged via Logstash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 input { http { # input plugin for HTTP and HTTPS traffic port = > 5044 # port for incoming requests ssl = > true # HTTPS traffic processing ssl_certificate = > \"/etc/server.crt\" # Logstash TLS certificate ssl_key = > \"/etc/server.key\" # private key for TLS certificate } } output { syslog { # output plugin to forward logs from Logstash via Syslog host = > \"109.111.35.11\" # IP address to forward logs to port = > \"514\" # port to forward logs to protocol = > \"tcp\" # connection protocol codec = > json # format of forwarded logs } stdout {} # output plugin to print Logstash logs on the command line } A more detailed description of the configuration files is available in the official Logstash documentation . Testing Logstash configuration To check that Logstash logs are created and forwarded to QRadar, the POST request can be sent to Logstash. Request example: curl -X POST 'https://logstash.example.domain.com' -H \"Content-Type: application/json\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Logstash logs: QRadar logs: QRadar log payload:","title":"Logstash configuration"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-qradar/#qradar-configuration-optional","text":"In QRadar, the log source is configured. It helps to easily find Logstash logs in the list of all logs in QRadar, and can also be used for further log filtering. The log source is configured as follows: Log Source Name : Logstash Log Source Description : Logs from Logstash Log Source Type : type of incoming logs parser used with Syslog standard Universal LEEF Protocol Configuration : standard of logs forwarding Syslog Log Source Identifier : Logstash IP address Other default settings A more detailed description of the QRadar log source setup is available in the official IBM documentation .","title":"QRadar configuration (optional)"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-qradar/#configuration-of-webhook-integration","text":"Webhooks are sent to https://logstash.example.domain.com Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes","title":"Configuration of webhook integration"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-qradar/#example-testing","text":"To test the configuration, a new user is added in Wallarm Console: Logstash will log the event as follows: The following data in JSON format will be displayed in the QRadar log payload:","title":"Example testing"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-splunk/","text":"Splunk Enterprise via Logstash \u00b6 Example overview \u00b6 Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Logstash log collector and forwarded to the Splunk SIEM system. Used resources \u00b6 Splunk Enterprise with WEB URL https://109.111.35.11:8000 and API URL https://109.111.35.11:8088 Logstash 7.7.0 installed on Debian 10.4 (Buster) and available on https://logstash.example.domain.com Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the Splunk Enterprise and Logstash services are cited as examples, they do not respond. Splunk Enterprise configuration \u00b6 Logstash logs are sent to Splunk HTTP Event Controller with the name Wallarm Logstash logs and other default settings: To access the HTTP Event Controller, generated token 93eaeba4-97a9-46c7-abf3-4e0c545fa5cb will be used. A more detailed description of Splunk HTTP Event Controller setup is available in the official Splunk documentation . Logstash configuration \u00b6 Logstash is configured in the logstash-sample.conf file: Incoming webhook processing is configured in the input section: Traffic is sent to port 5044 Logstash is configured to accept only HTTPS connections Logstash TLS certificate signed by a publicly trusted CA is located within the file /etc/server.crt Private key for TLS certificate is located within the file /etc/server.key Forwarding logs to Splunk and log output are configured in the output section: Logs are forwarded from Logstash to Splunk in the JSON format All event logs are forwarded from Logstash to Splunk API endpoint https://109.111.35.11:8088/services/collector/raw via POST requests. To authorize requests, the HTTPS Event Collector token is used Logstash logs are additionally printed on the command line (15 th code line). The setting is used to verify that events are logged via Logstash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 input { http { # input plugin for HTTP and HTTPS traffic port = > 5044 # port for incoming requests ssl = > true # HTTPS traffic processing ssl_certificate = > \"/etc/server.crt\" # Logstash TLS certificate ssl_key = > \"/etc/server.key\" # private key for TLS certificate } } output { http { # output plugin to forward logs from Logstash via HTTP/HTTPS protocol format = > \"json\" # format of forwarded logs http_method = > \"post\" # HTTP method used to forward logs url = > \"https://109.111.35.11:8088/services/collector/raw\" # ednpoint to forward logs to headers = > [ \"Authorization\" , \"Splunk 93eaeba4-97a9-46c7-abf3-4e0c545fa5cb\" ] # HTTP headers to authorize requests } stdout {} # output plugin to print Logstash logs on the command line } A more detailed description of configuration files is available in the official Logstash documentation . Testing Logstash configuration To check that Logstash logs are created and forwarded to Splunk, the POST request can be sent to Logstash. Request example: curl -X POST 'https://logstash.example.domain.com' -H \"Content-Type: application/json\" -H \"Authorization: Splunk 93eaeba4-97a9-46c7-abf3-4e0c545fa5cb\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Logstash logs: Splunk event: Configuration of webhook integration \u00b6 Webhooks are sent to https://logstash.example.domain.com Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes Example testing \u00b6 To test the configuration, a new user is added in Wallarm Console: Logstash will log the event as follows: The following entry will be displayed in Splunk events:","title":"Splunk Enterprise via Logstash"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-splunk/#splunk-enterprise-via-logstash","text":"","title":"Splunk Enterprise via Logstash"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-splunk/#example-overview","text":"Webhooks can be used as system log sources. The number of log sources depends on the system complexity: the more components in the system, the greater number of log sources and logs. The most common logging scheme in complex systems consists of the following components: Log collector: accepts logs from several sources and forwards logs to the SIEM system SIEM system: used to analyze logs and monitor the system status In the provided example, events are sent via webhooks to the Logstash log collector and forwarded to the Splunk SIEM system.","title":"Example overview"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-splunk/#used-resources","text":"Splunk Enterprise with WEB URL https://109.111.35.11:8000 and API URL https://109.111.35.11:8088 Logstash 7.7.0 installed on Debian 10.4 (Buster) and available on https://logstash.example.domain.com Administrator access to Wallarm Console in EU cloud to configure the webhook integration Since the links to the Splunk Enterprise and Logstash services are cited as examples, they do not respond.","title":"Used resources"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-splunk/#splunk-enterprise-configuration","text":"Logstash logs are sent to Splunk HTTP Event Controller with the name Wallarm Logstash logs and other default settings: To access the HTTP Event Controller, generated token 93eaeba4-97a9-46c7-abf3-4e0c545fa5cb will be used. A more detailed description of Splunk HTTP Event Controller setup is available in the official Splunk documentation .","title":"Splunk Enterprise configuration"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-splunk/#logstash-configuration","text":"Logstash is configured in the logstash-sample.conf file: Incoming webhook processing is configured in the input section: Traffic is sent to port 5044 Logstash is configured to accept only HTTPS connections Logstash TLS certificate signed by a publicly trusted CA is located within the file /etc/server.crt Private key for TLS certificate is located within the file /etc/server.key Forwarding logs to Splunk and log output are configured in the output section: Logs are forwarded from Logstash to Splunk in the JSON format All event logs are forwarded from Logstash to Splunk API endpoint https://109.111.35.11:8088/services/collector/raw via POST requests. To authorize requests, the HTTPS Event Collector token is used Logstash logs are additionally printed on the command line (15 th code line). The setting is used to verify that events are logged via Logstash 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 input { http { # input plugin for HTTP and HTTPS traffic port = > 5044 # port for incoming requests ssl = > true # HTTPS traffic processing ssl_certificate = > \"/etc/server.crt\" # Logstash TLS certificate ssl_key = > \"/etc/server.key\" # private key for TLS certificate } } output { http { # output plugin to forward logs from Logstash via HTTP/HTTPS protocol format = > \"json\" # format of forwarded logs http_method = > \"post\" # HTTP method used to forward logs url = > \"https://109.111.35.11:8088/services/collector/raw\" # ednpoint to forward logs to headers = > [ \"Authorization\" , \"Splunk 93eaeba4-97a9-46c7-abf3-4e0c545fa5cb\" ] # HTTP headers to authorize requests } stdout {} # output plugin to print Logstash logs on the command line } A more detailed description of configuration files is available in the official Logstash documentation . Testing Logstash configuration To check that Logstash logs are created and forwarded to Splunk, the POST request can be sent to Logstash. Request example: curl -X POST 'https://logstash.example.domain.com' -H \"Content-Type: application/json\" -H \"Authorization: Splunk 93eaeba4-97a9-46c7-abf3-4e0c545fa5cb\" -d '{\"key1\":\"value1\", \"key2\":\"value2\"}' Logstash logs: Splunk event:","title":"Logstash configuration"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-splunk/#configuration-of-webhook-integration","text":"Webhooks are sent to https://logstash.example.domain.com Webhooks are sent via POST requests Additional authentication parameter X-Auth-Token is passed in the request Webhooks sent to Webhook URLs are all available events: hits, system events, vulnerabilities, scope changes","title":"Configuration of webhook integration"},{"location":"user-guides/settings/integrations/webhook-examples/logstash-splunk/#example-testing","text":"To test the configuration, a new user is added in Wallarm Console: Logstash will log the event as follows: The following entry will be displayed in Splunk events:","title":"Example testing"},{"location":"user-guides/triggers/trigger-examples/","text":"Trigger examples \u00b6 Greylist IP if 4 or more attack vectors are detected in 1 hour \u00b6 If 4 or more different attack vectors are sent to the protected resource from one IP address, this IP address will be greylisted for 1 hour. This trigger will be run only if the WAF node filters requests in safe blocking mode . If you have recently created the Wallarm account, this trigger is already created but is in a disabled state. To test the trigger: Ensure that the WAF node filters requests in safe blocking mode. Send the following requests to the protected resource: curl http://localhost/instructions.php/etc/passwd curl http://localhost/?id = 'or+1=1--a-<script>prompt(1)</script>' There are 3 attack vectors in these requests: SQLi , XSS , and Path Traversal . Open the Wallarm Console \u2192 IP lists \u2192 Greylist and check that IP address from which the requests were originated is greylisted for 1 hour. Open the section Events and check that requests are displayed in the list as the SQLi , XSS , and Path Traversal attacks. To search for attacks, you can use the filters, for example: sqli for the SQLi attacks, xss for the XSS attacks, ptrav for the Path Traversal attacks. All filters are described in the instructions on search using . Blacklist IP if 4 or more attack vectors are detected in 1 hour \u00b6 If 4 or more different attack vectors are sent to the protected resource from one IP address, this IP address will be blacklisted for 1 hour. To test the trigger: Send the following requests to the protected resource: curl http://localhost/instructions.php/etc/passwd curl http://localhost/?id = 'or+1=1--a-<script>prompt(1)</script>' There are 3 attack vectors in these requests: SQLi , XSS , and Path Traversal . Open the Wallarm Console \u2192 Blacklist and check that IP address from which the requests were originated is blocked for 1 hour. Open the section Events and check that requests are displayed in the list as the SQLi , XSS , and Path Traversal attacks. To search for attacks, you can use the filters, for example: sqli for the SQLi attacks, xss for the XSS attacks, ptrav for the Path Traversal attacks. All filters are described in the instructions on search using . If an IP address was blacklisted by this trigger, the WAF node would block all malicious and legitimate requests originated from this IP. To allow legitimate requests, you can configure the greylisting trigger . Mark requests as a brute\u2011force or dirbust attack if 31 or more requests are sent to the protected resource \u00b6 With the filter by the counter name \u00b6 If 31 or more requests are sent to https://example.com/api/frontend/login in 30 seconds, these requests will be marked as brute\u2011force attack and the IP address from which requests were originated will be added to the blacklist. The request URL https://example.com/api/frontend/login is specified in the rule Tag requests as a brute-force attack . To mark requests as the dirbust (forced browsing) attack, it is required to use the rule Tag requests as a forced browsing attack . Details on configuration of brute force protection and trigger testing \u2192 With the filter by URL \u00b6 If 31 or more requests are sent to example.com:8888/api/frontend/login in 30 seconds: These requests will be marked as brute\u2011force attack and the IP address from which requests were originated will be added to the blacklist. If the code 404 was returned in the response to all requests, these requests will be marked as dirbust (forced browsing) attack and the IP address from which requests were originated will be added to the blacklist. URL value format The format of the URL filter value is host:port/path . The scheme should be omitted. The port value must contain a non\u2011standard port (to specify 80 or 443 port, please configure the URL via the Counter name filter). Details on configuration of brute force protection and trigger testing \u2192 Slack notification if 2 or more SQLi hits are detected in one minute \u00b6 If 2 or more SQLi hits are sent to the protected resource, then a notification about this event will be sent to the Slack channel. To test the trigger: Send the following requests to the protected resource: curl http://localhost/data/UNION%20SELECT curl http://localhost/?id = or+1 = 1 --a- Open the Wallarm Console \u2192 Events and check that 3 SQLi attacks are displayed in the list of events. The attack was detected in the second request twice, before and after the parser percent was applied. Open the Slack channel and check that the following notification from the user wallarm received: [Wallarm] Trigger: The number of detected hits exceeded the threshold Notification type: attacks_exceeded The number of detected hits exceeded 1 in 1 minute. This notification was triggered by the \"Notification about SQLi hits\" trigger. Additional trigger\u2019s clauses: Attack type: SQLi. View events: https://my.wallarm.com/search?q=attacks&time_from=XXXXXXXXXX&time_to=XXXXXXXXXX Client: TestCompany Cloud: EU Notification about SQLi hits is the trigger name TestCompany is the name of your company account in the Wallarm Console EU is the Wallarm Cloud where your company account is registered Slack and email notification if new user is added to the account \u00b6 If a new user with the Administrator or Analyst role is added to the company account in the Wallarm Console, notification about this event will be sent to the email address specified in the integration and to the Slack channel. To test the trigger: Open the Wallarm Console \u2192 Settings \u2192 Users and add a new user. For example: Open your email Inbox and check that the following message received: Open the Slack channel and check that the following notification from the user wallarm received: [Wallarm] Trigger: New user was added to the company account Notification type: create_user A new user John Smith <johnsmith@example.com> with the role Analyst was added to the company account by John Doe <johndoe@example.com>. This notification was triggered by the \"Added user\" trigger. Client: TestCompany Cloud: EU John Smith and johnsmith@example.com is information about the added user Analyst is the role of the added user John Doe and johndoe@example.com is information about the user who added a new user Added user is the trigger name TestCompany is the name of your company account in the Wallarm Console EU is the Wallarm Cloud where your company account is registered Opsgenie notification if 2 or more incidents are detected in one second \u00b6 If 2 or more incidents with the application server or database are detected in one second, the notification about this event will be sent to Opsgenie. To test the trigger , it is required to send the attack exploiting an active vulnerability to the protected resource. The Wallarm Console \u2192 Vulnerabilities section displays active vulnerabilities detected in your applications and the examples of attacks that exploit these vulnerabilities. If the attack example is sent to the protected resource, Wallarm will record the incident. Two or more recorded incidents will trigger sending the following notification to Opsgenie: [Wallarm] Trigger: The number of incidents exceeded the threshold Notification type: incidents_exceeded The number of detected incidents exceeded 1 in 1 second. This notification was triggered by the \"Notification about incidents\" trigger. Additional trigger\u2019s clauses: Target: server, database. View events: https://my.wallarm.com/search?q=incidents&time_from=XXXXXXXXXX&time_to=XXXXXXXXXX Client: TestCompany Cloud: EU Notification about incidents is the trigger name TestCompany is the name of your company account in the Wallarm Console EU is the Wallarm Cloud where your company account is registered Protecting the resource from active vulnerability exploitation To protect the resource from active vulnerability exploitation, we recommend to patch the vulnerability in a timely manner. If the vulnerability cannot be patched on the application side, please configure a virtual patch to block attacks exploiting this vulnerability. Notification to Webhook URL if IP address is added to the blacklist \u00b6 If an IP address was added to the blacklist, the webhook about this event will be sent to Webhook URL. To test the trigger: Open the Wallarm Console \u2192 Blacklist and add the IP address to the blacklist. For example: Check that the following webhook was sent to the Webhook URL: [ { \"summary\": \"[Wallarm] Trigger: New IP address was blacklisted\", \"description\": \"Notification type: ip_blocked\\n\\nIP address 1.1.1.1 was blacklisted until 2021-06-10 02:27:15 +0300. You can review blocked IP addresses in the \\\"Blacklist\\\" section of Wallarm Console.\\nThis notification was triggered by the \\\"Notification about blacklisted IP\\\" trigger.\\n\\nClient: TestCompany\\nCloud: EU\\n\", \"details\": { \"client_name\": \"TestCompany\", \"cloud\": \"EU\", \"notification_type\": \"ip_blocked\", \"trigger_name\": \"Notification about blacklisted IP\", \"expire_at\": \"2021-06-10 02:27:15 +0300\", \"ip\": \"1.1.1.1\" } } ] Notification about blacklisted IP is the trigger name TestCompany is the name of your company account in the Wallarm Console EU is the Wallarm Cloud where your company account is registered","title":"Trigger examples"},{"location":"user-guides/triggers/trigger-examples/#trigger-examples","text":"","title":"Trigger examples"},{"location":"user-guides/triggers/trigger-examples/#greylist-ip-if-4-or-more-attack-vectors-are-detected-in-1-hour","text":"If 4 or more different attack vectors are sent to the protected resource from one IP address, this IP address will be greylisted for 1 hour. This trigger will be run only if the WAF node filters requests in safe blocking mode . If you have recently created the Wallarm account, this trigger is already created but is in a disabled state. To test the trigger: Ensure that the WAF node filters requests in safe blocking mode. Send the following requests to the protected resource: curl http://localhost/instructions.php/etc/passwd curl http://localhost/?id = 'or+1=1--a-<script>prompt(1)</script>' There are 3 attack vectors in these requests: SQLi , XSS , and Path Traversal . Open the Wallarm Console \u2192 IP lists \u2192 Greylist and check that IP address from which the requests were originated is greylisted for 1 hour. Open the section Events and check that requests are displayed in the list as the SQLi , XSS , and Path Traversal attacks. To search for attacks, you can use the filters, for example: sqli for the SQLi attacks, xss for the XSS attacks, ptrav for the Path Traversal attacks. All filters are described in the instructions on search using .","title":"Greylist IP if 4 or more attack vectors are detected in 1 hour"},{"location":"user-guides/triggers/trigger-examples/#blacklist-ip-if-4-or-more-attack-vectors-are-detected-in-1-hour","text":"If 4 or more different attack vectors are sent to the protected resource from one IP address, this IP address will be blacklisted for 1 hour. To test the trigger: Send the following requests to the protected resource: curl http://localhost/instructions.php/etc/passwd curl http://localhost/?id = 'or+1=1--a-<script>prompt(1)</script>' There are 3 attack vectors in these requests: SQLi , XSS , and Path Traversal . Open the Wallarm Console \u2192 Blacklist and check that IP address from which the requests were originated is blocked for 1 hour. Open the section Events and check that requests are displayed in the list as the SQLi , XSS , and Path Traversal attacks. To search for attacks, you can use the filters, for example: sqli for the SQLi attacks, xss for the XSS attacks, ptrav for the Path Traversal attacks. All filters are described in the instructions on search using . If an IP address was blacklisted by this trigger, the WAF node would block all malicious and legitimate requests originated from this IP. To allow legitimate requests, you can configure the greylisting trigger .","title":"Blacklist IP if 4 or more attack vectors are detected in 1 hour"},{"location":"user-guides/triggers/trigger-examples/#mark-requests-as-a-bruteforce-or-dirbust-attack-if-31-or-more-requests-are-sent-to-the-protected-resource","text":"","title":"Mark requests as a brute\u2011force or dirbust attack if 31 or more requests are sent to the protected resource"},{"location":"user-guides/triggers/trigger-examples/#with-the-filter-by-the-counter-name","text":"If 31 or more requests are sent to https://example.com/api/frontend/login in 30 seconds, these requests will be marked as brute\u2011force attack and the IP address from which requests were originated will be added to the blacklist. The request URL https://example.com/api/frontend/login is specified in the rule Tag requests as a brute-force attack . To mark requests as the dirbust (forced browsing) attack, it is required to use the rule Tag requests as a forced browsing attack . Details on configuration of brute force protection and trigger testing \u2192","title":"With the filter by the counter name"},{"location":"user-guides/triggers/trigger-examples/#with-the-filter-by-url","text":"If 31 or more requests are sent to example.com:8888/api/frontend/login in 30 seconds: These requests will be marked as brute\u2011force attack and the IP address from which requests were originated will be added to the blacklist. If the code 404 was returned in the response to all requests, these requests will be marked as dirbust (forced browsing) attack and the IP address from which requests were originated will be added to the blacklist. URL value format The format of the URL filter value is host:port/path . The scheme should be omitted. The port value must contain a non\u2011standard port (to specify 80 or 443 port, please configure the URL via the Counter name filter). Details on configuration of brute force protection and trigger testing \u2192","title":"With the filter by URL"},{"location":"user-guides/triggers/trigger-examples/#slack-notification-if-2-or-more-sqli-hits-are-detected-in-one-minute","text":"If 2 or more SQLi hits are sent to the protected resource, then a notification about this event will be sent to the Slack channel. To test the trigger: Send the following requests to the protected resource: curl http://localhost/data/UNION%20SELECT curl http://localhost/?id = or+1 = 1 --a- Open the Wallarm Console \u2192 Events and check that 3 SQLi attacks are displayed in the list of events. The attack was detected in the second request twice, before and after the parser percent was applied. Open the Slack channel and check that the following notification from the user wallarm received: [Wallarm] Trigger: The number of detected hits exceeded the threshold Notification type: attacks_exceeded The number of detected hits exceeded 1 in 1 minute. This notification was triggered by the \"Notification about SQLi hits\" trigger. Additional trigger\u2019s clauses: Attack type: SQLi. View events: https://my.wallarm.com/search?q=attacks&time_from=XXXXXXXXXX&time_to=XXXXXXXXXX Client: TestCompany Cloud: EU Notification about SQLi hits is the trigger name TestCompany is the name of your company account in the Wallarm Console EU is the Wallarm Cloud where your company account is registered","title":"Slack notification if 2 or more SQLi hits are detected in one minute"},{"location":"user-guides/triggers/trigger-examples/#slack-and-email-notification-if-new-user-is-added-to-the-account","text":"If a new user with the Administrator or Analyst role is added to the company account in the Wallarm Console, notification about this event will be sent to the email address specified in the integration and to the Slack channel. To test the trigger: Open the Wallarm Console \u2192 Settings \u2192 Users and add a new user. For example: Open your email Inbox and check that the following message received: Open the Slack channel and check that the following notification from the user wallarm received: [Wallarm] Trigger: New user was added to the company account Notification type: create_user A new user John Smith <johnsmith@example.com> with the role Analyst was added to the company account by John Doe <johndoe@example.com>. This notification was triggered by the \"Added user\" trigger. Client: TestCompany Cloud: EU John Smith and johnsmith@example.com is information about the added user Analyst is the role of the added user John Doe and johndoe@example.com is information about the user who added a new user Added user is the trigger name TestCompany is the name of your company account in the Wallarm Console EU is the Wallarm Cloud where your company account is registered","title":"Slack and email notification if new user is added to the account"},{"location":"user-guides/triggers/trigger-examples/#opsgenie-notification-if-2-or-more-incidents-are-detected-in-one-second","text":"If 2 or more incidents with the application server or database are detected in one second, the notification about this event will be sent to Opsgenie. To test the trigger , it is required to send the attack exploiting an active vulnerability to the protected resource. The Wallarm Console \u2192 Vulnerabilities section displays active vulnerabilities detected in your applications and the examples of attacks that exploit these vulnerabilities. If the attack example is sent to the protected resource, Wallarm will record the incident. Two or more recorded incidents will trigger sending the following notification to Opsgenie: [Wallarm] Trigger: The number of incidents exceeded the threshold Notification type: incidents_exceeded The number of detected incidents exceeded 1 in 1 second. This notification was triggered by the \"Notification about incidents\" trigger. Additional trigger\u2019s clauses: Target: server, database. View events: https://my.wallarm.com/search?q=incidents&time_from=XXXXXXXXXX&time_to=XXXXXXXXXX Client: TestCompany Cloud: EU Notification about incidents is the trigger name TestCompany is the name of your company account in the Wallarm Console EU is the Wallarm Cloud where your company account is registered Protecting the resource from active vulnerability exploitation To protect the resource from active vulnerability exploitation, we recommend to patch the vulnerability in a timely manner. If the vulnerability cannot be patched on the application side, please configure a virtual patch to block attacks exploiting this vulnerability.","title":"Opsgenie notification if 2 or more incidents are detected in one second"},{"location":"user-guides/triggers/trigger-examples/#notification-to-webhook-url-if-ip-address-is-added-to-the-blacklist","text":"If an IP address was added to the blacklist, the webhook about this event will be sent to Webhook URL. To test the trigger: Open the Wallarm Console \u2192 Blacklist and add the IP address to the blacklist. For example: Check that the following webhook was sent to the Webhook URL: [ { \"summary\": \"[Wallarm] Trigger: New IP address was blacklisted\", \"description\": \"Notification type: ip_blocked\\n\\nIP address 1.1.1.1 was blacklisted until 2021-06-10 02:27:15 +0300. You can review blocked IP addresses in the \\\"Blacklist\\\" section of Wallarm Console.\\nThis notification was triggered by the \\\"Notification about blacklisted IP\\\" trigger.\\n\\nClient: TestCompany\\nCloud: EU\\n\", \"details\": { \"client_name\": \"TestCompany\", \"cloud\": \"EU\", \"notification_type\": \"ip_blocked\", \"trigger_name\": \"Notification about blacklisted IP\", \"expire_at\": \"2021-06-10 02:27:15 +0300\", \"ip\": \"1.1.1.1\" } } ] Notification about blacklisted IP is the trigger name TestCompany is the name of your company account in the Wallarm Console EU is the Wallarm Cloud where your company account is registered","title":"Notification to Webhook URL if IP address is added to the blacklist"},{"location":"user-guides/triggers/triggers/","text":"Working with triggers \u00b6 What are triggers \u00b6 Triggers are tools that are used to set up custom notifications and reactions to events. Using triggers, you can: Receive alerts on major events via the tools you use for your day-to-day workflow, for example via corporate messengers or incident management systems. Block IP addresses from which a certain number of requests or attack vectors were sent. Identify brute\u2011force and dirbust attacks by the number of requests sent to the application addresses. You can configure all the trigger components: Condition : system event to be notified about. For example: getting a certain amount of attacks, blacklisted IP address, and new user added to the account. Filters : condition details. For example: attack types. Reaction : action that should be performed if the specified condition and filters are met. For example: sending the notification to Slack or another system configured as the integration , blocking IP address, or marking requests as the brute\u2011force attack. Triggers are configured in the Triggers section of the Wallarm Console. The section is available only for users with the Administrator role . Creating triggers \u00b6 Click the Create trigger button. Choose conditions. Add filters. Add reactions. Save the trigger. Step 1: Choosing a condition \u00b6 A condition is a system event to be notified about. The following conditions are available for notification: Number of requests Number of attack vectors Number of attacks Number of hits Number of incidents Blacklisted IP User added Choose a condition in the Wallarm Console interface and set the lower threshold for the reaction, if the setting is available. Step 2: Adding filters \u00b6 Filters are used for condition detailing. For example, you can set up reactions to attacks of certain types, such as brute-force attacks, SQL injections and others. The following filters are available: URL (only for the number of requests): full URL to which the request was sent. This filter is used if the protected resource listens for incoming requests on port other than 80 or 443. If the resource listens for incoming requests on port 80 orr 443, please configure the URL via the Counter name filter. URL format is host:port/path . The scheme should be omitted. The port value must contain a non\u2011standard port (to specify 80 or 443 port, please configure the URL via the Counter name filter). For example: example.com:8888/login or 255.255.255.255:8888/login . Trigger example with the URL filter \u2192 Compatibility with other filters This filter can be used with all available filters except Counter name . Counter name (only for the number of requests): counter name created in the rule defining a counter . Trigger example with the counter name filter \u2192 Compatibility with other filters This filter can be used with all available filters except URL . Type is a type of attack detected in the request or a type of vulnerability the request is directed to. Application is the application that receives the request or in which an incident is detected. IP is the IP address from which the request is sent. Domain is the domain that receives the request or in which an incident is detected. Response status is the response code returned to the request. Target is an application architecture part that the attack is directed at or in which the incident is detected. It can take the following values: Server , Client , Database . User's role is the role of the added user. It can take the following values: Deploy , Analyst , Admin . Choose one or more filters in the Wallarm Console interface and set values for them. Step 3: Adding reactions \u00b6 A reaction is an action that should be performed if the specified condition and filters are met. The set of available reactions depends on the selected condition. Reactions can be of the following types: Mark the requests as brute\u2011force or forced browsing (dirbust) attack. Requests will be marked as attacks in the events list but will not be blocked. To block requests, you can add an additional reaction: blacklist IP address. Add IP to the blacklist . Add IP to the greylist . Send a notification to the messenger, SIEM system or Webhook URL configured in the integrations . Choose one or more reactions in the Wallarm Console interface. Reactions available for the condition are located at Number of attacks : Step 4: Saving the trigger \u00b6 Click the Create button in the trigger creation modal dialog. Specify the trigger's name and description (if required) and click the Done button. If the trigger name and description are not specified, then the trigger is created with the name New trigger by <username>, <creation_date> and an empty description. Disabling and deleting triggers \u00b6 To temporarily stop sending notifications and reactions to events, you can disable the trigger. A disabled trigger will be displayed in the lists with All and Disabled triggers. To re\u2011enable sending notifications and reactions to events, the Enable option is used. To permanently stop sending notifications and reactions to events, you can delete the trigger. Deleting a trigger cannot be undone. The trigger will be permanently removed from the trigger list. To disable or delete the trigger, please select an appropriate option from the trigger menu and confirm the action if required. Demo videos \u00b6","title":"Working with triggers"},{"location":"user-guides/triggers/triggers/#working-with-triggers","text":"","title":"Working with triggers"},{"location":"user-guides/triggers/triggers/#what-are-triggers","text":"Triggers are tools that are used to set up custom notifications and reactions to events. Using triggers, you can: Receive alerts on major events via the tools you use for your day-to-day workflow, for example via corporate messengers or incident management systems. Block IP addresses from which a certain number of requests or attack vectors were sent. Identify brute\u2011force and dirbust attacks by the number of requests sent to the application addresses. You can configure all the trigger components: Condition : system event to be notified about. For example: getting a certain amount of attacks, blacklisted IP address, and new user added to the account. Filters : condition details. For example: attack types. Reaction : action that should be performed if the specified condition and filters are met. For example: sending the notification to Slack or another system configured as the integration , blocking IP address, or marking requests as the brute\u2011force attack. Triggers are configured in the Triggers section of the Wallarm Console. The section is available only for users with the Administrator role .","title":"What are triggers"},{"location":"user-guides/triggers/triggers/#creating-triggers","text":"Click the Create trigger button. Choose conditions. Add filters. Add reactions. Save the trigger.","title":"Creating triggers"},{"location":"user-guides/triggers/triggers/#step-1-choosing-a-condition","text":"A condition is a system event to be notified about. The following conditions are available for notification: Number of requests Number of attack vectors Number of attacks Number of hits Number of incidents Blacklisted IP User added Choose a condition in the Wallarm Console interface and set the lower threshold for the reaction, if the setting is available.","title":"Step 1: Choosing a condition"},{"location":"user-guides/triggers/triggers/#step-2-adding-filters","text":"Filters are used for condition detailing. For example, you can set up reactions to attacks of certain types, such as brute-force attacks, SQL injections and others. The following filters are available: URL (only for the number of requests): full URL to which the request was sent. This filter is used if the protected resource listens for incoming requests on port other than 80 or 443. If the resource listens for incoming requests on port 80 orr 443, please configure the URL via the Counter name filter. URL format is host:port/path . The scheme should be omitted. The port value must contain a non\u2011standard port (to specify 80 or 443 port, please configure the URL via the Counter name filter). For example: example.com:8888/login or 255.255.255.255:8888/login . Trigger example with the URL filter \u2192 Compatibility with other filters This filter can be used with all available filters except Counter name . Counter name (only for the number of requests): counter name created in the rule defining a counter . Trigger example with the counter name filter \u2192 Compatibility with other filters This filter can be used with all available filters except URL . Type is a type of attack detected in the request or a type of vulnerability the request is directed to. Application is the application that receives the request or in which an incident is detected. IP is the IP address from which the request is sent. Domain is the domain that receives the request or in which an incident is detected. Response status is the response code returned to the request. Target is an application architecture part that the attack is directed at or in which the incident is detected. It can take the following values: Server , Client , Database . User's role is the role of the added user. It can take the following values: Deploy , Analyst , Admin . Choose one or more filters in the Wallarm Console interface and set values for them.","title":"Step 2: Adding filters"},{"location":"user-guides/triggers/triggers/#step-3-adding-reactions","text":"A reaction is an action that should be performed if the specified condition and filters are met. The set of available reactions depends on the selected condition. Reactions can be of the following types: Mark the requests as brute\u2011force or forced browsing (dirbust) attack. Requests will be marked as attacks in the events list but will not be blocked. To block requests, you can add an additional reaction: blacklist IP address. Add IP to the blacklist . Add IP to the greylist . Send a notification to the messenger, SIEM system or Webhook URL configured in the integrations . Choose one or more reactions in the Wallarm Console interface. Reactions available for the condition are located at Number of attacks :","title":"Step 3: Adding reactions"},{"location":"user-guides/triggers/triggers/#step-4-saving-the-trigger","text":"Click the Create button in the trigger creation modal dialog. Specify the trigger's name and description (if required) and click the Done button. If the trigger name and description are not specified, then the trigger is created with the name New trigger by <username>, <creation_date> and an empty description.","title":"Step 4: Saving the trigger"},{"location":"user-guides/triggers/triggers/#disabling-and-deleting-triggers","text":"To temporarily stop sending notifications and reactions to events, you can disable the trigger. A disabled trigger will be displayed in the lists with All and Disabled triggers. To re\u2011enable sending notifications and reactions to events, the Enable option is used. To permanently stop sending notifications and reactions to events, you can delete the trigger. Deleting a trigger cannot be undone. The trigger will be permanently removed from the trigger list. To disable or delete the trigger, please select an appropriate option from the trigger menu and confirm the action if required.","title":"Disabling and deleting triggers"},{"location":"user-guides/triggers/triggers/#demo-videos","text":"","title":"Demo videos"},{"location":"user-guides/vulnerabilities/analyze-vuln/","text":"Analyzing Vulnerabilities \u00b6 Check vulnerabilities on the Vulnerabilities tab of the Wallarm interface. Analyze a Vulnerability \u00b6 Click the vulnerability entry from the list to view detailed information about it. Wallarm displays the detailed information about the vulnerability: Internal ID Method by which the vulnerability was discovered Risk level Vulnerability status Last check date Domain Target resource Discovery date and time Path Request method Request parameter Related incidents Detailed description Additional information Exploit example If any malicious requests exploiting this vulnerability are discovered, the Exploit example field has the warning: Attention. Found by incidents . Clicking the link displays the associated security incidents. Vulnerability Detection Method \u00b6 Vulnerabilities can be detected in the protected applications by the following methods: Active Threat Verification : the vulnerability was found during the attack verification process. Passive Detection : the vulnerability was found due to the security incident that occurred. Vulnerability Scanner : the vulnerability was found during the scope scanning process. Test Run : the vulnerability was found during the test run conducted by FAST. If the method by which the vulnerability was discovered is unknown, this information is not shown.","title":"Analyzing vulnerabilities"},{"location":"user-guides/vulnerabilities/analyze-vuln/#analyzing-vulnerabilities","text":"Check vulnerabilities on the Vulnerabilities tab of the Wallarm interface.","title":"Analyzing Vulnerabilities"},{"location":"user-guides/vulnerabilities/analyze-vuln/#analyze-a-vulnerability","text":"Click the vulnerability entry from the list to view detailed information about it. Wallarm displays the detailed information about the vulnerability: Internal ID Method by which the vulnerability was discovered Risk level Vulnerability status Last check date Domain Target resource Discovery date and time Path Request method Request parameter Related incidents Detailed description Additional information Exploit example If any malicious requests exploiting this vulnerability are discovered, the Exploit example field has the warning: Attention. Found by incidents . Clicking the link displays the associated security incidents.","title":"Analyze a Vulnerability"},{"location":"user-guides/vulnerabilities/analyze-vuln/#vulnerability-detection-method","text":"Vulnerabilities can be detected in the protected applications by the following methods: Active Threat Verification : the vulnerability was found during the attack verification process. Passive Detection : the vulnerability was found due to the security incident that occurred. Vulnerability Scanner : the vulnerability was found during the scope scanning process. Test Run : the vulnerability was found during the test run conducted by FAST. If the method by which the vulnerability was discovered is unknown, this information is not shown.","title":"Vulnerability Detection Method"},{"location":"user-guides/vulnerabilities/check-vuln/","text":"Checking Vulnerabilities \u00b6 You can check vulnerabilities on the Vulnerabilities tab of the Wallarm interface. By default, all vulnerabilities are divided into groups by the risk level. Lists inside the groups are sorted by the vulnerability discovery date. Wallarm stores the history of all discovered vulnerabilities and checks them regularly \u2014 both the open and closed ones. If a closed vulnerability opens as a result of checking, you will receive a corresponding notification. Clicking a vulnerability displays its change log. Sort the Vulnerabilities by Risk or Date \u00b6 You can sort the vulnerabilities by the following criteria: Risk: High first Low first Date From latest From earliest You can filter the vulnerabilities by the risk level by pressing one of the following buttons: All \u2014 display the vulnerabilities from all of the risk level groups High risk \u2014 display the high risk vulnerabilities Medium risk \u2014 display the medium risk vulnerabilities Low risk \u2014 display the low risk vulnerabilities Filter the Active and Closed Vulnerabilities \u00b6 Click Active to see the active vulnerabilities. Click closed to see the closed vulnerabilities. You can filter the closed vulnerabilities by clicking the following selectors: all : The list of closed and false vulnerabilities. fixed : The list fixed vulnerabilities only. false : The list of false vulnerabilities only.","title":"Check vuln"},{"location":"user-guides/vulnerabilities/check-vuln/#checking-vulnerabilities","text":"You can check vulnerabilities on the Vulnerabilities tab of the Wallarm interface. By default, all vulnerabilities are divided into groups by the risk level. Lists inside the groups are sorted by the vulnerability discovery date. Wallarm stores the history of all discovered vulnerabilities and checks them regularly \u2014 both the open and closed ones. If a closed vulnerability opens as a result of checking, you will receive a corresponding notification. Clicking a vulnerability displays its change log.","title":"Checking Vulnerabilities"},{"location":"user-guides/vulnerabilities/check-vuln/#sort-the-vulnerabilities-by-risk-or-date","text":"You can sort the vulnerabilities by the following criteria: Risk: High first Low first Date From latest From earliest You can filter the vulnerabilities by the risk level by pressing one of the following buttons: All \u2014 display the vulnerabilities from all of the risk level groups High risk \u2014 display the high risk vulnerabilities Medium risk \u2014 display the medium risk vulnerabilities Low risk \u2014 display the low risk vulnerabilities","title":"Sort the Vulnerabilities by Risk or Date"},{"location":"user-guides/vulnerabilities/check-vuln/#filter-the-active-and-closed-vulnerabilities","text":"Click Active to see the active vulnerabilities. Click closed to see the closed vulnerabilities. You can filter the closed vulnerabilities by clicking the following selectors: all : The list of closed and false vulnerabilities. fixed : The list fixed vulnerabilities only. false : The list of false vulnerabilities only.","title":"Filter the Active and Closed Vulnerabilities"},{"location":"user-guides/vulnerabilities/close-open-vuln/","text":"Closing and Opening Vulnerabilities \u00b6 You can check vulnerabilities on the Vulnerabilities tab of the Wallarm interface. You can close issues on a fixed vulnerability. You can reopen a closed vulnerability if you think the vulnerability is not fixed. Close a Vulnerability \u00b6 Click the Close button next to the desired vulnerability in the list to close this vulnerability. You can also close the vulnerability by clicking the Close button on the page of the desired vulnerability. Wallarm will mark the vulnerability as closed. Open a Vulnerability \u00b6 Click the Reopen button next to the desired vulnerability in the list to reopen the previously closed vulnerability. You can also reopen the vulnerability by clicking the Open button on the page of the desired vulnerability. The vulnerability will be reopened.","title":"Close open vuln"},{"location":"user-guides/vulnerabilities/close-open-vuln/#closing-and-opening-vulnerabilities","text":"You can check vulnerabilities on the Vulnerabilities tab of the Wallarm interface. You can close issues on a fixed vulnerability. You can reopen a closed vulnerability if you think the vulnerability is not fixed.","title":"Closing and Opening Vulnerabilities"},{"location":"user-guides/vulnerabilities/close-open-vuln/#close-a-vulnerability","text":"Click the Close button next to the desired vulnerability in the list to close this vulnerability. You can also close the vulnerability by clicking the Close button on the page of the desired vulnerability. Wallarm will mark the vulnerability as closed.","title":"Close a Vulnerability"},{"location":"user-guides/vulnerabilities/close-open-vuln/#open-a-vulnerability","text":"Click the Reopen button next to the desired vulnerability in the list to reopen the previously closed vulnerability. You can also reopen the vulnerability by clicking the Open button on the page of the desired vulnerability. The vulnerability will be reopened.","title":"Open a Vulnerability"},{"location":"user-guides/vulnerabilities/false-vuln/","text":"Working with false vulnerabilities \u00b6 False positive occurs when legitimate entity is qualified as a vulnerability. After analyzing a vulnerability, you may conclude that the vulnerability is a false positive. A vulnerability marked as a false positive will be switched to an appropriate status and will not be rechecked. If the detected vulnerability exists but cannot be fixed If the detected vulnerability exists in the protected application but cannot be fixed, we recommend setting up the Create a virtual patch rule. This rule will allow blocking attacks exploiting the detected type of vulnerability and will eliminate the risk of an incident. Mark a vulnerability as a false positive \u00b6 Click the Mark as false button next to the desired vulnerability in the list to mark this vulnerability as a false positive. You can also mark the vulnerability as a false positive by clicking the Mark as false button on the page of the desired vulnerability. Wallarm will requalify the vulnerability as a false positive. Remove a false positive mark \u00b6 The vulnerability marked as a false positive, will be displayed on the Closed tab. To remove a false positive mark, please open a vulnerability card and click Reopen . The vulnerability will be switched to the status Open and will be rechecked with Wallarm WAF tools.","title":"False vuln"},{"location":"user-guides/vulnerabilities/false-vuln/#working-with-false-vulnerabilities","text":"False positive occurs when legitimate entity is qualified as a vulnerability. After analyzing a vulnerability, you may conclude that the vulnerability is a false positive. A vulnerability marked as a false positive will be switched to an appropriate status and will not be rechecked. If the detected vulnerability exists but cannot be fixed If the detected vulnerability exists in the protected application but cannot be fixed, we recommend setting up the Create a virtual patch rule. This rule will allow blocking attacks exploiting the detected type of vulnerability and will eliminate the risk of an incident.","title":"Working with false vulnerabilities"},{"location":"user-guides/vulnerabilities/false-vuln/#mark-a-vulnerability-as-a-false-positive","text":"Click the Mark as false button next to the desired vulnerability in the list to mark this vulnerability as a false positive. You can also mark the vulnerability as a false positive by clicking the Mark as false button on the page of the desired vulnerability. Wallarm will requalify the vulnerability as a false positive.","title":"Mark a vulnerability as a false positive"},{"location":"user-guides/vulnerabilities/false-vuln/#remove-a-false-positive-mark","text":"The vulnerability marked as a false positive, will be displayed on the Closed tab. To remove a false positive mark, please open a vulnerability card and click Reopen . The vulnerability will be switched to the status Open and will be rechecked with Wallarm WAF tools.","title":"Remove a false positive mark"},{"location":"user-guides/vulnerabilities/recheck-vuln/","text":"Rechecking a Vulnerability \u00b6 The Vulnerability tab displays information about vulnerabilities . You can recheck vulnerabilities. Recheck a Vulnerability \u00b6 Click a vulnerability. Click Recheck again . Wallarm will run the vulnerability recheck.","title":"Recheck vuln"},{"location":"user-guides/vulnerabilities/recheck-vuln/#rechecking-a-vulnerability","text":"The Vulnerability tab displays information about vulnerabilities . You can recheck vulnerabilities.","title":"Rechecking a Vulnerability"},{"location":"user-guides/vulnerabilities/recheck-vuln/#recheck-a-vulnerability","text":"Click a vulnerability. Click Recheck again . Wallarm will run the vulnerability recheck.","title":"Recheck a Vulnerability"},{"location":"waf-installation/nginx-plus/","text":"Installing dynamic WAF module for NGINX Plus \u00b6 These instructions describe the steps to install Wallarm WAF as a dynamic module for the official commercial version of NGINX Plus. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of an already existing Wallarm WAF or need to duplicate the installation in the same environment, then please keep the same WAF version as currently used or update all installations to the latest version. For the postanalytics installed separately, versions of substite or duplicate installations must be the same as already installed postanalytics too. To check the installed version of WAF node and postanalytics installed on the same server: Debian apt list wallarm-node Ubuntu apt list wallarm-node CentOS or Amazon Linux 2 yum list wallarm-node To check the versions of WAF node and postanalytics installed on different servers: Debian # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool Ubuntu # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool CentOS or Amazon Linux 2 # run from the server with installed WAF node yum list wallarm-node-nginx # run from the server with installed postanalytics yum list wallarm-node-tarantool If the version 3.0.x is installed, then follow the current instructions for the WAF node and for separate postanalytics . If the version 2.18.x is installed, then follow the instructions for WAF node 2.18 and for separate postanalytics 2.18 or update WAF node packages and separate postanalytics packages to the latest version in all installations. If the version 2.16.x or lower is installed, then please update the WAF node packages and separate postanalytics packages to the latest version in all installations. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Requirements \u00b6 Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud SELinux disabled or configured upon the instructions Executing all commands as a superuser (e.g. root ) For the request processing and postanalytics on different servers: postanalytics installed on the separate server upon the instructions Access to https://repo.wallarm.com to download packages. Ensure the access is not blocked by a firewall Access to https://api.wallarm.com:444 for working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 for working with US Wallarm Cloud. If access can be configured only via the proxy server, then use the instructions Access to GCP storage addresses to download an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers Installed text editor vim , nano , or any other. In the instruction, vim is used Installation options \u00b6 The processing of requests in the WAF is divided into two stages: Primary processing in the NGINX-Wallarm module. The processing is not memory demanding and can be put on frontend servers without changing the server requirements. Statistical analysis of the processed requests in the postanalytics module. Postanalytics is memory demanding, which may require changes in the server configuration or installation of postanalytics on a separate server. Depending on the system architecture, the NGINX-Wallarm and postanalytics modules can be installed on the same server or on different servers . Installation commands for both options are described in the further instructions. Installation \u00b6 1. Install NGINX Plus and dependencies \u00b6 Install NGINX Plus and its dependencies using these official NGINX instructions . Installing on Amazon Linux 2 To install NGINX Plus on Amazon Linux 2, use the CentOS 7 instructions. 2. Add Wallarm WAF repositories \u00b6 Wallarm WAF is installed and updated from the Wallarm repositories. To add repositories, use the commands for your platform: Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 10.x (buster) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 20.04 LTS (focal) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node focal/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Amazon Linux 2 sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm 3. Install Wallarm WAF packages \u00b6 Request processing and postanalytics on the same server \u00b6 To run postanalytics and process the requests on the same server, the following packages are required: nginx-plus-module-wallarm for the NGINX Plus-Wallarm module wallarm-node for the postanalytics module, Tarantool database, and additional NGINX Plus-Wallarm packages Debian sudo apt install --no-install-recommends wallarm-node nginx-plus-module-wallarm Ubuntu sudo apt install --no-install-recommends wallarm-node nginx-plus-module-wallarm CentOS or Amazon Linux 2 sudo yum install wallarm-node nginx-plus-module-wallarm Request processing and postanalytics on different servers \u00b6 To run postanalytics and process the requests on different servers, the following packages are required: wallarm-node-nginx and nginx-plus-module-wallarm for the NGINX Plus-Wallarm module Debian sudo apt install --no-install-recommends wallarm-node-nginx nginx-plus-module-wallarm Ubuntu sudo apt install --no-install-recommends wallarm-node-nginx nginx-plus-module-wallarm CentOS or Amazon Linux 2 sudo yum install wallarm-node-nginx nginx-plus-module-wallarm wallarm-node-tarantool on the separate server for the postanalytics module and Tarantool database (installation steps are described in the instructions ) 4. Connect the Wallarm WAF module \u00b6 Open the file /etc/nginx/nginx.conf : sudo vim /etc/nginx/nginx.conf Add the following directive right after the worker_processes directive: load_module modules/ngx_http_wallarm_module.so ; Configuration example with the added directive: user nginx; worker_processes auto; load_module modules/ngx_http_wallarm_module.so; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; Copy the configuration files for the system setup: sudo cp /usr/share/doc/nginx-plus-module-wallarm/examples/*.conf /etc/nginx/conf.d/ 5. Connect the WAF node to Wallarm Cloud \u00b6 The WAF node interacts with the Wallarm Cloud. To connect the WAF node to the Cloud, proceed with the following steps: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled in the Wallarm Console. You can check mentioned settings by navigating to the users list in the EU Cloud or US Cloud . Run the addnode script in a system with the installed WAF node: EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Input the email and password for your account in the Wallarm Console. Input the WAF node name or click Enter to use an automatically generated name. Open the Wallarm Console \u2192 Nodes section in the EU Cloud or US Cloud and ensure a new WAF node is added to the list. 6. Update Wallarm WAF configuration \u00b6 Main configuration files of NGINX and Wallarm WAF node are located in the directories: /etc/nginx/conf.d/default.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings The file is used for settings applied to all domains. To apply different settings to different domain groups, use the file default.conf or create new configuration files for each domain group (for example, example.com.conf and test.com.conf ). More detailed information about NGINX configuration files is available in the official NGINX documentation . /etc/nginx/conf.d/wallarm-status.conf with WAF node monitoring settings. Detailed description is available within the link /etc/default/wallarm-tarantool or /etc/sysconfig/wallarm-tarantool with the Tarantool database settings Request filtration mode \u00b6 By default, the WAF node is in the status off and does not analyze incoming requests. To enable requests analysis, please follow the steps: Open the file /etc/nginx/conf.d/default.conf : sudo vim /etc/nginx/conf.d/default.conf Add the line wallarm_mode monitoring; to the https , server or location block: Example of the file /etc/nginx/conf.d/default.conf server { # port for which requests are filtered listen 80 ; # domain for which requests are filtered server_name localhost ; # WAF node mode wallarm_mode monitoring ; location / { root /usr/share/nginx/html ; index index.html index.htm ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } When operating in the monitoring mode, the WAF node searches attack signs in requests but does not block detected attacks. We recommend keeping the traffic flowing via the WAF node in the monitoring mode for several days after the WAF node deployment and only then enable the block mode. Learn recommendations on the WAF node operation mode setup \u2192 Memory \u00b6 Postanalytics on the separate server If you installed postanalytics on a separate server, then skip this step as you already have your postanalytics configured. The WAF node uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 75% of the total server memory. To allocate memory for Tarantool: Open the Tarantool configuration file in the editing mode: Debian sudo vim /etc/default/wallarm-tarantool Ubuntu sudo vim /etc/default/wallarm-tarantool CentOS or Amazon Linux 2 sudo vim /etc/sysconfig/wallarm-tarantool Specify memory size in GB in the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 Detailed recommendations about allocating memory for Tarantool are described in these instructions . To apply changes, restart Tarantool: Debian sudo systemctl restart wallarm-tarantool Ubuntu sudo systemctl restart wallarm-tarantool CentOS or Amazon Linux 2 sudo systemctl restart wallarm-tarantool Address of the separate postanalytics server \u00b6 NGINX-Wallarm and postanalytics on the same server If the NGINX-Wallarm and postanalytics modules are installed on the same server, then skip this step. Add postanalytics server addresses to the file /etc/nginx/conf.d/wallarm.conf : upstream wallarm_tarantool { server <ip1>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; server <ip2>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 2 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; max_conns value must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. keepalive value must not be lower than the number of the Tarantool servers. Other configurations \u00b6 To update other NGINX and Wallarm WAF configurations, use the NGINX documentation and the list of available Wallarm WAF directives . 7. Restart NGINX Plus \u00b6 Providing user with root permission If you are running NGINX as a user that does not have root permission, then add this user to the wallarm group using the following command: usermod -aG wallarm <user_name>; where <user_name> is the name of the user without root permission. Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx 8. Test Wallarm WAF operation \u00b6 Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Settings customization \u00b6 Dynamic Wallarm WAF module with default settings is installed for NGINX Plus. To customize Wallarm WAF settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Double\u2011detection of attacks with libdetection","title":"Installing as a dynamic module for NGINX Plus"},{"location":"waf-installation/nginx-plus/#installing-dynamic-waf-module-for-nginx-plus","text":"These instructions describe the steps to install Wallarm WAF as a dynamic module for the official commercial version of NGINX Plus. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of an already existing Wallarm WAF or need to duplicate the installation in the same environment, then please keep the same WAF version as currently used or update all installations to the latest version. For the postanalytics installed separately, versions of substite or duplicate installations must be the same as already installed postanalytics too. To check the installed version of WAF node and postanalytics installed on the same server: Debian apt list wallarm-node Ubuntu apt list wallarm-node CentOS or Amazon Linux 2 yum list wallarm-node To check the versions of WAF node and postanalytics installed on different servers: Debian # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool Ubuntu # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool CentOS or Amazon Linux 2 # run from the server with installed WAF node yum list wallarm-node-nginx # run from the server with installed postanalytics yum list wallarm-node-tarantool If the version 3.0.x is installed, then follow the current instructions for the WAF node and for separate postanalytics . If the version 2.18.x is installed, then follow the instructions for WAF node 2.18 and for separate postanalytics 2.18 or update WAF node packages and separate postanalytics packages to the latest version in all installations. If the version 2.16.x or lower is installed, then please update the WAF node packages and separate postanalytics packages to the latest version in all installations. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"Installing dynamic WAF module for NGINX Plus"},{"location":"waf-installation/nginx-plus/#requirements","text":"Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud SELinux disabled or configured upon the instructions Executing all commands as a superuser (e.g. root ) For the request processing and postanalytics on different servers: postanalytics installed on the separate server upon the instructions Access to https://repo.wallarm.com to download packages. Ensure the access is not blocked by a firewall Access to https://api.wallarm.com:444 for working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 for working with US Wallarm Cloud. If access can be configured only via the proxy server, then use the instructions Access to GCP storage addresses to download an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers Installed text editor vim , nano , or any other. In the instruction, vim is used","title":"Requirements"},{"location":"waf-installation/nginx-plus/#installation-options","text":"The processing of requests in the WAF is divided into two stages: Primary processing in the NGINX-Wallarm module. The processing is not memory demanding and can be put on frontend servers without changing the server requirements. Statistical analysis of the processed requests in the postanalytics module. Postanalytics is memory demanding, which may require changes in the server configuration or installation of postanalytics on a separate server. Depending on the system architecture, the NGINX-Wallarm and postanalytics modules can be installed on the same server or on different servers . Installation commands for both options are described in the further instructions.","title":"Installation options"},{"location":"waf-installation/nginx-plus/#installation","text":"","title":"Installation"},{"location":"waf-installation/nginx-plus/#1-install-nginx-plus-and-dependencies","text":"Install NGINX Plus and its dependencies using these official NGINX instructions . Installing on Amazon Linux 2 To install NGINX Plus on Amazon Linux 2, use the CentOS 7 instructions.","title":"1. Install NGINX Plus and dependencies"},{"location":"waf-installation/nginx-plus/#2-add-wallarm-waf-repositories","text":"Wallarm WAF is installed and updated from the Wallarm repositories. To add repositories, use the commands for your platform: Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 10.x (buster) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 20.04 LTS (focal) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node focal/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Amazon Linux 2 sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm","title":"2. Add Wallarm WAF repositories"},{"location":"waf-installation/nginx-plus/#3-install-wallarm-waf-packages","text":"","title":"3. Install Wallarm WAF packages"},{"location":"waf-installation/nginx-plus/#request-processing-and-postanalytics-on-the-same-server","text":"To run postanalytics and process the requests on the same server, the following packages are required: nginx-plus-module-wallarm for the NGINX Plus-Wallarm module wallarm-node for the postanalytics module, Tarantool database, and additional NGINX Plus-Wallarm packages Debian sudo apt install --no-install-recommends wallarm-node nginx-plus-module-wallarm Ubuntu sudo apt install --no-install-recommends wallarm-node nginx-plus-module-wallarm CentOS or Amazon Linux 2 sudo yum install wallarm-node nginx-plus-module-wallarm","title":"Request processing and postanalytics on the same server"},{"location":"waf-installation/nginx-plus/#request-processing-and-postanalytics-on-different-servers","text":"To run postanalytics and process the requests on different servers, the following packages are required: wallarm-node-nginx and nginx-plus-module-wallarm for the NGINX Plus-Wallarm module Debian sudo apt install --no-install-recommends wallarm-node-nginx nginx-plus-module-wallarm Ubuntu sudo apt install --no-install-recommends wallarm-node-nginx nginx-plus-module-wallarm CentOS or Amazon Linux 2 sudo yum install wallarm-node-nginx nginx-plus-module-wallarm wallarm-node-tarantool on the separate server for the postanalytics module and Tarantool database (installation steps are described in the instructions )","title":"Request processing and postanalytics on different servers"},{"location":"waf-installation/nginx-plus/#4-connect-the-wallarm-waf-module","text":"Open the file /etc/nginx/nginx.conf : sudo vim /etc/nginx/nginx.conf Add the following directive right after the worker_processes directive: load_module modules/ngx_http_wallarm_module.so ; Configuration example with the added directive: user nginx; worker_processes auto; load_module modules/ngx_http_wallarm_module.so; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; Copy the configuration files for the system setup: sudo cp /usr/share/doc/nginx-plus-module-wallarm/examples/*.conf /etc/nginx/conf.d/","title":"4. Connect the Wallarm WAF module"},{"location":"waf-installation/nginx-plus/#5-connect-the-waf-node-to-wallarm-cloud","text":"The WAF node interacts with the Wallarm Cloud. To connect the WAF node to the Cloud, proceed with the following steps: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled in the Wallarm Console. You can check mentioned settings by navigating to the users list in the EU Cloud or US Cloud . Run the addnode script in a system with the installed WAF node: EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Input the email and password for your account in the Wallarm Console. Input the WAF node name or click Enter to use an automatically generated name. Open the Wallarm Console \u2192 Nodes section in the EU Cloud or US Cloud and ensure a new WAF node is added to the list.","title":"5. Connect the WAF node to Wallarm Cloud"},{"location":"waf-installation/nginx-plus/#6-update-wallarm-waf-configuration","text":"Main configuration files of NGINX and Wallarm WAF node are located in the directories: /etc/nginx/conf.d/default.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings The file is used for settings applied to all domains. To apply different settings to different domain groups, use the file default.conf or create new configuration files for each domain group (for example, example.com.conf and test.com.conf ). More detailed information about NGINX configuration files is available in the official NGINX documentation . /etc/nginx/conf.d/wallarm-status.conf with WAF node monitoring settings. Detailed description is available within the link /etc/default/wallarm-tarantool or /etc/sysconfig/wallarm-tarantool with the Tarantool database settings","title":"6. Update Wallarm WAF configuration"},{"location":"waf-installation/nginx-plus/#request-filtration-mode","text":"By default, the WAF node is in the status off and does not analyze incoming requests. To enable requests analysis, please follow the steps: Open the file /etc/nginx/conf.d/default.conf : sudo vim /etc/nginx/conf.d/default.conf Add the line wallarm_mode monitoring; to the https , server or location block: Example of the file /etc/nginx/conf.d/default.conf server { # port for which requests are filtered listen 80 ; # domain for which requests are filtered server_name localhost ; # WAF node mode wallarm_mode monitoring ; location / { root /usr/share/nginx/html ; index index.html index.htm ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } When operating in the monitoring mode, the WAF node searches attack signs in requests but does not block detected attacks. We recommend keeping the traffic flowing via the WAF node in the monitoring mode for several days after the WAF node deployment and only then enable the block mode. Learn recommendations on the WAF node operation mode setup \u2192","title":"Request filtration mode"},{"location":"waf-installation/nginx-plus/#memory","text":"Postanalytics on the separate server If you installed postanalytics on a separate server, then skip this step as you already have your postanalytics configured. The WAF node uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 75% of the total server memory. To allocate memory for Tarantool: Open the Tarantool configuration file in the editing mode: Debian sudo vim /etc/default/wallarm-tarantool Ubuntu sudo vim /etc/default/wallarm-tarantool CentOS or Amazon Linux 2 sudo vim /etc/sysconfig/wallarm-tarantool Specify memory size in GB in the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 Detailed recommendations about allocating memory for Tarantool are described in these instructions . To apply changes, restart Tarantool: Debian sudo systemctl restart wallarm-tarantool Ubuntu sudo systemctl restart wallarm-tarantool CentOS or Amazon Linux 2 sudo systemctl restart wallarm-tarantool","title":"Memory"},{"location":"waf-installation/nginx-plus/#address-of-the-separate-postanalytics-server","text":"NGINX-Wallarm and postanalytics on the same server If the NGINX-Wallarm and postanalytics modules are installed on the same server, then skip this step. Add postanalytics server addresses to the file /etc/nginx/conf.d/wallarm.conf : upstream wallarm_tarantool { server <ip1>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; server <ip2>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 2 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; max_conns value must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. keepalive value must not be lower than the number of the Tarantool servers.","title":"Address of the separate postanalytics server"},{"location":"waf-installation/nginx-plus/#other-configurations","text":"To update other NGINX and Wallarm WAF configurations, use the NGINX documentation and the list of available Wallarm WAF directives .","title":"Other configurations"},{"location":"waf-installation/nginx-plus/#7-restart-nginx-plus","text":"Providing user with root permission If you are running NGINX as a user that does not have root permission, then add this user to the wallarm group using the following command: usermod -aG wallarm <user_name>; where <user_name> is the name of the user without root permission. Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx","title":"7. Restart NGINX Plus"},{"location":"waf-installation/nginx-plus/#8-test-wallarm-waf-operation","text":"Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"8. Test Wallarm WAF operation"},{"location":"waf-installation/nginx-plus/#settings-customization","text":"Dynamic Wallarm WAF module with default settings is installed for NGINX Plus. To customize Wallarm WAF settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Double\u2011detection of attacks with libdetection","title":"Settings customization"},{"location":"waf-installation/cloud-platforms/private-cloud/","text":"Deployment of the WAF node to the private clouds \u00b6 Private clouds are cloud environments deployed solely to your infrastructure. This document overviews the principles of deploying the WAF node to the private clouds. Principles of deploying the WAF node Docker container to the private cloud \u00b6 One of the methods of deploying the WAF node to the private cloud is deploying the Docker image of the NGINX-based WAF node . Depending on the private cloud platform architecture and your application deployment scheme, you can deploy the Docker container to the private cloud in the following ways: Using a separate container deployment service provided by the cloud Using the standard docker run command in the instance based on any operating system Before deploying the WAF node Docker container to the private cloud, it is recommended to review the container deployment methods described in the documentation of this cloud and select the most suitable one. If you deployed a well-known cloud platform as the private cloud, you can follow the ready-made instructions developed by Wallarm . Principles of installing the WAF node from DEB and RPM packages on the private cloud \u00b6 One of the methods of installing the WAF node on the private cloud is installing from source DEB or RPM packages. Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can install the WAF node from DEB and RPM packages on the private cloud as follows: On the private cloud, create an instance from the supported operating system image. In the instance, install the packages of the WAF node and of the web server or API gateway suitable for your application architecture and supported by Wallarm. You can use one of the following instructions: Installing the WAF node as the NGINX Stable module Installing the WAF node as the NGINX Plus module Installing the WAF node as the Kong module Before installing the WAF node on the private cloud, it is recommended to review the instructions on creating and managing instances on the deployed cloud. If you deployed a well-known cloud platform as the private cloud, you can follow the ready-made instructions developed by Wallarm .","title":"Deployment to the private clouds"},{"location":"waf-installation/cloud-platforms/private-cloud/#deployment-of-the-waf-node-to-the-private-clouds","text":"Private clouds are cloud environments deployed solely to your infrastructure. This document overviews the principles of deploying the WAF node to the private clouds.","title":"Deployment of the WAF node to the private clouds"},{"location":"waf-installation/cloud-platforms/private-cloud/#principles-of-deploying-the-waf-node-docker-container-to-the-private-cloud","text":"One of the methods of deploying the WAF node to the private cloud is deploying the Docker image of the NGINX-based WAF node . Depending on the private cloud platform architecture and your application deployment scheme, you can deploy the Docker container to the private cloud in the following ways: Using a separate container deployment service provided by the cloud Using the standard docker run command in the instance based on any operating system Before deploying the WAF node Docker container to the private cloud, it is recommended to review the container deployment methods described in the documentation of this cloud and select the most suitable one. If you deployed a well-known cloud platform as the private cloud, you can follow the ready-made instructions developed by Wallarm .","title":"Principles of deploying the WAF node Docker container to the private cloud"},{"location":"waf-installation/cloud-platforms/private-cloud/#principles-of-installing-the-waf-node-from-deb-and-rpm-packages-on-the-private-cloud","text":"One of the methods of installing the WAF node on the private cloud is installing from source DEB or RPM packages. Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can install the WAF node from DEB and RPM packages on the private cloud as follows: On the private cloud, create an instance from the supported operating system image. In the instance, install the packages of the WAF node and of the web server or API gateway suitable for your application architecture and supported by Wallarm. You can use one of the following instructions: Installing the WAF node as the NGINX Stable module Installing the WAF node as the NGINX Plus module Installing the WAF node as the Kong module Before installing the WAF node on the private cloud, it is recommended to review the instructions on creating and managing instances on the deployed cloud. If you deployed a well-known cloud platform as the private cloud, you can follow the ready-made instructions developed by Wallarm .","title":"Principles of installing the WAF node from DEB and RPM packages on the private cloud"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/deb-rpm-packages/","text":"Installation of the WAF node from DEB or RPM packages on Alibaba Cloud \u00b6 This quick guide provides the steps to install the WAF node from the source packages on a separate Alibaba Cloud instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the instructions on Alibaba Cloud Elastic Compute Service (ECS) . Requirements \u00b6 Access to the Alibaba Cloud Console Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud WAF node installation options \u00b6 Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module Installing the WAF node as the NGINX Stable module \u00b6 To install the WAF node as the NGINX Stable module in the Alibaba Cloud instance: Create an Alibaba Cloud instance from the operating system image supported by Wallarm following the Alibaba Cloud instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Alibaba Cloud instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the NGINX Plus module \u00b6 To install the WAF node as the NGINX Plus module in the Alibaba Cloud instance: Create an Alibaba Cloud instance from the operating system image supported by Wallarm following the Alibaba Cloud instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Alibaba Cloud instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the Kong module \u00b6 To install the WAF node as the Kong module in the Alibaba Cloud instance: Create an Alibaba Cloud instance from the operating system image supported by Wallarm following the Alibaba Cloud instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the Alibaba Cloud instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installation from DEB or RPM packages on Alibaba Cloud"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/deb-rpm-packages/#installation-of-the-waf-node-from-deb-or-rpm-packages-on-alibaba-cloud","text":"This quick guide provides the steps to install the WAF node from the source packages on a separate Alibaba Cloud instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the instructions on Alibaba Cloud Elastic Compute Service (ECS) .","title":"Installation of the WAF node from DEB or RPM packages on Alibaba Cloud"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/deb-rpm-packages/#requirements","text":"Access to the Alibaba Cloud Console Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/deb-rpm-packages/#waf-node-installation-options","text":"Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module","title":"WAF node installation options"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-stable-module","text":"To install the WAF node as the NGINX Stable module in the Alibaba Cloud instance: Create an Alibaba Cloud instance from the operating system image supported by Wallarm following the Alibaba Cloud instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Alibaba Cloud instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Stable module"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-plus-module","text":"To install the WAF node as the NGINX Plus module in the Alibaba Cloud instance: Create an Alibaba Cloud instance from the operating system image supported by Wallarm following the Alibaba Cloud instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Alibaba Cloud instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Plus module"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/deb-rpm-packages/#installing-the-waf-node-as-the-kong-module","text":"To install the WAF node as the Kong module in the Alibaba Cloud instance: Create an Alibaba Cloud instance from the operating system image supported by Wallarm following the Alibaba Cloud instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the Alibaba Cloud instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the Kong module"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/docker-container/","text":"Deployment of the WAF node Docker image to Alibaba Cloud \u00b6 This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Alibaba Cloud platform using the Alibaba Cloud Elastic Compute Service (ECS) . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you read the appropriate Alibaba Cloud documentation . Requirements \u00b6 Access to the Alibaba Cloud Console Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud Options for the WAF node Docker container configuration \u00b6 The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container. Deploying the WAF node Docker container configured through environment variables \u00b6 To deploy the containerized WAF node configured only through environment variables, you should create the Alibaba Cloud instance and run the Docker container in this instance. You can perform these steps via the Alibaba Cloud Console or Alibaba Cloud CLI . In these instructions, Alibaba Cloud Console is used. Open the Alibaba Cloud Console \u2192 the list of services \u2192 Elastic Compute Service \u2192 Instances . Create the instance following the Alibaba Cloud instructions and the guidelines below: The instance can be based on the image of any operating system. Since the instance should be available for external resources, public IP address or domain should be configured in the instance settings. The instance settings should reflect the method used to connect to the instance . Connect to the instance by one of the methods described in the Alibaba Cloud documentation . Install the Docker packages in the instance following the instructions for an appropriate operating system . Set instance environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Run the WAF node Docker container by using the docker run command with passed environment variables and mounted configuration file: Command for the Wallarm EU Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> -p 80 :80 wallarm/node:3.0.0-2 Command for the Wallarm US Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> -e WALLARM_API_HOST = 'us1.api.wallarm.com' -p 80 :80 wallarm/node:3.0.0-2 -p : port the WAF node listens to. The value should be the same as the instance port. -e : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Test the WAF node operation . Deploying the WAF node Docker container configured through the mounted file \u00b6 To deploy the containerized WAF node configured through environment variables and mounted file, you should create the Alibaba Cloud instance, locate the WAF node configuration file in this instance file system and run the Docker container in this instance. You can perform these steps via the Alibaba Cloud Console or Alibaba Cloud CLI . In these instructions, Alibaba Cloud Console is used. Open the Alibaba Cloud Console \u2192 the list of services \u2192 Elastic Compute Service \u2192 Instances . Create the instance following the Alibaba Cloud instructions and the guidelines below: The instance can be based on the image of any operating system. Since the instance should be available for external resources, public IP address or domain should be configured in the instance settings. The instance settings should reflect the method used to connect to the instance . Connect to the instance by one of the methods described in the Alibaba Cloud documentation . Install the Docker packages in the instance following the instructions for an appropriate operating system . Set instance environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In the instance, create the directory with the file default containing the WAF node configuration (for example, the directory can be named as configs ). An example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Run the WAF node Docker container by using the docker run command with passed environment variables and mounted configuration file: Command for the Wallarm EU Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -v <INSTANCE_PATH_TO_CONFIG>:<CONTAINER_PATH_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 Command for the Wallarm US Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v <INSTANCE_PATH_TO_CONFIG>:<DIRECTORY_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 <INSTANCE_PATH_TO_CONFIG> : path to the configuration file created in the previous step. For example, configs . <DIRECTORY_FOR_MOUNTING> : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. -p : port the WAF node listens to. The value should be the same as the instance port. -e : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Test the WAF node operation . Testing the WAF node operation \u00b6 Open the Alibaba Cloud Console \u2192 the list of services \u2192 Elastic Compute Service \u2192 Instances and copy the public IP address of the instance from the IP address column. If the IP address is empty, please ensure the instance is in the Running status. Send the request with test SQLI and XSS attacks to the copied address: curl http://<COPIED_IP>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. To view details on errors that occurred during the container deployment, please connect to the instance by one of the methods and review the container logs . If the instance is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Deployment of Docker image to Alibaba Cloud"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/docker-container/#deployment-of-the-waf-node-docker-image-to-alibaba-cloud","text":"This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Alibaba Cloud platform using the Alibaba Cloud Elastic Compute Service (ECS) . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you read the appropriate Alibaba Cloud documentation .","title":"Deployment of the WAF node Docker image to Alibaba Cloud"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/docker-container/#requirements","text":"Access to the Alibaba Cloud Console Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/docker-container/#options-for-the-waf-node-docker-container-configuration","text":"The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container.","title":"Options for the WAF node Docker container configuration"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/docker-container/#deploying-the-waf-node-docker-container-configured-through-environment-variables","text":"To deploy the containerized WAF node configured only through environment variables, you should create the Alibaba Cloud instance and run the Docker container in this instance. You can perform these steps via the Alibaba Cloud Console or Alibaba Cloud CLI . In these instructions, Alibaba Cloud Console is used. Open the Alibaba Cloud Console \u2192 the list of services \u2192 Elastic Compute Service \u2192 Instances . Create the instance following the Alibaba Cloud instructions and the guidelines below: The instance can be based on the image of any operating system. Since the instance should be available for external resources, public IP address or domain should be configured in the instance settings. The instance settings should reflect the method used to connect to the instance . Connect to the instance by one of the methods described in the Alibaba Cloud documentation . Install the Docker packages in the instance following the instructions for an appropriate operating system . Set instance environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Run the WAF node Docker container by using the docker run command with passed environment variables and mounted configuration file: Command for the Wallarm EU Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> -p 80 :80 wallarm/node:3.0.0-2 Command for the Wallarm US Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> -e WALLARM_API_HOST = 'us1.api.wallarm.com' -p 80 :80 wallarm/node:3.0.0-2 -p : port the WAF node listens to. The value should be the same as the instance port. -e : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through environment variables"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/docker-container/#deploying-the-waf-node-docker-container-configured-through-the-mounted-file","text":"To deploy the containerized WAF node configured through environment variables and mounted file, you should create the Alibaba Cloud instance, locate the WAF node configuration file in this instance file system and run the Docker container in this instance. You can perform these steps via the Alibaba Cloud Console or Alibaba Cloud CLI . In these instructions, Alibaba Cloud Console is used. Open the Alibaba Cloud Console \u2192 the list of services \u2192 Elastic Compute Service \u2192 Instances . Create the instance following the Alibaba Cloud instructions and the guidelines below: The instance can be based on the image of any operating system. Since the instance should be available for external resources, public IP address or domain should be configured in the instance settings. The instance settings should reflect the method used to connect to the instance . Connect to the instance by one of the methods described in the Alibaba Cloud documentation . Install the Docker packages in the instance following the instructions for an appropriate operating system . Set instance environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In the instance, create the directory with the file default containing the WAF node configuration (for example, the directory can be named as configs ). An example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Run the WAF node Docker container by using the docker run command with passed environment variables and mounted configuration file: Command for the Wallarm EU Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -v <INSTANCE_PATH_TO_CONFIG>:<CONTAINER_PATH_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 Command for the Wallarm US Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v <INSTANCE_PATH_TO_CONFIG>:<DIRECTORY_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 <INSTANCE_PATH_TO_CONFIG> : path to the configuration file created in the previous step. For example, configs . <DIRECTORY_FOR_MOUNTING> : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. -p : port the WAF node listens to. The value should be the same as the instance port. -e : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through the mounted file"},{"location":"waf-installation/cloud-platforms/alibaba-cloud/docker-container/#testing-the-waf-node-operation","text":"Open the Alibaba Cloud Console \u2192 the list of services \u2192 Elastic Compute Service \u2192 Instances and copy the public IP address of the instance from the IP address column. If the IP address is empty, please ensure the instance is in the Running status. Send the request with test SQLI and XSS attacks to the copied address: curl http://<COPIED_IP>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. To view details on errors that occurred during the container deployment, please connect to the instance by one of the methods and review the container logs . If the instance is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Testing the WAF node operation"},{"location":"waf-installation/cloud-platforms/aws/deb-rpm-packages/","text":"Installation of the WAF node from DEB or RPM packages on AWS \u00b6 This quick guide provides the steps to install the WAF node from the source packages on a separate Amazon EC2 instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the AWS instructions on the Elastic Load Balancing service . Requirements \u00b6 AWS account and user with the admin permissions Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud WAF node installation options \u00b6 Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module Installing the WAF node as the NGINX Stable module \u00b6 To install the WAF node as the NGINX Stable module in the Amazon EC2 instance: Create an Amazon EC2 instance from the operating system image supported by Wallarm following the AWS instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Amazon Linux 2 Connect to the created instance following the AWS instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the NGINX Plus module \u00b6 To install the WAF node as the NGINX Plus module in the Amazon EC2 instance: Create an Amazon EC2 instance from the operating system image supported by Wallarm following the AWS instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Amazon Linux 2 Connect to the created instance following the AWS instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the Kong module \u00b6 To install the WAF node as the Kong module in the Amazon EC2 instance: Create an Amazon EC2 instance from the operating system image supported by Wallarm following the AWS instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the AWS instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installation from DEB or RPM packages on AWS"},{"location":"waf-installation/cloud-platforms/aws/deb-rpm-packages/#installation-of-the-waf-node-from-deb-or-rpm-packages-on-aws","text":"This quick guide provides the steps to install the WAF node from the source packages on a separate Amazon EC2 instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the AWS instructions on the Elastic Load Balancing service .","title":"Installation of the WAF node from DEB or RPM packages on AWS"},{"location":"waf-installation/cloud-platforms/aws/deb-rpm-packages/#requirements","text":"AWS account and user with the admin permissions Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/aws/deb-rpm-packages/#waf-node-installation-options","text":"Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module","title":"WAF node installation options"},{"location":"waf-installation/cloud-platforms/aws/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-stable-module","text":"To install the WAF node as the NGINX Stable module in the Amazon EC2 instance: Create an Amazon EC2 instance from the operating system image supported by Wallarm following the AWS instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Amazon Linux 2 Connect to the created instance following the AWS instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Stable module"},{"location":"waf-installation/cloud-platforms/aws/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-plus-module","text":"To install the WAF node as the NGINX Plus module in the Amazon EC2 instance: Create an Amazon EC2 instance from the operating system image supported by Wallarm following the AWS instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Amazon Linux 2 Connect to the created instance following the AWS instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Plus module"},{"location":"waf-installation/cloud-platforms/aws/deb-rpm-packages/#installing-the-waf-node-as-the-kong-module","text":"To install the WAF node as the Kong module in the Amazon EC2 instance: Create an Amazon EC2 instance from the operating system image supported by Wallarm following the AWS instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the AWS instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the Kong module"},{"location":"waf-installation/cloud-platforms/aws/docker-container/","text":"Deployment of the WAF node Docker image to AWS \u00b6 This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Amazon cloud platform using the Amazon Elastic Container Service (Amazon ECS) . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review an appropriate part of the AWS instructions . Requirements \u00b6 AWS account and user with the admin permissions AWS CLI 1 or AWS CLI 2 properly installed and configured Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud Options for the WAF node Docker container configuration \u00b6 The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container. Deploying the WAF node Docker container configured through environment variables \u00b6 To deploy the containerized WAF node configured only through environment variables, the AWS Management Console and AWS CLI are used. Sign in to the AWS Management Console \u2192 the Services list \u2192 Elastic Container Service . Proceed to cluster creation by the button Create Cluster : Select the template EC2 Linux + Networking . Specify the cluster name, for example: waf-cluster . If required, set other settings following the AWS instructions . Save the cluster. Save the sensitive data used for the WAF node and Wallarm Cloud connection setup in the AWS Secrets Manager or AWS Systems Manager \u2192 Parameter Store . Sensitive data includes the following environment variables: <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In these instructions, sensitive data is stored in the AWS Secrets Manager. Access to the sensitive data storage To allow the Docker container to read the encrypted sensitive data, please ensure the AWS settings meet the following requirements: Sensitive data is stored in the region used to run the Docker container. The IAM policy SecretsManagerReadWrite is attached to the user specified in the executionRoleArn parameter of the task definition. More details on the IAM policies setup \u2192 Create the following local JSON file with the task definition (task definition sets the Docker container operating scenario): If you use the Wallarm EU Cloud { \"executionRoleArn\" : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/ecsTaskExecutionRole\" , \"containerDefinitions\" : [ { \"memory\" : 128 , \"portMappings\" : [ { \"hostPort\" : 80 , \"containerPort\" : 80 , \"protocol\" : \"tcp\" } ], \"essential\" : true , \"environment\" : [ { \"name\" : \"NGINX_BACKEND\" , \"value\" : \"<HOST_TO_PROTECT_WITH_WAF>\" } ], \"secrets\" : [ { \"name\" : \"DEPLOY_PASSWORD\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_PASSWORD_PARAMETER_NAME>::\" }, { \"name\" : \"DEPLOY_USER\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_USER_PARAMETER_NAME>::\" } ], \"name\" : \"waf-container\" , \"image\" : \"registry-1.docker.io/wallarm/node:3.0.0-2\" } ], \"family\" : \"wallarm-waf-node\" } If you use the Wallarm US Cloud { \"executionRoleArn\" : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/ecsTaskExecutionRole\" , \"containerDefinitions\" : [ { \"memory\" : 128 , \"portMappings\" : [ { \"hostPort\" : 80 , \"containerPort\" : 80 , \"protocol\" : \"tcp\" } ], \"essential\" : true , \"environment\" : [ { \"name\" : \"WALLARM_API_HOST\" , \"value\" : \"us1.api.wallarm.com\" }, { \"name\" : \"NGINX_BACKEND\" , \"value\" : \"<HOST_TO_PROTECT_WITH_WAF>\" } ], \"secrets\" : [ { \"name\" : \"DEPLOY_PASSWORD\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_PASSWORD_PARAMETER_NAME>::\" }, { \"name\" : \"DEPLOY_USER\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_USER_PARAMETER_NAME>::\" } ], \"name\" : \"waf-container\" , \"image\" : \"registry-1.docker.io/wallarm/node:3.0.0-2\" } ], \"family\" : \"wallarm-waf-node\" } <AWS_ACCOUNT_ID> : your AWS account ID . The environment object sets the environment variables that should be passed to the Docker container in a text format. The set of available environment variables is described in the table below. It is recommended to pass the variables DEPLOY_USER and DEPLOY_PASSWORD in the secrets object. The secret object sets the environment variables that should be passed to the Docker container as the links to the sensitive data storage. The format of values depends on the selected storage (see more details in the AWS Secrets Manager or AWS Systems Manager \u2192 Parameter Store documentation). It is recommended to pass the variables DEPLOY_USER and DEPLOY_PASSWORD in the secrets object. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No All configuration file parameters are described in the AWS documentation . Register the task definition based on the JSON configuration file by using the aws ecs register\u2011task\u2011definition command: aws ecs register-task-definition --cli-input-json file://<PATH_TO_JSON_FILE>/<JSON_FILE_NAME> <PATH_TO_JSON_FILE> : path to the JSON file with the task definition on the local machine. <JSON_FILE_NAME> : name and extension of the JSON file with the task definition. Run the task in the cluster by using the aws ecs run-task command: aws ecs run-task --cluster <CLUSTER_NAME> --launch-type EC2 --task-definition <FAMILY_PARAM_VALUE> <CLUSTER_NAME> : name of the cluster created in the first step. For example, waf-cluster . <FAMILY_PARAM_VALUE> : name of the created task definition. The value should correspond to the family parameter value specified in the JSON file with the task definition. For example, wallarm-waf-node . Open the AWS Management Console \u2192 Elastic Container Service \u2192 cluster with the running task \u2192 Tasks and ensure the task is displayed in the list. Test the WAF node operation . Deploying the WAF node Docker container configured through the mounted file \u00b6 To deploy the containerized WAF node configured through environment variables and mounted file, the AWS Management Console and AWS CLI are used. In these instructions, the configuration file is mounted from the AWS EFS file system. You can review other methods for mounting the file in the AWS documentation . To deploy the container with environment variables and configuration file mounted from AWS EFS: Sign in to the AWS Management Console \u2192 the Services list \u2192 Elastic Container Service . Proceed to cluster creation by the button Create Cluster : Template : EC2 Linux + Networking . Cluster name : waf-cluster (as an example). Provisioning Model : On-Demand Instance . EC2 instance type : t2.micro . Number of instances : 1 . EC2 AMI ID : Amazon Linux 2 Amazon ECS-optimized AMI . Key pair : key pair for SSH connection to the instance. You will need to connect to the instance via SSH to upload the configuration file to the storage. Other settings can be left as default. When changing other settings, it is recommended to follow the instructions on AWS EFS setup . Configure the AWS EFS storage following steps 2-4 of the AWS instructions . In the 4 th step of AWS instructions, create the configuration file default and place the file in the directory that stores the files for mounting by default. The file default should cover the WAF node configuration. An example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Save the sensitive data used for the WAF node and Wallarm Cloud connection setup in the AWS Secrets Manager or AWS Systems Manager \u2192 Parameter Store . Sensitive data includes the following environment variables: <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In these instructions, sensitive data is stored in the AWS Secrets Manager. Access to the sensitive data storage To allow the Docker container to read the encrypted sensitive data, please ensure the AWS settings meet the following requirements: Sensitive data is stored in the region used to run the Docker container. The IAM policy SecretsManagerReadWrite is attached to the user specified in the executionRoleArn parameter of the task definition. More details on the IAM policies setup \u2192 Create the following local JSON file with the task definition (task definition sets the Docker container operating scenario): If you use the Wallarm EU Cloud { \"executionRoleArn\" : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/ecsTaskExecutionRole\" , \"containerDefinitions\" : [ { \"memory\" : 128 , \"portMappings\" : [ { \"hostPort\" : 80 , \"containerPort\" : 80 , \"protocol\" : \"tcp\" } ], \"essential\" : true , \"mountPoints\" : [ { \"containerPath\" : \"/etc/nginx/sites-enabled\" , \"sourceVolume\" : \"default\" } ], \"secrets\" : [ { \"name\" : \"DEPLOY_PASSWORD\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_PASSWORD_PARAMETER_NAME>::\" }, { \"name\" : \"DEPLOY_USER\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_USER_PARAMETER_NAME>::\" } ], \"name\" : \"waf-container\" , \"image\" : \"registry-1.docker.io/wallarm/node:3.0.0-2\" } ], \"volumes\" : [ { \"name\" : \"default\" , \"efsVolumeConfiguration\" : { \"fileSystemId\" : \"<EFS_FILE_SYSTEM_ID>\" , \"transitEncryption\" : \"ENABLED\" } } ], \"family\" : \"wallarm-waf-node\" } If you use the Wallarm US Cloud { \"executionRoleArn\" : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/ecsTaskExecutionRole\" , \"containerDefinitions\" : [ { \"memory\" : 128 , \"portMappings\" : [ { \"hostPort\" : 80 , \"containerPort\" : 80 , \"protocol\" : \"tcp\" } ], \"essential\" : true , \"mountPoints\" : [ { \"containerPath\" : \"<PATH_FOR_MOUNTED_CONFIG>\" , \"sourceVolume\" : \"<NAME_FROM_VOLUMES_OBJECT>\" } ], \"environment\" : [ { \"name\" : \"WALLARM_API_HOST\" , \"value\" : \"us1.api.wallarm.com\" } ], \"secrets\" : [ { \"name\" : \"DEPLOY_PASSWORD\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_PASSWORD_PARAMETER_NAME>::\" }, { \"name\" : \"DEPLOY_USER\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_USER_PARAMETER_NAME>::\" } ], \"name\" : \"waf-container\" , \"image\" : \"registry-1.docker.io/wallarm/node:3.0.0-2\" } ], \"volumes\" : [ { \"name\" : \"<VOLUME_NAME>\" , \"efsVolumeConfiguration\" : { \"fileSystemId\" : \"<EFS_FILE_SYSTEM_ID>\" , \"transitEncryption\" : \"ENABLED\" } } ], \"family\" : \"wallarm-waf-node\" } <AWS_ACCOUNT_ID> : your AWS account ID . <PATH_FOR_MOUNTED_CONFIG> : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. <NAME_FROM_VOLUMES_OBJECT> : name of the volumes object containing the configuration of the mounted file AWS EFS storage (the value should be the same as <VOLUME_NAME> ). <VOLUME_NAME> : name of the volumes object that contains the configuration of the mounted file AWS EFS storage. <EFS_FILE_SYSTEM_ID> : ID of the AWS EFS file system containing the file that should be mounted to the container. ID is displayed in the AWS Management Console \u2192 Services \u2192 EFS \u2192 File systems . The environment object sets the environment variables that should be passed to the Docker container in a text format. The set of available environment variables is described in the table below. It is recommended to pass the variables DEPLOY_USER and DEPLOY_PASSWORD in the secrets object. The secret object sets the environment variables that should be passed to the Docker container as the links to the sensitive data storage. The format of values depends on the selected storage (see more details in the AWS Secrets Manager or AWS Systems Manager \u2192 Parameter Store documentation). It is recommended to pass the variables DEPLOY_USER and DEPLOY_PASSWORD in the secrets object. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No All configuration file parameters are described in the AWS documentation . Register the task definition based on the JSON configuration file by using the aws ecs register\u2011task\u2011definition command: aws ecs register-task-definition --cli-input-json file://<PATH_TO_JSON_FILE>/<JSON_FILE_NAME> <PATH_TO_JSON_FILE> : path to the JSON file with the task definition on the local machine. <JSON_FILE_NAME> : name and extension of the JSON file with the task definition. Run the task in the cluster by using the aws ecs run-task command: aws ecs run-task --cluster <CLUSTER_NAME> --launch-type EC2 --task-definition <FAMILY_PARAM_VALUE> <CLUSTER_NAME> : name of the cluster created in the first step. For example, waf-cluster . <FAMILY_PARAM_VALUE> : name of the created task definition. The value should correspond to the family parameter value specified in the JSON file with the task definition. For example, wallarm-waf-node . Open the AWS Management Console \u2192 Elastic Container Service \u2192 cluster with the running task \u2192 Tasks and ensure the task is displayed in the list. Test the WAF node operation . Testing the WAF node operation \u00b6 In the AWS Management Console, open the running task and copy the container IP address from the field External Link . If the IP address is empty, please ensure the container is in the RUNNING status. Send the request with test SQLI and XSS attacks to the copied address: curl http://<COPIED_IP>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Details on errors that occurred during the container deployment are displayed in the task details in the AWS Management Console. If the container is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Deployment of Docker image to AWS"},{"location":"waf-installation/cloud-platforms/aws/docker-container/#deployment-of-the-waf-node-docker-image-to-aws","text":"This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Amazon cloud platform using the Amazon Elastic Container Service (Amazon ECS) . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review an appropriate part of the AWS instructions .","title":"Deployment of the WAF node Docker image to AWS"},{"location":"waf-installation/cloud-platforms/aws/docker-container/#requirements","text":"AWS account and user with the admin permissions AWS CLI 1 or AWS CLI 2 properly installed and configured Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/aws/docker-container/#options-for-the-waf-node-docker-container-configuration","text":"The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container.","title":"Options for the WAF node Docker container configuration"},{"location":"waf-installation/cloud-platforms/aws/docker-container/#deploying-the-waf-node-docker-container-configured-through-environment-variables","text":"To deploy the containerized WAF node configured only through environment variables, the AWS Management Console and AWS CLI are used. Sign in to the AWS Management Console \u2192 the Services list \u2192 Elastic Container Service . Proceed to cluster creation by the button Create Cluster : Select the template EC2 Linux + Networking . Specify the cluster name, for example: waf-cluster . If required, set other settings following the AWS instructions . Save the cluster. Save the sensitive data used for the WAF node and Wallarm Cloud connection setup in the AWS Secrets Manager or AWS Systems Manager \u2192 Parameter Store . Sensitive data includes the following environment variables: <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In these instructions, sensitive data is stored in the AWS Secrets Manager. Access to the sensitive data storage To allow the Docker container to read the encrypted sensitive data, please ensure the AWS settings meet the following requirements: Sensitive data is stored in the region used to run the Docker container. The IAM policy SecretsManagerReadWrite is attached to the user specified in the executionRoleArn parameter of the task definition. More details on the IAM policies setup \u2192 Create the following local JSON file with the task definition (task definition sets the Docker container operating scenario): If you use the Wallarm EU Cloud { \"executionRoleArn\" : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/ecsTaskExecutionRole\" , \"containerDefinitions\" : [ { \"memory\" : 128 , \"portMappings\" : [ { \"hostPort\" : 80 , \"containerPort\" : 80 , \"protocol\" : \"tcp\" } ], \"essential\" : true , \"environment\" : [ { \"name\" : \"NGINX_BACKEND\" , \"value\" : \"<HOST_TO_PROTECT_WITH_WAF>\" } ], \"secrets\" : [ { \"name\" : \"DEPLOY_PASSWORD\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_PASSWORD_PARAMETER_NAME>::\" }, { \"name\" : \"DEPLOY_USER\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_USER_PARAMETER_NAME>::\" } ], \"name\" : \"waf-container\" , \"image\" : \"registry-1.docker.io/wallarm/node:3.0.0-2\" } ], \"family\" : \"wallarm-waf-node\" } If you use the Wallarm US Cloud { \"executionRoleArn\" : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/ecsTaskExecutionRole\" , \"containerDefinitions\" : [ { \"memory\" : 128 , \"portMappings\" : [ { \"hostPort\" : 80 , \"containerPort\" : 80 , \"protocol\" : \"tcp\" } ], \"essential\" : true , \"environment\" : [ { \"name\" : \"WALLARM_API_HOST\" , \"value\" : \"us1.api.wallarm.com\" }, { \"name\" : \"NGINX_BACKEND\" , \"value\" : \"<HOST_TO_PROTECT_WITH_WAF>\" } ], \"secrets\" : [ { \"name\" : \"DEPLOY_PASSWORD\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_PASSWORD_PARAMETER_NAME>::\" }, { \"name\" : \"DEPLOY_USER\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_USER_PARAMETER_NAME>::\" } ], \"name\" : \"waf-container\" , \"image\" : \"registry-1.docker.io/wallarm/node:3.0.0-2\" } ], \"family\" : \"wallarm-waf-node\" } <AWS_ACCOUNT_ID> : your AWS account ID . The environment object sets the environment variables that should be passed to the Docker container in a text format. The set of available environment variables is described in the table below. It is recommended to pass the variables DEPLOY_USER and DEPLOY_PASSWORD in the secrets object. The secret object sets the environment variables that should be passed to the Docker container as the links to the sensitive data storage. The format of values depends on the selected storage (see more details in the AWS Secrets Manager or AWS Systems Manager \u2192 Parameter Store documentation). It is recommended to pass the variables DEPLOY_USER and DEPLOY_PASSWORD in the secrets object. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No All configuration file parameters are described in the AWS documentation . Register the task definition based on the JSON configuration file by using the aws ecs register\u2011task\u2011definition command: aws ecs register-task-definition --cli-input-json file://<PATH_TO_JSON_FILE>/<JSON_FILE_NAME> <PATH_TO_JSON_FILE> : path to the JSON file with the task definition on the local machine. <JSON_FILE_NAME> : name and extension of the JSON file with the task definition. Run the task in the cluster by using the aws ecs run-task command: aws ecs run-task --cluster <CLUSTER_NAME> --launch-type EC2 --task-definition <FAMILY_PARAM_VALUE> <CLUSTER_NAME> : name of the cluster created in the first step. For example, waf-cluster . <FAMILY_PARAM_VALUE> : name of the created task definition. The value should correspond to the family parameter value specified in the JSON file with the task definition. For example, wallarm-waf-node . Open the AWS Management Console \u2192 Elastic Container Service \u2192 cluster with the running task \u2192 Tasks and ensure the task is displayed in the list. Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through environment variables"},{"location":"waf-installation/cloud-platforms/aws/docker-container/#deploying-the-waf-node-docker-container-configured-through-the-mounted-file","text":"To deploy the containerized WAF node configured through environment variables and mounted file, the AWS Management Console and AWS CLI are used. In these instructions, the configuration file is mounted from the AWS EFS file system. You can review other methods for mounting the file in the AWS documentation . To deploy the container with environment variables and configuration file mounted from AWS EFS: Sign in to the AWS Management Console \u2192 the Services list \u2192 Elastic Container Service . Proceed to cluster creation by the button Create Cluster : Template : EC2 Linux + Networking . Cluster name : waf-cluster (as an example). Provisioning Model : On-Demand Instance . EC2 instance type : t2.micro . Number of instances : 1 . EC2 AMI ID : Amazon Linux 2 Amazon ECS-optimized AMI . Key pair : key pair for SSH connection to the instance. You will need to connect to the instance via SSH to upload the configuration file to the storage. Other settings can be left as default. When changing other settings, it is recommended to follow the instructions on AWS EFS setup . Configure the AWS EFS storage following steps 2-4 of the AWS instructions . In the 4 th step of AWS instructions, create the configuration file default and place the file in the directory that stores the files for mounting by default. The file default should cover the WAF node configuration. An example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Save the sensitive data used for the WAF node and Wallarm Cloud connection setup in the AWS Secrets Manager or AWS Systems Manager \u2192 Parameter Store . Sensitive data includes the following environment variables: <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In these instructions, sensitive data is stored in the AWS Secrets Manager. Access to the sensitive data storage To allow the Docker container to read the encrypted sensitive data, please ensure the AWS settings meet the following requirements: Sensitive data is stored in the region used to run the Docker container. The IAM policy SecretsManagerReadWrite is attached to the user specified in the executionRoleArn parameter of the task definition. More details on the IAM policies setup \u2192 Create the following local JSON file with the task definition (task definition sets the Docker container operating scenario): If you use the Wallarm EU Cloud { \"executionRoleArn\" : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/ecsTaskExecutionRole\" , \"containerDefinitions\" : [ { \"memory\" : 128 , \"portMappings\" : [ { \"hostPort\" : 80 , \"containerPort\" : 80 , \"protocol\" : \"tcp\" } ], \"essential\" : true , \"mountPoints\" : [ { \"containerPath\" : \"/etc/nginx/sites-enabled\" , \"sourceVolume\" : \"default\" } ], \"secrets\" : [ { \"name\" : \"DEPLOY_PASSWORD\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_PASSWORD_PARAMETER_NAME>::\" }, { \"name\" : \"DEPLOY_USER\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_USER_PARAMETER_NAME>::\" } ], \"name\" : \"waf-container\" , \"image\" : \"registry-1.docker.io/wallarm/node:3.0.0-2\" } ], \"volumes\" : [ { \"name\" : \"default\" , \"efsVolumeConfiguration\" : { \"fileSystemId\" : \"<EFS_FILE_SYSTEM_ID>\" , \"transitEncryption\" : \"ENABLED\" } } ], \"family\" : \"wallarm-waf-node\" } If you use the Wallarm US Cloud { \"executionRoleArn\" : \"arn:aws:iam::<AWS_ACCOUNT_ID>:role/ecsTaskExecutionRole\" , \"containerDefinitions\" : [ { \"memory\" : 128 , \"portMappings\" : [ { \"hostPort\" : 80 , \"containerPort\" : 80 , \"protocol\" : \"tcp\" } ], \"essential\" : true , \"mountPoints\" : [ { \"containerPath\" : \"<PATH_FOR_MOUNTED_CONFIG>\" , \"sourceVolume\" : \"<NAME_FROM_VOLUMES_OBJECT>\" } ], \"environment\" : [ { \"name\" : \"WALLARM_API_HOST\" , \"value\" : \"us1.api.wallarm.com\" } ], \"secrets\" : [ { \"name\" : \"DEPLOY_PASSWORD\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_PASSWORD_PARAMETER_NAME>::\" }, { \"name\" : \"DEPLOY_USER\" , \"valueFrom\" : \"arn:aws:secretsmanager:<SECRETS_MANAGER_AWS_REGION>:<AWS_ACCOUNT_ID>:secret:<SECRET_NAME>:<DEPLOY_USER_PARAMETER_NAME>::\" } ], \"name\" : \"waf-container\" , \"image\" : \"registry-1.docker.io/wallarm/node:3.0.0-2\" } ], \"volumes\" : [ { \"name\" : \"<VOLUME_NAME>\" , \"efsVolumeConfiguration\" : { \"fileSystemId\" : \"<EFS_FILE_SYSTEM_ID>\" , \"transitEncryption\" : \"ENABLED\" } } ], \"family\" : \"wallarm-waf-node\" } <AWS_ACCOUNT_ID> : your AWS account ID . <PATH_FOR_MOUNTED_CONFIG> : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. <NAME_FROM_VOLUMES_OBJECT> : name of the volumes object containing the configuration of the mounted file AWS EFS storage (the value should be the same as <VOLUME_NAME> ). <VOLUME_NAME> : name of the volumes object that contains the configuration of the mounted file AWS EFS storage. <EFS_FILE_SYSTEM_ID> : ID of the AWS EFS file system containing the file that should be mounted to the container. ID is displayed in the AWS Management Console \u2192 Services \u2192 EFS \u2192 File systems . The environment object sets the environment variables that should be passed to the Docker container in a text format. The set of available environment variables is described in the table below. It is recommended to pass the variables DEPLOY_USER and DEPLOY_PASSWORD in the secrets object. The secret object sets the environment variables that should be passed to the Docker container as the links to the sensitive data storage. The format of values depends on the selected storage (see more details in the AWS Secrets Manager or AWS Systems Manager \u2192 Parameter Store documentation). It is recommended to pass the variables DEPLOY_USER and DEPLOY_PASSWORD in the secrets object. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No All configuration file parameters are described in the AWS documentation . Register the task definition based on the JSON configuration file by using the aws ecs register\u2011task\u2011definition command: aws ecs register-task-definition --cli-input-json file://<PATH_TO_JSON_FILE>/<JSON_FILE_NAME> <PATH_TO_JSON_FILE> : path to the JSON file with the task definition on the local machine. <JSON_FILE_NAME> : name and extension of the JSON file with the task definition. Run the task in the cluster by using the aws ecs run-task command: aws ecs run-task --cluster <CLUSTER_NAME> --launch-type EC2 --task-definition <FAMILY_PARAM_VALUE> <CLUSTER_NAME> : name of the cluster created in the first step. For example, waf-cluster . <FAMILY_PARAM_VALUE> : name of the created task definition. The value should correspond to the family parameter value specified in the JSON file with the task definition. For example, wallarm-waf-node . Open the AWS Management Console \u2192 Elastic Container Service \u2192 cluster with the running task \u2192 Tasks and ensure the task is displayed in the list. Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through the mounted file"},{"location":"waf-installation/cloud-platforms/aws/docker-container/#testing-the-waf-node-operation","text":"In the AWS Management Console, open the running task and copy the container IP address from the field External Link . If the IP address is empty, please ensure the container is in the RUNNING status. Send the request with test SQLI and XSS attacks to the copied address: curl http://<COPIED_IP>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Details on errors that occurred during the container deployment are displayed in the task details in the AWS Management Console. If the container is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Testing the WAF node operation"},{"location":"waf-installation/cloud-platforms/azure/deb-rpm-packages/","text":"Installation of the WAF node from DEB or RPM packages on Azure \u00b6 This quick guide provides the steps to install the WAF node from the source packages on a separate Azure instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the Azure instructions . Requirements \u00b6 Active Azure subscription Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud WAF node installation options \u00b6 Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module Installing the WAF node as the NGINX Stable module \u00b6 To install the WAF node as the NGINX Stable module in the Azure instance: Create an Azure instance from the operating system image supported by Wallarm following the Azure instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Azure instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the NGINX Plus module \u00b6 To install the WAF node as the NGINX Plus module in the Azure instance: Create an Azure instance from the operating system image supported by Wallarm following the Azure instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Azure instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the Kong module \u00b6 To install the WAF node as the Kong module in the Azure instance: Create an Azure instance from the operating system image supported by Wallarm following the Azure instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the Azure instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installation from DEB or RPM packages on Azure"},{"location":"waf-installation/cloud-platforms/azure/deb-rpm-packages/#installation-of-the-waf-node-from-deb-or-rpm-packages-on-azure","text":"This quick guide provides the steps to install the WAF node from the source packages on a separate Azure instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the Azure instructions .","title":"Installation of the WAF node from DEB or RPM packages on Azure"},{"location":"waf-installation/cloud-platforms/azure/deb-rpm-packages/#requirements","text":"Active Azure subscription Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/azure/deb-rpm-packages/#waf-node-installation-options","text":"Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module","title":"WAF node installation options"},{"location":"waf-installation/cloud-platforms/azure/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-stable-module","text":"To install the WAF node as the NGINX Stable module in the Azure instance: Create an Azure instance from the operating system image supported by Wallarm following the Azure instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Azure instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Stable module"},{"location":"waf-installation/cloud-platforms/azure/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-plus-module","text":"To install the WAF node as the NGINX Plus module in the Azure instance: Create an Azure instance from the operating system image supported by Wallarm following the Azure instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Azure instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Plus module"},{"location":"waf-installation/cloud-platforms/azure/deb-rpm-packages/#installing-the-waf-node-as-the-kong-module","text":"To install the WAF node as the Kong module in the Azure instance: Create an Azure instance from the operating system image supported by Wallarm following the Azure instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the Azure instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the Kong module"},{"location":"waf-installation/cloud-platforms/azure/docker-container/","text":"Deployment of the WAF node Docker image to Azure \u00b6 This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Microsoft Azure cloud platform using the Azure Container Instances service . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you read the documentation on Azure Application Gateway . Requirements \u00b6 Active Azure subscription Azure CLI installed Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud Options for the WAF node Docker container configuration \u00b6 The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container. Deploying the WAF node Docker container configured through environment variables \u00b6 To deploy the containerized WAF node configured only through environment variables, you can use the following tools: Azure CLI Azure portal Azure PowerShell ARM template Docker CLI In these instructions, the container is deployed using the Azure CLI as follows: Sign in to the Azure CLI by using the az login command: az login Create a resource group by using the az group create command. For example, create the group myResourceGroup in the East US region with the following command: az group create --name myResourceGroup --location eastus Set local environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Create an Azure resource from the WAF node Docker container by using the az container create command: Command for the Wallarm EU Cloud az container create \\ --resource-group myResourceGroup \\ --name waf-node \\ --dns-name-label wallarm-waf \\ --ports 80 \\ --image registry-1.docker.io/wallarm/node:3.0.0-2 \\ --environment-variables DEPLOY_USER = ${ DEPLOY_USER } DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } NGINX_BACKEND = 'example.com' Command for the Wallarm US Cloud az container create \\ --resource-group myResourceGroup \\ --name waf-node \\ --dns-name-label wallarm-waf \\ --ports 80 \\ --image registry-1.docker.io/wallarm/node:3.0.0-2 \\ --environment-variables DEPLOY_USER = ${ DEPLOY_USER } DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } NGINX_BACKEND = 'example.com' WALLARM_API_HOST = 'us1.api.wallarm.com' --resource-group : name of the resource group created in the second step. --name : name of the container. --dns-name-label : DNS name label for the container. --ports : port on which the WAF node listens. --image : name of the WAF node Docker image. --environment-variables : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Open the Azure portal and ensure the created resource is displayed in the list of resources. Test the WAF node operation . Deploying the WAF node Docker container configured through the mounted file \u00b6 To deploy the containerized WAF node configured through environment variables and mounted file, only Azure CLI can be used. To deploy the container with environment variables and mounted configuration file: Sign in to the Azure CLI by using the az login command: az login Create a resource group by using the az group create command. For example, create the group myResourceGroup in the East US region with the following command: az group create --name myResourceGroup --location eastus Create a configuration file with the WAF node settings locally. A example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Locate the configuration file in one of the ways suitable for mounting data volumes in Azure. All methods are described in the Mount data volumes section of the Azure documentation . In these instructions, the configuration file is mounted from the Git repository. Set local environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Create an Azure resource from the WAF node Docker container by using the az container create command: Command for the Wallarm EU Cloud az container create \\ --resource-group myResourceGroup \\ --name waf-node \\ --dns-name-label wallarm-waf \\ --ports 80 \\ --image registry-1.docker.io/wallarm/node:3.0.0-2 \\ --gitrepo-url <URL_OF_GITREPO> \\ --gitrepo-mount-path /etc/nginx/sites-enabled \\ --environment-variables DEPLOY_USER = ${ DEPLOY_USER } DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } Command for the Wallarm US Cloud az container create \\ --resource-group myResourceGroup \\ --name waf-node \\ --dns-name-label wallarm-waf \\ --ports 80 \\ --image registry-1.docker.io/wallarm/node:3.0.0-2 \\ --gitrepo-url <URL_OF_GITREPO> \\ --gitrepo-mount-path /etc/nginx/sites-enabled \\ --environment-variables DEPLOY_USER = ${ DEPLOY_USER } DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } WALLARM_API_HOST = 'us1.api.wallarm.com' --resource-group : name of the resource group created in the 2 nd step. --name : name of the container. --dns-name-label : DNS name label for the container. --ports : port on which the WAF node listens. --image : name of the WAF node Docker image. --gitrepo-url : URL of the Git repository containing the configuration file. If the file is located in the repository root, you need to pass only this parameter. If the file is located in a separate Git repository directory, please also pass the path to the directory in the --gitrepo-dir parameter (for example, --gitrepo-dir ./dir1 ). --gitrepo-mount-path : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. --environment-variables : environment variables containing settings for the WAF node and Wallarm Cloud connection (available variables are listed in the table below). Please note that it is not recommended to explicitly pass the values of DEPLOY_USER and DEPLOY_PASSWORD . Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Open the Azure portal and ensure the created resource is displayed in the list of resources. Test the WAF node operation . Testing the WAF node operation \u00b6 Open the created resource on the Azure portal and copy the FQDN value. If the FQDN field is empty, please ensure the container is in the Running status. Send the request with test SQLI and XSS attacks to the copied domain: curl http://<COPIED_DOMAIN>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Details on errors occurred during the container deployment are displayed on the Containers \u2192 Logs tab of the resource details on the Azure portal. If the resource is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Deployment of Docker image to Azure"},{"location":"waf-installation/cloud-platforms/azure/docker-container/#deployment-of-the-waf-node-docker-image-to-azure","text":"This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Microsoft Azure cloud platform using the Azure Container Instances service . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you read the documentation on Azure Application Gateway .","title":"Deployment of the WAF node Docker image to Azure"},{"location":"waf-installation/cloud-platforms/azure/docker-container/#requirements","text":"Active Azure subscription Azure CLI installed Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/azure/docker-container/#options-for-the-waf-node-docker-container-configuration","text":"The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container.","title":"Options for the WAF node Docker container configuration"},{"location":"waf-installation/cloud-platforms/azure/docker-container/#deploying-the-waf-node-docker-container-configured-through-environment-variables","text":"To deploy the containerized WAF node configured only through environment variables, you can use the following tools: Azure CLI Azure portal Azure PowerShell ARM template Docker CLI In these instructions, the container is deployed using the Azure CLI as follows: Sign in to the Azure CLI by using the az login command: az login Create a resource group by using the az group create command. For example, create the group myResourceGroup in the East US region with the following command: az group create --name myResourceGroup --location eastus Set local environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Create an Azure resource from the WAF node Docker container by using the az container create command: Command for the Wallarm EU Cloud az container create \\ --resource-group myResourceGroup \\ --name waf-node \\ --dns-name-label wallarm-waf \\ --ports 80 \\ --image registry-1.docker.io/wallarm/node:3.0.0-2 \\ --environment-variables DEPLOY_USER = ${ DEPLOY_USER } DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } NGINX_BACKEND = 'example.com' Command for the Wallarm US Cloud az container create \\ --resource-group myResourceGroup \\ --name waf-node \\ --dns-name-label wallarm-waf \\ --ports 80 \\ --image registry-1.docker.io/wallarm/node:3.0.0-2 \\ --environment-variables DEPLOY_USER = ${ DEPLOY_USER } DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } NGINX_BACKEND = 'example.com' WALLARM_API_HOST = 'us1.api.wallarm.com' --resource-group : name of the resource group created in the second step. --name : name of the container. --dns-name-label : DNS name label for the container. --ports : port on which the WAF node listens. --image : name of the WAF node Docker image. --environment-variables : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Open the Azure portal and ensure the created resource is displayed in the list of resources. Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through environment variables"},{"location":"waf-installation/cloud-platforms/azure/docker-container/#deploying-the-waf-node-docker-container-configured-through-the-mounted-file","text":"To deploy the containerized WAF node configured through environment variables and mounted file, only Azure CLI can be used. To deploy the container with environment variables and mounted configuration file: Sign in to the Azure CLI by using the az login command: az login Create a resource group by using the az group create command. For example, create the group myResourceGroup in the East US region with the following command: az group create --name myResourceGroup --location eastus Create a configuration file with the WAF node settings locally. A example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Locate the configuration file in one of the ways suitable for mounting data volumes in Azure. All methods are described in the Mount data volumes section of the Azure documentation . In these instructions, the configuration file is mounted from the Git repository. Set local environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Create an Azure resource from the WAF node Docker container by using the az container create command: Command for the Wallarm EU Cloud az container create \\ --resource-group myResourceGroup \\ --name waf-node \\ --dns-name-label wallarm-waf \\ --ports 80 \\ --image registry-1.docker.io/wallarm/node:3.0.0-2 \\ --gitrepo-url <URL_OF_GITREPO> \\ --gitrepo-mount-path /etc/nginx/sites-enabled \\ --environment-variables DEPLOY_USER = ${ DEPLOY_USER } DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } Command for the Wallarm US Cloud az container create \\ --resource-group myResourceGroup \\ --name waf-node \\ --dns-name-label wallarm-waf \\ --ports 80 \\ --image registry-1.docker.io/wallarm/node:3.0.0-2 \\ --gitrepo-url <URL_OF_GITREPO> \\ --gitrepo-mount-path /etc/nginx/sites-enabled \\ --environment-variables DEPLOY_USER = ${ DEPLOY_USER } DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } WALLARM_API_HOST = 'us1.api.wallarm.com' --resource-group : name of the resource group created in the 2 nd step. --name : name of the container. --dns-name-label : DNS name label for the container. --ports : port on which the WAF node listens. --image : name of the WAF node Docker image. --gitrepo-url : URL of the Git repository containing the configuration file. If the file is located in the repository root, you need to pass only this parameter. If the file is located in a separate Git repository directory, please also pass the path to the directory in the --gitrepo-dir parameter (for example, --gitrepo-dir ./dir1 ). --gitrepo-mount-path : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. --environment-variables : environment variables containing settings for the WAF node and Wallarm Cloud connection (available variables are listed in the table below). Please note that it is not recommended to explicitly pass the values of DEPLOY_USER and DEPLOY_PASSWORD . Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Open the Azure portal and ensure the created resource is displayed in the list of resources. Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through the mounted file"},{"location":"waf-installation/cloud-platforms/azure/docker-container/#testing-the-waf-node-operation","text":"Open the created resource on the Azure portal and copy the FQDN value. If the FQDN field is empty, please ensure the container is in the Running status. Send the request with test SQLI and XSS attacks to the copied domain: curl http://<COPIED_DOMAIN>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Details on errors occurred during the container deployment are displayed on the Containers \u2192 Logs tab of the resource details on the Azure portal. If the resource is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Testing the WAF node operation"},{"location":"waf-installation/cloud-platforms/gcp/deb-rpm-packages/","text":"Installation of the WAF node from DEB or RPM packages on GCP \u00b6 This quick guide provides the steps to install the WAF node from the source packages on a separate Google Engine instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the GCP instructions . Requirements \u00b6 Active GCP account GCP project created Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud WAF node installation options \u00b6 Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module Installing the WAF node as the NGINX Stable module \u00b6 To install the WAF node as the NGINX Stable module in the Google Engine instance: Create a Google Engine instance from the operating system image supported by Wallarm following the GCP instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the GCP instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the NGINX Plus module \u00b6 To install the WAF node as the NGINX Plus module in the Google Engine instance: Create a Google Engine instance from the operating system image supported by Wallarm following the GCP instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the GCP instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the Kong module \u00b6 To install the WAF node as the Kong module in the Google Engine instance: Create a Google Engine instance from the operating system image supported by Wallarm following the GCP instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the GCP instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installation from DEB or RPM packages on GCP"},{"location":"waf-installation/cloud-platforms/gcp/deb-rpm-packages/#installation-of-the-waf-node-from-deb-or-rpm-packages-on-gcp","text":"This quick guide provides the steps to install the WAF node from the source packages on a separate Google Engine instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the GCP instructions .","title":"Installation of the WAF node from DEB or RPM packages on GCP"},{"location":"waf-installation/cloud-platforms/gcp/deb-rpm-packages/#requirements","text":"Active GCP account GCP project created Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/gcp/deb-rpm-packages/#waf-node-installation-options","text":"Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module","title":"WAF node installation options"},{"location":"waf-installation/cloud-platforms/gcp/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-stable-module","text":"To install the WAF node as the NGINX Stable module in the Google Engine instance: Create a Google Engine instance from the operating system image supported by Wallarm following the GCP instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the GCP instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Stable module"},{"location":"waf-installation/cloud-platforms/gcp/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-plus-module","text":"To install the WAF node as the NGINX Plus module in the Google Engine instance: Create a Google Engine instance from the operating system image supported by Wallarm following the GCP instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the GCP instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Plus module"},{"location":"waf-installation/cloud-platforms/gcp/deb-rpm-packages/#installing-the-waf-node-as-the-kong-module","text":"To install the WAF node as the Kong module in the Google Engine instance: Create a Google Engine instance from the operating system image supported by Wallarm following the GCP instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the GCP instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the Kong module"},{"location":"waf-installation/cloud-platforms/gcp/docker-container/","text":"Deployment of the WAF node Docker image to GCP \u00b6 This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Google Cloud Platform using the component Google Compute Engine (GCE) . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you read the appropriate GCP documentation . Requirements \u00b6 Active GCP account GCP project created Compute Engine API enabled Google Cloud SDK (gcloud CLI) installed and configured Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud Options for the WAF node Docker container configuration \u00b6 The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container. Deploying the WAF node Docker container configured through environment variables \u00b6 To deploy the containerized WAF node configured only through environment variables, you can use the GCP Console or gcloud CLI . In these instructions, gcloud CLI is used. Set local environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Create the instance with the running Docker container by using the gcloud compute instances create-with-container command: Command for the Wallarm EU Cloud gcloud compute instances create-with-container <INSTANCE_NAME> \\ --zone <DEPLOYMENT_ZONE> \\ --tags http-server \\ --container-env DEPLOY_USER = ${ DEPLOY_USER } \\ --container-env DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } \\ --container-env NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> --container-image registry-1.docker.io/wallarm/node:3.0.0-2 Command for the Wallarm US Cloud gcloud compute instances create-with-container <INSTANCE_NAME> \\ --zone <DEPLOYMENT_ZONE> \\ --tags http-server \\ --container-env DEPLOY_USER = ${ DEPLOY_USER } \\ --container-env DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } \\ --container-env NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> \\ --container-env WALLARM_API_HOST = us1.api.wallarm.com \\ --container-image registry-1.docker.io/wallarm/node:3.0.0-2 <INSTANCE_NAME> : name of the instance, for example: wallarm-waf . --zone : zone that will host the instance. --tags : instance tags. Tags are used to configure the availability of the instance for other resources. In the present case, the tag http-server opening port 80 is assigned to the instance. --container-image : link to the Docker image of the WAF node. --container-env : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No All parameters of the gcloud compute instances create-with-container command are described in the GCP documentation . Open the GCP Console \u2192 Compute Engine \u2192 VM instances and ensure the instance is displayed in the list. Test the WAF node operation . Deploying the WAF node Docker container configured through the mounted file \u00b6 To deploy the containerized WAF node configured through environment variables and mounted file, you should create the instance, locate the WAF node configuration file in this instance file system and run the Docker container in this instance. You can perform these steps via the GCP Console or gcloud CLI . In these instructions, gcloud CLI is used. Create the instace based on any operating system image from the Compute Engine registry by using the gcloud compute instances create comand: gcloud compute instances create <INSTANCE_NAME> \\ --image <PUBLIC_IMAGE_NAME> \\ --zone <DEPLOYMENT_ZONE> \\ --tags http-server <INSTANCE_NAME> : name of the instance. --image : name of the operating system image from the Compute Engine registry. The created instance will be based on this image and will be used to run the Docker container later. If this parameter is omitted, the instance will be based on the Debian 10 image. --zone : zone that will host the instance. --tags : instance tags. Tags are used to configure the availability of the instance for other resources. In the present case, the tag http-server opening port 80 is assigned to the instance. All parameters of the gcloud compute instances create command are described in the GCP documentation . Open the GCP Console \u2192 Compute Engine \u2192 VM instances and ensure the instance is displayed in the list and is in the RUNNING status. Connect to the instance via SSH following the GCP instructions . Install the Docker packages in the instance following the instrauctions for an appropriate operating system . Set instance environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In the instance, create the directory with the file default containing the WAF node configuration (for example, the directory can be named as configs ). An example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Run the WAF node Docker container by using the docker run command with passed environment variables and mounted configuration file: Command for the Wallarm EU Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -v <INSTANCE_PATH_TO_CONFIG>:<CONTAINER_PATH_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 Command for the Wallarm US Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v <INSTANCE_PATH_TO_CONFIG>:<DIRECTORY_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 <INSTANCE_PATH_TO_CONFIG> : path to the configuration file created in the previous step. For example, configs . <DIRECTORY_FOR_MOUNTING> : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. -p : port the WAF node listens to. The value should be the same as the instance port. -e : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Test the WAF node operation . Testing the WAF node operation \u00b6 Open the GCP Console \u2192 Compute Engine \u2192 VM instances and copy the instance IP address from the External IP column. If the IP address is empty, please ensure the instance is in the RUNNING status. Send the request with test SQLI and XSS attacks to the copied address: curl http://<COPIED_IP>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Details on errors that occurred during the container deployment are displayed in the View logs instance menu. If the instance is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Deployment of Docker image to GCP"},{"location":"waf-installation/cloud-platforms/gcp/docker-container/#deployment-of-the-waf-node-docker-image-to-gcp","text":"This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Google Cloud Platform using the component Google Compute Engine (GCE) . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you read the appropriate GCP documentation .","title":"Deployment of the WAF node Docker image to GCP"},{"location":"waf-installation/cloud-platforms/gcp/docker-container/#requirements","text":"Active GCP account GCP project created Compute Engine API enabled Google Cloud SDK (gcloud CLI) installed and configured Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/gcp/docker-container/#options-for-the-waf-node-docker-container-configuration","text":"The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container.","title":"Options for the WAF node Docker container configuration"},{"location":"waf-installation/cloud-platforms/gcp/docker-container/#deploying-the-waf-node-docker-container-configured-through-environment-variables","text":"To deploy the containerized WAF node configured only through environment variables, you can use the GCP Console or gcloud CLI . In these instructions, gcloud CLI is used. Set local environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Create the instance with the running Docker container by using the gcloud compute instances create-with-container command: Command for the Wallarm EU Cloud gcloud compute instances create-with-container <INSTANCE_NAME> \\ --zone <DEPLOYMENT_ZONE> \\ --tags http-server \\ --container-env DEPLOY_USER = ${ DEPLOY_USER } \\ --container-env DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } \\ --container-env NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> --container-image registry-1.docker.io/wallarm/node:3.0.0-2 Command for the Wallarm US Cloud gcloud compute instances create-with-container <INSTANCE_NAME> \\ --zone <DEPLOYMENT_ZONE> \\ --tags http-server \\ --container-env DEPLOY_USER = ${ DEPLOY_USER } \\ --container-env DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } \\ --container-env NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> \\ --container-env WALLARM_API_HOST = us1.api.wallarm.com \\ --container-image registry-1.docker.io/wallarm/node:3.0.0-2 <INSTANCE_NAME> : name of the instance, for example: wallarm-waf . --zone : zone that will host the instance. --tags : instance tags. Tags are used to configure the availability of the instance for other resources. In the present case, the tag http-server opening port 80 is assigned to the instance. --container-image : link to the Docker image of the WAF node. --container-env : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No All parameters of the gcloud compute instances create-with-container command are described in the GCP documentation . Open the GCP Console \u2192 Compute Engine \u2192 VM instances and ensure the instance is displayed in the list. Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through environment variables"},{"location":"waf-installation/cloud-platforms/gcp/docker-container/#deploying-the-waf-node-docker-container-configured-through-the-mounted-file","text":"To deploy the containerized WAF node configured through environment variables and mounted file, you should create the instance, locate the WAF node configuration file in this instance file system and run the Docker container in this instance. You can perform these steps via the GCP Console or gcloud CLI . In these instructions, gcloud CLI is used. Create the instace based on any operating system image from the Compute Engine registry by using the gcloud compute instances create comand: gcloud compute instances create <INSTANCE_NAME> \\ --image <PUBLIC_IMAGE_NAME> \\ --zone <DEPLOYMENT_ZONE> \\ --tags http-server <INSTANCE_NAME> : name of the instance. --image : name of the operating system image from the Compute Engine registry. The created instance will be based on this image and will be used to run the Docker container later. If this parameter is omitted, the instance will be based on the Debian 10 image. --zone : zone that will host the instance. --tags : instance tags. Tags are used to configure the availability of the instance for other resources. In the present case, the tag http-server opening port 80 is assigned to the instance. All parameters of the gcloud compute instances create command are described in the GCP documentation . Open the GCP Console \u2192 Compute Engine \u2192 VM instances and ensure the instance is displayed in the list and is in the RUNNING status. Connect to the instance via SSH following the GCP instructions . Install the Docker packages in the instance following the instrauctions for an appropriate operating system . Set instance environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In the instance, create the directory with the file default containing the WAF node configuration (for example, the directory can be named as configs ). An example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Run the WAF node Docker container by using the docker run command with passed environment variables and mounted configuration file: Command for the Wallarm EU Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -v <INSTANCE_PATH_TO_CONFIG>:<CONTAINER_PATH_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 Command for the Wallarm US Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v <INSTANCE_PATH_TO_CONFIG>:<DIRECTORY_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 <INSTANCE_PATH_TO_CONFIG> : path to the configuration file created in the previous step. For example, configs . <DIRECTORY_FOR_MOUNTING> : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. -p : port the WAF node listens to. The value should be the same as the instance port. -e : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through the mounted file"},{"location":"waf-installation/cloud-platforms/gcp/docker-container/#testing-the-waf-node-operation","text":"Open the GCP Console \u2192 Compute Engine \u2192 VM instances and copy the instance IP address from the External IP column. If the IP address is empty, please ensure the instance is in the RUNNING status. Send the request with test SQLI and XSS attacks to the copied address: curl http://<COPIED_IP>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Details on errors that occurred during the container deployment are displayed in the View logs instance menu. If the instance is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Testing the WAF node operation"},{"location":"waf-installation/cloud-platforms/yandex-cloud/deb-rpm-packages/","text":"Installation of the WAF node from DEB or RPM packages on Yandex.Cloud \u00b6 This quick guide provides the steps to install the WAF node from the source packages on a separate Yandex Compute Cloud instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the instructions on Yandex Network Load Balancer . Requirements \u00b6 Access to the Yandex.Cloud management console Payment account in the ACTIVE or TRIAL_ACTIVE status displayed on the billing page Folder created. By default, the folder default will be created. To create a new folder, please follow these instructions Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud WAF node installation options \u00b6 Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module Installing the WAF node as the NGINX Stable module \u00b6 To install the WAF node as the NGINX Stable module in the Yandex Compute Cloud instance: Create a Yandex.Cloud instance from the operating system image supported by Wallarm following the Yandex.Cloud instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Yandex.Cloud instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the NGINX Plus module \u00b6 To install the WAF node as the NGINX Plus module in the Yandex Compute Cloud instance: Create a Yandex.Cloud instance from the operating system image supported by Wallarm following the Yandex.Cloud instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Yandex.Cloud instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions . Installing the WAF node as the Kong module \u00b6 To install the WAF node as the Kong module in the Yandex Compute Cloud instance: Create a Yandex.Cloud instance from the operating system image supported by Wallarm following the Yandex.Cloud instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the Yandex.Cloud instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installation from DEB or RPM packages on Yandex.Cloud"},{"location":"waf-installation/cloud-platforms/yandex-cloud/deb-rpm-packages/#installation-of-the-waf-node-from-deb-or-rpm-packages-on-yandexcloud","text":"This quick guide provides the steps to install the WAF node from the source packages on a separate Yandex Compute Cloud instance. By following this guide, you will create an instance from the supported operating system image and install the Wallarm WAF node on this operating system. The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you review the instructions on Yandex Network Load Balancer .","title":"Installation of the WAF node from DEB or RPM packages on Yandex.Cloud"},{"location":"waf-installation/cloud-platforms/yandex-cloud/deb-rpm-packages/#requirements","text":"Access to the Yandex.Cloud management console Payment account in the ACTIVE or TRIAL_ACTIVE status displayed on the billing page Folder created. By default, the folder default will be created. To create a new folder, please follow these instructions Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/yandex-cloud/deb-rpm-packages/#waf-node-installation-options","text":"Since the WAF node operates as the web server or API gateway module, web server or API gateway packages should be installed on the operating system along with the WAF node packages. You can select the web server or API gateway that is the most suitable for your application architecture from the following list: Install the WAF node as the NGINX Stable module Install the WAF node as the NGINX Plus module Install the WAF node as the Kong module","title":"WAF node installation options"},{"location":"waf-installation/cloud-platforms/yandex-cloud/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-stable-module","text":"To install the WAF node as the NGINX Stable module in the Yandex Compute Cloud instance: Create a Yandex.Cloud instance from the operating system image supported by Wallarm following the Yandex.Cloud instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Yandex.Cloud instructions . In the instance, install the packages of NGINX Stable and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Stable module"},{"location":"waf-installation/cloud-platforms/yandex-cloud/deb-rpm-packages/#installing-the-waf-node-as-the-nginx-plus-module","text":"To install the WAF node as the NGINX Plus module in the Yandex Compute Cloud instance: Create a Yandex.Cloud instance from the operating system image supported by Wallarm following the Yandex.Cloud instructions : Debian 9.x Stretch Debian 10.x Buster Ubuntu 18.04 Bionic Ubuntu 20.04 Focal CentOS 7.x CentOS 8.x Connect to the created instance following the Yandex.Cloud instructions . In the instance, install the packages of NGINX Plus and Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the NGINX Plus module"},{"location":"waf-installation/cloud-platforms/yandex-cloud/deb-rpm-packages/#installing-the-waf-node-as-the-kong-module","text":"To install the WAF node as the Kong module in the Yandex Compute Cloud instance: Create a Yandex.Cloud instance from the operating system image supported by Wallarm following the Yandex.Cloud instructions : Debian 9.x Stretch Ubuntu 18.04 Bionic CentOS 7.x Connect to the created instance following the Yandex.Cloud instructions . In the instance, install Kong of version 1.4.3 or lower following the Kong instructions . In the instance, install the packages of Wallarm WAF node following the Wallarm instructions . To install the postanalytics module in a separate instance, please repeat steps 1-2 and install the postanalytics module following the Wallarm instructions .","title":"Installing the WAF node as the Kong module"},{"location":"waf-installation/cloud-platforms/yandex-cloud/docker-container/","text":"Deployment of the WAF node Docker image to Yandex.Cloud \u00b6 This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Yandex.Cloud platform using the Yandex Container Solution service . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you read the appropriate Yandex.Cloud instructions . Requirements \u00b6 Access to the Yandex.Cloud management console Payment account in the ACTIVE or TRIAL_ACTIVE status displayed on the billing page Folder created. By default, the folder default will be created. To create a new folder, please follow these instructions If you deploy the Docker container with the WAF node configured only through environment variables, Yandex.Cloud CLI installed and configured Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud Options for the WAF node Docker container configuration \u00b6 The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container. Deploying the WAF node Docker container configured through environment variables \u00b6 To deploy the containerized WAF node configured only through environment variables, you can use the Yandex.Cloud management console or CLI . In these instructions, Yandex.Cloud CLI is used. Set local environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Create the instance with the running Docker container by using the yc compute instance create-with-container command: Command for the Wallarm EU Cloud yc compute instance create-with-container \\ --name <INSTANCE_NAME> \\ --zone = <DEPLOYMENT_ZONE> \\ --public-ip \\ --container-image = wallarm/node:3.0.0-2 \\ --container-env = DEPLOY_USER = ${ DEPLOY_USER } ,DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } ,NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> Command for the Wallarm US Cloud yc compute instance create-with-container \\ --name <INSTANCE_NAME> \\ --zone = <DEPLOYMENT_ZONE> \\ --public-ip \\ --container-image = wallarm/node:3.0.0-2 \\ --container-env = DEPLOY_USER = ${ DEPLOY_USER } ,DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } ,NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF>,WALLARM_API_HOST = us1.api.wallarm.com --name : name of the instance, for example: wallarm-waf . --zone : zone that will host the instance. --public-ip : if this parameter is passed, the public IP address will be assigned to the instance. --container-image : link to the Docker image of the WAF node. --container-env : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No All parameters of the yc compute instance create-with-container command are described in the Yandex.Cloud documentation . Open the Yandex.Cloud management console \u2192 Compute Cloud \u2192 Virtual machines and ensure the instance is displayed in the list. Test the WAF node operation . Deploying the WAF node Docker container configured through the mounted file \u00b6 To deploy the containerized WAF node configured through environment variables and mounted file, you should create the instance, locate the WAF node configuration file in this instance file system and run the Docker container in this instance. You can perform these steps via the Yandex.Cloud management console or Yandex.Cloud CLI . In these instructions, Yandex.Cloud CLI is used. Create the instance based on any operating system image following the Yandex.Cloud instructions . An example of the instance settings: Connect to the instance via SSH following the Yandex.Cloud instructions . Install the Docker packages in the instance following the instructions for an appropriate operating system . Set instance environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In the instance, create the directory with the file default containing the WAF node configuration (for example, the directory can be named as configs ). An example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Run the WAF node Docker container by using the docker run command with passed environment variables and mounted configuration file: Command for the Wallarm EU Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -v <INSTANCE_PATH_TO_CONFIG>:<CONTAINER_PATH_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 Command for the Wallarm US Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v <INSTANCE_PATH_TO_CONFIG>:<DIRECTORY_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 <INSTANCE_PATH_TO_CONFIG> : path to the configuration file created in the previous step. For example, configs . <DIRECTORY_FOR_MOUNTING> : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. -p : port the WAF node listens to. The value should be the same as the instance port. -e : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Test the WAF node operation . Testing the WAF node operation \u00b6 Open the Yandex.Cloud management console \u2192 Compute Cloud \u2192 Virtual machines and copy the instance IP address from the Public IPv4 column. If the IP address is empty, please ensure the instance is in the Running status. Send the request with test SQLI and XSS attacks to the copied address: curl http://<COPIED_IP>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. To view details on errors that occurred during the container deployment, please connect to the instance via SSH and review the container logs . If the instance is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Deployment of Docker image to Yandex.Cloud"},{"location":"waf-installation/cloud-platforms/yandex-cloud/docker-container/#deployment-of-the-waf-node-docker-image-to-yandexcloud","text":"This quick guide provides the steps to deploy the Docker image of the NGINX-based WAF node to the Yandex.Cloud platform using the Yandex Container Solution service . The instructions limitations These instructions do not cover the configuration of load balancing and WAF node autoscaling. If setting up these components yourself, we recommend that you read the appropriate Yandex.Cloud instructions .","title":"Deployment of the WAF node Docker image to Yandex.Cloud"},{"location":"waf-installation/cloud-platforms/yandex-cloud/docker-container/#requirements","text":"Access to the Yandex.Cloud management console Payment account in the ACTIVE or TRIAL_ACTIVE status displayed on the billing page Folder created. By default, the folder default will be created. To create a new folder, please follow these instructions If you deploy the Docker container with the WAF node configured only through environment variables, Yandex.Cloud CLI installed and configured Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud","title":"Requirements"},{"location":"waf-installation/cloud-platforms/yandex-cloud/docker-container/#options-for-the-waf-node-docker-container-configuration","text":"The WAF node configuration parameters should be passed to the deployed Docker container in one of the following ways: In the environment variables . This option allows for the configuration of only basic WAF node parameters. Most directives cannot be configured through environment variables. In the mounted configuration file . This option allows full WAF node configuration via any directives . With this configuration method, environment variables with the WAF node and Wallarm Cloud connection settings are also passed to the container.","title":"Options for the WAF node Docker container configuration"},{"location":"waf-installation/cloud-platforms/yandex-cloud/docker-container/#deploying-the-waf-node-docker-container-configured-through-environment-variables","text":"To deploy the containerized WAF node configured only through environment variables, you can use the Yandex.Cloud management console or CLI . In these instructions, Yandex.Cloud CLI is used. Set local environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. Create the instance with the running Docker container by using the yc compute instance create-with-container command: Command for the Wallarm EU Cloud yc compute instance create-with-container \\ --name <INSTANCE_NAME> \\ --zone = <DEPLOYMENT_ZONE> \\ --public-ip \\ --container-image = wallarm/node:3.0.0-2 \\ --container-env = DEPLOY_USER = ${ DEPLOY_USER } ,DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } ,NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF> Command for the Wallarm US Cloud yc compute instance create-with-container \\ --name <INSTANCE_NAME> \\ --zone = <DEPLOYMENT_ZONE> \\ --public-ip \\ --container-image = wallarm/node:3.0.0-2 \\ --container-env = DEPLOY_USER = ${ DEPLOY_USER } ,DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } ,NGINX_BACKEND = <HOST_TO_PROTECT_WITH_WAF>,WALLARM_API_HOST = us1.api.wallarm.com --name : name of the instance, for example: wallarm-waf . --zone : zone that will host the instance. --public-ip : if this parameter is passed, the public IP address will be assigned to the instance. --container-image : link to the Docker image of the WAF node. --container-env : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes NGINX_BACKEND Domain or IP address of the resource to protect with WAF. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No WALLARM_MODE WAF node mode: block to block malicious requests safe_blocking to block only those malicious requests originated from greylisted IP addresses monitoring to analyze but not block requests off to disable traffic analyzing and processing By default: monitoring . Detailed description of filtration modes \u2192 No TARANTOOL_MEMORY_GB Amount of memory allocated to Tarantool. The value can be an integer or a float (a dot . is a decimal separator). By default: 0.2 gygabytes. No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No All parameters of the yc compute instance create-with-container command are described in the Yandex.Cloud documentation . Open the Yandex.Cloud management console \u2192 Compute Cloud \u2192 Virtual machines and ensure the instance is displayed in the list. Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through environment variables"},{"location":"waf-installation/cloud-platforms/yandex-cloud/docker-container/#deploying-the-waf-node-docker-container-configured-through-the-mounted-file","text":"To deploy the containerized WAF node configured through environment variables and mounted file, you should create the instance, locate the WAF node configuration file in this instance file system and run the Docker container in this instance. You can perform these steps via the Yandex.Cloud management console or Yandex.Cloud CLI . In these instructions, Yandex.Cloud CLI is used. Create the instance based on any operating system image following the Yandex.Cloud instructions . An example of the instance settings: Connect to the instance via SSH following the Yandex.Cloud instructions . Install the Docker packages in the instance following the instructions for an appropriate operating system . Set instance environment variables with email and password used for authentication in the Wallarm Cloud: export DEPLOY_USER = '<DEPLOY_USER>' export DEPLOY_PASSWORD = '<DEPLOY_PASSWORD>' <DEPLOY_USER> : email to the Deploy or Administrator user account in the Wallarm Console. <DEPLOY_PASSWORD> : password to the Deploy or Administrator user account in the Wallarm Console. In the instance, create the directory with the file default containing the WAF node configuration (for example, the directory can be named as configs ). An example of the file with minimal settings: server { listen 80 default_server ; listen [ :: ] :80 default_server ipv6only = on ; #listen 443 ssl; server_name localhost ; #ssl_certificate cert.pem; #ssl_certificate_key cert.key; root /usr/share/nginx/html ; index index.html index.htm ; wallarm_mode monitoring ; # wallarm_instance 1; location / { proxy_pass http://example.com ; include proxy_params ; } } Set of WAF node directives that can be specified in the configuration file \u2192 Run the WAF node Docker container by using the docker run command with passed environment variables and mounted configuration file: Command for the Wallarm EU Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -v <INSTANCE_PATH_TO_CONFIG>:<CONTAINER_PATH_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 Command for the Wallarm US Cloud docker run -d -e DEPLOY_USER = ${ DEPLOY_USER } -e DEPLOY_PASSWORD = ${ DEPLOY_PASSWORD } -e WALLARM_API_HOST = 'us1.api.wallarm.com' -v <INSTANCE_PATH_TO_CONFIG>:<DIRECTORY_FOR_MOUNTING> -p 80 :80 wallarm/node:3.0.0-2 <INSTANCE_PATH_TO_CONFIG> : path to the configuration file created in the previous step. For example, configs . <DIRECTORY_FOR_MOUNTING> : directory of the container to mount the configuration file to. Configuration files can be mounted to the following container directories used by NGINX: /etc/nginx/conf.d \u2014 common settings /etc/nginx/sites-enabled \u2014 virtual host settings /var/www/html \u2014 static files The WAF node directives should be described in the /etc/nginx/sites-enabled/default file. -p : port the WAF node listens to. The value should be the same as the instance port. -e : environment variables with the WAF node configuration (available variables are listed in the table below). Please note that it is not recommended to pass the values of DEPLOY_USER and DEPLOY_PASSWORD explicitly. Environment variable Description Required DEPLOY_USER Email to the Deploy or Administrator user account in the Wallarm Console. Yes DEPLOY_PASSWORD Password to the Deploy or Administrator user account in the Wallarm Console. Yes WALLARM_API_HOST Wallarm API server: api.wallarm.com for the EU Cloud us1.api.wallarm.com for the US Cloud By default: api.wallarm.com . No DEPLOY_FORCE Replaces an existing WAF node with a new one if an existing WAF node name matches the identifier of the container you are running. The following values can be assigned to a variable: true to replace the WAF node false to disable the replacement of the WAF node Default value (if the variable is not passed to the container) is false . The WAF node name always matches the identifier of the container you are running. WAF node replacement is helpful if the Docker container identifiers in your environment are static and you are trying to run another Docker container with the WAF node (for example, a container with a new version of the image). If in this case the variable value is false , the WAF node creation process will fail. No Test the WAF node operation .","title":"Deploying the WAF node Docker container configured through the mounted file"},{"location":"waf-installation/cloud-platforms/yandex-cloud/docker-container/#testing-the-waf-node-operation","text":"Open the Yandex.Cloud management console \u2192 Compute Cloud \u2192 Virtual machines and copy the instance IP address from the Public IPv4 column. If the IP address is empty, please ensure the instance is in the Running status. Send the request with test SQLI and XSS attacks to the copied address: curl http://<COPIED_IP>/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. To view details on errors that occurred during the container deployment, please connect to the instance via SSH and review the container logs . If the instance is unavailable, please ensure required WAF node parameters with correct values are passed to the container.","title":"Testing the WAF node operation"},{"location":"waf-installation/nginx/dynamic-module-from-distr/","text":"Installing dynamic WAF module for NGINX from Debian/CentOS repositories \u00b6 These instructions describe the steps to install Wallarm WAF as a dynamic module for the open source version of NGINX installed from the Debian/CentOS repositories. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of an already existing Wallarm WAF or need to duplicate the installation in the same environment, then please keep the same WAF version as currently used or update all installations to the latest version. For the postanalytics installed separately, versions of substite or duplicate installations must be the same as already installed postanalytics too. To check the installed version of WAF node and postanalytics installed on the same server: Debian apt list wallarm-node CentOS yum list wallarm-node To check the versions of WAF node and postanalytics installed on different servers: Debian # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool CentOS # run from the server with installed WAF node yum list wallarm-node-nginx # run from the server with installed postanalytics yum list wallarm-node-tarantool If the version 3.0.x is installed, then follow the current instructions for the WAF node and for separate postanalytics . If the version 2.18.x is installed, then follow the instructions for WAF node 2.18 and for separate postanalytics 2.18 or update WAF node packages and separate postanalytics packages to the latest version in all installations. If the version 2.16.x or lower is installed, then please update the WAF node packages and separate postanalytics packages to the latest version in all installations. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Requirements \u00b6 Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud SELinux disabled or configured upon the instructions Executing all commands as a superuser (e.g. root ) For the request processing and postanalytics on different servers: postanalytics installed on the separate server upon the instructions Access to https://repo.wallarm.com to download packages. Ensure the access is not blocked by a firewall Access to https://api.wallarm.com:444 for working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 for working with US Wallarm Cloud. If access can be configured only via the proxy server, then use the instructions Access to GCP storage addresses to download an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers Installed text editor vim , nano , or any other. In the instruction, vim is used Installation options \u00b6 The processing of requests in the WAF is divided into two stages: Primary processing in the NGINX-Wallarm module. The processing is not memory demanding and can be put on frontend servers without changing the server requirements. Statistical analysis of the processed requests in the postanalytics module. Postanalytics is memory demanding, which may require changes in the server configuration or installation of postanalytics on a separate server. Depending on the system architecture, the NGINX-Wallarm and postanalytics modules can be installed on the same server or on different servers . Installation commands for both options are described in the further instructions. Installation \u00b6 1. Add Debian/CentOS repositories \u00b6 Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 9.x (stretch-backports) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch-backports/3.0/' | sudo tee --append /etc/apt/sources.list.d/wallarm.list\" # for correct WAF operation, uncomment the following line in /etc/apt/sources.list`: # deb http://deb.debian.org/debian stretch-backports main contrib non-free sudo apt update Debian 10.x (buster) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm 2. Install NGINX with Wallarm WAF packages \u00b6 Request processing and postanalytics on the same server \u00b6 The command installs the following packages: nginx for NGINX libnginx-mod-http-wallarm or nginx-mod-http-wallarm for the NGINX-Wallarm module wallarm-node for the postanalytics module, Tarantool database, and additional NGINX-Wallarm packages Debian 9.x (stretch) sudo apt install --no-install-recommends nginx wallarm-node libnginx-mod-http-wallarm Debian 9.x (stretch-backports) sudo apt install --no-install-recommends nginx wallarm-node libnginx-mod-http-wallarm -t stretch-backports Debian 10.x (buster) sudo apt install --no-install-recommends nginx wallarm-node libnginx-mod-http-wallarm CentOS 7.x sudo yum install nginx wallarm-node nginx-mod-http-wallarm CentOS 8.x sudo yum install nginx wallarm-node nginx-mod-http-wallarm Request processing and postanalytics on different servers \u00b6 To run postanalytics and process the requests on different servers, the following packages are required: wallarm-node-tarantool on the separate server for the postanalytics module and Tarantool database (installation steps are described in the instructions ) wallarm-node-nginx and libnginx-mod-http-wallarm / nginx-mod-http-wallarm for the NGINX-Wallarm module The commands install packages for NGINX and for the NGINX-Wallarm module: Debian 9.x (stretch) sudo apt install --no-install-recommends nginx wallarm-node-nginx libnginx-mod-http-wallarm Debian 9.x (stretch-backports) sudo apt install --no-install-recommends nginx wallarm-node-nginx libnginx-mod-http-wallarm -t stretch-backports Debian 10.x (buster) sudo apt install --no-install-recommends nginx wallarm-node-nginx libnginx-mod-http-wallarm CentOS 7.x sudo yum install nginx wallarm-node-nginx nginx-mod-http-wallarm CentOS 8.x sudo yum install nginx wallarm-node-nginx nginx-mod-http-wallarm 3. Connect the Wallarm WAF module \u00b6 Copy the configuration files for the system setup: Debian sudo cp /usr/share/doc/libnginx-mod-http-wallarm/examples/*conf /etc/nginx/conf.d/ CentOS sudo cp /usr/share/doc/nginx-mod-http-wallarm/examples/*conf /etc/nginx/conf.d/ 4. Connect the WAF node to Wallarm Cloud \u00b6 The WAF node interacts with the Wallarm Cloud. To connect the WAF node to the Cloud, proceed with the following steps: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled in the Wallarm Console. You can check mentioned settings by navigating to the users list in the EU Cloud or US Cloud . Run the addnode script in a system with the installed WAF node: EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Input the email and password for your account in the Wallarm Console. Input the WAF node name or click Enter to use an automatically generated name. Open the Wallarm Console \u2192 Nodes section in the EU Cloud or US Cloud and ensure a new WAF node is added to the list. 5. Update Wallarm WAF configuration \u00b6 Main configuration files of NGINX and Wallarm WAF node are located in the directories: /etc/nginx/conf.d/default.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings The file is used for settings applied to all domains. To apply different settings to different domain groups, use the file default.conf or create new configuration files for each domain group (for example, example.com.conf and test.com.conf ). More detailed information about NGINX configuration files is available in the official NGINX documentation . /etc/nginx/conf.d/wallarm-status.conf with WAF node monitoring settings. Detailed description is available within the link /etc/default/wallarm-tarantool or /etc/sysconfig/wallarm-tarantool with the Tarantool database settings Request filtration mode \u00b6 By default, the WAF node is in the status off and does not analyze incoming requests. To enable requests analysis, please follow the steps: Open the file /etc/nginx/conf.d/default.conf : sudo vim /etc/nginx/conf.d/default.conf Add the line wallarm_mode monitoring; to the https , server or location block: Example of the file /etc/nginx/conf.d/default.conf server { # port for which requests are filtered listen 80 ; # domain for which requests are filtered server_name localhost ; # WAF node mode wallarm_mode monitoring ; location / { root /usr/share/nginx/html ; index index.html index.htm ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } When operating in the monitoring mode, the WAF node searches attack signs in requests but does not block detected attacks. We recommend keeping the traffic flowing via the WAF node in the monitoring mode for several days after the WAF node deployment and only then enable the block mode. Learn recommendations on the WAF node operation mode setup \u2192 Memory \u00b6 Postanalytics on the separate server If you installed postanalytics on a separate server, then skip this step as you already have your postanalytics configured. The WAF node uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 75% of the total server memory. To allocate memory for Tarantool: Open the Tarantool configuration file in the editing mode: Debian sudo vim /etc/default/wallarm-tarantool CentOS sudo vim /etc/sysconfig/wallarm-tarantool Specify memory size in GB in the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 Detailed recommendations about allocating memory for Tarantool are described in these instructions . To apply changes, restart Tarantool: Debian sudo systemctl restart wallarm-tarantool CentOS 7.x sudo systemctl restart wallarm-tarantool Address of the separate postanalytics server \u00b6 NGINX-Wallarm and postanalytics on the same server If the NGINX-Wallarm and postanalytics modules are installed on the same server, then skip this step. Add postanalytics server addresses to the file /etc/nginx/conf.d/wallarm.conf : upstream wallarm_tarantool { server <ip1>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; server <ip2>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 2 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; max_conns value must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. keepalive value must not be lower than the number of the Tarantool servers. Other configurations \u00b6 To update other NGINX and Wallarm WAF configurations, use the NGINX documentation and the list of available Wallarm WAF directives . 6. Restart NGINX \u00b6 Providing user with root permission If you are running NGINX as a user that does not have root permission, then add this user to the wallarm group using the following command: usermod -aG wallarm <user_name>; where <user_name> is the name of the user without root permission. Debian sudo systemctl restart nginx CentOS 7.x sudo systemctl restart nginx 7. Test Wallarm WAF operation \u00b6 Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Settings customization \u00b6 Dynamic Wallarm WAF module with default settings is installed for NGINX from the Debian/CentOS repositories. To customize Wallarm WAF settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Configuring dynamic DNS resolution in NGINX Double\u2011detection of attacks with libdetection","title":"Installing as a dynamic module for NGINX from Debian/CentOS repositories"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#installing-dynamic-waf-module-for-nginx-from-debiancentos-repositories","text":"These instructions describe the steps to install Wallarm WAF as a dynamic module for the open source version of NGINX installed from the Debian/CentOS repositories. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of an already existing Wallarm WAF or need to duplicate the installation in the same environment, then please keep the same WAF version as currently used or update all installations to the latest version. For the postanalytics installed separately, versions of substite or duplicate installations must be the same as already installed postanalytics too. To check the installed version of WAF node and postanalytics installed on the same server: Debian apt list wallarm-node CentOS yum list wallarm-node To check the versions of WAF node and postanalytics installed on different servers: Debian # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool CentOS # run from the server with installed WAF node yum list wallarm-node-nginx # run from the server with installed postanalytics yum list wallarm-node-tarantool If the version 3.0.x is installed, then follow the current instructions for the WAF node and for separate postanalytics . If the version 2.18.x is installed, then follow the instructions for WAF node 2.18 and for separate postanalytics 2.18 or update WAF node packages and separate postanalytics packages to the latest version in all installations. If the version 2.16.x or lower is installed, then please update the WAF node packages and separate postanalytics packages to the latest version in all installations. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"Installing dynamic WAF module for NGINX from Debian/CentOS repositories"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#requirements","text":"Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud SELinux disabled or configured upon the instructions Executing all commands as a superuser (e.g. root ) For the request processing and postanalytics on different servers: postanalytics installed on the separate server upon the instructions Access to https://repo.wallarm.com to download packages. Ensure the access is not blocked by a firewall Access to https://api.wallarm.com:444 for working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 for working with US Wallarm Cloud. If access can be configured only via the proxy server, then use the instructions Access to GCP storage addresses to download an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers Installed text editor vim , nano , or any other. In the instruction, vim is used","title":"Requirements"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#installation-options","text":"The processing of requests in the WAF is divided into two stages: Primary processing in the NGINX-Wallarm module. The processing is not memory demanding and can be put on frontend servers without changing the server requirements. Statistical analysis of the processed requests in the postanalytics module. Postanalytics is memory demanding, which may require changes in the server configuration or installation of postanalytics on a separate server. Depending on the system architecture, the NGINX-Wallarm and postanalytics modules can be installed on the same server or on different servers . Installation commands for both options are described in the further instructions.","title":"Installation options"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#installation","text":"","title":"Installation"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#1-add-debiancentos-repositories","text":"Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 9.x (stretch-backports) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch-backports/3.0/' | sudo tee --append /etc/apt/sources.list.d/wallarm.list\" # for correct WAF operation, uncomment the following line in /etc/apt/sources.list`: # deb http://deb.debian.org/debian stretch-backports main contrib non-free sudo apt update Debian 10.x (buster) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm","title":"1. Add Debian/CentOS repositories"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#2-install-nginx-with-wallarm-waf-packages","text":"","title":"2. Install NGINX with Wallarm WAF packages"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#request-processing-and-postanalytics-on-the-same-server","text":"The command installs the following packages: nginx for NGINX libnginx-mod-http-wallarm or nginx-mod-http-wallarm for the NGINX-Wallarm module wallarm-node for the postanalytics module, Tarantool database, and additional NGINX-Wallarm packages Debian 9.x (stretch) sudo apt install --no-install-recommends nginx wallarm-node libnginx-mod-http-wallarm Debian 9.x (stretch-backports) sudo apt install --no-install-recommends nginx wallarm-node libnginx-mod-http-wallarm -t stretch-backports Debian 10.x (buster) sudo apt install --no-install-recommends nginx wallarm-node libnginx-mod-http-wallarm CentOS 7.x sudo yum install nginx wallarm-node nginx-mod-http-wallarm CentOS 8.x sudo yum install nginx wallarm-node nginx-mod-http-wallarm","title":"Request processing and postanalytics on the same server"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#request-processing-and-postanalytics-on-different-servers","text":"To run postanalytics and process the requests on different servers, the following packages are required: wallarm-node-tarantool on the separate server for the postanalytics module and Tarantool database (installation steps are described in the instructions ) wallarm-node-nginx and libnginx-mod-http-wallarm / nginx-mod-http-wallarm for the NGINX-Wallarm module The commands install packages for NGINX and for the NGINX-Wallarm module: Debian 9.x (stretch) sudo apt install --no-install-recommends nginx wallarm-node-nginx libnginx-mod-http-wallarm Debian 9.x (stretch-backports) sudo apt install --no-install-recommends nginx wallarm-node-nginx libnginx-mod-http-wallarm -t stretch-backports Debian 10.x (buster) sudo apt install --no-install-recommends nginx wallarm-node-nginx libnginx-mod-http-wallarm CentOS 7.x sudo yum install nginx wallarm-node-nginx nginx-mod-http-wallarm CentOS 8.x sudo yum install nginx wallarm-node-nginx nginx-mod-http-wallarm","title":"Request processing and postanalytics on different servers"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#3-connect-the-wallarm-waf-module","text":"Copy the configuration files for the system setup: Debian sudo cp /usr/share/doc/libnginx-mod-http-wallarm/examples/*conf /etc/nginx/conf.d/ CentOS sudo cp /usr/share/doc/nginx-mod-http-wallarm/examples/*conf /etc/nginx/conf.d/","title":"3. Connect the Wallarm WAF module"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#4-connect-the-waf-node-to-wallarm-cloud","text":"The WAF node interacts with the Wallarm Cloud. To connect the WAF node to the Cloud, proceed with the following steps: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled in the Wallarm Console. You can check mentioned settings by navigating to the users list in the EU Cloud or US Cloud . Run the addnode script in a system with the installed WAF node: EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Input the email and password for your account in the Wallarm Console. Input the WAF node name or click Enter to use an automatically generated name. Open the Wallarm Console \u2192 Nodes section in the EU Cloud or US Cloud and ensure a new WAF node is added to the list.","title":"4. Connect the WAF node to Wallarm Cloud"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#5-update-wallarm-waf-configuration","text":"Main configuration files of NGINX and Wallarm WAF node are located in the directories: /etc/nginx/conf.d/default.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings The file is used for settings applied to all domains. To apply different settings to different domain groups, use the file default.conf or create new configuration files for each domain group (for example, example.com.conf and test.com.conf ). More detailed information about NGINX configuration files is available in the official NGINX documentation . /etc/nginx/conf.d/wallarm-status.conf with WAF node monitoring settings. Detailed description is available within the link /etc/default/wallarm-tarantool or /etc/sysconfig/wallarm-tarantool with the Tarantool database settings","title":"5. Update Wallarm WAF configuration"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#request-filtration-mode","text":"By default, the WAF node is in the status off and does not analyze incoming requests. To enable requests analysis, please follow the steps: Open the file /etc/nginx/conf.d/default.conf : sudo vim /etc/nginx/conf.d/default.conf Add the line wallarm_mode monitoring; to the https , server or location block: Example of the file /etc/nginx/conf.d/default.conf server { # port for which requests are filtered listen 80 ; # domain for which requests are filtered server_name localhost ; # WAF node mode wallarm_mode monitoring ; location / { root /usr/share/nginx/html ; index index.html index.htm ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } When operating in the monitoring mode, the WAF node searches attack signs in requests but does not block detected attacks. We recommend keeping the traffic flowing via the WAF node in the monitoring mode for several days after the WAF node deployment and only then enable the block mode. Learn recommendations on the WAF node operation mode setup \u2192","title":"Request filtration mode"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#memory","text":"Postanalytics on the separate server If you installed postanalytics on a separate server, then skip this step as you already have your postanalytics configured. The WAF node uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 75% of the total server memory. To allocate memory for Tarantool: Open the Tarantool configuration file in the editing mode: Debian sudo vim /etc/default/wallarm-tarantool CentOS sudo vim /etc/sysconfig/wallarm-tarantool Specify memory size in GB in the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 Detailed recommendations about allocating memory for Tarantool are described in these instructions . To apply changes, restart Tarantool: Debian sudo systemctl restart wallarm-tarantool CentOS 7.x sudo systemctl restart wallarm-tarantool","title":"Memory"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#address-of-the-separate-postanalytics-server","text":"NGINX-Wallarm and postanalytics on the same server If the NGINX-Wallarm and postanalytics modules are installed on the same server, then skip this step. Add postanalytics server addresses to the file /etc/nginx/conf.d/wallarm.conf : upstream wallarm_tarantool { server <ip1>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; server <ip2>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 2 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; max_conns value must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. keepalive value must not be lower than the number of the Tarantool servers.","title":"Address of the separate postanalytics server"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#other-configurations","text":"To update other NGINX and Wallarm WAF configurations, use the NGINX documentation and the list of available Wallarm WAF directives .","title":"Other configurations"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#6-restart-nginx","text":"Providing user with root permission If you are running NGINX as a user that does not have root permission, then add this user to the wallarm group using the following command: usermod -aG wallarm <user_name>; where <user_name> is the name of the user without root permission. Debian sudo systemctl restart nginx CentOS 7.x sudo systemctl restart nginx","title":"6. Restart NGINX"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#7-test-wallarm-waf-operation","text":"Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"7. Test Wallarm WAF operation"},{"location":"waf-installation/nginx/dynamic-module-from-distr/#settings-customization","text":"Dynamic Wallarm WAF module with default settings is installed for NGINX from the Debian/CentOS repositories. To customize Wallarm WAF settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Configuring dynamic DNS resolution in NGINX Double\u2011detection of attacks with libdetection","title":"Settings customization"},{"location":"waf-installation/nginx/dynamic-module/","text":"Installing dynamic WAF module for NGINX stable from NGINX repository \u00b6 These instructions describe the steps to install Wallarm WAF as a dynamic module for the open source version of NGINX stable that was installed from the NGINX repository. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of an already existing Wallarm WAF or need to duplicate the installation in the same environment, then please keep the same WAF version as currently used or update all installations to the latest version. For the postanalytics installed separately, versions of substite or duplicate installations must be the same as already installed postanalytics too. To check the installed version of WAF node and postanalytics installed on the same server: Debian apt list wallarm-node Ubuntu apt list wallarm-node CentOS or Amazon Linux 2 yum list wallarm-node To check the versions of WAF node and postanalytics installed on different servers: Debian # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool Ubuntu # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool CentOS or Amazon Linux 2 # run from the server with installed WAF node yum list wallarm-node-nginx # run from the server with installed postanalytics yum list wallarm-node-tarantool If the version 3.0.x is installed, then follow the current instructions for the WAF node and for separate postanalytics . If the version 2.18.x is installed, then follow the instructions for WAF node 2.18 and for separate postanalytics 2.18 or update WAF node packages and separate postanalytics packages to the latest version in all installations. If the version 2.16.x or lower is installed, then please update the WAF node packages and separate postanalytics packages to the latest version in all installations. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy . Requirements \u00b6 Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud SELinux disabled or configured upon the instructions Executing all commands as a superuser (e.g. root ) For the request processing and postanalytics on different servers: postanalytics installed on the separate server upon the instructions Access to https://repo.wallarm.com to download packages. Ensure the access is not blocked by a firewall Access to https://api.wallarm.com:444 for working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 for working with US Wallarm Cloud. If access can be configured only via the proxy server, then use the instructions Access to GCP storage addresses to download an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers Installed text editor vim , nano , or any other. In the instruction, vim is used Installation options \u00b6 The processing of requests in the WAF is divided into two stages: Primary processing in the NGINX-Wallarm module. The processing is not memory demanding and can be put on frontend servers without changing the server requirements. Statistical analysis of the processed requests in the postanalytics module. Postanalytics is memory demanding, which may require changes in the server configuration or installation of postanalytics on a separate server. Depending on the system architecture, the NGINX-Wallarm and postanalytics modules can be installed on the same server or on different servers . Installation commands for both options are described in the further instructions. Installation \u00b6 1. Install NGINX stable and dependencies \u00b6 These are the following options to install NGINX stable from the NGINX repository: Installation from the built package Debian sudo apt install curl gnupg2 ca-certificates lsb-release echo \"deb http://nginx.org/packages/debian `lsb_release -cs` nginx\" | sudo tee /etc/apt/sources.list.d/nginx.list curl -fsSL https://nginx.org/keys/nginx_signing.key | sudo apt-key add - sudo apt update sudo apt install nginx Ubuntu sudo apt install curl gnupg2 ca-certificates lsb-release echo \"deb http://nginx.org/packages/ubuntu `lsb_release -cs` nginx\" | sudo tee /etc/apt/sources.list.d/nginx.list curl -fsSL https://nginx.org/keys/nginx_signing.key | sudo apt-key add - sudo apt update sudo apt install nginx CentOS or Amazon Linux 2 If an EPEL repository is added in CentOS 7.x, please disable installation of NGINX stable from this repository by adding exclude=nginx* to the file /etc/yum.repos.d/epel.repo . Example of the changed file /etc/yum.repos.d/epel.repo : [ epel ] name = Extra Packages for Enterprise Linux 7 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch metalink = https://mirrors.fedoraproject.org/metalink?repo = epel-7 & arch = $basearch failovermethod = priority enabled = 1 gpgcheck = 1 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 exclude = nginx* [ epel-debuginfo ] name = Extra Packages for Enterprise Linux 7 - $basearch - Debug #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch/debug metalink = https://mirrors.fedoraproject.org/metalink?repo = epel-debug-7 & arch = $basearch failovermethod = priority enabled = 0 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 gpgcheck = 1 [ epel-source ] name = Extra Packages for Enterprise Linux 7 - $basearch - Source #baseurl=http://download.fedoraproject.org/pub/epel/7/SRPMS metalink = https://mirrors.fedoraproject.org/metalink?repo = epel-source-7 & arch = $basearch failovermethod = priority enabled = 0 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 gpgcheck = 1 Install NGINX stable from the official repository: echo -e '\\n[nginx-stable] \\nname=nginx stable repo \\nbaseurl=http://nginx.org/packages/centos/$releasever/$basearch/ \\ngpgcheck=1 \\nenabled=1 \\ngpgkey=https://nginx.org/keys/nginx_signing.key \\nmodule_hotfixes=true' | sudo tee /etc/yum.repos.d/nginx.repo sudo yum install nginx Compilation of the source code from the stable branch of the NGINX repository and installation with the same options More detailed information about installation is available in the official NGINX documentation . Installing on Amazon Linux 2 To install NGINX Plus on Amazon Linux 2, use the CentOS 7 instructions. 2. Add Wallarm WAF repositories \u00b6 Wallarm WAF is installed and updated from the Wallarm repositories. To add repositories, use the commands for your platform: Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 10.x (buster) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 20.04 LTS (focal) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node focal/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Amazon Linux 2 sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm 3. Install Wallarm WAF packages \u00b6 Request processing and postanalytics on the same server \u00b6 To run postanalytics and process the requests on the same server, the following packages are required: nginx-module-wallarm for the NGINX-Wallarm module wallarm-node for the postanalytics module, Tarantool database, and additional NGINX-Wallarm packages Debian sudo apt install --no-install-recommends wallarm-node nginx-module-wallarm Ubuntu sudo apt install --no-install-recommends wallarm-node nginx-module-wallarm CentOS or Amazon Linux 2 sudo yum install wallarm-node nginx-module-wallarm Request processing and postanalytics on different servers \u00b6 To run postanalytics and process the requests on different servers, the following packages are required: wallarm-node-nginx and nginx-module-wallarm for the NGINX-Wallarm module Debian sudo apt install --no-install-recommends wallarm-node-nginx nginx-module-wallarm Ubuntu sudo apt install --no-install-recommends wallarm-node-nginx nginx-module-wallarm CentOS or Amazon Linux 2 sudo yum install wallarm-node-nginx nginx-module-wallarm wallarm-node-tarantool on the separate server for the postanalytics module and Tarantool database (installation steps are described in the instructions ) 4. Connect the Wallarm WAF module \u00b6 Open the file /etc/nginx/nginx.conf : sudo vim /etc/nginx/nginx.conf Ensure that the include /etc/nginx/conf.d/* line is added to the file. If there is no such line, add it. Add the following directive right after the worker_processes directive: load_module modules/ngx_http_wallarm_module.so ; Configuration example with the added directive: user nginx; worker_processes auto; load_module modules/ngx_http_wallarm_module.so; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; Copy the configuration files for the system setup: sudo cp /usr/share/doc/nginx-module-wallarm/examples/*.conf /etc/nginx/conf.d/ 5. Connect the WAF node to Wallarm Cloud \u00b6 The WAF node interacts with the Wallarm Cloud. To connect the WAF node to the Cloud, proceed with the following steps: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled in the Wallarm Console. You can check mentioned settings by navigating to the users list in the EU Cloud or US Cloud . Run the addnode script in a system with the installed WAF node: EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Input the email and password for your account in the Wallarm Console. Input the WAF node name or click Enter to use an automatically generated name. Open the Wallarm Console \u2192 Nodes section in the EU Cloud or US Cloud and ensure a new WAF node is added to the list. 6. Update Wallarm WAF configuration \u00b6 Main configuration files of NGINX and Wallarm WAF node are located in the directories: /etc/nginx/conf.d/default.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings The file is used for settings applied to all domains. To apply different settings to different domain groups, use the file default.conf or create new configuration files for each domain group (for example, example.com.conf and test.com.conf ). More detailed information about NGINX configuration files is available in the official NGINX documentation . /etc/nginx/conf.d/wallarm-status.conf with WAF node monitoring settings. Detailed description is available within the link /etc/default/wallarm-tarantool or /etc/sysconfig/wallarm-tarantool with the Tarantool database settings Request filtration mode \u00b6 By default, the WAF node is in the status off and does not analyze incoming requests. To enable requests analysis, please follow the steps: Open the file /etc/nginx/conf.d/default.conf : sudo vim /etc/nginx/conf.d/default.conf Add the line wallarm_mode monitoring; to the https , server or location block: Example of the file /etc/nginx/conf.d/default.conf server { # port for which requests are filtered listen 80 ; # domain for which requests are filtered server_name localhost ; # WAF node mode wallarm_mode monitoring ; location / { root /usr/share/nginx/html ; index index.html index.htm ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } When operating in the monitoring mode, the WAF node searches attack signs in requests but does not block detected attacks. We recommend keeping the traffic flowing via the WAF node in the monitoring mode for several days after the WAF node deployment and only then enable the block mode. Learn recommendations on the WAF node operation mode setup \u2192 Memory \u00b6 Postanalytics on the separate server If you installed postanalytics on a separate server, then skip this step as you already have your postanalytics configured. The WAF node uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 75% of the total server memory. To allocate memory for Tarantool: Open the Tarantool configuration file in the editing mode: Debian sudo vim /etc/default/wallarm-tarantool Ubuntu sudo vim /etc/default/wallarm-tarantool CentOS or Amazon Linux 2 sudo vim /etc/sysconfig/wallarm-tarantool Specify memory size in GB in the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 Detailed recommendations about allocating memory for Tarantool are described in these instructions . To apply changes, restart Tarantool: Debian sudo systemctl restart wallarm-tarantool Ubuntu sudo systemctl restart wallarm-tarantool CentOS or Amazon Linux 2 sudo systemctl restart wallarm-tarantool Address of the separate postanalytics server \u00b6 NGINX-Wallarm and postanalytics on the same server If the NGINX-Wallarm and postanalytics modules are installed on the same server, then skip this step. Add postanalytics server addresses to the file /etc/nginx/conf.d/wallarm.conf : upstream wallarm_tarantool { server <ip1>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; server <ip2>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 2 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; max_conns value must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. keepalive value must not be lower than the number of the Tarantool servers. Other configurations \u00b6 To update other NGINX and Wallarm WAF configurations, use the NGINX documentation and the list of available Wallarm WAF directives . 7. Restart NGINX \u00b6 Providing user with root permission If you are running NGINX as a user that does not have root permission, then add this user to the wallarm group using the following command: usermod -aG wallarm <user_name>; where <user_name> is the name of the user without root permission. Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx 8. Test Wallarm WAF operation \u00b6 Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list. Settings customization \u00b6 Dynamic Wallarm WAF module with default settings is installed for NGINX stable . To customize Wallarm WAF settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Configuring dynamic DNS resolution in NGINX Double\u2011detection of attacks with libdetection","title":"Installing as a dynamic module for NGINX stable"},{"location":"waf-installation/nginx/dynamic-module/#installing-dynamic-waf-module-for-nginx-stable-from-nginx-repository","text":"These instructions describe the steps to install Wallarm WAF as a dynamic module for the open source version of NGINX stable that was installed from the NGINX repository. If Wallarm WAF is already installed in your environment If you install Wallarm WAF instead of an already existing Wallarm WAF or need to duplicate the installation in the same environment, then please keep the same WAF version as currently used or update all installations to the latest version. For the postanalytics installed separately, versions of substite or duplicate installations must be the same as already installed postanalytics too. To check the installed version of WAF node and postanalytics installed on the same server: Debian apt list wallarm-node Ubuntu apt list wallarm-node CentOS or Amazon Linux 2 yum list wallarm-node To check the versions of WAF node and postanalytics installed on different servers: Debian # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool Ubuntu # run from the server with installed WAF node apt list wallarm-node-nginx # run from the server with installed postanalytics apt list wallarm-node-tarantool CentOS or Amazon Linux 2 # run from the server with installed WAF node yum list wallarm-node-nginx # run from the server with installed postanalytics yum list wallarm-node-tarantool If the version 3.0.x is installed, then follow the current instructions for the WAF node and for separate postanalytics . If the version 2.18.x is installed, then follow the instructions for WAF node 2.18 and for separate postanalytics 2.18 or update WAF node packages and separate postanalytics packages to the latest version in all installations. If the version 2.16.x or lower is installed, then please update the WAF node packages and separate postanalytics packages to the latest version in all installations. Support for installed versions will be deprecated soon. More information about WAF node versioning is available in the WAF node versioning policy .","title":"Installing dynamic WAF module for NGINX stable from NGINX repository"},{"location":"waf-installation/nginx/dynamic-module/#requirements","text":"Access to the account with the Administrator or Deploy role and two\u2011factor authentication disabled in the Wallarm Console for the EU Cloud or US Cloud SELinux disabled or configured upon the instructions Executing all commands as a superuser (e.g. root ) For the request processing and postanalytics on different servers: postanalytics installed on the separate server upon the instructions Access to https://repo.wallarm.com to download packages. Ensure the access is not blocked by a firewall Access to https://api.wallarm.com:444 for working with EU Wallarm Cloud or to https://us1.api.wallarm.com:444 for working with US Wallarm Cloud. If access can be configured only via the proxy server, then use the instructions Access to GCP storage addresses to download an actual list of IP addresses registered in whitelisted, blacklisted, or greylisted countries or data centers Installed text editor vim , nano , or any other. In the instruction, vim is used","title":"Requirements"},{"location":"waf-installation/nginx/dynamic-module/#installation-options","text":"The processing of requests in the WAF is divided into two stages: Primary processing in the NGINX-Wallarm module. The processing is not memory demanding and can be put on frontend servers without changing the server requirements. Statistical analysis of the processed requests in the postanalytics module. Postanalytics is memory demanding, which may require changes in the server configuration or installation of postanalytics on a separate server. Depending on the system architecture, the NGINX-Wallarm and postanalytics modules can be installed on the same server or on different servers . Installation commands for both options are described in the further instructions.","title":"Installation options"},{"location":"waf-installation/nginx/dynamic-module/#installation","text":"","title":"Installation"},{"location":"waf-installation/nginx/dynamic-module/#1-install-nginx-stable-and-dependencies","text":"These are the following options to install NGINX stable from the NGINX repository: Installation from the built package Debian sudo apt install curl gnupg2 ca-certificates lsb-release echo \"deb http://nginx.org/packages/debian `lsb_release -cs` nginx\" | sudo tee /etc/apt/sources.list.d/nginx.list curl -fsSL https://nginx.org/keys/nginx_signing.key | sudo apt-key add - sudo apt update sudo apt install nginx Ubuntu sudo apt install curl gnupg2 ca-certificates lsb-release echo \"deb http://nginx.org/packages/ubuntu `lsb_release -cs` nginx\" | sudo tee /etc/apt/sources.list.d/nginx.list curl -fsSL https://nginx.org/keys/nginx_signing.key | sudo apt-key add - sudo apt update sudo apt install nginx CentOS or Amazon Linux 2 If an EPEL repository is added in CentOS 7.x, please disable installation of NGINX stable from this repository by adding exclude=nginx* to the file /etc/yum.repos.d/epel.repo . Example of the changed file /etc/yum.repos.d/epel.repo : [ epel ] name = Extra Packages for Enterprise Linux 7 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch metalink = https://mirrors.fedoraproject.org/metalink?repo = epel-7 & arch = $basearch failovermethod = priority enabled = 1 gpgcheck = 1 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 exclude = nginx* [ epel-debuginfo ] name = Extra Packages for Enterprise Linux 7 - $basearch - Debug #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch/debug metalink = https://mirrors.fedoraproject.org/metalink?repo = epel-debug-7 & arch = $basearch failovermethod = priority enabled = 0 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 gpgcheck = 1 [ epel-source ] name = Extra Packages for Enterprise Linux 7 - $basearch - Source #baseurl=http://download.fedoraproject.org/pub/epel/7/SRPMS metalink = https://mirrors.fedoraproject.org/metalink?repo = epel-source-7 & arch = $basearch failovermethod = priority enabled = 0 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 gpgcheck = 1 Install NGINX stable from the official repository: echo -e '\\n[nginx-stable] \\nname=nginx stable repo \\nbaseurl=http://nginx.org/packages/centos/$releasever/$basearch/ \\ngpgcheck=1 \\nenabled=1 \\ngpgkey=https://nginx.org/keys/nginx_signing.key \\nmodule_hotfixes=true' | sudo tee /etc/yum.repos.d/nginx.repo sudo yum install nginx Compilation of the source code from the stable branch of the NGINX repository and installation with the same options More detailed information about installation is available in the official NGINX documentation . Installing on Amazon Linux 2 To install NGINX Plus on Amazon Linux 2, use the CentOS 7 instructions.","title":"1. Install NGINX stable and dependencies"},{"location":"waf-installation/nginx/dynamic-module/#2-add-wallarm-waf-repositories","text":"Wallarm WAF is installed and updated from the Wallarm repositories. To add repositories, use the commands for your platform: Debian 9.x (stretch) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node stretch/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Debian 10.x (buster) sudo apt install dirmngr curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/debian/wallarm-node buster/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 18.04 LTS (bionic) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node bionic/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update Ubuntu 20.04 LTS (focal) curl -fsSL https://repo.wallarm.com/wallarm.gpg | sudo apt-key add - sh -c \"echo 'deb http://repo.wallarm.com/ubuntu/wallarm-node focal/3.0/' | sudo tee /etc/apt/sources.list.d/wallarm.list\" sudo apt update CentOS 7.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm Amazon Linux 2 sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/7/3.0/x86_64/Packages/wallarm-node-repo-1-6.el7.noarch.rpm CentOS 8.x sudo yum install -y epel-release sudo rpm -i https://repo.wallarm.com/centos/wallarm-node/8/3.0/x86_64/Packages/wallarm-node-repo-1-6.el8.noarch.rpm","title":"2. Add Wallarm WAF repositories"},{"location":"waf-installation/nginx/dynamic-module/#3-install-wallarm-waf-packages","text":"","title":"3. Install Wallarm WAF packages"},{"location":"waf-installation/nginx/dynamic-module/#request-processing-and-postanalytics-on-the-same-server","text":"To run postanalytics and process the requests on the same server, the following packages are required: nginx-module-wallarm for the NGINX-Wallarm module wallarm-node for the postanalytics module, Tarantool database, and additional NGINX-Wallarm packages Debian sudo apt install --no-install-recommends wallarm-node nginx-module-wallarm Ubuntu sudo apt install --no-install-recommends wallarm-node nginx-module-wallarm CentOS or Amazon Linux 2 sudo yum install wallarm-node nginx-module-wallarm","title":"Request processing and postanalytics on the same server"},{"location":"waf-installation/nginx/dynamic-module/#request-processing-and-postanalytics-on-different-servers","text":"To run postanalytics and process the requests on different servers, the following packages are required: wallarm-node-nginx and nginx-module-wallarm for the NGINX-Wallarm module Debian sudo apt install --no-install-recommends wallarm-node-nginx nginx-module-wallarm Ubuntu sudo apt install --no-install-recommends wallarm-node-nginx nginx-module-wallarm CentOS or Amazon Linux 2 sudo yum install wallarm-node-nginx nginx-module-wallarm wallarm-node-tarantool on the separate server for the postanalytics module and Tarantool database (installation steps are described in the instructions )","title":"Request processing and postanalytics on different servers"},{"location":"waf-installation/nginx/dynamic-module/#4-connect-the-wallarm-waf-module","text":"Open the file /etc/nginx/nginx.conf : sudo vim /etc/nginx/nginx.conf Ensure that the include /etc/nginx/conf.d/* line is added to the file. If there is no such line, add it. Add the following directive right after the worker_processes directive: load_module modules/ngx_http_wallarm_module.so ; Configuration example with the added directive: user nginx; worker_processes auto; load_module modules/ngx_http_wallarm_module.so; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; Copy the configuration files for the system setup: sudo cp /usr/share/doc/nginx-module-wallarm/examples/*.conf /etc/nginx/conf.d/","title":"4. Connect the Wallarm WAF module"},{"location":"waf-installation/nginx/dynamic-module/#5-connect-the-waf-node-to-wallarm-cloud","text":"The WAF node interacts with the Wallarm Cloud. To connect the WAF node to the Cloud, proceed with the following steps: Make sure that your Wallarm account has the Administrator or Deploy role enabled and two-factor authentication disabled in the Wallarm Console. You can check mentioned settings by navigating to the users list in the EU Cloud or US Cloud . Run the addnode script in a system with the installed WAF node: EU Cloud sudo /usr/share/wallarm-common/addnode US Cloud sudo /usr/share/wallarm-common/addnode -H us1.api.wallarm.com Input the email and password for your account in the Wallarm Console. Input the WAF node name or click Enter to use an automatically generated name. Open the Wallarm Console \u2192 Nodes section in the EU Cloud or US Cloud and ensure a new WAF node is added to the list.","title":"5. Connect the WAF node to Wallarm Cloud"},{"location":"waf-installation/nginx/dynamic-module/#6-update-wallarm-waf-configuration","text":"Main configuration files of NGINX and Wallarm WAF node are located in the directories: /etc/nginx/conf.d/default.conf with NGINX settings /etc/nginx/conf.d/wallarm.conf with global WAF node settings The file is used for settings applied to all domains. To apply different settings to different domain groups, use the file default.conf or create new configuration files for each domain group (for example, example.com.conf and test.com.conf ). More detailed information about NGINX configuration files is available in the official NGINX documentation . /etc/nginx/conf.d/wallarm-status.conf with WAF node monitoring settings. Detailed description is available within the link /etc/default/wallarm-tarantool or /etc/sysconfig/wallarm-tarantool with the Tarantool database settings","title":"6. Update Wallarm WAF configuration"},{"location":"waf-installation/nginx/dynamic-module/#request-filtration-mode","text":"By default, the WAF node is in the status off and does not analyze incoming requests. To enable requests analysis, please follow the steps: Open the file /etc/nginx/conf.d/default.conf : sudo vim /etc/nginx/conf.d/default.conf Add the line wallarm_mode monitoring; to the https , server or location block: Example of the file /etc/nginx/conf.d/default.conf server { # port for which requests are filtered listen 80 ; # domain for which requests are filtered server_name localhost ; # WAF node mode wallarm_mode monitoring ; location / { root /usr/share/nginx/html ; index index.html index.htm ; } error_page 500 502 503 504 /50x.html ; location = /50x.html { root /usr/share/nginx/html ; } } When operating in the monitoring mode, the WAF node searches attack signs in requests but does not block detected attacks. We recommend keeping the traffic flowing via the WAF node in the monitoring mode for several days after the WAF node deployment and only then enable the block mode. Learn recommendations on the WAF node operation mode setup \u2192","title":"Request filtration mode"},{"location":"waf-installation/nginx/dynamic-module/#memory","text":"Postanalytics on the separate server If you installed postanalytics on a separate server, then skip this step as you already have your postanalytics configured. The WAF node uses the in-memory storage Tarantool. The recommended memory size for Tarantool is 75% of the total server memory. To allocate memory for Tarantool: Open the Tarantool configuration file in the editing mode: Debian sudo vim /etc/default/wallarm-tarantool Ubuntu sudo vim /etc/default/wallarm-tarantool CentOS or Amazon Linux 2 sudo vim /etc/sysconfig/wallarm-tarantool Specify memory size in GB in the SLAB_ALLOC_ARENA directive. The value can be an integer or a float (a dot . is a decimal separator). For example, 24 GB: SLAB_ALLOC_ARENA = 24 Detailed recommendations about allocating memory for Tarantool are described in these instructions . To apply changes, restart Tarantool: Debian sudo systemctl restart wallarm-tarantool Ubuntu sudo systemctl restart wallarm-tarantool CentOS or Amazon Linux 2 sudo systemctl restart wallarm-tarantool","title":"Memory"},{"location":"waf-installation/nginx/dynamic-module/#address-of-the-separate-postanalytics-server","text":"NGINX-Wallarm and postanalytics on the same server If the NGINX-Wallarm and postanalytics modules are installed on the same server, then skip this step. Add postanalytics server addresses to the file /etc/nginx/conf.d/wallarm.conf : upstream wallarm_tarantool { server <ip1>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; server <ip2>:3313 max_fails = 0 fail_timeout = 0 max_conns = 1 ; keepalive 2 ; } # omitted wallarm_tarantool_upstream wallarm_tarantool ; max_conns value must be specified for each of the upstream Tarantool servers to prevent the creation of excessive connections. keepalive value must not be lower than the number of the Tarantool servers.","title":"Address of the separate postanalytics server"},{"location":"waf-installation/nginx/dynamic-module/#other-configurations","text":"To update other NGINX and Wallarm WAF configurations, use the NGINX documentation and the list of available Wallarm WAF directives .","title":"Other configurations"},{"location":"waf-installation/nginx/dynamic-module/#7-restart-nginx","text":"Providing user with root permission If you are running NGINX as a user that does not have root permission, then add this user to the wallarm group using the following command: usermod -aG wallarm <user_name>; where <user_name> is the name of the user without root permission. Debian sudo systemctl restart nginx Ubuntu sudo service nginx restart CentOS or Amazon Linux 2 sudo systemctl restart nginx","title":"7. Restart NGINX"},{"location":"waf-installation/nginx/dynamic-module/#8-test-wallarm-waf-operation","text":"Send the request with test SQLI and XSS attacks to the application address: curl http://localhost/?id='or+1=1--a-<script>prompt(1)</script>' Open the Wallarm Console \u2192 Events section in the EU Cloud or US Cloud and ensure attacks are displayed in the list.","title":"8. Test Wallarm WAF operation"},{"location":"waf-installation/nginx/dynamic-module/#settings-customization","text":"Dynamic Wallarm WAF module with default settings is installed for NGINX stable . To customize Wallarm WAF settings, use the available directives . Common customization options: Configuration of the filtration mode Logging WAF node variables Using the balancer of the proxy server behind the WAF node Limiting the single request processing time in the directive wallarm_process_time_limit Limiting the server reply waiting time in the NGINX directive proxy_read_timeout Limiting the maximum request size in the NGINX directive client_max_body_size Configuring dynamic DNS resolution in NGINX Double\u2011detection of attacks with libdetection","title":"Settings customization"}]}